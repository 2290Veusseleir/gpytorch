{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from gpytorch.utils import LanczosBidiagonalize, StochasticLQ\n",
    "from gpytorch.kernels import RBFKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 500\n",
    "max_iter = 50\n",
    "\n",
    "kern = RBFKernel(log_lengthscale_bounds=(1, 1))\n",
    "X = torch.linspace(0, 1, N)\n",
    "var_X = Variable(X)\n",
    "rhs_vec = torch.randn(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = kern(var_X, var_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = (K + Variable(torch.diag(1e-4 * torch.ones(len(var_X))))).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P, B, V = LanczosBidiagonalize(max_iter=500).lanczos_bidiagonalize(lambda v: K.matmul(v), rhs_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Qs, Ts = StochasticLQ(max_iter=15).lanczos_batch(lambda v: K.matmul(v), rhs_vec)\n",
    "Q = Qs[0]\n",
    "T = Ts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000  0.0000 -0.0000  0.0000 -0.0000 -0.0000 -0.0000\n",
       " 0.0000  1.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.0000\n",
       "-0.0000  0.0000  1.0000 -0.0000 -0.0000  0.0000  0.0000\n",
       " 0.0000 -0.0000 -0.0000  1.0000  0.0000  0.0000  0.0000\n",
       "-0.0000 -0.0000 -0.0000  0.0000  1.0000 -0.0000  0.0000\n",
       "-0.0000 -0.0000  0.0000  0.0000 -0.0000  1.0000  0.0000\n",
       "-0.0000 -0.0000  0.0000 -0.0000  0.0000  0.0000  1.0000\n",
       "[torch.FloatTensor of size 7x7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.t().matmul(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000  0.0000 -0.0000  ...  -0.0000  0.0000  0.0000\n",
       " 0.0000  1.0000  0.0000  ...  -0.0000 -0.0000  0.0000\n",
       "-0.0000  0.0000  1.0000  ...  -0.0000  0.0000  0.0000\n",
       "          ...             â‹±             ...          \n",
       "-0.0000  0.0000  0.0000  ...   1.0000  0.0000 -0.0000\n",
       "-0.0000 -0.0000  0.0000  ...   0.0000  1.0000  0.0000\n",
       "-0.0000  0.0000  0.0000  ...  -0.0000  0.0000  1.0000\n",
       "[torch.FloatTensor of size 500x500]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.t().matmul(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 4.7166e+02\n",
       " 2.7638e+01\n",
       " 6.8806e-01\n",
       " 1.1015e-02\n",
       " 2.2784e-04\n",
       " 1.3527e-04\n",
       " 1.3175e-04\n",
       " 1.2916e-04\n",
       " 1.2460e-04\n",
       " 1.1905e-04\n",
       " 1.1577e-04\n",
       " 1.1366e-04\n",
       " 1.1120e-04\n",
       " 1.1040e-04\n",
       " 1.0751e-04\n",
       " 1.0690e-04\n",
       " 1.0661e-04\n",
       " 1.0573e-04\n",
       " 1.0550e-04\n",
       " 1.0484e-04\n",
       " 1.0437e-04\n",
       " 1.0362e-04\n",
       " 1.0336e-04\n",
       " 1.0329e-04\n",
       " 1.0322e-04\n",
       " 1.0316e-04\n",
       " 1.0303e-04\n",
       " 1.0298e-04\n",
       " 1.0292e-04\n",
       " 1.0287e-04\n",
       " 1.0282e-04\n",
       " 1.0274e-04\n",
       " 1.0271e-04\n",
       " 1.0268e-04\n",
       " 1.0265e-04\n",
       " 1.0260e-04\n",
       " 1.0257e-04\n",
       " 1.0255e-04\n",
       " 1.0253e-04\n",
       " 1.0249e-04\n",
       " 1.0245e-04\n",
       " 1.0242e-04\n",
       " 1.0239e-04\n",
       " 1.0237e-04\n",
       " 1.0235e-04\n",
       " 1.0233e-04\n",
       " 1.0230e-04\n",
       " 1.0227e-04\n",
       " 1.0225e-04\n",
       " 1.0223e-04\n",
       " 1.0222e-04\n",
       " 1.0220e-04\n",
       " 1.0218e-04\n",
       " 1.0215e-04\n",
       " 1.0213e-04\n",
       " 1.0211e-04\n",
       " 1.0209e-04\n",
       " 1.0207e-04\n",
       " 1.0204e-04\n",
       " 1.0203e-04\n",
       " 1.0201e-04\n",
       " 1.0197e-04\n",
       " 1.0196e-04\n",
       " 1.0194e-04\n",
       " 1.0193e-04\n",
       " 1.0192e-04\n",
       " 1.0189e-04\n",
       " 1.0188e-04\n",
       " 1.0186e-04\n",
       " 1.0186e-04\n",
       " 1.0183e-04\n",
       " 1.0183e-04\n",
       " 1.0181e-04\n",
       " 1.0177e-04\n",
       " 1.0175e-04\n",
       " 1.0174e-04\n",
       " 1.0173e-04\n",
       " 1.0172e-04\n",
       " 1.0171e-04\n",
       " 1.0170e-04\n",
       " 1.0168e-04\n",
       " 1.0167e-04\n",
       " 1.0164e-04\n",
       " 1.0163e-04\n",
       " 1.0161e-04\n",
       " 1.0160e-04\n",
       " 1.0159e-04\n",
       " 1.0159e-04\n",
       " 1.0155e-04\n",
       " 1.0153e-04\n",
       " 1.0153e-04\n",
       " 1.0152e-04\n",
       " 1.0151e-04\n",
       " 1.0149e-04\n",
       " 1.0149e-04\n",
       " 1.0148e-04\n",
       " 1.0147e-04\n",
       " 1.0145e-04\n",
       " 1.0144e-04\n",
       " 1.0143e-04\n",
       " 1.0142e-04\n",
       " 1.0141e-04\n",
       " 1.0140e-04\n",
       " 1.0138e-04\n",
       " 1.0137e-04\n",
       " 1.0136e-04\n",
       " 1.0134e-04\n",
       " 1.0133e-04\n",
       " 1.0132e-04\n",
       " 1.0131e-04\n",
       " 1.0130e-04\n",
       " 1.0129e-04\n",
       " 1.0128e-04\n",
       " 1.0126e-04\n",
       " 1.0126e-04\n",
       " 1.0125e-04\n",
       " 1.0123e-04\n",
       " 1.0121e-04\n",
       " 1.0121e-04\n",
       " 1.0119e-04\n",
       " 1.0118e-04\n",
       " 1.0116e-04\n",
       " 1.0115e-04\n",
       " 1.0114e-04\n",
       " 1.0113e-04\n",
       " 1.0112e-04\n",
       " 1.0112e-04\n",
       " 1.0111e-04\n",
       " 1.0110e-04\n",
       " 1.0109e-04\n",
       " 1.0107e-04\n",
       " 1.0106e-04\n",
       " 1.0105e-04\n",
       " 1.0104e-04\n",
       " 1.0103e-04\n",
       " 1.0101e-04\n",
       " 1.0100e-04\n",
       " 1.0100e-04\n",
       " 1.0100e-04\n",
       " 1.0098e-04\n",
       " 1.0097e-04\n",
       " 1.0096e-04\n",
       " 1.0095e-04\n",
       " 1.0095e-04\n",
       " 1.0094e-04\n",
       " 1.0094e-04\n",
       " 1.0092e-04\n",
       " 1.0091e-04\n",
       " 1.0090e-04\n",
       " 1.0089e-04\n",
       " 1.0088e-04\n",
       " 1.0087e-04\n",
       " 1.0087e-04\n",
       " 1.0086e-04\n",
       " 1.0084e-04\n",
       " 1.0084e-04\n",
       " 1.0083e-04\n",
       " 1.0081e-04\n",
       " 1.0081e-04\n",
       " 1.0080e-04\n",
       " 1.0078e-04\n",
       " 1.0078e-04\n",
       " 1.0076e-04\n",
       " 1.0075e-04\n",
       " 1.0074e-04\n",
       " 1.0074e-04\n",
       " 1.0073e-04\n",
       " 1.0072e-04\n",
       " 1.0071e-04\n",
       " 1.0071e-04\n",
       " 1.0070e-04\n",
       " 1.0068e-04\n",
       " 1.0068e-04\n",
       " 1.0067e-04\n",
       " 1.0066e-04\n",
       " 1.0066e-04\n",
       " 1.0065e-04\n",
       " 1.0064e-04\n",
       " 1.0062e-04\n",
       " 1.0062e-04\n",
       " 1.0061e-04\n",
       " 1.0060e-04\n",
       " 1.0059e-04\n",
       " 1.0059e-04\n",
       " 1.0058e-04\n",
       " 1.0057e-04\n",
       " 1.0056e-04\n",
       " 1.0055e-04\n",
       " 1.0055e-04\n",
       " 1.0054e-04\n",
       " 1.0053e-04\n",
       " 1.0053e-04\n",
       " 1.0052e-04\n",
       " 1.0050e-04\n",
       " 1.0050e-04\n",
       " 1.0049e-04\n",
       " 1.0049e-04\n",
       " 1.0048e-04\n",
       " 1.0047e-04\n",
       " 1.0046e-04\n",
       " 1.0045e-04\n",
       " 1.0044e-04\n",
       " 1.0043e-04\n",
       " 1.0042e-04\n",
       " 1.0042e-04\n",
       " 1.0040e-04\n",
       " 1.0039e-04\n",
       " 1.0039e-04\n",
       " 1.0038e-04\n",
       " 1.0038e-04\n",
       " 1.0036e-04\n",
       " 1.0036e-04\n",
       " 1.0034e-04\n",
       " 1.0034e-04\n",
       " 1.0033e-04\n",
       " 1.0032e-04\n",
       " 1.0031e-04\n",
       " 1.0031e-04\n",
       " 1.0030e-04\n",
       " 1.0029e-04\n",
       " 1.0029e-04\n",
       " 1.0028e-04\n",
       " 1.0027e-04\n",
       " 1.0026e-04\n",
       " 1.0025e-04\n",
       " 1.0025e-04\n",
       " 1.0024e-04\n",
       " 1.0023e-04\n",
       " 1.0023e-04\n",
       " 1.0022e-04\n",
       " 1.0022e-04\n",
       " 1.0020e-04\n",
       " 1.0020e-04\n",
       " 1.0017e-04\n",
       " 1.0017e-04\n",
       " 1.0017e-04\n",
       " 1.0016e-04\n",
       " 1.0015e-04\n",
       " 1.0015e-04\n",
       " 1.0014e-04\n",
       " 1.0013e-04\n",
       " 1.0013e-04\n",
       " 1.0011e-04\n",
       " 1.0011e-04\n",
       " 1.0010e-04\n",
       " 1.0009e-04\n",
       " 1.0009e-04\n",
       " 1.0008e-04\n",
       " 1.0007e-04\n",
       " 1.0006e-04\n",
       " 1.0005e-04\n",
       " 1.0005e-04\n",
       " 1.0004e-04\n",
       " 1.0003e-04\n",
       " 1.0002e-04\n",
       " 1.0002e-04\n",
       " 1.0001e-04\n",
       " 1.0000e-04\n",
       " 9.9991e-05\n",
       " 9.9979e-05\n",
       " 9.9971e-05\n",
       " 9.9969e-05\n",
       " 9.9957e-05\n",
       " 9.9954e-05\n",
       " 9.9947e-05\n",
       " 9.9932e-05\n",
       " 9.9926e-05\n",
       " 9.9919e-05\n",
       " 9.9907e-05\n",
       " 9.9903e-05\n",
       " 9.9892e-05\n",
       " 9.9881e-05\n",
       " 9.9878e-05\n",
       " 9.9866e-05\n",
       " 9.9860e-05\n",
       " 9.9854e-05\n",
       " 9.9844e-05\n",
       " 9.9836e-05\n",
       " 9.9823e-05\n",
       " 9.9819e-05\n",
       " 9.9818e-05\n",
       " 9.9802e-05\n",
       " 9.9798e-05\n",
       " 9.9789e-05\n",
       " 9.9783e-05\n",
       " 9.9778e-05\n",
       " 9.9762e-05\n",
       " 9.9754e-05\n",
       " 9.9751e-05\n",
       " 9.9745e-05\n",
       " 9.9736e-05\n",
       " 9.9728e-05\n",
       " 9.9720e-05\n",
       " 9.9708e-05\n",
       " 9.9700e-05\n",
       " 9.9698e-05\n",
       " 9.9684e-05\n",
       " 9.9680e-05\n",
       " 9.9672e-05\n",
       " 9.9658e-05\n",
       " 9.9656e-05\n",
       " 9.9640e-05\n",
       " 9.9636e-05\n",
       " 9.9625e-05\n",
       " 9.9619e-05\n",
       " 9.9616e-05\n",
       " 9.9600e-05\n",
       " 9.9594e-05\n",
       " 9.9584e-05\n",
       " 9.9578e-05\n",
       " 9.9559e-05\n",
       " 9.9556e-05\n",
       " 9.9553e-05\n",
       " 9.9544e-05\n",
       " 9.9537e-05\n",
       " 9.9533e-05\n",
       " 9.9523e-05\n",
       " 9.9517e-05\n",
       " 9.9510e-05\n",
       " 9.9501e-05\n",
       " 9.9495e-05\n",
       " 9.9489e-05\n",
       " 9.9469e-05\n",
       " 9.9460e-05\n",
       " 9.9454e-05\n",
       " 9.9448e-05\n",
       " 9.9443e-05\n",
       " 9.9433e-05\n",
       " 9.9426e-05\n",
       " 9.9419e-05\n",
       " 9.9406e-05\n",
       " 9.9402e-05\n",
       " 9.9385e-05\n",
       " 9.9379e-05\n",
       " 9.9361e-05\n",
       " 9.9350e-05\n",
       " 9.9344e-05\n",
       " 9.9336e-05\n",
       " 9.9329e-05\n",
       " 9.9319e-05\n",
       " 9.9310e-05\n",
       " 9.9296e-05\n",
       " 9.9291e-05\n",
       " 9.9287e-05\n",
       " 9.9281e-05\n",
       " 9.9271e-05\n",
       " 9.9267e-05\n",
       " 9.9246e-05\n",
       " 9.9239e-05\n",
       " 9.9231e-05\n",
       " 9.9224e-05\n",
       " 9.9206e-05\n",
       " 9.9204e-05\n",
       " 9.9201e-05\n",
       " 9.9187e-05\n",
       " 9.9172e-05\n",
       " 9.9159e-05\n",
       " 9.9153e-05\n",
       " 9.9145e-05\n",
       " 9.9142e-05\n",
       " 9.9128e-05\n",
       " 9.9122e-05\n",
       " 9.9109e-05\n",
       " 9.9095e-05\n",
       " 9.9091e-05\n",
       " 9.9080e-05\n",
       " 9.9072e-05\n",
       " 9.9064e-05\n",
       " 9.9048e-05\n",
       " 9.9039e-05\n",
       " 9.9021e-05\n",
       " 9.9017e-05\n",
       " 9.9010e-05\n",
       " 9.8999e-05\n",
       " 9.8987e-05\n",
       " 9.8982e-05\n",
       " 9.8972e-05\n",
       " 9.8954e-05\n",
       " 9.8951e-05\n",
       " 9.8936e-05\n",
       " 9.8927e-05\n",
       " 9.8920e-05\n",
       " 9.8908e-05\n",
       " 9.8904e-05\n",
       " 9.8888e-05\n",
       " 9.8870e-05\n",
       " 9.8865e-05\n",
       " 9.8862e-05\n",
       " 9.8851e-05\n",
       " 9.8835e-05\n",
       " 9.8825e-05\n",
       " 9.8804e-05\n",
       " 9.8800e-05\n",
       " 9.8776e-05\n",
       " 9.8763e-05\n",
       " 9.8761e-05\n",
       " 9.8755e-05\n",
       " 9.8746e-05\n",
       " 9.8730e-05\n",
       " 9.8711e-05\n",
       " 9.8708e-05\n",
       " 9.8696e-05\n",
       " 9.8689e-05\n",
       " 9.8687e-05\n",
       " 9.8645e-05\n",
       " 9.8642e-05\n",
       " 9.8621e-05\n",
       " 9.8612e-05\n",
       " 9.8607e-05\n",
       " 9.8593e-05\n",
       " 9.8576e-05\n",
       " 9.8572e-05\n",
       " 9.8558e-05\n",
       " 9.8549e-05\n",
       " 9.8531e-05\n",
       " 9.8523e-05\n",
       " 9.8494e-05\n",
       " 9.8489e-05\n",
       " 9.8485e-05\n",
       " 9.8480e-05\n",
       " 9.8455e-05\n",
       " 9.8438e-05\n",
       " 9.8419e-05\n",
       " 9.8411e-05\n",
       " 9.8391e-05\n",
       " 9.8388e-05\n",
       " 9.8374e-05\n",
       " 9.8359e-05\n",
       " 9.8339e-05\n",
       " 9.8335e-05\n",
       " 9.8329e-05\n",
       " 9.8296e-05\n",
       " 9.8283e-05\n",
       " 9.8277e-05\n",
       " 9.8263e-05\n",
       " 9.8250e-05\n",
       " 9.8230e-05\n",
       " 9.8213e-05\n",
       " 9.8207e-05\n",
       " 9.8197e-05\n",
       " 9.8165e-05\n",
       " 9.8117e-05\n",
       " 9.8102e-05\n",
       " 9.8102e-05\n",
       " 9.8078e-05\n",
       " 9.8064e-05\n",
       " 9.8045e-05\n",
       " 9.8012e-05\n",
       " 9.7986e-05\n",
       " 9.7966e-05\n",
       " 9.7950e-05\n",
       " 9.7931e-05\n",
       " 9.7921e-05\n",
       " 9.7903e-05\n",
       " 9.7880e-05\n",
       " 9.7848e-05\n",
       " 9.7824e-05\n",
       " 9.7791e-05\n",
       " 9.7756e-05\n",
       " 9.7737e-05\n",
       " 9.7725e-05\n",
       " 9.7698e-05\n",
       " 9.7671e-05\n",
       " 9.7663e-05\n",
       " 9.7639e-05\n",
       " 9.7611e-05\n",
       " 9.7585e-05\n",
       " 9.7549e-05\n",
       " 9.7539e-05\n",
       " 9.7475e-05\n",
       " 9.7428e-05\n",
       " 9.7413e-05\n",
       " 9.7339e-05\n",
       " 9.7295e-05\n",
       " 9.7262e-05\n",
       " 9.7216e-05\n",
       " 9.7163e-05\n",
       " 9.7113e-05\n",
       " 9.7087e-05\n",
       " 9.7049e-05\n",
       " 9.6959e-05\n",
       " 9.6913e-05\n",
       " 9.6761e-05\n",
       " 9.6329e-05\n",
       " 9.6094e-05\n",
       " 9.5767e-05\n",
       " 9.4818e-05\n",
       " 9.4298e-05\n",
       " 9.3592e-05\n",
       " 9.3122e-05\n",
       " 9.2529e-05\n",
       " 9.0875e-05\n",
       " 8.9213e-05\n",
       " 8.7557e-05\n",
       " 8.6781e-05\n",
       " 8.4812e-05\n",
       " 8.3877e-05\n",
       " 8.1992e-05\n",
       " 7.7815e-05\n",
       " 7.2523e-05\n",
       "[torch.FloatTensor of size 500]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.svd(K)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vec = torch.randn(N)\n",
    "test_vec = test_vec / torch.norm(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6171483005901385e-06"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(P.matmul(B).matmul(V.t().matmul(test_vec)) - K.matmul(test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.98176422058089e-05"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(Q.matmul(T.matmul(Q.t().matmul(test_vec))) - K.matmul(test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-03 *\n",
       "  7.6811\n",
       "  7.6705\n",
       "  7.6597\n",
       "  7.6499\n",
       "  7.6404\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.matmul(T.matmul(Q.t().matmul(test_vec)))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4679\n",
       "-0.4682\n",
       "-0.4686\n",
       "-0.4690\n",
       "-0.4693\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.matmul(B).matmul(V.t().matmul(test_vec))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4679\n",
       "-0.4683\n",
       "-0.4686\n",
       "-0.4689\n",
       "-0.4693\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.matmul(test_vec)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 5 \n",
       " 1.0000e+00 -6.7055e-08  5.9605e-08  1.4901e-08  5.5879e-08 -1.1057e-05\n",
       "-6.7055e-08  1.0000e+00  8.1956e-08  7.4506e-09  1.6065e-07 -2.2037e-05\n",
       " 5.9605e-08  8.1956e-08  1.0000e+00  2.6822e-07 -4.8429e-08  7.8976e-06\n",
       " 1.4901e-08  7.4506e-09  2.6822e-07  1.0000e+00 -2.2352e-08 -2.9311e-05\n",
       " 5.5879e-08  1.6065e-07 -4.8429e-08 -2.2352e-08  1.0000e+00  1.1157e-04\n",
       "-1.1057e-05 -2.2037e-05  7.8976e-06 -2.9311e-05  1.1157e-04  1.0000e+00\n",
       " 4.0631e-04  1.9263e-03 -1.7542e-04 -2.4059e-04 -4.6696e-04  6.3809e-02\n",
       " 6.8763e-04  3.5326e-03 -3.3697e-04 -3.8305e-04 -7.4591e-04  1.0382e-01\n",
       " 7.8510e-04  4.0080e-03 -3.6802e-04 -4.5680e-04 -8.6511e-04  1.2026e-01\n",
       " 8.5121e-04  4.3227e-03 -4.0093e-04 -4.8507e-04 -9.2720e-04  1.2919e-01\n",
       "\n",
       "Columns 6 to 9 \n",
       " 4.0631e-04  6.8763e-04  7.8510e-04  8.5121e-04\n",
       " 1.9263e-03  3.5326e-03  4.0080e-03  4.3227e-03\n",
       "-1.7542e-04 -3.3697e-04 -3.6802e-04 -4.0093e-04\n",
       "-2.4059e-04 -3.8305e-04 -4.5680e-04 -4.8507e-04\n",
       "-4.6696e-04 -7.4591e-04 -8.6511e-04 -9.2720e-04\n",
       " 6.3809e-02  1.0382e-01  1.2026e-01  1.2919e-01\n",
       " 1.0000e+00  9.9918e-01  9.9839e-01  9.9784e-01\n",
       " 9.9918e-01  1.0000e+00  9.9986e-01  9.9967e-01\n",
       " 9.9839e-01  9.9986e-01  1.0000e+00  9.9996e-01\n",
       " 9.9784e-01  9.9967e-01  9.9996e-01  1.0000e+00\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.t().matmul(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms = P[:, :-1].t().matmul(P[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0009\n",
       " 0.0043\n",
       "-0.0004\n",
       "-0.0005\n",
       "-0.0009\n",
       " 0.1292\n",
       " 0.9978\n",
       " 0.9997\n",
       " 1.0000\n",
       "[torch.FloatTensor of size 9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P[:, -1] = P[:, -1] - torch.sum(norms * P[:, :-1], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 5 \n",
       " 1.0000e+00 -6.7055e-08  5.9605e-08  1.4901e-08  5.5879e-08 -1.1057e-05\n",
       "-6.7055e-08  1.0000e+00  8.1956e-08  7.4506e-09  1.6065e-07 -2.2037e-05\n",
       " 5.9605e-08  8.1956e-08  1.0000e+00  2.6822e-07 -4.8429e-08  7.8976e-06\n",
       " 1.4901e-08  7.4506e-09  2.6822e-07  1.0000e+00 -2.2352e-08 -2.9311e-05\n",
       " 5.5879e-08  1.6065e-07 -4.8429e-08 -2.2352e-08  1.0000e+00  1.1157e-04\n",
       "-1.1057e-05 -2.2037e-05  7.8976e-06 -2.9311e-05  1.1157e-04  1.0000e+00\n",
       " 4.0631e-04  1.9263e-03 -1.7542e-04 -2.4059e-04 -4.6696e-04  6.3809e-02\n",
       " 6.8763e-04  3.5326e-03 -3.3697e-04 -3.8305e-04 -7.4591e-04  1.0382e-01\n",
       " 7.8510e-04  4.0080e-03 -3.6802e-04 -4.5680e-04 -8.6511e-04  1.2026e-01\n",
       "-1.8765e-03 -9.4586e-03  8.7882e-04  1.0835e-03  2.0623e-03 -2.8771e-01\n",
       "\n",
       "Columns 6 to 9 \n",
       " 4.0631e-04  6.8763e-04  7.8510e-04 -1.8765e-03\n",
       " 1.9263e-03  3.5326e-03  4.0080e-03 -9.4586e-03\n",
       "-1.7542e-04 -3.3697e-04 -3.6802e-04  8.7882e-04\n",
       "-2.4059e-04 -3.8305e-04 -4.5680e-04  1.0835e-03\n",
       "-4.6696e-04 -7.4591e-04 -8.6511e-04  2.0623e-03\n",
       " 6.3809e-02  1.0382e-01  1.2026e-01 -2.8771e-01\n",
       " 1.0000e+00  9.9918e-01  9.9839e-01 -2.0055e+00\n",
       " 9.9918e-01  1.0000e+00  9.9986e-01 -2.0103e+00\n",
       " 9.9839e-01  9.9986e-01  1.0000e+00 -2.0113e+00\n",
       "-2.0055e+00 -2.0103e+00 -2.0113e+00  4.0475e+00\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.t().matmul(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(P.t().matmul(P)[:4, 5] > 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
