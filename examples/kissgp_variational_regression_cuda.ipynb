{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to perform GP regression, but using **variational inference** rather than exact inference. There are a few cases where variational inference may be prefereable:\n",
    "\n",
    "1) If you have lots of data, and want to perform **stochastic optimization**\n",
    "\n",
    "2) If you have a model where you want to use other variational distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set\n",
    "# We're going to learn a sine function\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "train_y = torch.sin(train_x * (4 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing SGD - the dataloader\n",
    "\n",
    "Because we want to do stochastic optimization, we have to put the dataset in a pytorch **DataLoader**.\n",
    "This creates easy minibatches of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "This is pretty similar to a normal regression model, except now we're using a `gpytorch.models.GridInducingVariationalGP` instead of a `gpytorch.models.ExactGP`.\n",
    "\n",
    "Any of the variational models would work. We're using the `GridInducingVariationalGP` because we have many data points, but only 1 dimensional data.\n",
    "\n",
    "Similar to exact regression, we use a `GaussianLikelihood`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPRegressionModel, self).__init__(grid_size=64, grid_bounds=[(-0.05, 1.05)])\n",
    "        self.mean_module = gpytorch.means.ConstantMean(constant_bounds=[-1e-5,1e-5])\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        self.register_parameter('log_outputscale', torch.nn.Parameter(torch.Tensor([0])), bounds=(-5,6))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean_x, covar_x)\n",
    "    \n",
    "model = GPRegressionModel().cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "This training loop will use **stochastic optimization** rather than batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/40 - Loss: 387.373 (0.100)\n",
      "Iter 1/40 - Loss: 1.319 (0.100)\n",
      "Iter 1/40 - Loss: 5.413 (0.100)\n",
      "Iter 1/40 - Loss: 1.339 (0.100)\n",
      "Iter 1/40 - Loss: 2.537 (0.100)\n",
      "Iter 1/40 - Loss: 1.402 (0.100)\n",
      "Iter 1/40 - Loss: 1.340 (0.100)\n",
      "Iter 1/40 - Loss: 2.036 (0.100)\n",
      "Iter 1/40 - Loss: 1.454 (0.100)\n",
      "Iter 1/40 - Loss: 1.359 (0.100)\n",
      "Iter 1/40 - Loss: 1.780 (0.100)\n",
      "Iter 1/40 - Loss: 1.519 (0.100)\n",
      "Iter 1/40 - Loss: 1.403 (0.100)\n",
      "Iter 1/40 - Loss: 1.336 (0.100)\n",
      "Iter 1/40 - Loss: 1.582 (0.100)\n",
      "Iter 1/40 - Loss: 1.342 (0.100)\n",
      "Iter 2/40 - Loss: 1.375 (0.100)\n",
      "Iter 2/40 - Loss: 1.357 (0.100)\n",
      "Iter 2/40 - Loss: 1.252 (0.100)\n",
      "Iter 2/40 - Loss: 1.349 (0.100)\n",
      "Iter 2/40 - Loss: 1.260 (0.100)\n",
      "Iter 2/40 - Loss: 1.392 (0.100)\n",
      "Iter 2/40 - Loss: 1.484 (0.100)\n",
      "Iter 2/40 - Loss: 1.312 (0.100)\n",
      "Iter 2/40 - Loss: 1.378 (0.100)\n",
      "Iter 2/40 - Loss: 1.363 (0.100)\n",
      "Iter 2/40 - Loss: 1.336 (0.100)\n",
      "Iter 2/40 - Loss: 1.455 (0.100)\n",
      "Iter 2/40 - Loss: 1.322 (0.100)\n",
      "Iter 2/40 - Loss: 1.278 (0.100)\n",
      "Iter 2/40 - Loss: 1.257 (0.100)\n",
      "Iter 2/40 - Loss: 1.241 (0.100)\n",
      "Iter 3/40 - Loss: 1.269 (0.100)\n",
      "Iter 3/40 - Loss: 1.294 (0.100)\n",
      "Iter 3/40 - Loss: 1.368 (0.100)\n",
      "Iter 3/40 - Loss: 1.302 (0.100)\n",
      "Iter 3/40 - Loss: 1.255 (0.100)\n",
      "Iter 3/40 - Loss: 1.223 (0.100)\n",
      "Iter 3/40 - Loss: 1.315 (0.100)\n",
      "Iter 3/40 - Loss: 1.216 (0.100)\n",
      "Iter 3/40 - Loss: 1.247 (0.100)\n",
      "Iter 3/40 - Loss: 1.247 (0.100)\n",
      "Iter 3/40 - Loss: 1.183 (0.100)\n",
      "Iter 3/40 - Loss: 1.268 (0.100)\n",
      "Iter 3/40 - Loss: 1.272 (0.100)\n",
      "Iter 3/40 - Loss: 1.223 (0.100)\n",
      "Iter 3/40 - Loss: 1.281 (0.100)\n",
      "Iter 3/40 - Loss: 1.173 (0.100)\n",
      "Iter 4/40 - Loss: 1.146 (0.100)\n",
      "Iter 4/40 - Loss: 1.248 (0.100)\n",
      "Iter 4/40 - Loss: 1.231 (0.100)\n",
      "Iter 4/40 - Loss: 1.281 (0.100)\n",
      "Iter 4/40 - Loss: 1.213 (0.100)\n",
      "Iter 4/40 - Loss: 1.180 (0.100)\n",
      "Iter 4/40 - Loss: 1.227 (0.100)\n",
      "Iter 4/40 - Loss: 1.187 (0.100)\n",
      "Iter 4/40 - Loss: 1.297 (0.100)\n",
      "Iter 4/40 - Loss: 1.236 (0.100)\n",
      "Iter 4/40 - Loss: 1.235 (0.100)\n",
      "Iter 4/40 - Loss: 1.148 (0.100)\n",
      "Iter 4/40 - Loss: 1.128 (0.100)\n",
      "Iter 4/40 - Loss: 1.234 (0.100)\n",
      "Iter 4/40 - Loss: 1.188 (0.100)\n",
      "Iter 4/40 - Loss: 1.216 (0.100)\n",
      "Iter 5/40 - Loss: 1.217 (0.100)\n",
      "Iter 5/40 - Loss: 1.175 (0.100)\n",
      "Iter 5/40 - Loss: 1.217 (0.100)\n",
      "Iter 5/40 - Loss: 1.152 (0.100)\n",
      "Iter 5/40 - Loss: 1.276 (0.100)\n",
      "Iter 5/40 - Loss: 1.196 (0.100)\n",
      "Iter 5/40 - Loss: 1.201 (0.100)\n",
      "Iter 5/40 - Loss: 1.168 (0.100)\n",
      "Iter 5/40 - Loss: 1.175 (0.100)\n",
      "Iter 5/40 - Loss: 1.172 (0.100)\n",
      "Iter 5/40 - Loss: 1.130 (0.100)\n",
      "Iter 5/40 - Loss: 1.091 (0.100)\n",
      "Iter 5/40 - Loss: 1.105 (0.100)\n",
      "Iter 5/40 - Loss: 1.164 (0.100)\n",
      "Iter 5/40 - Loss: 1.102 (0.100)\n",
      "Iter 5/40 - Loss: 1.045 (0.100)\n",
      "Iter 6/40 - Loss: 1.145 (0.100)\n",
      "Iter 6/40 - Loss: 1.101 (0.100)\n",
      "Iter 6/40 - Loss: 1.107 (0.100)\n",
      "Iter 6/40 - Loss: 1.107 (0.100)\n",
      "Iter 6/40 - Loss: 1.097 (0.100)\n",
      "Iter 6/40 - Loss: 1.176 (0.100)\n",
      "Iter 6/40 - Loss: 1.097 (0.100)\n",
      "Iter 6/40 - Loss: 1.074 (0.100)\n",
      "Iter 6/40 - Loss: 1.161 (0.100)\n",
      "Iter 6/40 - Loss: 1.121 (0.100)\n",
      "Iter 6/40 - Loss: 1.210 (0.100)\n",
      "Iter 6/40 - Loss: 1.068 (0.100)\n",
      "Iter 6/40 - Loss: 1.071 (0.100)\n",
      "Iter 6/40 - Loss: 1.082 (0.100)\n",
      "Iter 6/40 - Loss: 1.141 (0.100)\n",
      "Iter 6/40 - Loss: 1.024 (0.100)\n",
      "Iter 7/40 - Loss: 1.080 (0.100)\n",
      "Iter 7/40 - Loss: 1.120 (0.100)\n",
      "Iter 7/40 - Loss: 1.064 (0.100)\n",
      "Iter 7/40 - Loss: 1.083 (0.100)\n",
      "Iter 7/40 - Loss: 1.042 (0.100)\n",
      "Iter 7/40 - Loss: 1.170 (0.100)\n",
      "Iter 7/40 - Loss: 1.034 (0.100)\n",
      "Iter 7/40 - Loss: 1.008 (0.100)\n",
      "Iter 7/40 - Loss: 1.100 (0.100)\n",
      "Iter 7/40 - Loss: 1.085 (0.100)\n",
      "Iter 7/40 - Loss: 1.049 (0.100)\n",
      "Iter 7/40 - Loss: 0.980 (0.100)\n",
      "Iter 7/40 - Loss: 1.005 (0.100)\n",
      "Iter 7/40 - Loss: 1.079 (0.100)\n",
      "Iter 7/40 - Loss: 1.064 (0.100)\n",
      "Iter 7/40 - Loss: 0.868 (0.100)\n",
      "Iter 8/40 - Loss: 1.072 (0.100)\n",
      "Iter 8/40 - Loss: 1.024 (0.100)\n",
      "Iter 8/40 - Loss: 1.071 (0.100)\n",
      "Iter 8/40 - Loss: 0.987 (0.100)\n",
      "Iter 8/40 - Loss: 1.064 (0.100)\n",
      "Iter 8/40 - Loss: 0.947 (0.100)\n",
      "Iter 8/40 - Loss: 0.991 (0.100)\n",
      "Iter 8/40 - Loss: 1.034 (0.100)\n",
      "Iter 8/40 - Loss: 0.996 (0.100)\n",
      "Iter 8/40 - Loss: 1.004 (0.100)\n",
      "Iter 8/40 - Loss: 0.943 (0.100)\n",
      "Iter 8/40 - Loss: 0.935 (0.100)\n",
      "Iter 8/40 - Loss: 0.974 (0.100)\n",
      "Iter 8/40 - Loss: 1.055 (0.100)\n",
      "Iter 8/40 - Loss: 0.935 (0.100)\n",
      "Iter 8/40 - Loss: 0.816 (0.100)\n",
      "Iter 9/40 - Loss: 0.898 (0.100)\n",
      "Iter 9/40 - Loss: 0.975 (0.100)\n",
      "Iter 9/40 - Loss: 0.961 (0.100)\n",
      "Iter 9/40 - Loss: 0.973 (0.100)\n",
      "Iter 9/40 - Loss: 0.899 (0.100)\n",
      "Iter 9/40 - Loss: 0.969 (0.100)\n",
      "Iter 9/40 - Loss: 0.889 (0.100)\n",
      "Iter 9/40 - Loss: 1.039 (0.100)\n",
      "Iter 9/40 - Loss: 0.907 (0.100)\n",
      "Iter 9/40 - Loss: 0.938 (0.100)\n",
      "Iter 9/40 - Loss: 0.958 (0.100)\n",
      "Iter 9/40 - Loss: 0.871 (0.100)\n",
      "Iter 9/40 - Loss: 0.856 (0.100)\n",
      "Iter 9/40 - Loss: 0.932 (0.100)\n",
      "Iter 9/40 - Loss: 0.903 (0.100)\n",
      "Iter 9/40 - Loss: 0.854 (0.100)\n",
      "Iter 10/40 - Loss: 0.962 (0.100)\n",
      "Iter 10/40 - Loss: 0.930 (0.100)\n",
      "Iter 10/40 - Loss: 0.823 (0.100)\n",
      "Iter 10/40 - Loss: 0.846 (0.100)\n",
      "Iter 10/40 - Loss: 0.921 (0.100)\n",
      "Iter 10/40 - Loss: 0.782 (0.100)\n",
      "Iter 10/40 - Loss: 0.898 (0.100)\n",
      "Iter 10/40 - Loss: 0.861 (0.100)\n",
      "Iter 10/40 - Loss: 0.792 (0.100)\n",
      "Iter 10/40 - Loss: 0.838 (0.100)\n",
      "Iter 10/40 - Loss: 0.822 (0.100)\n",
      "Iter 10/40 - Loss: 0.819 (0.100)\n",
      "Iter 10/40 - Loss: 0.797 (0.100)\n",
      "Iter 10/40 - Loss: 0.863 (0.100)\n",
      "Iter 10/40 - Loss: 0.879 (0.100)\n",
      "Iter 10/40 - Loss: 0.797 (0.100)\n",
      "Iter 11/40 - Loss: 0.847 (0.100)\n",
      "Iter 11/40 - Loss: 0.824 (0.100)\n",
      "Iter 11/40 - Loss: 0.804 (0.100)\n",
      "Iter 11/40 - Loss: 0.811 (0.100)\n",
      "Iter 11/40 - Loss: 0.757 (0.100)\n",
      "Iter 11/40 - Loss: 0.793 (0.100)\n",
      "Iter 11/40 - Loss: 0.826 (0.100)\n",
      "Iter 11/40 - Loss: 0.741 (0.100)\n",
      "Iter 11/40 - Loss: 0.717 (0.100)\n",
      "Iter 11/40 - Loss: 0.861 (0.100)\n",
      "Iter 11/40 - Loss: 0.763 (0.100)\n",
      "Iter 11/40 - Loss: 0.727 (0.100)\n",
      "Iter 11/40 - Loss: 0.771 (0.100)\n",
      "Iter 11/40 - Loss: 0.675 (0.100)\n",
      "Iter 11/40 - Loss: 0.695 (0.100)\n",
      "Iter 11/40 - Loss: 0.682 (0.100)\n",
      "Iter 12/40 - Loss: 0.738 (0.100)\n",
      "Iter 12/40 - Loss: 0.789 (0.100)\n",
      "Iter 12/40 - Loss: 0.698 (0.100)\n",
      "Iter 12/40 - Loss: 0.728 (0.100)\n",
      "Iter 12/40 - Loss: 0.690 (0.100)\n",
      "Iter 12/40 - Loss: 0.652 (0.100)\n",
      "Iter 12/40 - Loss: 0.697 (0.100)\n",
      "Iter 12/40 - Loss: 0.632 (0.100)\n",
      "Iter 12/40 - Loss: 0.608 (0.100)\n",
      "Iter 12/40 - Loss: 0.705 (0.100)\n",
      "Iter 12/40 - Loss: 0.790 (0.100)\n",
      "Iter 12/40 - Loss: 0.735 (0.100)\n",
      "Iter 12/40 - Loss: 0.589 (0.100)\n",
      "Iter 12/40 - Loss: 0.680 (0.100)\n",
      "Iter 12/40 - Loss: 0.559 (0.100)\n",
      "Iter 12/40 - Loss: 0.594 (0.100)\n",
      "Iter 13/40 - Loss: 0.614 (0.100)\n",
      "Iter 13/40 - Loss: 0.565 (0.100)\n",
      "Iter 13/40 - Loss: 0.682 (0.100)\n",
      "Iter 13/40 - Loss: 0.645 (0.100)\n",
      "Iter 13/40 - Loss: 0.629 (0.100)\n",
      "Iter 13/40 - Loss: 0.624 (0.100)\n",
      "Iter 13/40 - Loss: 0.584 (0.100)\n",
      "Iter 13/40 - Loss: 0.599 (0.100)\n",
      "Iter 13/40 - Loss: 0.615 (0.100)\n",
      "Iter 13/40 - Loss: 0.607 (0.100)\n",
      "Iter 13/40 - Loss: 0.550 (0.100)\n",
      "Iter 13/40 - Loss: 0.518 (0.100)\n",
      "Iter 13/40 - Loss: 0.540 (0.100)\n",
      "Iter 13/40 - Loss: 0.533 (0.100)\n",
      "Iter 13/40 - Loss: 0.610 (0.100)\n",
      "Iter 13/40 - Loss: 0.637 (0.100)\n",
      "Iter 14/40 - Loss: 0.603 (0.100)\n",
      "Iter 14/40 - Loss: 0.586 (0.100)\n",
      "Iter 14/40 - Loss: 0.618 (0.100)\n",
      "Iter 14/40 - Loss: 0.489 (0.100)\n",
      "Iter 14/40 - Loss: 0.421 (0.100)\n",
      "Iter 14/40 - Loss: 0.464 (0.100)\n",
      "Iter 14/40 - Loss: 0.427 (0.100)\n",
      "Iter 14/40 - Loss: 0.431 (0.100)\n",
      "Iter 14/40 - Loss: 0.453 (0.100)\n",
      "Iter 14/40 - Loss: 0.540 (0.100)\n",
      "Iter 14/40 - Loss: 0.461 (0.100)\n",
      "Iter 14/40 - Loss: 0.426 (0.100)\n",
      "Iter 14/40 - Loss: 0.423 (0.100)\n",
      "Iter 14/40 - Loss: 0.463 (0.100)\n",
      "Iter 14/40 - Loss: 0.406 (0.100)\n",
      "Iter 14/40 - Loss: 0.411 (0.100)\n",
      "Iter 15/40 - Loss: 0.434 (0.100)\n",
      "Iter 15/40 - Loss: 0.556 (0.100)\n",
      "Iter 15/40 - Loss: 0.408 (0.100)\n",
      "Iter 15/40 - Loss: 0.424 (0.100)\n",
      "Iter 15/40 - Loss: 0.326 (0.100)\n",
      "Iter 15/40 - Loss: 0.347 (0.100)\n",
      "Iter 15/40 - Loss: 0.351 (0.100)\n",
      "Iter 15/40 - Loss: 0.360 (0.100)\n",
      "Iter 15/40 - Loss: 0.353 (0.100)\n",
      "Iter 15/40 - Loss: 0.255 (0.100)\n",
      "Iter 15/40 - Loss: 0.242 (0.100)\n",
      "Iter 15/40 - Loss: 0.374 (0.100)\n",
      "Iter 15/40 - Loss: 0.306 (0.100)\n",
      "Iter 15/40 - Loss: 0.299 (0.100)\n",
      "Iter 15/40 - Loss: 0.330 (0.100)\n",
      "Iter 15/40 - Loss: 0.413 (0.100)\n",
      "Iter 16/40 - Loss: 0.312 (0.100)\n",
      "Iter 16/40 - Loss: 0.238 (0.100)\n",
      "Iter 16/40 - Loss: 0.440 (0.100)\n",
      "Iter 16/40 - Loss: 0.198 (0.100)\n",
      "Iter 16/40 - Loss: 0.199 (0.100)\n",
      "Iter 16/40 - Loss: 0.286 (0.100)\n",
      "Iter 16/40 - Loss: 0.295 (0.100)\n",
      "Iter 16/40 - Loss: 0.100 (0.100)\n",
      "Iter 16/40 - Loss: 0.337 (0.100)\n",
      "Iter 16/40 - Loss: 0.248 (0.100)\n",
      "Iter 16/40 - Loss: 0.324 (0.100)\n",
      "Iter 16/40 - Loss: 0.230 (0.100)\n",
      "Iter 16/40 - Loss: 0.252 (0.100)\n",
      "Iter 16/40 - Loss: 0.292 (0.100)\n",
      "Iter 16/40 - Loss: 0.149 (0.100)\n",
      "Iter 16/40 - Loss: 0.202 (0.100)\n",
      "Iter 17/40 - Loss: 0.230 (0.100)\n",
      "Iter 17/40 - Loss: 0.198 (0.100)\n",
      "Iter 17/40 - Loss: 0.055 (0.100)\n",
      "Iter 17/40 - Loss: 0.224 (0.100)\n",
      "Iter 17/40 - Loss: 0.257 (0.100)\n",
      "Iter 17/40 - Loss: 0.157 (0.100)\n",
      "Iter 17/40 - Loss: 0.258 (0.100)\n",
      "Iter 17/40 - Loss: 0.279 (0.100)\n",
      "Iter 17/40 - Loss: 0.156 (0.100)\n",
      "Iter 17/40 - Loss: 0.242 (0.100)\n",
      "Iter 17/40 - Loss: 0.090 (0.100)\n",
      "Iter 17/40 - Loss: 0.043 (0.100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17/40 - Loss: 0.136 (0.100)\n",
      "Iter 17/40 - Loss: 0.170 (0.100)\n",
      "Iter 17/40 - Loss: 0.160 (0.100)\n",
      "Iter 17/40 - Loss: 0.236 (0.100)\n",
      "Iter 18/40 - Loss: 0.269 (0.100)\n",
      "Iter 18/40 - Loss: 0.124 (0.100)\n",
      "Iter 18/40 - Loss: 0.024 (0.100)\n",
      "Iter 18/40 - Loss: 0.126 (0.100)\n",
      "Iter 18/40 - Loss: 0.183 (0.100)\n",
      "Iter 18/40 - Loss: 0.080 (0.100)\n",
      "Iter 18/40 - Loss: 0.174 (0.100)\n",
      "Iter 18/40 - Loss: 0.076 (0.100)\n",
      "Iter 18/40 - Loss: 0.082 (0.100)\n",
      "Iter 18/40 - Loss: 0.058 (0.100)\n",
      "Iter 18/40 - Loss: 0.062 (0.100)\n",
      "Iter 18/40 - Loss: 0.224 (0.100)\n",
      "Iter 18/40 - Loss: 0.180 (0.100)\n",
      "Iter 18/40 - Loss: 0.074 (0.100)\n",
      "Iter 18/40 - Loss: 0.042 (0.100)\n",
      "Iter 18/40 - Loss: 0.284 (0.100)\n",
      "Iter 19/40 - Loss: 0.162 (0.100)\n",
      "Iter 19/40 - Loss: 0.084 (0.100)\n",
      "Iter 19/40 - Loss: 0.074 (0.100)\n",
      "Iter 19/40 - Loss: 0.103 (0.100)\n",
      "Iter 19/40 - Loss: 0.069 (0.100)\n",
      "Iter 19/40 - Loss: 0.101 (0.100)\n",
      "Iter 19/40 - Loss: 0.057 (0.100)\n",
      "Iter 19/40 - Loss: -0.007 (0.100)\n",
      "Iter 19/40 - Loss: 0.205 (0.100)\n",
      "Iter 19/40 - Loss: 0.156 (0.100)\n",
      "Iter 19/40 - Loss: 0.047 (0.100)\n",
      "Iter 19/40 - Loss: 0.018 (0.100)\n",
      "Iter 19/40 - Loss: -0.016 (0.100)\n",
      "Iter 19/40 - Loss: 0.097 (0.100)\n",
      "Iter 19/40 - Loss: 0.150 (0.100)\n",
      "Iter 19/40 - Loss: 0.174 (0.100)\n",
      "Iter 20/40 - Loss: 0.105 (0.100)\n",
      "Iter 20/40 - Loss: 0.212 (0.100)\n",
      "Iter 20/40 - Loss: 0.035 (0.100)\n",
      "Iter 20/40 - Loss: 0.046 (0.100)\n",
      "Iter 20/40 - Loss: 0.006 (0.100)\n",
      "Iter 20/40 - Loss: 0.128 (0.100)\n",
      "Iter 20/40 - Loss: -0.004 (0.100)\n",
      "Iter 20/40 - Loss: 0.127 (0.100)\n",
      "Iter 20/40 - Loss: 0.039 (0.100)\n",
      "Iter 20/40 - Loss: 0.056 (0.100)\n",
      "Iter 20/40 - Loss: 0.272 (0.100)\n",
      "Iter 20/40 - Loss: 0.020 (0.100)\n",
      "Iter 20/40 - Loss: -0.047 (0.100)\n",
      "Iter 20/40 - Loss: 0.237 (0.100)\n",
      "Iter 20/40 - Loss: 0.054 (0.100)\n",
      "Iter 20/40 - Loss: -0.067 (0.100)\n",
      "Iter 21/40 - Loss: 0.104 (0.100)\n",
      "Iter 21/40 - Loss: 0.009 (0.100)\n",
      "Iter 21/40 - Loss: -0.033 (0.100)\n",
      "Iter 21/40 - Loss: 0.043 (0.100)\n",
      "Iter 21/40 - Loss: -0.144 (0.100)\n",
      "Iter 21/40 - Loss: 0.165 (0.100)\n",
      "Iter 21/40 - Loss: -0.008 (0.100)\n",
      "Iter 21/40 - Loss: 0.130 (0.100)\n",
      "Iter 21/40 - Loss: -0.047 (0.100)\n",
      "Iter 21/40 - Loss: 0.157 (0.100)\n",
      "Iter 21/40 - Loss: 0.134 (0.100)\n",
      "Iter 21/40 - Loss: 0.142 (0.100)\n",
      "Iter 21/40 - Loss: 0.163 (0.100)\n",
      "Iter 21/40 - Loss: 0.069 (0.100)\n",
      "Iter 21/40 - Loss: 0.289 (0.100)\n",
      "Iter 21/40 - Loss: 0.009 (0.100)\n",
      "Iter 22/40 - Loss: -0.059 (0.100)\n",
      "Iter 22/40 - Loss: 0.202 (0.100)\n",
      "Iter 22/40 - Loss: 0.131 (0.100)\n",
      "Iter 22/40 - Loss: 0.255 (0.100)\n",
      "Iter 22/40 - Loss: 0.078 (0.100)\n",
      "Iter 22/40 - Loss: 0.087 (0.100)\n",
      "Iter 22/40 - Loss: 0.113 (0.100)\n",
      "Iter 22/40 - Loss: 0.054 (0.100)\n",
      "Iter 22/40 - Loss: -0.005 (0.100)\n",
      "Iter 22/40 - Loss: 0.102 (0.100)\n",
      "Iter 22/40 - Loss: 0.217 (0.100)\n",
      "Iter 22/40 - Loss: -0.005 (0.100)\n",
      "Iter 22/40 - Loss: -0.066 (0.100)\n",
      "Iter 22/40 - Loss: -0.026 (0.100)\n",
      "Iter 22/40 - Loss: 0.085 (0.100)\n",
      "Iter 22/40 - Loss: -0.067 (0.100)\n",
      "Iter 23/40 - Loss: 0.049 (0.100)\n",
      "Iter 23/40 - Loss: 0.144 (0.100)\n",
      "Iter 23/40 - Loss: 0.099 (0.100)\n",
      "Iter 23/40 - Loss: 0.019 (0.100)\n",
      "Iter 23/40 - Loss: 0.143 (0.100)\n",
      "Iter 23/40 - Loss: 0.076 (0.100)\n",
      "Iter 23/40 - Loss: 0.074 (0.100)\n",
      "Iter 23/40 - Loss: 0.120 (0.100)\n",
      "Iter 23/40 - Loss: 0.005 (0.100)\n",
      "Iter 23/40 - Loss: 0.005 (0.100)\n",
      "Iter 23/40 - Loss: 0.171 (0.100)\n",
      "Iter 23/40 - Loss: -0.064 (0.100)\n",
      "Iter 23/40 - Loss: -0.005 (0.100)\n",
      "Iter 23/40 - Loss: -0.030 (0.100)\n",
      "Iter 23/40 - Loss: 0.150 (0.100)\n",
      "Iter 23/40 - Loss: 0.220 (0.100)\n",
      "Iter 24/40 - Loss: 0.132 (0.100)\n",
      "Iter 24/40 - Loss: 0.104 (0.100)\n",
      "Iter 24/40 - Loss: -0.026 (0.100)\n",
      "Iter 24/40 - Loss: -0.006 (0.100)\n",
      "Iter 24/40 - Loss: 0.152 (0.100)\n",
      "Iter 24/40 - Loss: 0.295 (0.100)\n",
      "Iter 24/40 - Loss: -0.028 (0.100)\n",
      "Iter 24/40 - Loss: -0.084 (0.100)\n",
      "Iter 24/40 - Loss: 0.079 (0.100)\n",
      "Iter 24/40 - Loss: 0.134 (0.100)\n",
      "Iter 24/40 - Loss: -0.011 (0.100)\n",
      "Iter 24/40 - Loss: -0.122 (0.100)\n",
      "Iter 24/40 - Loss: 0.317 (0.100)\n",
      "Iter 24/40 - Loss: -0.052 (0.100)\n",
      "Iter 24/40 - Loss: -0.021 (0.100)\n",
      "Iter 24/40 - Loss: 0.119 (0.100)\n",
      "Iter 25/40 - Loss: 0.088 (0.100)\n",
      "Iter 25/40 - Loss: 0.037 (0.100)\n",
      "Iter 25/40 - Loss: -0.035 (0.100)\n",
      "Iter 25/40 - Loss: 0.063 (0.100)\n",
      "Iter 25/40 - Loss: 0.209 (0.100)\n",
      "Iter 25/40 - Loss: -0.008 (0.100)\n",
      "Iter 25/40 - Loss: 0.039 (0.100)\n",
      "Iter 25/40 - Loss: 0.151 (0.100)\n",
      "Iter 25/40 - Loss: -0.114 (0.100)\n",
      "Iter 25/40 - Loss: 0.051 (0.100)\n",
      "Iter 25/40 - Loss: 0.155 (0.100)\n",
      "Iter 25/40 - Loss: 0.121 (0.100)\n",
      "Iter 25/40 - Loss: 0.162 (0.100)\n",
      "Iter 25/40 - Loss: 0.026 (0.100)\n",
      "Iter 25/40 - Loss: 0.028 (0.100)\n",
      "Iter 25/40 - Loss: -0.111 (0.100)\n",
      "Iter 26/40 - Loss: 0.101 (0.100)\n",
      "Iter 26/40 - Loss: 0.042 (0.100)\n",
      "Iter 26/40 - Loss: 0.047 (0.100)\n",
      "Iter 26/40 - Loss: 0.105 (0.100)\n",
      "Iter 26/40 - Loss: -0.061 (0.100)\n",
      "Iter 26/40 - Loss: 0.077 (0.100)\n",
      "Iter 26/40 - Loss: 0.140 (0.100)\n",
      "Iter 26/40 - Loss: 0.042 (0.100)\n",
      "Iter 26/40 - Loss: 0.052 (0.100)\n",
      "Iter 26/40 - Loss: 0.094 (0.100)\n",
      "Iter 26/40 - Loss: 0.079 (0.100)\n",
      "Iter 26/40 - Loss: -0.107 (0.100)\n",
      "Iter 26/40 - Loss: 0.013 (0.100)\n",
      "Iter 26/40 - Loss: 0.004 (0.100)\n",
      "Iter 26/40 - Loss: 0.361 (0.100)\n",
      "Iter 26/40 - Loss: -0.018 (0.100)\n",
      "Iter 27/40 - Loss: 0.113 (0.100)\n",
      "Iter 27/40 - Loss: 0.116 (0.100)\n",
      "Iter 27/40 - Loss: -0.034 (0.100)\n",
      "Iter 27/40 - Loss: 0.009 (0.100)\n",
      "Iter 27/40 - Loss: -0.051 (0.100)\n",
      "Iter 27/40 - Loss: -0.008 (0.100)\n",
      "Iter 27/40 - Loss: 0.122 (0.100)\n",
      "Iter 27/40 - Loss: 0.100 (0.100)\n",
      "Iter 27/40 - Loss: 0.058 (0.100)\n",
      "Iter 27/40 - Loss: 0.266 (0.100)\n",
      "Iter 27/40 - Loss: 0.009 (0.100)\n",
      "Iter 27/40 - Loss: -0.106 (0.100)\n",
      "Iter 27/40 - Loss: 0.008 (0.100)\n",
      "Iter 27/40 - Loss: 0.181 (0.100)\n",
      "Iter 27/40 - Loss: 0.029 (0.100)\n",
      "Iter 27/40 - Loss: 0.055 (0.100)\n",
      "Iter 28/40 - Loss: 0.202 (0.100)\n",
      "Iter 28/40 - Loss: 0.060 (0.100)\n",
      "Iter 28/40 - Loss: 0.028 (0.100)\n",
      "Iter 28/40 - Loss: -0.051 (0.100)\n",
      "Iter 28/40 - Loss: 0.073 (0.100)\n",
      "Iter 28/40 - Loss: 0.047 (0.100)\n",
      "Iter 28/40 - Loss: -0.042 (0.100)\n",
      "Iter 28/40 - Loss: -0.055 (0.100)\n",
      "Iter 28/40 - Loss: 0.148 (0.100)\n",
      "Iter 28/40 - Loss: 0.342 (0.100)\n",
      "Iter 28/40 - Loss: 0.134 (0.100)\n",
      "Iter 28/40 - Loss: 0.196 (0.100)\n",
      "Iter 28/40 - Loss: -0.032 (0.100)\n",
      "Iter 28/40 - Loss: -0.116 (0.100)\n",
      "Iter 28/40 - Loss: -0.062 (0.100)\n",
      "Iter 28/40 - Loss: -0.076 (0.100)\n",
      "Iter 29/40 - Loss: 0.071 (0.100)\n",
      "Iter 29/40 - Loss: 0.111 (0.100)\n",
      "Iter 29/40 - Loss: 0.126 (0.100)\n",
      "Iter 29/40 - Loss: 0.147 (0.100)\n",
      "Iter 29/40 - Loss: -0.087 (0.100)\n",
      "Iter 29/40 - Loss: -0.006 (0.100)\n",
      "Iter 29/40 - Loss: 0.168 (0.100)\n",
      "Iter 29/40 - Loss: 0.103 (0.100)\n",
      "Iter 29/40 - Loss: 0.051 (0.100)\n",
      "Iter 29/40 - Loss: 0.089 (0.100)\n",
      "Iter 29/40 - Loss: 0.002 (0.100)\n",
      "Iter 29/40 - Loss: -0.074 (0.100)\n",
      "Iter 29/40 - Loss: 0.079 (0.100)\n",
      "Iter 29/40 - Loss: -0.021 (0.100)\n",
      "Iter 29/40 - Loss: 0.044 (0.100)\n",
      "Iter 29/40 - Loss: 0.173 (0.100)\n",
      "Iter 30/40 - Loss: 0.055 (0.100)\n",
      "Iter 30/40 - Loss: 0.100 (0.100)\n",
      "Iter 30/40 - Loss: 0.163 (0.100)\n",
      "Iter 30/40 - Loss: 0.043 (0.100)\n",
      "Iter 30/40 - Loss: -0.029 (0.100)\n",
      "Iter 30/40 - Loss: 0.093 (0.100)\n",
      "Iter 30/40 - Loss: 0.084 (0.100)\n",
      "Iter 30/40 - Loss: -0.151 (0.100)\n",
      "Iter 30/40 - Loss: -0.013 (0.100)\n",
      "Iter 30/40 - Loss: -0.089 (0.100)\n",
      "Iter 30/40 - Loss: 0.022 (0.100)\n",
      "Iter 30/40 - Loss: 0.155 (0.100)\n",
      "Iter 30/40 - Loss: 0.329 (0.100)\n",
      "Iter 30/40 - Loss: 0.080 (0.100)\n",
      "Iter 30/40 - Loss: 0.118 (0.100)\n",
      "Iter 30/40 - Loss: -0.170 (0.100)\n",
      "Iter 31/40 - Loss: 0.110 (0.010)\n",
      "Iter 31/40 - Loss: 0.025 (0.010)\n",
      "Iter 31/40 - Loss: 0.082 (0.010)\n",
      "Iter 31/40 - Loss: 0.072 (0.010)\n",
      "Iter 31/40 - Loss: -0.082 (0.010)\n",
      "Iter 31/40 - Loss: 0.148 (0.010)\n",
      "Iter 31/40 - Loss: -0.122 (0.010)\n",
      "Iter 31/40 - Loss: -0.009 (0.010)\n",
      "Iter 31/40 - Loss: -0.033 (0.010)\n",
      "Iter 31/40 - Loss: 0.173 (0.010)\n",
      "Iter 31/40 - Loss: 0.135 (0.010)\n",
      "Iter 31/40 - Loss: -0.007 (0.010)\n",
      "Iter 31/40 - Loss: 0.086 (0.010)\n",
      "Iter 31/40 - Loss: -0.073 (0.010)\n",
      "Iter 31/40 - Loss: -0.046 (0.010)\n",
      "Iter 31/40 - Loss: 0.362 (0.010)\n",
      "Iter 32/40 - Loss: 0.105 (0.010)\n",
      "Iter 32/40 - Loss: -0.042 (0.010)\n",
      "Iter 32/40 - Loss: -0.031 (0.010)\n",
      "Iter 32/40 - Loss: 0.295 (0.010)\n",
      "Iter 32/40 - Loss: -0.106 (0.010)\n",
      "Iter 32/40 - Loss: 0.178 (0.010)\n",
      "Iter 32/40 - Loss: 0.021 (0.010)\n",
      "Iter 32/40 - Loss: 0.185 (0.010)\n",
      "Iter 32/40 - Loss: 0.134 (0.010)\n",
      "Iter 32/40 - Loss: 0.046 (0.010)\n",
      "Iter 32/40 - Loss: -0.016 (0.010)\n",
      "Iter 32/40 - Loss: 0.043 (0.010)\n",
      "Iter 32/40 - Loss: 0.022 (0.010)\n",
      "Iter 32/40 - Loss: 0.020 (0.010)\n",
      "Iter 32/40 - Loss: -0.071 (0.010)\n",
      "Iter 32/40 - Loss: 0.020 (0.010)\n",
      "Iter 33/40 - Loss: 0.085 (0.010)\n",
      "Iter 33/40 - Loss: 0.185 (0.010)\n",
      "Iter 33/40 - Loss: -0.022 (0.010)\n",
      "Iter 33/40 - Loss: -0.031 (0.010)\n",
      "Iter 33/40 - Loss: -0.034 (0.010)\n",
      "Iter 33/40 - Loss: 0.200 (0.010)\n",
      "Iter 33/40 - Loss: 0.050 (0.010)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 33/40 - Loss: 0.026 (0.010)\n",
      "Iter 33/40 - Loss: 0.026 (0.010)\n",
      "Iter 33/40 - Loss: 0.101 (0.010)\n",
      "Iter 33/40 - Loss: 0.007 (0.010)\n",
      "Iter 33/40 - Loss: 0.076 (0.010)\n",
      "Iter 33/40 - Loss: 0.024 (0.010)\n",
      "Iter 33/40 - Loss: 0.172 (0.010)\n",
      "Iter 33/40 - Loss: 0.018 (0.010)\n",
      "Iter 33/40 - Loss: 0.102 (0.010)\n",
      "Iter 34/40 - Loss: 0.137 (0.010)\n",
      "Iter 34/40 - Loss: 0.232 (0.010)\n",
      "Iter 34/40 - Loss: 0.047 (0.010)\n",
      "Iter 34/40 - Loss: -0.042 (0.010)\n",
      "Iter 34/40 - Loss: 0.172 (0.010)\n",
      "Iter 34/40 - Loss: -0.018 (0.010)\n",
      "Iter 34/40 - Loss: -0.021 (0.010)\n",
      "Iter 34/40 - Loss: 0.055 (0.010)\n",
      "Iter 34/40 - Loss: 0.144 (0.010)\n",
      "Iter 34/40 - Loss: 0.090 (0.010)\n",
      "Iter 34/40 - Loss: 0.014 (0.010)\n",
      "Iter 34/40 - Loss: 0.106 (0.010)\n",
      "Iter 34/40 - Loss: 0.087 (0.010)\n",
      "Iter 34/40 - Loss: 0.040 (0.010)\n",
      "Iter 34/40 - Loss: -0.033 (0.010)\n",
      "Iter 34/40 - Loss: 0.136 (0.010)\n",
      "Iter 35/40 - Loss: 0.010 (0.010)\n",
      "Iter 35/40 - Loss: 0.129 (0.010)\n",
      "Iter 35/40 - Loss: -0.054 (0.010)\n",
      "Iter 35/40 - Loss: -0.094 (0.010)\n",
      "Iter 35/40 - Loss: 0.264 (0.010)\n",
      "Iter 35/40 - Loss: 0.209 (0.010)\n",
      "Iter 35/40 - Loss: 0.158 (0.010)\n",
      "Iter 35/40 - Loss: 0.141 (0.010)\n",
      "Iter 35/40 - Loss: 0.106 (0.010)\n",
      "Iter 35/40 - Loss: -0.047 (0.010)\n",
      "Iter 35/40 - Loss: 0.028 (0.010)\n",
      "Iter 35/40 - Loss: 0.026 (0.010)\n",
      "Iter 35/40 - Loss: 0.043 (0.010)\n",
      "Iter 35/40 - Loss: 0.240 (0.010)\n",
      "Iter 35/40 - Loss: 0.030 (0.010)\n",
      "Iter 35/40 - Loss: 0.080 (0.010)\n",
      "Iter 36/40 - Loss: 0.106 (0.010)\n",
      "Iter 36/40 - Loss: -0.000 (0.010)\n",
      "Iter 36/40 - Loss: 0.189 (0.010)\n",
      "Iter 36/40 - Loss: 0.194 (0.010)\n",
      "Iter 36/40 - Loss: -0.041 (0.010)\n",
      "Iter 36/40 - Loss: -0.000 (0.010)\n",
      "Iter 36/40 - Loss: 0.213 (0.010)\n",
      "Iter 36/40 - Loss: 0.093 (0.010)\n",
      "Iter 36/40 - Loss: 0.148 (0.010)\n",
      "Iter 36/40 - Loss: 0.009 (0.010)\n",
      "Iter 36/40 - Loss: 0.079 (0.010)\n",
      "Iter 36/40 - Loss: -0.048 (0.010)\n",
      "Iter 36/40 - Loss: 0.137 (0.010)\n",
      "Iter 36/40 - Loss: 0.188 (0.010)\n",
      "Iter 36/40 - Loss: 0.095 (0.010)\n",
      "Iter 36/40 - Loss: 0.036 (0.010)\n",
      "Iter 37/40 - Loss: -0.011 (0.010)\n",
      "Iter 37/40 - Loss: 0.118 (0.010)\n",
      "Iter 37/40 - Loss: 0.166 (0.010)\n",
      "Iter 37/40 - Loss: 0.084 (0.010)\n",
      "Iter 37/40 - Loss: 0.163 (0.010)\n",
      "Iter 37/40 - Loss: 0.124 (0.010)\n",
      "Iter 37/40 - Loss: 0.036 (0.010)\n",
      "Iter 37/40 - Loss: 0.203 (0.010)\n",
      "Iter 37/40 - Loss: 0.069 (0.010)\n",
      "Iter 37/40 - Loss: 0.088 (0.010)\n",
      "Iter 37/40 - Loss: 0.222 (0.010)\n",
      "Iter 37/40 - Loss: 0.026 (0.010)\n",
      "Iter 37/40 - Loss: 0.201 (0.010)\n",
      "Iter 37/40 - Loss: 0.028 (0.010)\n",
      "Iter 37/40 - Loss: 0.031 (0.010)\n",
      "Iter 37/40 - Loss: -0.071 (0.010)\n",
      "Iter 38/40 - Loss: 0.122 (0.010)\n",
      "Iter 38/40 - Loss: 0.158 (0.010)\n",
      "Iter 38/40 - Loss: 0.093 (0.010)\n",
      "Iter 38/40 - Loss: 0.064 (0.010)\n",
      "Iter 38/40 - Loss: 0.133 (0.010)\n",
      "Iter 38/40 - Loss: 0.194 (0.010)\n",
      "Iter 38/40 - Loss: 0.221 (0.010)\n",
      "Iter 38/40 - Loss: 0.159 (0.010)\n",
      "Iter 38/40 - Loss: 0.114 (0.010)\n",
      "Iter 38/40 - Loss: 0.022 (0.010)\n",
      "Iter 38/40 - Loss: 0.051 (0.010)\n",
      "Iter 38/40 - Loss: -0.016 (0.010)\n",
      "Iter 38/40 - Loss: 0.233 (0.010)\n",
      "Iter 38/40 - Loss: 0.087 (0.010)\n",
      "Iter 38/40 - Loss: -0.034 (0.010)\n",
      "Iter 38/40 - Loss: 0.051 (0.010)\n",
      "Iter 39/40 - Loss: 0.017 (0.010)\n",
      "Iter 39/40 - Loss: 0.056 (0.010)\n",
      "Iter 39/40 - Loss: 0.068 (0.010)\n",
      "Iter 39/40 - Loss: 0.045 (0.010)\n",
      "Iter 39/40 - Loss: 0.076 (0.010)\n",
      "Iter 39/40 - Loss: 0.111 (0.010)\n",
      "Iter 39/40 - Loss: 0.082 (0.010)\n",
      "Iter 39/40 - Loss: 0.250 (0.010)\n",
      "Iter 39/40 - Loss: -0.028 (0.010)\n",
      "Iter 39/40 - Loss: 0.254 (0.010)\n",
      "Iter 39/40 - Loss: 0.186 (0.010)\n",
      "Iter 39/40 - Loss: 0.105 (0.010)\n",
      "Iter 39/40 - Loss: 0.172 (0.010)\n",
      "Iter 39/40 - Loss: 0.182 (0.010)\n",
      "Iter 39/40 - Loss: 0.114 (0.010)\n",
      "Iter 39/40 - Loss: 0.108 (0.010)\n",
      "Iter 40/40 - Loss: 0.155 (0.010)\n",
      "Iter 40/40 - Loss: 0.140 (0.010)\n",
      "Iter 40/40 - Loss: 0.038 (0.010)\n",
      "Iter 40/40 - Loss: 0.149 (0.010)\n",
      "Iter 40/40 - Loss: 0.064 (0.010)\n",
      "Iter 40/40 - Loss: 0.116 (0.010)\n",
      "Iter 40/40 - Loss: 0.170 (0.010)\n",
      "Iter 40/40 - Loss: 0.105 (0.010)\n",
      "Iter 40/40 - Loss: 0.181 (0.010)\n",
      "Iter 40/40 - Loss: 0.225 (0.010)\n",
      "Iter 40/40 - Loss: 0.126 (0.010)\n",
      "Iter 40/40 - Loss: -0.004 (0.010)\n",
      "Iter 40/40 - Loss: 0.038 (0.010)\n",
      "Iter 40/40 - Loss: 0.101 (0.010)\n",
      "Iter 40/40 - Loss: 0.096 (0.010)\n",
      "Iter 40/40 - Loss: 0.254 (0.010)\n",
      "CPU times: user 7.58 s, sys: 96 ms, total: 7.67 s\n",
      "Wall time: 7.61 s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 40 iterations of optimization\n",
    "n_iter = 40\n",
    "\n",
    "# We use SGD here, rather than Adam\n",
    "# Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# We use a Learning rate scheduler from PyTorch to lower the learning rate during optimization\n",
    "# We're going to drop the learning rate by 1/10 after 3/4 of training\n",
    "# This helps the model converge to a minimum\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.75 * n_iter], gamma=0.1)\n",
    "\n",
    "# Our loss object\n",
    "# We're using the VariationalMarginalLogLikelihood object\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=train_y.size(0))\n",
    "\n",
    "# The training loop\n",
    "def train():\n",
    "    for i in range(n_iter):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = torch.autograd.Variable(x_batch.float().cuda())\n",
    "            y_batch = torch.autograd.Variable(y_batch.float().cuda())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # We're going to use two context managers here\n",
    "            \n",
    "            # The use_toeplitz flag makes learning faster on the GPU\n",
    "            # See the DKL-MNIST notebook for an explanation\n",
    "            \n",
    "            # The diagonal_correction flag improves the approximations we're making for variational inference\n",
    "            # It makes running time a bit slower, but improves the optimization and predictions\n",
    "            with gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "                output = model(x_batch)\n",
    "                loss = -mll(output, y_batch)\n",
    "                print('Iter %d/%d - Loss: %.3f (%.3f)' % (i + 1, n_iter, loss.data[0], optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # The actual optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7e4c065550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e56bb16a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "test_x = torch.autograd.Variable(torch.linspace(0, 1, 51)).cuda()\n",
    "with gpytorch.settings.max_cg_iterations(2000), gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "lower, upper = observed_pred.confidence_region()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), 'k*')\n",
    "ax.plot(test_x.data.cpu().numpy(), observed_pred.mean().data.cpu().numpy(), 'b')\n",
    "ax.fill_between(test_x.data.cpu().numpy(), lower.data.cpu().numpy(), upper.data.cpu().numpy(), alpha=0.5)\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
