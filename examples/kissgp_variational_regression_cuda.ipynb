{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to perform GP regression, but using **variational inference** rather than exact inference. There are a few cases where variational inference may be prefereable:\n",
    "\n",
    "1) If you have lots of data, and want to perform **stochastic optimization**\n",
    "\n",
    "2) If you have a model where you want to use other variational distributions\n",
    "\n",
    "KISS-GP with SVI was introduced in:\n",
    "https://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a training set\n",
    "# We're going to learn a sine function\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "train_y = torch.sin(train_x * (4 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing SGD - the dataloader\n",
    "\n",
    "Because we want to do stochastic optimization, we have to put the dataset in a pytorch **DataLoader**.\n",
    "This creates easy minibatches of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "This is pretty similar to a normal regression model, except now we're using a `gpytorch.models.GridInducingVariationalGP` instead of a `gpytorch.models.ExactGP`.\n",
    "\n",
    "Any of the variational models would work. We're using the `GridInducingVariationalGP` because we have many data points, but only 1 dimensional data.\n",
    "\n",
    "Similar to exact regression, we use a `GaussianLikelihood`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.priors import SmoothedBoxPrior\n",
    "from gpytorch.random_variables import GaussianRandomVariable\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPRegressionModel, self).__init__(grid_size=20, grid_bounds=[(-0.05, 1.05)])\n",
    "        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n",
    "        self.covar_module = RBFKernel(\n",
    "            log_lengthscale_prior=SmoothedBoxPrior(math.exp(-3), math.exp(6), sigma=0.1, log_transform=True)\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            name=\"log_outputscale\",\n",
    "            parameter=torch.nn.Parameter(torch.Tensor([0])),\n",
    "            prior=SmoothedBoxPrior(math.exp(-5), math.exp(1), sigma=0.1, log_transform=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x) * self.log_outputscale.exp()\n",
    "        return GaussianRandomVariable(mean_x, covar_x)\n",
    "    \n",
    "model = GPRegressionModel().cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "This training loop will use **stochastic optimization** rather than batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/40 - Loss: 11.036 (0.100)\n",
      "Iter 1/40 - Loss: 8.227 (0.100)\n",
      "Iter 1/40 - Loss: 6.163 (0.100)\n",
      "Iter 1/40 - Loss: 4.766 (0.100)\n",
      "Iter 1/40 - Loss: 3.743 (0.100)\n",
      "Iter 1/40 - Loss: 2.963 (0.100)\n",
      "Iter 1/40 - Loss: 2.473 (0.100)\n",
      "Iter 1/40 - Loss: 2.279 (0.100)\n",
      "Iter 1/40 - Loss: 1.975 (0.100)\n",
      "Iter 1/40 - Loss: 1.926 (0.100)\n",
      "Iter 1/40 - Loss: 1.867 (0.100)\n",
      "Iter 1/40 - Loss: 1.813 (0.100)\n",
      "Iter 1/40 - Loss: 1.842 (0.100)\n",
      "Iter 1/40 - Loss: 1.784 (0.100)\n",
      "Iter 1/40 - Loss: 1.765 (0.100)\n",
      "Iter 1/40 - Loss: 1.739 (0.100)\n",
      "Iter 2/40 - Loss: 1.669 (0.100)\n",
      "Iter 2/40 - Loss: 1.606 (0.100)\n",
      "Iter 2/40 - Loss: 1.560 (0.100)\n",
      "Iter 2/40 - Loss: 1.537 (0.100)\n",
      "Iter 2/40 - Loss: 1.495 (0.100)\n",
      "Iter 2/40 - Loss: 1.411 (0.100)\n",
      "Iter 2/40 - Loss: 1.401 (0.100)\n",
      "Iter 2/40 - Loss: 1.342 (0.100)\n",
      "Iter 2/40 - Loss: 1.273 (0.100)\n",
      "Iter 2/40 - Loss: 1.237 (0.100)\n",
      "Iter 2/40 - Loss: 1.161 (0.100)\n",
      "Iter 2/40 - Loss: 1.123 (0.100)\n",
      "Iter 2/40 - Loss: 1.069 (0.100)\n",
      "Iter 2/40 - Loss: 1.004 (0.100)\n",
      "Iter 2/40 - Loss: 0.944 (0.100)\n",
      "Iter 2/40 - Loss: 0.874 (0.100)\n",
      "Iter 3/40 - Loss: 0.833 (0.100)\n",
      "Iter 3/40 - Loss: 0.799 (0.100)\n",
      "Iter 3/40 - Loss: 0.737 (0.100)\n",
      "Iter 3/40 - Loss: 0.669 (0.100)\n",
      "Iter 3/40 - Loss: 0.667 (0.100)\n",
      "Iter 3/40 - Loss: 0.604 (0.100)\n",
      "Iter 3/40 - Loss: 0.569 (0.100)\n",
      "Iter 3/40 - Loss: 0.507 (0.100)\n",
      "Iter 3/40 - Loss: 0.485 (0.100)\n",
      "Iter 3/40 - Loss: 0.384 (0.100)\n",
      "Iter 3/40 - Loss: 0.334 (0.100)\n",
      "Iter 3/40 - Loss: 0.301 (0.100)\n",
      "Iter 3/40 - Loss: 0.257 (0.100)\n",
      "Iter 3/40 - Loss: 0.198 (0.100)\n",
      "Iter 3/40 - Loss: 0.182 (0.100)\n",
      "Iter 3/40 - Loss: 0.238 (0.100)\n",
      "Iter 4/40 - Loss: 0.106 (0.100)\n",
      "Iter 4/40 - Loss: 0.048 (0.100)\n",
      "Iter 4/40 - Loss: 0.142 (0.100)\n",
      "Iter 4/40 - Loss: 0.128 (0.100)\n",
      "Iter 4/40 - Loss: 0.083 (0.100)\n",
      "Iter 4/40 - Loss: 0.010 (0.100)\n",
      "Iter 4/40 - Loss: -0.081 (0.100)\n",
      "Iter 4/40 - Loss: 0.150 (0.100)\n",
      "Iter 4/40 - Loss: 0.071 (0.100)\n",
      "Iter 4/40 - Loss: 0.062 (0.100)\n",
      "Iter 4/40 - Loss: -0.089 (0.100)\n",
      "Iter 4/40 - Loss: 0.137 (0.100)\n",
      "Iter 4/40 - Loss: 0.080 (0.100)\n",
      "Iter 4/40 - Loss: -0.046 (0.100)\n",
      "Iter 4/40 - Loss: -0.112 (0.100)\n",
      "Iter 4/40 - Loss: -0.076 (0.100)\n",
      "Iter 5/40 - Loss: 0.147 (0.100)\n",
      "Iter 5/40 - Loss: 0.060 (0.100)\n",
      "Iter 5/40 - Loss: -0.237 (0.100)\n",
      "Iter 5/40 - Loss: -0.156 (0.100)\n",
      "Iter 5/40 - Loss: -0.068 (0.100)\n",
      "Iter 5/40 - Loss: 0.013 (0.100)\n",
      "Iter 5/40 - Loss: 0.162 (0.100)\n",
      "Iter 5/40 - Loss: 0.042 (0.100)\n",
      "Iter 5/40 - Loss: 0.014 (0.100)\n",
      "Iter 5/40 - Loss: -0.044 (0.100)\n",
      "Iter 5/40 - Loss: -0.073 (0.100)\n",
      "Iter 5/40 - Loss: -0.072 (0.100)\n",
      "Iter 5/40 - Loss: -0.012 (0.100)\n",
      "Iter 5/40 - Loss: 0.122 (0.100)\n",
      "Iter 5/40 - Loss: -0.143 (0.100)\n",
      "Iter 5/40 - Loss: 0.173 (0.100)\n",
      "Iter 6/40 - Loss: -0.009 (0.100)\n",
      "Iter 6/40 - Loss: 0.197 (0.100)\n",
      "Iter 6/40 - Loss: -0.041 (0.100)\n",
      "Iter 6/40 - Loss: 0.051 (0.100)\n",
      "Iter 6/40 - Loss: 0.029 (0.100)\n",
      "Iter 6/40 - Loss: -0.001 (0.100)\n",
      "Iter 6/40 - Loss: -0.035 (0.100)\n",
      "Iter 6/40 - Loss: -0.030 (0.100)\n",
      "Iter 6/40 - Loss: -0.078 (0.100)\n",
      "Iter 6/40 - Loss: 0.034 (0.100)\n",
      "Iter 6/40 - Loss: 0.055 (0.100)\n",
      "Iter 6/40 - Loss: -0.059 (0.100)\n",
      "Iter 6/40 - Loss: -0.148 (0.100)\n",
      "Iter 6/40 - Loss: -0.106 (0.100)\n",
      "Iter 6/40 - Loss: -0.108 (0.100)\n",
      "Iter 6/40 - Loss: -0.123 (0.100)\n",
      "Iter 7/40 - Loss: -0.149 (0.100)\n",
      "Iter 7/40 - Loss: -0.014 (0.100)\n",
      "Iter 7/40 - Loss: 0.136 (0.100)\n",
      "Iter 7/40 - Loss: -0.036 (0.100)\n",
      "Iter 7/40 - Loss: 0.178 (0.100)\n",
      "Iter 7/40 - Loss: -0.069 (0.100)\n",
      "Iter 7/40 - Loss: -0.084 (0.100)\n",
      "Iter 7/40 - Loss: -0.061 (0.100)\n",
      "Iter 7/40 - Loss: 0.012 (0.100)\n",
      "Iter 7/40 - Loss: 0.053 (0.100)\n",
      "Iter 7/40 - Loss: -0.039 (0.100)\n",
      "Iter 7/40 - Loss: -0.134 (0.100)\n",
      "Iter 7/40 - Loss: -0.160 (0.100)\n",
      "Iter 7/40 - Loss: -0.115 (0.100)\n",
      "Iter 7/40 - Loss: 0.028 (0.100)\n",
      "Iter 7/40 - Loss: -0.071 (0.100)\n",
      "Iter 8/40 - Loss: -0.062 (0.100)\n",
      "Iter 8/40 - Loss: -0.048 (0.100)\n",
      "Iter 8/40 - Loss: 0.115 (0.100)\n",
      "Iter 8/40 - Loss: -0.154 (0.100)\n",
      "Iter 8/40 - Loss: -0.220 (0.100)\n",
      "Iter 8/40 - Loss: -0.079 (0.100)\n",
      "Iter 8/40 - Loss: 0.085 (0.100)\n",
      "Iter 8/40 - Loss: -0.133 (0.100)\n",
      "Iter 8/40 - Loss: 0.020 (0.100)\n",
      "Iter 8/40 - Loss: -0.035 (0.100)\n",
      "Iter 8/40 - Loss: -0.025 (0.100)\n",
      "Iter 8/40 - Loss: -0.102 (0.100)\n",
      "Iter 8/40 - Loss: -0.102 (0.100)\n",
      "Iter 8/40 - Loss: -0.067 (0.100)\n",
      "Iter 8/40 - Loss: -0.151 (0.100)\n",
      "Iter 8/40 - Loss: 0.163 (0.100)\n",
      "Iter 9/40 - Loss: -0.072 (0.100)\n",
      "Iter 9/40 - Loss: -0.159 (0.100)\n",
      "Iter 9/40 - Loss: -0.021 (0.100)\n",
      "Iter 9/40 - Loss: -0.121 (0.100)\n",
      "Iter 9/40 - Loss: -0.097 (0.100)\n",
      "Iter 9/40 - Loss: -0.110 (0.100)\n",
      "Iter 9/40 - Loss: -0.032 (0.100)\n",
      "Iter 9/40 - Loss: -0.141 (0.100)\n",
      "Iter 9/40 - Loss: 0.079 (0.100)\n",
      "Iter 9/40 - Loss: -0.020 (0.100)\n",
      "Iter 9/40 - Loss: -0.091 (0.100)\n",
      "Iter 9/40 - Loss: -0.045 (0.100)\n",
      "Iter 9/40 - Loss: -0.063 (0.100)\n",
      "Iter 9/40 - Loss: -0.037 (0.100)\n",
      "Iter 9/40 - Loss: -0.125 (0.100)\n",
      "Iter 9/40 - Loss: -0.094 (0.100)\n",
      "Iter 10/40 - Loss: -0.078 (0.100)\n",
      "Iter 10/40 - Loss: -0.122 (0.100)\n",
      "Iter 10/40 - Loss: 0.054 (0.100)\n",
      "Iter 10/40 - Loss: -0.094 (0.100)\n",
      "Iter 10/40 - Loss: -0.151 (0.100)\n",
      "Iter 10/40 - Loss: -0.104 (0.100)\n",
      "Iter 10/40 - Loss: -0.100 (0.100)\n",
      "Iter 10/40 - Loss: 0.131 (0.100)\n",
      "Iter 10/40 - Loss: -0.070 (0.100)\n",
      "Iter 10/40 - Loss: 0.032 (0.100)\n",
      "Iter 10/40 - Loss: 0.064 (0.100)\n",
      "Iter 10/40 - Loss: -0.119 (0.100)\n",
      "Iter 10/40 - Loss: -0.235 (0.100)\n",
      "Iter 10/40 - Loss: 0.043 (0.100)\n",
      "Iter 10/40 - Loss: -0.136 (0.100)\n",
      "Iter 10/40 - Loss: -0.072 (0.100)\n",
      "Iter 11/40 - Loss: -0.166 (0.100)\n",
      "Iter 11/40 - Loss: 0.015 (0.100)\n",
      "Iter 11/40 - Loss: -0.050 (0.100)\n",
      "Iter 11/40 - Loss: -0.117 (0.100)\n",
      "Iter 11/40 - Loss: -0.088 (0.100)\n",
      "Iter 11/40 - Loss: 0.021 (0.100)\n",
      "Iter 11/40 - Loss: -0.099 (0.100)\n",
      "Iter 11/40 - Loss: -0.080 (0.100)\n",
      "Iter 11/40 - Loss: -0.094 (0.100)\n",
      "Iter 11/40 - Loss: -0.062 (0.100)\n",
      "Iter 11/40 - Loss: 0.144 (0.100)\n",
      "Iter 11/40 - Loss: -0.020 (0.100)\n",
      "Iter 11/40 - Loss: -0.131 (0.100)\n",
      "Iter 11/40 - Loss: -0.093 (0.100)\n",
      "Iter 11/40 - Loss: -0.060 (0.100)\n",
      "Iter 11/40 - Loss: -0.056 (0.100)\n",
      "Iter 12/40 - Loss: -0.062 (0.100)\n",
      "Iter 12/40 - Loss: -0.023 (0.100)\n",
      "Iter 12/40 - Loss: 0.003 (0.100)\n",
      "Iter 12/40 - Loss: -0.035 (0.100)\n",
      "Iter 12/40 - Loss: -0.196 (0.100)\n",
      "Iter 12/40 - Loss: 0.014 (0.100)\n",
      "Iter 12/40 - Loss: -0.047 (0.100)\n",
      "Iter 12/40 - Loss: -0.060 (0.100)\n",
      "Iter 12/40 - Loss: -0.112 (0.100)\n",
      "Iter 12/40 - Loss: -0.014 (0.100)\n",
      "Iter 12/40 - Loss: -0.108 (0.100)\n",
      "Iter 12/40 - Loss: -0.073 (0.100)\n",
      "Iter 12/40 - Loss: -0.112 (0.100)\n",
      "Iter 12/40 - Loss: -0.107 (0.100)\n",
      "Iter 12/40 - Loss: -0.052 (0.100)\n",
      "Iter 12/40 - Loss: -0.159 (0.100)\n",
      "Iter 13/40 - Loss: -0.089 (0.100)\n",
      "Iter 13/40 - Loss: -0.145 (0.100)\n",
      "Iter 13/40 - Loss: -0.076 (0.100)\n",
      "Iter 13/40 - Loss: 0.047 (0.100)\n",
      "Iter 13/40 - Loss: -0.090 (0.100)\n",
      "Iter 13/40 - Loss: 0.006 (0.100)\n",
      "Iter 13/40 - Loss: -0.176 (0.100)\n",
      "Iter 13/40 - Loss: -0.043 (0.100)\n",
      "Iter 13/40 - Loss: -0.062 (0.100)\n",
      "Iter 13/40 - Loss: -0.131 (0.100)\n",
      "Iter 13/40 - Loss: 0.014 (0.100)\n",
      "Iter 13/40 - Loss: -0.119 (0.100)\n",
      "Iter 13/40 - Loss: -0.200 (0.100)\n",
      "Iter 13/40 - Loss: 0.040 (0.100)\n",
      "Iter 13/40 - Loss: -0.096 (0.100)\n",
      "Iter 13/40 - Loss: 0.126 (0.100)\n",
      "Iter 14/40 - Loss: -0.221 (0.100)\n",
      "Iter 14/40 - Loss: -0.012 (0.100)\n",
      "Iter 14/40 - Loss: 0.008 (0.100)\n",
      "Iter 14/40 - Loss: 0.081 (0.100)\n",
      "Iter 14/40 - Loss: -0.145 (0.100)\n",
      "Iter 14/40 - Loss: -0.095 (0.100)\n",
      "Iter 14/40 - Loss: -0.007 (0.100)\n",
      "Iter 14/40 - Loss: -0.032 (0.100)\n",
      "Iter 14/40 - Loss: -0.205 (0.100)\n",
      "Iter 14/40 - Loss: -0.103 (0.100)\n",
      "Iter 14/40 - Loss: -0.126 (0.100)\n",
      "Iter 14/40 - Loss: -0.014 (0.100)\n",
      "Iter 14/40 - Loss: -0.177 (0.100)\n",
      "Iter 14/40 - Loss: -0.116 (0.100)\n",
      "Iter 14/40 - Loss: 0.081 (0.100)\n",
      "Iter 14/40 - Loss: -0.053 (0.100)\n",
      "Iter 15/40 - Loss: -0.029 (0.100)\n",
      "Iter 15/40 - Loss: -0.028 (0.100)\n",
      "Iter 15/40 - Loss: -0.121 (0.100)\n",
      "Iter 15/40 - Loss: 0.068 (0.100)\n",
      "Iter 15/40 - Loss: -0.022 (0.100)\n",
      "Iter 15/40 - Loss: -0.220 (0.100)\n",
      "Iter 15/40 - Loss: -0.189 (0.100)\n",
      "Iter 15/40 - Loss: -0.034 (0.100)\n",
      "Iter 15/40 - Loss: -0.063 (0.100)\n",
      "Iter 15/40 - Loss: -0.009 (0.100)\n",
      "Iter 15/40 - Loss: 0.001 (0.100)\n",
      "Iter 15/40 - Loss: -0.082 (0.100)\n",
      "Iter 15/40 - Loss: -0.136 (0.100)\n",
      "Iter 15/40 - Loss: -0.018 (0.100)\n",
      "Iter 15/40 - Loss: -0.049 (0.100)\n",
      "Iter 15/40 - Loss: 0.010 (0.100)\n",
      "Iter 16/40 - Loss: -0.107 (0.100)\n",
      "Iter 16/40 - Loss: 0.026 (0.100)\n",
      "Iter 16/40 - Loss: 0.040 (0.100)\n",
      "Iter 16/40 - Loss: 0.029 (0.100)\n",
      "Iter 16/40 - Loss: -0.148 (0.100)\n",
      "Iter 16/40 - Loss: -0.143 (0.100)\n",
      "Iter 16/40 - Loss: -0.077 (0.100)\n",
      "Iter 16/40 - Loss: -0.071 (0.100)\n",
      "Iter 16/40 - Loss: 0.032 (0.100)\n",
      "Iter 16/40 - Loss: -0.066 (0.100)\n",
      "Iter 16/40 - Loss: -0.098 (0.100)\n",
      "Iter 16/40 - Loss: -0.052 (0.100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16/40 - Loss: 0.033 (0.100)\n",
      "Iter 16/40 - Loss: -0.033 (0.100)\n",
      "Iter 16/40 - Loss: -0.053 (0.100)\n",
      "Iter 16/40 - Loss: 0.000 (0.100)\n",
      "Iter 17/40 - Loss: 0.021 (0.100)\n",
      "Iter 17/40 - Loss: -0.277 (0.100)\n",
      "Iter 17/40 - Loss: -0.177 (0.100)\n",
      "Iter 17/40 - Loss: -0.074 (0.100)\n",
      "Iter 17/40 - Loss: 0.091 (0.100)\n",
      "Iter 17/40 - Loss: -0.037 (0.100)\n",
      "Iter 17/40 - Loss: -0.160 (0.100)\n",
      "Iter 17/40 - Loss: -0.051 (0.100)\n",
      "Iter 17/40 - Loss: -0.248 (0.100)\n",
      "Iter 17/40 - Loss: -0.012 (0.100)\n",
      "Iter 17/40 - Loss: -0.018 (0.100)\n",
      "Iter 17/40 - Loss: -0.052 (0.100)\n",
      "Iter 17/40 - Loss: 0.092 (0.100)\n",
      "Iter 17/40 - Loss: -0.033 (0.100)\n",
      "Iter 17/40 - Loss: -0.021 (0.100)\n",
      "Iter 17/40 - Loss: 0.168 (0.100)\n",
      "Iter 18/40 - Loss: -0.082 (0.100)\n",
      "Iter 18/40 - Loss: 0.086 (0.100)\n",
      "Iter 18/40 - Loss: -0.046 (0.100)\n",
      "Iter 18/40 - Loss: -0.014 (0.100)\n",
      "Iter 18/40 - Loss: -0.081 (0.100)\n",
      "Iter 18/40 - Loss: -0.052 (0.100)\n",
      "Iter 18/40 - Loss: 0.090 (0.100)\n",
      "Iter 18/40 - Loss: 0.035 (0.100)\n",
      "Iter 18/40 - Loss: -0.158 (0.100)\n",
      "Iter 18/40 - Loss: -0.162 (0.100)\n",
      "Iter 18/40 - Loss: -0.112 (0.100)\n",
      "Iter 18/40 - Loss: 0.018 (0.100)\n",
      "Iter 18/40 - Loss: -0.068 (0.100)\n",
      "Iter 18/40 - Loss: -0.043 (0.100)\n",
      "Iter 18/40 - Loss: 0.050 (0.100)\n",
      "Iter 18/40 - Loss: -0.056 (0.100)\n",
      "Iter 19/40 - Loss: -0.037 (0.100)\n",
      "Iter 19/40 - Loss: -0.072 (0.100)\n",
      "Iter 19/40 - Loss: 0.031 (0.100)\n",
      "Iter 19/40 - Loss: 0.025 (0.100)\n",
      "Iter 19/40 - Loss: -0.192 (0.100)\n",
      "Iter 19/40 - Loss: -0.013 (0.100)\n",
      "Iter 19/40 - Loss: -0.107 (0.100)\n",
      "Iter 19/40 - Loss: 0.161 (0.100)\n",
      "Iter 19/40 - Loss: -0.026 (0.100)\n",
      "Iter 19/40 - Loss: -0.056 (0.100)\n",
      "Iter 19/40 - Loss: -0.049 (0.100)\n",
      "Iter 19/40 - Loss: -0.112 (0.100)\n",
      "Iter 19/40 - Loss: -0.095 (0.100)\n",
      "Iter 19/40 - Loss: -0.111 (0.100)\n",
      "Iter 19/40 - Loss: 0.008 (0.100)\n",
      "Iter 19/40 - Loss: -0.084 (0.100)\n",
      "Iter 20/40 - Loss: -0.163 (0.100)\n",
      "Iter 20/40 - Loss: 0.033 (0.100)\n",
      "Iter 20/40 - Loss: 0.065 (0.100)\n",
      "Iter 20/40 - Loss: -0.141 (0.100)\n",
      "Iter 20/40 - Loss: 0.011 (0.100)\n",
      "Iter 20/40 - Loss: 0.020 (0.100)\n",
      "Iter 20/40 - Loss: -0.067 (0.100)\n",
      "Iter 20/40 - Loss: -0.091 (0.100)\n",
      "Iter 20/40 - Loss: -0.090 (0.100)\n",
      "Iter 20/40 - Loss: -0.145 (0.100)\n",
      "Iter 20/40 - Loss: -0.095 (0.100)\n",
      "Iter 20/40 - Loss: -0.046 (0.100)\n",
      "Iter 20/40 - Loss: -0.131 (0.100)\n",
      "Iter 20/40 - Loss: 0.027 (0.100)\n",
      "Iter 20/40 - Loss: 0.043 (0.100)\n",
      "Iter 20/40 - Loss: -0.156 (0.100)\n",
      "Iter 21/40 - Loss: -0.069 (0.100)\n",
      "Iter 21/40 - Loss: 0.247 (0.100)\n",
      "Iter 21/40 - Loss: -0.102 (0.100)\n",
      "Iter 21/40 - Loss: -0.085 (0.100)\n",
      "Iter 21/40 - Loss: -0.097 (0.100)\n",
      "Iter 21/40 - Loss: -0.065 (0.100)\n",
      "Iter 21/40 - Loss: -0.086 (0.100)\n",
      "Iter 21/40 - Loss: -0.140 (0.100)\n",
      "Iter 21/40 - Loss: 0.060 (0.100)\n",
      "Iter 21/40 - Loss: 0.001 (0.100)\n",
      "Iter 21/40 - Loss: -0.076 (0.100)\n",
      "Iter 21/40 - Loss: -0.161 (0.100)\n",
      "Iter 21/40 - Loss: -0.122 (0.100)\n",
      "Iter 21/40 - Loss: -0.164 (0.100)\n",
      "Iter 21/40 - Loss: -0.036 (0.100)\n",
      "Iter 21/40 - Loss: -0.129 (0.100)\n",
      "Iter 22/40 - Loss: -0.117 (0.100)\n",
      "Iter 22/40 - Loss: -0.028 (0.100)\n",
      "Iter 22/40 - Loss: -0.146 (0.100)\n",
      "Iter 22/40 - Loss: 0.241 (0.100)\n",
      "Iter 22/40 - Loss: -0.194 (0.100)\n",
      "Iter 22/40 - Loss: -0.103 (0.100)\n",
      "Iter 22/40 - Loss: -0.165 (0.100)\n",
      "Iter 22/40 - Loss: 0.049 (0.100)\n",
      "Iter 22/40 - Loss: -0.029 (0.100)\n",
      "Iter 22/40 - Loss: -0.035 (0.100)\n",
      "Iter 22/40 - Loss: 0.004 (0.100)\n",
      "Iter 22/40 - Loss: 0.002 (0.100)\n",
      "Iter 22/40 - Loss: -0.118 (0.100)\n",
      "Iter 22/40 - Loss: 0.029 (0.100)\n",
      "Iter 22/40 - Loss: -0.026 (0.100)\n",
      "Iter 22/40 - Loss: -0.122 (0.100)\n",
      "Iter 23/40 - Loss: -0.014 (0.100)\n",
      "Iter 23/40 - Loss: -0.127 (0.100)\n",
      "Iter 23/40 - Loss: -0.105 (0.100)\n",
      "Iter 23/40 - Loss: 0.062 (0.100)\n",
      "Iter 23/40 - Loss: -0.055 (0.100)\n",
      "Iter 23/40 - Loss: -0.134 (0.100)\n",
      "Iter 23/40 - Loss: -0.095 (0.100)\n",
      "Iter 23/40 - Loss: -0.040 (0.100)\n",
      "Iter 23/40 - Loss: -0.097 (0.100)\n",
      "Iter 23/40 - Loss: -0.100 (0.100)\n",
      "Iter 23/40 - Loss: -0.039 (0.100)\n",
      "Iter 23/40 - Loss: -0.132 (0.100)\n",
      "Iter 23/40 - Loss: 0.059 (0.100)\n",
      "Iter 23/40 - Loss: 0.021 (0.100)\n",
      "Iter 23/40 - Loss: -0.052 (0.100)\n",
      "Iter 23/40 - Loss: -0.234 (0.100)\n",
      "Iter 24/40 - Loss: -0.114 (0.100)\n",
      "Iter 24/40 - Loss: 0.118 (0.100)\n",
      "Iter 24/40 - Loss: -0.012 (0.100)\n",
      "Iter 24/40 - Loss: 0.018 (0.100)\n",
      "Iter 24/40 - Loss: 0.045 (0.100)\n",
      "Iter 24/40 - Loss: -0.137 (0.100)\n",
      "Iter 24/40 - Loss: -0.178 (0.100)\n",
      "Iter 24/40 - Loss: -0.039 (0.100)\n",
      "Iter 24/40 - Loss: -0.151 (0.100)\n",
      "Iter 24/40 - Loss: -0.019 (0.100)\n",
      "Iter 24/40 - Loss: -0.178 (0.100)\n",
      "Iter 24/40 - Loss: -0.048 (0.100)\n",
      "Iter 24/40 - Loss: -0.039 (0.100)\n",
      "Iter 24/40 - Loss: -0.002 (0.100)\n",
      "Iter 24/40 - Loss: -0.237 (0.100)\n",
      "Iter 24/40 - Loss: -0.190 (0.100)\n",
      "Iter 25/40 - Loss: -0.048 (0.100)\n",
      "Iter 25/40 - Loss: -0.109 (0.100)\n",
      "Iter 25/40 - Loss: -0.211 (0.100)\n",
      "Iter 25/40 - Loss: 0.144 (0.100)\n",
      "Iter 25/40 - Loss: 0.126 (0.100)\n",
      "Iter 25/40 - Loss: -0.143 (0.100)\n",
      "Iter 25/40 - Loss: -0.063 (0.100)\n",
      "Iter 25/40 - Loss: -0.052 (0.100)\n",
      "Iter 25/40 - Loss: -0.201 (0.100)\n",
      "Iter 25/40 - Loss: -0.013 (0.100)\n",
      "Iter 25/40 - Loss: -0.066 (0.100)\n",
      "Iter 25/40 - Loss: -0.130 (0.100)\n",
      "Iter 25/40 - Loss: -0.024 (0.100)\n",
      "Iter 25/40 - Loss: -0.143 (0.100)\n",
      "Iter 25/40 - Loss: -0.043 (0.100)\n",
      "Iter 25/40 - Loss: -0.062 (0.100)\n",
      "Iter 26/40 - Loss: -0.060 (0.100)\n",
      "Iter 26/40 - Loss: -0.222 (0.100)\n",
      "Iter 26/40 - Loss: -0.052 (0.100)\n",
      "Iter 26/40 - Loss: -0.053 (0.100)\n",
      "Iter 26/40 - Loss: -0.290 (0.100)\n",
      "Iter 26/40 - Loss: -0.101 (0.100)\n",
      "Iter 26/40 - Loss: 0.041 (0.100)\n",
      "Iter 26/40 - Loss: 0.065 (0.100)\n",
      "Iter 26/40 - Loss: 0.050 (0.100)\n",
      "Iter 26/40 - Loss: -0.177 (0.100)\n",
      "Iter 26/40 - Loss: -0.063 (0.100)\n",
      "Iter 26/40 - Loss: 0.089 (0.100)\n",
      "Iter 26/40 - Loss: -0.084 (0.100)\n",
      "Iter 26/40 - Loss: 0.052 (0.100)\n",
      "Iter 26/40 - Loss: -0.160 (0.100)\n",
      "Iter 26/40 - Loss: -0.060 (0.100)\n",
      "Iter 27/40 - Loss: -0.149 (0.100)\n",
      "Iter 27/40 - Loss: -0.119 (0.100)\n",
      "Iter 27/40 - Loss: -0.081 (0.100)\n",
      "Iter 27/40 - Loss: -0.184 (0.100)\n",
      "Iter 27/40 - Loss: 0.005 (0.100)\n",
      "Iter 27/40 - Loss: -0.049 (0.100)\n",
      "Iter 27/40 - Loss: -0.155 (0.100)\n",
      "Iter 27/40 - Loss: -0.011 (0.100)\n",
      "Iter 27/40 - Loss: -0.103 (0.100)\n",
      "Iter 27/40 - Loss: 0.093 (0.100)\n",
      "Iter 27/40 - Loss: -0.196 (0.100)\n",
      "Iter 27/40 - Loss: -0.051 (0.100)\n",
      "Iter 27/40 - Loss: -0.128 (0.100)\n",
      "Iter 27/40 - Loss: -0.017 (0.100)\n",
      "Iter 27/40 - Loss: -0.070 (0.100)\n",
      "Iter 27/40 - Loss: 0.097 (0.100)\n",
      "Iter 28/40 - Loss: -0.094 (0.100)\n",
      "Iter 28/40 - Loss: -0.025 (0.100)\n",
      "Iter 28/40 - Loss: -0.043 (0.100)\n",
      "Iter 28/40 - Loss: 0.099 (0.100)\n",
      "Iter 28/40 - Loss: -0.117 (0.100)\n",
      "Iter 28/40 - Loss: -0.128 (0.100)\n",
      "Iter 28/40 - Loss: -0.216 (0.100)\n",
      "Iter 28/40 - Loss: 0.031 (0.100)\n",
      "Iter 28/40 - Loss: -0.100 (0.100)\n",
      "Iter 28/40 - Loss: 0.045 (0.100)\n",
      "Iter 28/40 - Loss: -0.010 (0.100)\n",
      "Iter 28/40 - Loss: -0.281 (0.100)\n",
      "Iter 28/40 - Loss: -0.020 (0.100)\n",
      "Iter 28/40 - Loss: -0.089 (0.100)\n",
      "Iter 28/40 - Loss: 0.091 (0.100)\n",
      "Iter 28/40 - Loss: -0.183 (0.100)\n",
      "Iter 29/40 - Loss: -0.161 (0.100)\n",
      "Iter 29/40 - Loss: -0.235 (0.100)\n",
      "Iter 29/40 - Loss: -0.127 (0.100)\n",
      "Iter 29/40 - Loss: 0.062 (0.100)\n",
      "Iter 29/40 - Loss: 0.103 (0.100)\n",
      "Iter 29/40 - Loss: 0.013 (0.100)\n",
      "Iter 29/40 - Loss: -0.108 (0.100)\n",
      "Iter 29/40 - Loss: -0.160 (0.100)\n",
      "Iter 29/40 - Loss: -0.110 (0.100)\n",
      "Iter 29/40 - Loss: -0.031 (0.100)\n",
      "Iter 29/40 - Loss: -0.078 (0.100)\n",
      "Iter 29/40 - Loss: -0.013 (0.100)\n",
      "Iter 29/40 - Loss: -0.032 (0.100)\n",
      "Iter 29/40 - Loss: 0.020 (0.100)\n",
      "Iter 29/40 - Loss: -0.163 (0.100)\n",
      "Iter 29/40 - Loss: -0.049 (0.100)\n",
      "Iter 30/40 - Loss: 0.034 (0.100)\n",
      "Iter 30/40 - Loss: -0.075 (0.100)\n",
      "Iter 30/40 - Loss: -0.112 (0.100)\n",
      "Iter 30/40 - Loss: -0.076 (0.100)\n",
      "Iter 30/40 - Loss: -0.006 (0.100)\n",
      "Iter 30/40 - Loss: -0.112 (0.100)\n",
      "Iter 30/40 - Loss: -0.021 (0.100)\n",
      "Iter 30/40 - Loss: -0.007 (0.100)\n",
      "Iter 30/40 - Loss: -0.104 (0.100)\n",
      "Iter 30/40 - Loss: -0.046 (0.100)\n",
      "Iter 30/40 - Loss: -0.156 (0.100)\n",
      "Iter 30/40 - Loss: -0.156 (0.100)\n",
      "Iter 30/40 - Loss: -0.089 (0.100)\n",
      "Iter 30/40 - Loss: -0.102 (0.100)\n",
      "Iter 30/40 - Loss: -0.012 (0.100)\n",
      "Iter 30/40 - Loss: 0.074 (0.100)\n",
      "Iter 31/40 - Loss: -0.062 (0.010)\n",
      "Iter 31/40 - Loss: -0.165 (0.010)\n",
      "Iter 31/40 - Loss: -0.183 (0.010)\n",
      "Iter 31/40 - Loss: -0.095 (0.010)\n",
      "Iter 31/40 - Loss: -0.007 (0.010)\n",
      "Iter 31/40 - Loss: -0.061 (0.010)\n",
      "Iter 31/40 - Loss: -0.125 (0.010)\n",
      "Iter 31/40 - Loss: -0.129 (0.010)\n",
      "Iter 31/40 - Loss: -0.126 (0.010)\n",
      "Iter 31/40 - Loss: -0.141 (0.010)\n",
      "Iter 31/40 - Loss: -0.086 (0.010)\n",
      "Iter 31/40 - Loss: -0.145 (0.010)\n",
      "Iter 31/40 - Loss: -0.225 (0.010)\n",
      "Iter 31/40 - Loss: 0.002 (0.010)\n",
      "Iter 31/40 - Loss: 0.002 (0.010)\n",
      "Iter 31/40 - Loss: -0.095 (0.010)\n",
      "Iter 32/40 - Loss: -0.154 (0.010)\n",
      "Iter 32/40 - Loss: -0.055 (0.010)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32/40 - Loss: -0.070 (0.010)\n",
      "Iter 32/40 - Loss: -0.106 (0.010)\n",
      "Iter 32/40 - Loss: -0.159 (0.010)\n",
      "Iter 32/40 - Loss: -0.092 (0.010)\n",
      "Iter 32/40 - Loss: -0.256 (0.010)\n",
      "Iter 32/40 - Loss: -0.141 (0.010)\n",
      "Iter 32/40 - Loss: -0.154 (0.010)\n",
      "Iter 32/40 - Loss: -0.068 (0.010)\n",
      "Iter 32/40 - Loss: -0.110 (0.010)\n",
      "Iter 32/40 - Loss: -0.013 (0.010)\n",
      "Iter 32/40 - Loss: -0.060 (0.010)\n",
      "Iter 32/40 - Loss: -0.169 (0.010)\n",
      "Iter 32/40 - Loss: -0.266 (0.010)\n",
      "Iter 32/40 - Loss: -0.225 (0.010)\n",
      "Iter 33/40 - Loss: -0.225 (0.010)\n",
      "Iter 33/40 - Loss: -0.159 (0.010)\n",
      "Iter 33/40 - Loss: -0.088 (0.010)\n",
      "Iter 33/40 - Loss: -0.047 (0.010)\n",
      "Iter 33/40 - Loss: -0.190 (0.010)\n",
      "Iter 33/40 - Loss: -0.139 (0.010)\n",
      "Iter 33/40 - Loss: -0.176 (0.010)\n",
      "Iter 33/40 - Loss: -0.059 (0.010)\n",
      "Iter 33/40 - Loss: -0.151 (0.010)\n",
      "Iter 33/40 - Loss: -0.162 (0.010)\n",
      "Iter 33/40 - Loss: -0.149 (0.010)\n",
      "Iter 33/40 - Loss: -0.121 (0.010)\n",
      "Iter 33/40 - Loss: -0.042 (0.010)\n",
      "Iter 33/40 - Loss: -0.225 (0.010)\n",
      "Iter 33/40 - Loss: -0.105 (0.010)\n",
      "Iter 33/40 - Loss: -0.025 (0.010)\n",
      "Iter 34/40 - Loss: -0.310 (0.010)\n",
      "Iter 34/40 - Loss: -0.065 (0.010)\n",
      "Iter 34/40 - Loss: -0.017 (0.010)\n",
      "Iter 34/40 - Loss: -0.149 (0.010)\n",
      "Iter 34/40 - Loss: -0.107 (0.010)\n",
      "Iter 34/40 - Loss: -0.167 (0.010)\n",
      "Iter 34/40 - Loss: -0.049 (0.010)\n",
      "Iter 34/40 - Loss: -0.070 (0.010)\n",
      "Iter 34/40 - Loss: -0.042 (0.010)\n",
      "Iter 34/40 - Loss: -0.015 (0.010)\n",
      "Iter 34/40 - Loss: -0.137 (0.010)\n",
      "Iter 34/40 - Loss: -0.201 (0.010)\n",
      "Iter 34/40 - Loss: -0.163 (0.010)\n",
      "Iter 34/40 - Loss: -0.197 (0.010)\n",
      "Iter 34/40 - Loss: -0.263 (0.010)\n",
      "Iter 34/40 - Loss: -0.198 (0.010)\n",
      "Iter 35/40 - Loss: -0.191 (0.010)\n",
      "Iter 35/40 - Loss: -0.057 (0.010)\n",
      "Iter 35/40 - Loss: -0.154 (0.010)\n",
      "Iter 35/40 - Loss: -0.156 (0.010)\n",
      "Iter 35/40 - Loss: -0.156 (0.010)\n",
      "Iter 35/40 - Loss: -0.067 (0.010)\n",
      "Iter 35/40 - Loss: -0.170 (0.010)\n",
      "Iter 35/40 - Loss: -0.176 (0.010)\n",
      "Iter 35/40 - Loss: -0.020 (0.010)\n",
      "Iter 35/40 - Loss: -0.161 (0.010)\n",
      "Iter 35/40 - Loss: -0.078 (0.010)\n",
      "Iter 35/40 - Loss: -0.147 (0.010)\n",
      "Iter 35/40 - Loss: -0.117 (0.010)\n",
      "Iter 35/40 - Loss: -0.068 (0.010)\n",
      "Iter 35/40 - Loss: -0.172 (0.010)\n",
      "Iter 35/40 - Loss: -0.279 (0.010)\n",
      "Iter 36/40 - Loss: 0.029 (0.010)\n",
      "Iter 36/40 - Loss: -0.166 (0.010)\n",
      "Iter 36/40 - Loss: -0.125 (0.010)\n",
      "Iter 36/40 - Loss: -0.164 (0.010)\n",
      "Iter 36/40 - Loss: -0.098 (0.010)\n",
      "Iter 36/40 - Loss: -0.140 (0.010)\n",
      "Iter 36/40 - Loss: -0.185 (0.010)\n",
      "Iter 36/40 - Loss: -0.287 (0.010)\n",
      "Iter 36/40 - Loss: -0.135 (0.010)\n",
      "Iter 36/40 - Loss: -0.289 (0.010)\n",
      "Iter 36/40 - Loss: -0.156 (0.010)\n",
      "Iter 36/40 - Loss: -0.056 (0.010)\n",
      "Iter 36/40 - Loss: -0.058 (0.010)\n",
      "Iter 36/40 - Loss: 0.032 (0.010)\n",
      "Iter 36/40 - Loss: -0.233 (0.010)\n",
      "Iter 36/40 - Loss: -0.146 (0.010)\n",
      "Iter 37/40 - Loss: -0.141 (0.010)\n",
      "Iter 37/40 - Loss: -0.265 (0.010)\n",
      "Iter 37/40 - Loss: -0.101 (0.010)\n",
      "Iter 37/40 - Loss: -0.116 (0.010)\n",
      "Iter 37/40 - Loss: -0.074 (0.010)\n",
      "Iter 37/40 - Loss: -0.171 (0.010)\n",
      "Iter 37/40 - Loss: -0.177 (0.010)\n",
      "Iter 37/40 - Loss: -0.213 (0.010)\n",
      "Iter 37/40 - Loss: -0.084 (0.010)\n",
      "Iter 37/40 - Loss: -0.123 (0.010)\n",
      "Iter 37/40 - Loss: -0.171 (0.010)\n",
      "Iter 37/40 - Loss: -0.056 (0.010)\n",
      "Iter 37/40 - Loss: 0.094 (0.010)\n",
      "Iter 37/40 - Loss: -0.265 (0.010)\n",
      "Iter 37/40 - Loss: -0.157 (0.010)\n",
      "Iter 37/40 - Loss: -0.199 (0.010)\n",
      "Iter 38/40 - Loss: -0.184 (0.010)\n",
      "Iter 38/40 - Loss: -0.260 (0.010)\n",
      "Iter 38/40 - Loss: -0.139 (0.010)\n",
      "Iter 38/40 - Loss: -0.138 (0.010)\n",
      "Iter 38/40 - Loss: -0.260 (0.010)\n",
      "Iter 38/40 - Loss: -0.126 (0.010)\n",
      "Iter 38/40 - Loss: -0.205 (0.010)\n",
      "Iter 38/40 - Loss: -0.024 (0.010)\n",
      "Iter 38/40 - Loss: 0.003 (0.010)\n",
      "Iter 38/40 - Loss: -0.168 (0.010)\n",
      "Iter 38/40 - Loss: -0.120 (0.010)\n",
      "Iter 38/40 - Loss: -0.017 (0.010)\n",
      "Iter 38/40 - Loss: -0.174 (0.010)\n",
      "Iter 38/40 - Loss: -0.221 (0.010)\n",
      "Iter 38/40 - Loss: -0.175 (0.010)\n",
      "Iter 38/40 - Loss: 0.102 (0.010)\n",
      "Iter 39/40 - Loss: -0.177 (0.010)\n",
      "Iter 39/40 - Loss: -0.083 (0.010)\n",
      "Iter 39/40 - Loss: -0.172 (0.010)\n",
      "Iter 39/40 - Loss: -0.167 (0.010)\n",
      "Iter 39/40 - Loss: -0.178 (0.010)\n",
      "Iter 39/40 - Loss: -0.166 (0.010)\n",
      "Iter 39/40 - Loss: -0.006 (0.010)\n",
      "Iter 39/40 - Loss: -0.260 (0.010)\n",
      "Iter 39/40 - Loss: -0.220 (0.010)\n",
      "Iter 39/40 - Loss: -0.080 (0.010)\n",
      "Iter 39/40 - Loss: -0.100 (0.010)\n",
      "Iter 39/40 - Loss: -0.158 (0.010)\n",
      "Iter 39/40 - Loss: -0.116 (0.010)\n",
      "Iter 39/40 - Loss: -0.114 (0.010)\n",
      "Iter 39/40 - Loss: 0.023 (0.010)\n",
      "Iter 39/40 - Loss: -0.269 (0.010)\n",
      "Iter 40/40 - Loss: -0.197 (0.010)\n",
      "Iter 40/40 - Loss: -0.109 (0.010)\n",
      "Iter 40/40 - Loss: -0.156 (0.010)\n",
      "Iter 40/40 - Loss: -0.131 (0.010)\n",
      "Iter 40/40 - Loss: -0.251 (0.010)\n",
      "Iter 40/40 - Loss: -0.083 (0.010)\n",
      "Iter 40/40 - Loss: -0.067 (0.010)\n",
      "Iter 40/40 - Loss: -0.023 (0.010)\n",
      "Iter 40/40 - Loss: -0.263 (0.010)\n",
      "Iter 40/40 - Loss: -0.150 (0.010)\n",
      "Iter 40/40 - Loss: 0.006 (0.010)\n",
      "Iter 40/40 - Loss: -0.128 (0.010)\n",
      "Iter 40/40 - Loss: -0.174 (0.010)\n",
      "Iter 40/40 - Loss: -0.212 (0.010)\n",
      "Iter 40/40 - Loss: -0.053 (0.010)\n",
      "Iter 40/40 - Loss: -0.250 (0.010)\n",
      "CPU times: user 1min 9s, sys: 828 ms, total: 1min 10s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 40 iterations of optimization\n",
    "n_iter = 40\n",
    "\n",
    "# We use SGD here, rather than Adam\n",
    "# Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# We use a Learning rate scheduler from PyTorch to lower the learning rate during optimization\n",
    "# We're going to drop the learning rate by 1/10 after 3/4 of training\n",
    "# This helps the model converge to a minimum\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.75 * n_iter], gamma=0.1)\n",
    "\n",
    "# Our loss object\n",
    "# We're using the VariationalMarginalLogLikelihood object\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=train_y.size(0))\n",
    "\n",
    "# The training loop\n",
    "def train():\n",
    "    for i in range(n_iter):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = torch.autograd.Variable(x_batch.float().cuda())\n",
    "            y_batch = torch.autograd.Variable(y_batch.float().cuda())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # We're going to use two context managers here\n",
    "            \n",
    "            # The use_toeplitz flag makes learning faster on the GPU\n",
    "            # See the DKL-MNIST notebook for an explanation\n",
    "            \n",
    "            # The diagonal_correction flag improves the approximations we're making for variational inference\n",
    "            # It makes running time a bit slower, but improves the optimization and predictions\n",
    "            with gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "                output = model(x_batch)\n",
    "                loss = -mll(output, y_batch)\n",
    "                print('Iter %d/%d - Loss: %.3f (%.3f)' % (i + 1, n_iter, loss.data[0], optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # The actual optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f81b0664fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADGCAYAAAAwqi48AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b9896f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "test_x = torch.autograd.Variable(torch.linspace(0, 1, 51)).cuda()\n",
    "with gpytorch.settings.max_cg_iterations(2000), gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "lower, upper = observed_pred.confidence_region()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), 'k*')\n",
    "ax.plot(test_x.data.cpu().numpy(), observed_pred.mean().data.cpu().numpy(), 'b')\n",
    "ax.fill_between(test_x.data.cpu().numpy(), lower.data.cpu().numpy(), upper.data.cpu().numpy(), alpha=0.5)\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
