{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# the train data points are spaced every 0.001 between 0 and 1 inclusive\n",
    "train_x = Variable(torch.linspace(0, 1, 1001)).cuda()\n",
    "# Use the sign function (-1 if value <0, 1 if value>0) to assign periodic labels to the data\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (8 * math.pi)))).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel, GridInterpolationKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood, BernoulliLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable\n",
    "gpytorch.functions.use_toeplitz = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model to classify, we use a GridInducingVariationalGP which exploits\n",
    "# grid structure (the x data points are linspace)\n",
    "# to get fast predictive distributions\n",
    "class GPClassificationModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPClassificationModel, self).__init__(grid_size=100, grid_bounds=[(0, 1)])\n",
    "        # Near-zero constant mean\n",
    "        self.mean_module = ConstantMean(constant_bounds=[-1e-5,1e-5])\n",
    "        # RBF kernel as universal approximator\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        # Register RBF lengthscale as hyperparameter\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-5,6))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Calc predictive mean (zero)\n",
    "        mean_x = self.mean_module(x)\n",
    "        # Calc predictive covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        # Make predictive distribution from predictive mean and covariance\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "# Initialize model\n",
    "model = GPClassificationModel().cuda()\n",
    "# Use Bernoulli Likelihood (warps via normal CDF to (0,1))\n",
    "likelihood = BernoulliLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 476.310   log_lengthscale: 0.000\n",
      "Iter 2/200 - Loss: 393.227   log_lengthscale: -0.100\n",
      "Iter 3/200 - Loss: 275.376   log_lengthscale: -0.181\n",
      "Iter 4/200 - Loss: 236.201   log_lengthscale: -0.270\n",
      "Iter 5/200 - Loss: 181.623   log_lengthscale: -0.353\n",
      "Iter 6/200 - Loss: 161.046   log_lengthscale: -0.430\n",
      "Iter 7/200 - Loss: 92.589   log_lengthscale: -0.500\n",
      "Iter 8/200 - Loss: 49.672   log_lengthscale: -0.571\n",
      "Iter 9/200 - Loss: 39.353   log_lengthscale: -0.639\n",
      "Iter 10/200 - Loss: 23.415   log_lengthscale: -0.707\n",
      "Iter 11/200 - Loss: 12.675   log_lengthscale: -0.767\n",
      "Iter 12/200 - Loss: 7.710   log_lengthscale: -0.824\n",
      "Iter 13/200 - Loss: 9.105   log_lengthscale: -0.882\n",
      "Iter 14/200 - Loss: 9.167   log_lengthscale: -0.937\n",
      "Iter 15/200 - Loss: 11.865   log_lengthscale: -0.994\n",
      "Iter 16/200 - Loss: 10.952   log_lengthscale: -1.048\n",
      "Iter 17/200 - Loss: 7.558   log_lengthscale: -1.104\n",
      "Iter 18/200 - Loss: 7.142   log_lengthscale: -1.153\n",
      "Iter 19/200 - Loss: 10.795   log_lengthscale: -1.200\n",
      "Iter 20/200 - Loss: 19.036   log_lengthscale: -1.242\n",
      "Iter 21/200 - Loss: 17.437   log_lengthscale: -1.286\n",
      "Iter 22/200 - Loss: 14.530   log_lengthscale: -1.334\n",
      "Iter 23/200 - Loss: 16.573   log_lengthscale: -1.384\n",
      "Iter 24/200 - Loss: 12.831   log_lengthscale: -1.437\n",
      "Iter 25/200 - Loss: 8.874   log_lengthscale: -1.492\n",
      "Iter 26/200 - Loss: 5.054   log_lengthscale: -1.544\n",
      "Iter 27/200 - Loss: 5.214   log_lengthscale: -1.596\n",
      "Iter 28/200 - Loss: 4.073   log_lengthscale: -1.648\n",
      "Iter 29/200 - Loss: 2.362   log_lengthscale: -1.697\n",
      "Iter 30/200 - Loss: 2.134   log_lengthscale: -1.745\n",
      "Iter 31/200 - Loss: 2.197   log_lengthscale: -1.788\n",
      "Iter 32/200 - Loss: 1.866   log_lengthscale: -1.830\n",
      "Iter 33/200 - Loss: 1.524   log_lengthscale: -1.873\n",
      "Iter 34/200 - Loss: 1.804   log_lengthscale: -1.915\n",
      "Iter 35/200 - Loss: 1.434   log_lengthscale: -1.956\n",
      "Iter 36/200 - Loss: 1.936   log_lengthscale: -1.998\n",
      "Iter 37/200 - Loss: 1.333   log_lengthscale: -2.037\n",
      "Iter 38/200 - Loss: 1.360   log_lengthscale: -2.077\n",
      "Iter 39/200 - Loss: 1.448   log_lengthscale: -2.116\n",
      "Iter 40/200 - Loss: 1.235   log_lengthscale: -2.151\n",
      "Iter 41/200 - Loss: 1.254   log_lengthscale: -2.186\n",
      "Iter 42/200 - Loss: 1.097   log_lengthscale: -2.219\n",
      "Iter 43/200 - Loss: 1.441   log_lengthscale: -2.250\n",
      "Iter 44/200 - Loss: 1.260   log_lengthscale: -2.277\n",
      "Iter 45/200 - Loss: 0.857   log_lengthscale: -2.302\n",
      "Iter 46/200 - Loss: 0.944   log_lengthscale: -2.325\n",
      "Iter 47/200 - Loss: 1.129   log_lengthscale: -2.349\n",
      "Iter 48/200 - Loss: 1.123   log_lengthscale: -2.370\n",
      "Iter 49/200 - Loss: 0.919   log_lengthscale: -2.391\n",
      "Iter 50/200 - Loss: 0.789   log_lengthscale: -2.411\n",
      "Iter 51/200 - Loss: 0.962   log_lengthscale: -2.428\n",
      "Iter 52/200 - Loss: 0.922   log_lengthscale: -2.445\n",
      "Iter 53/200 - Loss: 1.002   log_lengthscale: -2.459\n",
      "Iter 54/200 - Loss: 0.966   log_lengthscale: -2.473\n",
      "Iter 55/200 - Loss: 0.787   log_lengthscale: -2.485\n",
      "Iter 56/200 - Loss: 0.897   log_lengthscale: -2.498\n",
      "Iter 57/200 - Loss: 0.764   log_lengthscale: -2.509\n",
      "Iter 58/200 - Loss: 0.939   log_lengthscale: -2.520\n",
      "Iter 59/200 - Loss: 0.736   log_lengthscale: -2.530\n",
      "Iter 60/200 - Loss: 0.997   log_lengthscale: -2.539\n",
      "Iter 61/200 - Loss: 0.902   log_lengthscale: -2.547\n",
      "Iter 62/200 - Loss: 0.898   log_lengthscale: -2.553\n",
      "Iter 63/200 - Loss: 0.827   log_lengthscale: -2.561\n",
      "Iter 64/200 - Loss: 1.004   log_lengthscale: -2.568\n",
      "Iter 65/200 - Loss: 1.143   log_lengthscale: -2.574\n",
      "Iter 66/200 - Loss: 1.182   log_lengthscale: -2.580\n",
      "Iter 67/200 - Loss: 0.834   log_lengthscale: -2.584\n",
      "Iter 68/200 - Loss: 0.925   log_lengthscale: -2.590\n",
      "Iter 69/200 - Loss: 0.808   log_lengthscale: -2.594\n",
      "Iter 70/200 - Loss: 1.052   log_lengthscale: -2.597\n",
      "Iter 71/200 - Loss: 0.923   log_lengthscale: -2.602\n",
      "Iter 72/200 - Loss: 1.042   log_lengthscale: -2.607\n",
      "Iter 73/200 - Loss: 0.802   log_lengthscale: -2.613\n",
      "Iter 74/200 - Loss: 0.777   log_lengthscale: -2.621\n",
      "Iter 75/200 - Loss: 0.692   log_lengthscale: -2.630\n",
      "Iter 76/200 - Loss: 0.769   log_lengthscale: -2.639\n",
      "Iter 77/200 - Loss: 0.871   log_lengthscale: -2.649\n",
      "Iter 78/200 - Loss: 0.938   log_lengthscale: -2.657\n",
      "Iter 79/200 - Loss: 1.081   log_lengthscale: -2.664\n",
      "Iter 80/200 - Loss: 0.850   log_lengthscale: -2.672\n",
      "Iter 81/200 - Loss: 0.842   log_lengthscale: -2.679\n",
      "Iter 82/200 - Loss: 0.942   log_lengthscale: -2.687\n",
      "Iter 83/200 - Loss: 0.733   log_lengthscale: -2.693\n",
      "Iter 84/200 - Loss: 0.641   log_lengthscale: -2.699\n",
      "Iter 85/200 - Loss: 0.800   log_lengthscale: -2.705\n",
      "Iter 86/200 - Loss: 0.626   log_lengthscale: -2.710\n",
      "Iter 87/200 - Loss: 0.679   log_lengthscale: -2.716\n",
      "Iter 88/200 - Loss: 0.889   log_lengthscale: -2.721\n",
      "Iter 89/200 - Loss: 0.669   log_lengthscale: -2.726\n",
      "Iter 90/200 - Loss: 0.703   log_lengthscale: -2.730\n",
      "Iter 91/200 - Loss: 0.737   log_lengthscale: -2.734\n",
      "Iter 92/200 - Loss: 0.717   log_lengthscale: -2.739\n",
      "Iter 93/200 - Loss: 0.597   log_lengthscale: -2.742\n",
      "Iter 94/200 - Loss: 0.888   log_lengthscale: -2.746\n",
      "Iter 95/200 - Loss: 0.872   log_lengthscale: -2.749\n",
      "Iter 96/200 - Loss: 0.693   log_lengthscale: -2.752\n",
      "Iter 97/200 - Loss: 0.623   log_lengthscale: -2.754\n",
      "Iter 98/200 - Loss: 0.721   log_lengthscale: -2.757\n",
      "Iter 99/200 - Loss: 0.673   log_lengthscale: -2.760\n",
      "Iter 100/200 - Loss: 0.940   log_lengthscale: -2.762\n",
      "Iter 101/200 - Loss: 0.746   log_lengthscale: -2.764\n",
      "Iter 102/200 - Loss: 0.821   log_lengthscale: -2.766\n",
      "Iter 103/200 - Loss: 0.711   log_lengthscale: -2.769\n",
      "Iter 104/200 - Loss: 0.834   log_lengthscale: -2.771\n",
      "Iter 105/200 - Loss: 0.677   log_lengthscale: -2.773\n",
      "Iter 106/200 - Loss: 0.723   log_lengthscale: -2.776\n",
      "Iter 107/200 - Loss: 0.807   log_lengthscale: -2.779\n",
      "Iter 108/200 - Loss: 0.647   log_lengthscale: -2.781\n",
      "Iter 109/200 - Loss: 0.718   log_lengthscale: -2.783\n",
      "Iter 110/200 - Loss: 0.740   log_lengthscale: -2.784\n",
      "Iter 111/200 - Loss: 0.644   log_lengthscale: -2.786\n",
      "Iter 112/200 - Loss: 0.647   log_lengthscale: -2.787\n",
      "Iter 113/200 - Loss: 0.618   log_lengthscale: -2.789\n",
      "Iter 114/200 - Loss: 0.702   log_lengthscale: -2.791\n",
      "Iter 115/200 - Loss: 0.740   log_lengthscale: -2.792\n",
      "Iter 116/200 - Loss: 0.610   log_lengthscale: -2.795\n",
      "Iter 117/200 - Loss: 0.621   log_lengthscale: -2.797\n",
      "Iter 118/200 - Loss: 0.588   log_lengthscale: -2.799\n",
      "Iter 119/200 - Loss: 0.677   log_lengthscale: -2.801\n",
      "Iter 120/200 - Loss: 0.676   log_lengthscale: -2.804\n",
      "Iter 121/200 - Loss: 0.616   log_lengthscale: -2.807\n",
      "Iter 122/200 - Loss: 0.678   log_lengthscale: -2.810\n",
      "Iter 123/200 - Loss: 0.652   log_lengthscale: -2.813\n",
      "Iter 124/200 - Loss: 0.644   log_lengthscale: -2.816\n",
      "Iter 125/200 - Loss: 0.617   log_lengthscale: -2.818\n",
      "Iter 126/200 - Loss: 0.555   log_lengthscale: -2.821\n",
      "Iter 127/200 - Loss: 0.698   log_lengthscale: -2.823\n",
      "Iter 128/200 - Loss: 0.548   log_lengthscale: -2.825\n",
      "Iter 129/200 - Loss: 0.649   log_lengthscale: -2.827\n",
      "Iter 130/200 - Loss: 0.566   log_lengthscale: -2.829\n",
      "Iter 131/200 - Loss: 0.740   log_lengthscale: -2.831\n",
      "Iter 132/200 - Loss: 0.615   log_lengthscale: -2.833\n",
      "Iter 133/200 - Loss: 0.700   log_lengthscale: -2.835\n",
      "Iter 134/200 - Loss: 0.616   log_lengthscale: -2.837\n",
      "Iter 135/200 - Loss: 0.579   log_lengthscale: -2.839\n",
      "Iter 136/200 - Loss: 0.579   log_lengthscale: -2.841\n",
      "Iter 137/200 - Loss: 0.581   log_lengthscale: -2.843\n",
      "Iter 138/200 - Loss: 0.705   log_lengthscale: -2.844\n",
      "Iter 139/200 - Loss: 0.585   log_lengthscale: -2.845\n",
      "Iter 140/200 - Loss: 0.591   log_lengthscale: -2.845\n",
      "Iter 141/200 - Loss: 0.666   log_lengthscale: -2.846\n",
      "Iter 142/200 - Loss: 0.552   log_lengthscale: -2.846\n",
      "Iter 143/200 - Loss: 0.640   log_lengthscale: -2.846\n",
      "Iter 144/200 - Loss: 0.590   log_lengthscale: -2.846\n",
      "Iter 145/200 - Loss: 0.622   log_lengthscale: -2.845\n",
      "Iter 146/200 - Loss: 0.573   log_lengthscale: -2.844\n",
      "Iter 147/200 - Loss: 0.584   log_lengthscale: -2.845\n",
      "Iter 148/200 - Loss: 0.535   log_lengthscale: -2.845\n",
      "Iter 149/200 - Loss: 0.582   log_lengthscale: -2.845\n",
      "Iter 150/200 - Loss: 0.595   log_lengthscale: -2.845\n",
      "Iter 151/200 - Loss: 0.540   log_lengthscale: -2.847\n",
      "Iter 152/200 - Loss: 0.554   log_lengthscale: -2.848\n",
      "Iter 153/200 - Loss: 0.569   log_lengthscale: -2.850\n",
      "Iter 154/200 - Loss: 0.586   log_lengthscale: -2.851\n",
      "Iter 155/200 - Loss: 0.572   log_lengthscale: -2.853\n",
      "Iter 156/200 - Loss: 0.540   log_lengthscale: -2.854\n",
      "Iter 157/200 - Loss: 0.623   log_lengthscale: -2.856\n",
      "Iter 158/200 - Loss: 0.568   log_lengthscale: -2.859\n",
      "Iter 159/200 - Loss: 0.620   log_lengthscale: -2.862\n",
      "Iter 160/200 - Loss: 0.539   log_lengthscale: -2.865\n",
      "Iter 161/200 - Loss: 0.547   log_lengthscale: -2.868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 162/200 - Loss: 0.592   log_lengthscale: -2.871\n",
      "Iter 163/200 - Loss: 0.517   log_lengthscale: -2.873\n",
      "Iter 164/200 - Loss: 0.498   log_lengthscale: -2.876\n",
      "Iter 165/200 - Loss: 0.614   log_lengthscale: -2.878\n",
      "Iter 166/200 - Loss: 0.609   log_lengthscale: -2.881\n",
      "Iter 167/200 - Loss: 0.555   log_lengthscale: -2.883\n",
      "Iter 168/200 - Loss: 0.561   log_lengthscale: -2.885\n",
      "Iter 169/200 - Loss: 0.507   log_lengthscale: -2.887\n",
      "Iter 170/200 - Loss: 0.504   log_lengthscale: -2.889\n",
      "Iter 171/200 - Loss: 0.561   log_lengthscale: -2.890\n",
      "Iter 172/200 - Loss: 0.560   log_lengthscale: -2.892\n",
      "Iter 173/200 - Loss: 0.633   log_lengthscale: -2.893\n",
      "Iter 174/200 - Loss: 0.567   log_lengthscale: -2.895\n",
      "Iter 175/200 - Loss: 0.506   log_lengthscale: -2.896\n",
      "Iter 176/200 - Loss: 0.490   log_lengthscale: -2.897\n",
      "Iter 177/200 - Loss: 0.579   log_lengthscale: -2.898\n",
      "Iter 178/200 - Loss: 0.693   log_lengthscale: -2.899\n",
      "Iter 179/200 - Loss: 0.571   log_lengthscale: -2.900\n",
      "Iter 180/200 - Loss: 0.727   log_lengthscale: -2.904\n",
      "Iter 181/200 - Loss: 0.522   log_lengthscale: -2.906\n",
      "Iter 182/200 - Loss: 0.537   log_lengthscale: -2.909\n",
      "Iter 183/200 - Loss: 0.501   log_lengthscale: -2.911\n",
      "Iter 184/200 - Loss: 0.531   log_lengthscale: -2.914\n",
      "Iter 185/200 - Loss: 0.920   log_lengthscale: -2.917\n",
      "Iter 186/200 - Loss: 0.763   log_lengthscale: -2.921\n",
      "Iter 187/200 - Loss: 0.901   log_lengthscale: -2.926\n",
      "Iter 188/200 - Loss: 0.848   log_lengthscale: -2.931\n",
      "Iter 189/200 - Loss: 0.590   log_lengthscale: -2.935\n",
      "Iter 190/200 - Loss: 0.495   log_lengthscale: -2.939\n",
      "Iter 191/200 - Loss: 0.507   log_lengthscale: -2.942\n",
      "Iter 192/200 - Loss: 0.635   log_lengthscale: -2.945\n",
      "Iter 193/200 - Loss: 1.027   log_lengthscale: -2.951\n",
      "Iter 194/200 - Loss: 0.963   log_lengthscale: -2.954\n",
      "Iter 195/200 - Loss: 1.248   log_lengthscale: -2.956\n",
      "Iter 196/200 - Loss: 0.623   log_lengthscale: -2.958\n",
      "Iter 197/200 - Loss: 0.525   log_lengthscale: -2.961\n",
      "Iter 198/200 - Loss: 0.484   log_lengthscale: -2.963\n",
      "Iter 199/200 - Loss: 0.609   log_lengthscale: -2.966\n",
      "Iter 200/200 - Loss: 0.612   log_lengthscale: -2.967\n",
      "CPU times: user 14.1 s, sys: 188 ms, total: 14.3 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "def train():\n",
    "    # Use adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    optimizer.n_iter = 0\n",
    "    total_iter = 200\n",
    "    for i in range(total_iter):\n",
    "        # Zero gradients out for new iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calculate loss\n",
    "        loss = -model.marginal_log_likelihood(likelihood, output, train_y)\n",
    "        # Calc gradients\n",
    "        loss.backward()\n",
    "        optimizer.n_iter += 1\n",
    "        print('Iter %d/%d - Loss: %.3f   log_lengthscale: %.3f' % (\n",
    "            i + 1, total_iter, loss.data[0],\n",
    "            model.covar_module.base_kernel_module.log_lengthscale.data.squeeze()[0],\n",
    "        ))\n",
    "        optimizer.step()\n",
    "# Get time spent\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADSCAYAAACo7W6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnXmYFdWZ/z9vN2A3CIIEI9CyKLI1\nNIst4gOK4oIitpFIFNFoQCOdISMzmlHcgo7G/MYMJmPMz8EZFY3SKMZoIkZQJCpIFGwgLNoiEmlZ\nRBYF2g5Lv/PHqdtW9+3l9r11l668n+ep51bVOfc971vLt845tRxRVQzDMPxkpdsBwzAyDxMGwzCi\nMGEwDCMKEwbDMKIwYTAMIwoTBsMwoviHFQYRmSkiv023H01BRK4Vkbebm+1Gyr1BRH4ZQ77bROR/\nvPkeIqIi0iKO8paIyHXe/CQRWehLUxHp1VSbcfjwhIjc680XiMiyZJfZVEIrDN6B/lcRqRCR7SLy\n/0Wkfbr9SgYikiMie0VkdB1pD4rI/HT41Rgi0gq4A3jAW673hFfVn6nqdUGWr6pPq+r5QdqMw4c1\nwF4RuTidftQmlMIgIjcB/w/4CXAMMBzoDizyDsZU+dHkK1o8qGolMA/4fq3ys4GJwJxU+BEHlwAf\nqOpn6XYkzTwN3JBuJ/yEThhEpB1wN/BjVf2Tqh5S1c3A93DicJUve46IzBORfSLyvogM8tm5RUQ+\n89I+FJFzvPVZInKriHwsIrtE5FkROdZLi1zxpojIp8BiEfmTiEyr5eNqERnvzfcVkUUistsr53u+\nfB1F5CUR+UpE3gVOaiD0OcB3RaS1b90Y3D5+xbMX8XufiKwXkUvr2YZRV25/FdxbniwiG0Rkj4i8\nKiLdvfXi1VI+F5EvRWSNiAyox+cLgT83EJPfp3qbfiLyXRHZHClHRIaLyDKvFrVaRM6q5391NZ/O\nFZGPvLgeFhHx8maJyB0i8jcvtidF5BifrSIRWeeVuURE+vnShnjH1z4RmQfk1CpzCXCOiBwVy7ZI\nCaoaqgm4ADgMtKgjbQ4w15ufCRwCLgNaAjcDn3jzfYAtQBcvbw/gJG9+OrAcyAOOAv7bZ7MHoMCT\nQBsgF3cVX+rzoT+w1/tvG6+cHwAtgKHAF0C+l7cEeNbLNwD4DHi7gdjLgKt8y3OBX/qWJwBdcGJx\nOXAA6OylXRux7Yujhe+/S4DrvPnvABuBfp7fdwDLvLQxwEqgPSBens71+PseMMG3HFWuL20m8Nva\n+bxttxHo5aV1BXYBY704z/OWO9URR3XM3rICf/R87wbsBC7w0iZ75ZwIHA38DnjKS+vtbcvzcMfP\nv3l5W3nT34B/8dIuwx1399aK7yugIN3nT7U/6XYg8IBcjWB7PWk/Bxb5DrTlvrQsYBtwBtAL+Bw4\nF2hZy8YG4BzfcmdvR7fwHbAn+tLbegdNd2/5PuAxb/5y4K1a9v8b+CmQ7dnt60v7GQ0Lwx3AQm++\nHVABDGkg/yrgEm+++iSp6wStdUK9Akypte0qcDWy0TiBGg5kNbKvPoqcePWV60ubSbQw3AysB/J8\n+W6JnLC+da8C19QRR3XM3rICI33LzwK3evOvAz/ypfXx7fc7gWdrbY/PgLOAM4GtgPjSlxEtDJ8B\nZ6b7/IlMoWtK4K6436qnfd/ZS4+wJTKjqlVAOa6WsBFXM5gJfC4iJSLSxcvaHXjBqzLuxQnFEeDb\n9djdB7wMXOGtugLXpozYOi1iy7M3CTge6IQ76Kpt4a48DfEkcLaIdMVdmTaqamkkUUS+LyKrfGUN\nAL7ViM266A78ymdnN6520FVVFwO/Bh4GdojIbK95Vxd7cMIZLz8BHlbV8lq+Tai1TUfi9n0sbPfN\nV+BqB+BqWv7t/zfc/vl27TTvWNqCq710AT5T7+z3/bc2bXE1yYwgjMLwDvB3YLx/pYi0wbVpX/et\nPsGXnoVrHmwFUNVnVHUk7kBTXGcmuB1+oaq29005WrMDrfYrq3OBiSJyOq558YbP1p9r2TpaVYtx\n1djDfh9x1dt6UdVPgbdw4nI1Tigi8XUHHgWmAR1VtT2wFndC1+aA9+vvrzjeN78FuKGW37mquszz\n479U9RQgH1fN/kk9Lq/x0uPlfOAOEfluLd+equVbG1X9eQLlgDsuuvuWu+H2z47aaV6/xAm4WsA2\noGukr8L3X3z5u+CaHB8m6GNghE4YVPVLXOfjQyJygYi0FJEewHO4GsFTvuyniMh4r3YxHScoy0Wk\nj4iM9jqDKoGvcbUCgEeA+3ydbZ1E5JJG3FqAO3DuAeZ5VxRw7dneInK152dLETlVRPqp6hFcO3am\niLQWkf7ANTFsgjm4k38E39RMwPVTKE5wEJEf4GoMUajqTtxBfZWIZIvIZGp2fD4CzBCRfM/WMSIy\nwZs/VUROE5GWOIGp5JttV9d2GVXH+qPE3YKNTPUdp+twfUoPi0iRt+63wMUiMsbzPUdEzhKRvHps\nxMpc4F9EpKeIHI1r1s1T1cO4JsdFInKOF/dNuGNpGe5CdRj4ZxFpIa7TeVgt22cBi1X17wn6GBih\nEwYAVf0P4DbgF7hOnb/griTn1Nr4L+La+XtwV9jxqnoI1zH4c1yzYztwnGcP4FfAS8BCEdmH64g8\nrRF//o47yc8FnvGt34e76l2Bu+psx9VMIr3T03BV2e3AE8DjMYQ/H+gAvK6q23xlrQf+E3eg7gAG\nAksbsHM97kq/C3flr34IR1Vf8PwsEZGvcDWPC73kdriayR5clXkXbj/UxR+Avr5mWoT9ODGOTFHP\nZ/h8WQ2MAx4VkQtVdQvuNuhtOBHc4sWR6LH+GO6i8iauk7oS+LHnw4e4vq2HcMfMxcDFqnpQVQ/i\naq/X4rbJ5bhjwc8knNhmDFKz6WMYqUVEfgj0V9Xp6fYlHYjIQGC2qp6ebl/8mDAYhhFFwk0Jrw33\nrvcgyToRuTsIxwzDSB8J1xi83tY2qrrf63h5G7hRVZcH4aBhGKkn4Wf5vfuz+73Flt5k7RPDaMYE\nclfCuy20Cve04CJV/UsQdg3DSA+BvP3n3XMfLO615hdEZICqrvXn8XqffwjQpk2bU/r27RtE0YZh\nNIGVK1d+oaqdGssX+F0JEfkpcEBV67t3TWFhoa5YsSLQcg3DaBwRWamqhY3lC+KuRCevpoCI5OIe\n4vkgUbuGYaSPIJoSnYE54j4KkoV7y+yPAdg1DCNNBHFXYg0wJABfDMPIEFLy6TGj+XLo0CHKy8up\nrKxMtytGE8jJySEvL4+WLVvG9X8TBqNBysvLadu2LT169KDmm8NGpqKq7Nq1i/Lycnr27BmXjVC+\nXWkER2VlJR07djRRaEaICB07dkyolmfCYDSKiULzI9F9ZsJgZDzl5eVccsklnHzyyZx00knceOON\nHDx4EIAnnniCadOmNWIh9Rx99NF1rs/Ozmbw4MHk5+czaNAgZs2aRVVVVZ15I2zevJlnnnmmwTxB\nY8JgBM62bdsYNWoU27dvbzxzI6gq48eP5zvf+Q4fffQRZWVl7N+/n9tvvz0AT+vm8OHDSbOdm5vL\nqlWrWLduHYsWLWLBggXcfXfDLySnQxjS8gXaU045RY3mwfr165v8n+LiYs3KytLi4uKEy3/ttdf0\njDPOqLHuyy+/1GOPPVYPHDigjz/+uBYVFemYMWO0d+/eOnPmTFVV3b9/v44dO1YLCgo0Pz9fS0pK\nVFV1xYoVeuaZZ+rQoUP1/PPP161bt6qq6qhRo3TGjBl65pln6syZM7V79+565MgRVVU9cOCA5uXl\n6cGDB3Xjxo06ZswYHTp0qI4cOVI3bNigqqqbNm3S4cOHa2Fhod5xxx3apk2bOuOpvf7jjz/WY489\nVquqqvSTTz7RkSNH6pAhQ3TIkCG6dOlSVVU97bTTtF27djpo0CCdNWtWvflqU9e+A1ZoDOeoCYPR\nIE0RhpycHMW9WVtjysnJibv8X/3qVzp9+vSo9YMHD9bVq1fr448/rscff7x+8cUXWlFRofn5+fre\ne+/p/Pnz9brrrqvOv3fvXj148KCefvrp+vnnn6uqaklJif7gBz9QVScMfiErKirSxYsXV+ebMmWK\nqqqOHj1ay8rKVFV1+fLlevbZZ6uq6sUXX6xz5sxRVdVf//rXMQuDqmr79u11+/bteuDAAf36669V\nVbWsrEwj58kbb7yhF110UXX++vLVJhFhsKaEERibNm3iyiuvpHVr93Hp1q1bM2nSJD755JO4bapq\nnR1p/vXnnXceHTt2JDc3l/Hjx/P2228zcOBAXnvtNW655RbeeustjjnmGD788EPWrl3Leeedx+DB\ng7n33nspL//my/OXX355jfl58+YBUFJSwuWXX87+/ftZtmwZEyZMYPDgwdxwww1s2+Y+q7l06VIm\nTpwIwNVXX93kGME9M3L99dczcOBAJkyYwPr16+vMH2u+RLDnGIzA6Ny5M+3ataOyspKcnBwqKytp\n164dxx9/fON/rof8/Hyef/75Guu++uortmzZwkknncTKlSujhENE6N27NytXrmTBggXMmDGD888/\nn0svvZT8/HzeeeedOstq06ZN9XxRUREzZsxg9+7drFy5ktGjR3PgwAHat2/PqlWr6vx/PHcCNm3a\nRHZ2Nscddxx333033/72t1m9ejVVVVXk5NQeyc7x4IMPxpQvEazGYATKjh07mDp1KsuXL2fq1KkJ\nd0Cec845VFRU8OSTboiMI0eOcNNNN3HttddW10wWLVrE7t27+frrr/n973/PiBEj2Lp1K61bt+aq\nq67i5ptv5v3336dPnz7s3LmzWhgOHTrEunXr6iz36KOPZtiwYdx4442MGzeO7Oxs2rVrR8+ePXnu\nuecAd6VfvXo1ACNGjKCkpASAp59+uk6btdm5cydTp05l2rRpiAhffvklnTt3Jisri6eeeoojR9xX\n99u2bcu+ffuq/1dfvkCJpb0R9GR9DM2HeDofg+bTTz/VcePGaa9evfTEE0/UadOmaWVlpaqqPv74\n4zphwgQdO3Zsjc7HP/3pTzpw4EAdNGiQFhYW6nvvvaeqqqWlpXrGGWdoQUGB9u/fX2fPnq2qro8h\nkifCc889p4AuWbKket2mTZt0zJgxWlBQoP369dO77767en2k8/H++++vt48hKytLBw0apP3799eC\nggJ94IEHqjs5y8rKdODAgXraaafprbfeWm3j4MGDOnr0aC0oKNBZs2bVm682ifQxpOUr0fY9hubD\nhg0b6NevX+MZjYyjrn2Xsu8xGIYRPkwYDMOIwoTBMIwoTBgMw4jChMEwjCiC+BjsCSLyhohs8Iao\nuzEIxwzDSB9B1BgOAzepaj9gOPBPItI/ALuGAbgnCv2PGR8+fJhOnToxbty4NHoVbhIWBlXdpqrv\ne/P7gA1A10TtGkaENm3asHbtWr7++mvAPenYtasdYskk0D4GEemB+2K0DVFnBMqFF17Iyy+/DMDc\nuXOrX1gCOHDgAJMnT+bUU09lyJAhvPjii4D7jsEZZ5zB0KFDGTp0KMuWLQNgyZIlnHXWWVx22WX0\n7duXSZMmkY4H/TKZwF6iEpGjgeeB6ar6VR3p1UPUdevWLahijRQyfTrU8/5Q3AweDL/8ZeP5rrji\nCu655x7GjRvHmjVrmDx5Mm+99RYA9913H6NHj+axxx5j7969DBs2jHPPPZfjjjuORYsWkZOTw0cf\nfcTEiROJPHFbWlrKunXr6NKlCyNGjGDp0qWMHDky2OCaMYEIg4i0xInC06r6u7ryqOpsYDa4R6KD\nKNf4x6GgoIDNmzczd+5cxo4dWyNt4cKFvPTSS/ziF25UxMrKSj799FO6dOnCtGnTWLVqFdnZ2ZSV\nlVX/Z9iwYeTl5QEwePBgNm/ebMLgI2FhEPeu6f8CG1R1VuIuGZlKLFf2ZFJUVMTNN9/MkiVL2LVr\nV/V6VeX555+nT58+NfLPnDmz3teTjzrqqOr57OzspH7OrTkSRB/DCOBqYLSIrPKmsY39yTCayuTJ\nk7nrrrsYOHBgjfVjxozhoYcequ4nKC0tBVL0enJICeKuxNuqKqpaoKqDvWlBEM4Zhp+8vDxuvDH6\nMZk777yTQ4cOUVBQwIABA7jzzjsB+NGPfsScOXMYPnw4ZWVlNT7EYjSMvXZtNIi9dt18sdeuDcMI\nFBMGwzCiMGEwDCMKEwajUeypwOZHovvMhMFokJycHHbt2mXi0IxQVXbt2pXQZ+VtXAmjQfLy8igv\nL2fnzp3pdsVoAjk5OdVPdsaDCYPRIC1btqRnz57pdsNIMdaUMAwjChMGwzCiMGEwDCMKEwbDMKIw\nYTAMIwoTBsMwojBhMAwjChMGwzCiMGEwDCMKEwbDMKIwYTAMI4pAhEFEHhORz0VkbRD2AFatWkVO\nTg4i0qSpd+/ebN++PSg3AvMrnunRRx9NWhzgRnRKRRwiwvz585MaS0lJSUriyM3NZc2aNUmLY9u2\nbfTq1avJfmVlZbF48eLgHFHVhCfgTGAosDaW/Keccoo2Rn5+vgJxTcXFxY3aj5dE/GrqlJWVlbQ4\nVFU7dOiQslhatWqV1FhatWqVsljy8/OTFkdxcXHcfnXo0KFR+8AKjeEcDexjsN7wdH9U1QGN5W3o\nY7BumAqA6bgv08fCXuDHQGWNtTk5OdXjHSaK8+s44BdAbiA2G+Z94P7qpaD2E/i38U+BRndXABwA\n/hXYDSQrlvOB6wOzWz9fAv8MVADBxZKbm0tlZWUdKdOAUTFYWI/bnzToV6wfg03Za9exDlFXWlrK\n2Wefzd69XYG+MVhuB3QDfgOUVq8dNmxY9RiGQVBaWsqIEbOoqLga2EhtEQqWzsAYIsIwe/bsQK0v\nXLiQiy66iEOH7sSdrDsCtV+T1sCJQAkir/Lss88Gav2bcSyvBYqAsob/kBBtge7AbDp2/DjQqvum\nTZsoKioi+oJ5M3AMUN6Ihf0AtGrVildeeSVxh2KpVsQyAT0IqCnRv3//JlShLlZQhaFJb07k5U33\nyhqQ5OrqAwr7ktqccM2IIwr3JDmWU71tdmHSmhOuGTFX4YMkxzLGi+W0pDQnpk6dWkeZf1P438Ca\nE8TYlMjIuxJ79uxpQu5IlUlqrE1GB+T+/QdqlZkslEg8VVVVSSmhoqIC1/ecilgAJGnDwDm7Qipj\n2b17d+DWd+yoq+bWtLjcfg2AWNQjlokAawxN4Q9/UAXVd98NzGS9zJvnylq7Nrnl/OQnqrm5yS1D\n1cVy113JLeO991w5f/hDcsuZMEG1b9/klvHqqy6WpUuTW46frl1Vp0wJzh6prDGIyFzgHaCPiJSL\nyJQg7MZWtvtNxbdKI2WINJwvUUSSH08qY/GXlyxUwxOLn1TEVReBdD6q6sQg7MRDGIXBX1ay7afq\noAuDMPjLShWpLMtPRvYxNIUwCoPVGJqO1RiCpdkLQ4RUCkOySaUwJJtUCkOyMWFoRliNIT6sxtB0\nTBiaESYM8WHC0HRMGJoRJgzxYcLQdEwYmhEmDPFhwtB0TBiaESYM8WHC0HRMGJoRYRWGZGPC0HRM\nGJoRYRaGZMYUplgi9sMSix8ThjgxYYiPMMUSsR+WWPyYMMSJCUN8hCmWiP2wxOLHhCFOTBjiI0yx\nROyHJRY/Jgxxko6NFobOx1SVFcZYTBiaAVZjiI8wxRKxH5ZY/JgwxIkJQ3yEKZaI/bDE4seEIU5M\nGOIjTLFE7IclFj8mDHFiwhAfYYolYj8ssfgxYYgTE4b4CFMsEfthicVPsxYGEblARD4UkY0icmsQ\nNmMv2/2aMDSNMMUSsR+WWPw0W2EQkWzgYeBCoD8wUUT6J2o39vLdb5iEoXZ5ybQdhlgi9sMSS7rK\n8hNEjWEYsFFVN6nqQaAEuCQAuzERRmGwGkPTsRpDsAQhDF2BLb7lcm9dDUTkhyKyQkRW7Ny5M4Bi\nI3bdrwlD0whTLBH7YYnFT3MWhrrcjtp0qjpbVQtVtbBTp04BFOsVbsIQF2GKJWI/LLH4ac7CUA6c\n4FvOA7YGYDcmTBjiI0yxROyHJRY/zVkY3gNOFpGeItIKuAJ4KQC7MWHCEB9hiiViP0zvfaS73IRH\nolLVwyIyDXgVyAYeU9V1CXsWIyYM8RGmWCL2wxKLn3TVGIIaom4BsCAIW03FhCE+whRLxH5YYqmv\n3FRiTz42gTCdTGGKJWI/LLFESPWzJn5MGJpAmE6mMMUSsR+WWCKYMCSACUN8hCmWiP2wxBLBhCEB\nTBjiI0yxROyHJZYIJgwJYMIQH2GKJWI/LLFEMGFIABOG+AhTLBH7YYklgglDAqRjo4XpQRqLpen2\nTRiaEamsMSSbVNYYkk0qawzJxoShGWFNifgIUywR+2GJJYIJQwKYMMRHmGKJ2A9LLBFMGBLAhCE+\nwhRLxH5YYolgwpAAJgzxEaZYIvbDEksEE4YEMGGIjzDFErEfllgimDAkQBiFoXZ5ybQdhlgi9sMS\nS6rLqQsThiYQpqtsmGKJ2A9LLBGsxpAAJgzxEaZYIvbDEksEE4YESIcwJBsThqZjwhAsJgxNIFXt\nWBOGpmPCECwJCYOITBCRdSJSJSKFQTnVNB/cryqUlJQgIkmb7r33XlSPkJWVxeLFi5MeU69evZIW\nS0FBAQBXXjmRNWvWJC2W7du3AXDbbbcldd988MEHfPLJx0mLA+DNN/8MQHFxcVJjERHmz5/ffIUB\nWAuMB94MwJe48AvDNddck+zSAEVVueyyy5JXSvWBkMwjwtk+dOjvXHnllUkr5f77f1ajvOQhrFix\nIqklTJ16Q3VZyWbSpElpFYaEPgarqhsAJB2ee0SKvv76h4ELklxab/DG0tmzZ0913Bpg3dLZvBJ4\nGjgXNxxoMujp/Srr1q0LPJbc3FwqKytxh9hDuDiKArFdN22BqiTuE4DjvN9BJDcWOHgQOnW6Gngq\nLcKAqiY8AUuAwkby/BBYAazo1q2bBsXevarZ2UfU1RlSMe1QnDpoq1at9PXXXw8sFlXV3/zmNwoX\npDCeUQpox44ddfXq1YHFsXXrVi0sLFQQhb0piuVhBXT27NmBxaGqunDhQm3ZsqVCG4XKFO4bNz31\nVHCxACs0hnNaXN76EZHXgOPrSLpdVV/08iwBblbVmOpyhYWFGmS1b8sWOPHEYRw+fCgwm/WzFfgc\ngA4dOrB79+7AS8jKyka1N5ATuO2aVABlAOTn57N27dpArRcXF/PII4/gDp+6DqGgWU9W1mGOHDkS\nuOVjjz2WPXv24IZlDW6IxYZo0aIlpaXvkp8fXHNCRFaqaqP9gY02JVT13GBcSh4nnABVVSuBqpSW\nW1FRkRS7qlXAB0mxXR/JELgdO3Z4c9u9KflUJekQ+GZff+ZNyaeqKosBA1JSVBSBDDiTCSTjKpEu\nGqvFNRd+97vfpduFwHD9Jf84JHq78lIRKQdOB14WkVeDccswjHSS6F2JF4AXAvLFMIwModk/+WgY\nRvCYMBiGEYUJg2EYUZgwGIYRhQmDYRhRmDAYhhGFCYNhGFGYMBiGEYUJg2EYUZgwGIYRhQmDYRhR\nmDAYhhGFCYNhGFGYMBiGEYUJg2EYUZgwGIYRhQmDYRhRmDAYhhFFot98fEBEPhCRNSLygoi0D8ox\nwzDSR6I1hkXAAFUtwA1QMCNxlwzDSDcJCYOqLlTVw97iciAvcZcMw0g3QfYxTAZeCdCeYRhpotHP\nx8c4RN3twGHcSKz12fkhbvxKunXrFpezhmGkhoSHqBORa4BxwDnawBBKqjobmA1u7Mom+mkYRgpJ\naMAZEbkAuAUYparJGcjRMIyUk2gfw6+BtsAiEVklIo8E4JNhGGkm0SHqegXliGEYmYM9+WgYRhQm\nDIZhRGHCYBhGFCYMhmFEYcJgGEYUJgyGYURhwmAYRhQmDIZhRGHCYBhGFCYMhmFEYcJgGEYUJgyG\nYURhwmAYRhQmDIZhRGHCYBhGFCYMhmFEYcJgGEYUJgyGYUSR6BB1/+4NT7dKRBaKSJegHDMMI30k\nWmN4QFULVHUw8EfgrgB8MgwjzSQ6RN1XvsU2gI0XYRghIKGvRAOIyH3A94EvgbMT9sgwjLQjDQwe\n5TLEMESdl28GkKOqP63HTvUQdUAf4MMY/PsW8EUM+dJJpvuY6f5B5vuY6f5B7D52V9VOjWVqVBhi\nRUS6Ay+r6oBADDqbK1S1MCh7ySDTfcx0/yDzfcx0/yB4HxO9K3Gyb7EI+CAxdwzDyAQS7WP4uYj0\nAaqAvwFTE3fJMIx0k+gQdd8NypF6mJ1k+0GQ6T5mun+Q+T5mun8QsI+B9TEYhhEe7JFowzCiyAhh\nEJELRORDEdkoIrfWkX6UiMzz0v8iIj0yzL9/FZH13uPhr3t3aFJKYz768l0mIioiKe9lj8VHEfme\nty3XicgzmeSfiHQTkTdEpNTb12NT7N9jIvK5iKytJ11E5L88/9eIyNC4C1PVtE5ANvAxcCLQClgN\n9K+V50fAI978FcC8DPPvbKC1N1+cSv9i9dHL1xZ4E1gOFGaaj8DJQCnQwVs+LsP8mw0Ue/P9gc0p\n3oZnAkOBtfWkjwVeAQQYDvwl3rIyocYwDNioqptU9SBQAlxSK88lwBxvfj5wjohIpvinqm+oaoW3\nuBzIS5FvMfvo8e/AfwCVqXTOIxYfrwceVtU9AKr6eYb5p0A7b/4YYGsK/UNV3wR2N5DlEuBJdSwH\n2otI53jKygRh6Aps8S2Xe+vqzKOqh3GPX3dMiXex+ednCk61U0mjPorIEOAEVf1jKh3zEct27A30\nFpGlIrJcRC5ImXex+TcTuEpEyoEFwI9T41rMNPVYrZeE35UIgLqu/LVvlcSSJ1nEXLaIXAUUAqOS\n6lEdRdexrtpHEckCHgSuTZVDdRDLdmyBa06chat1vSUiA1R1b5J9g9j8mwg8oar/KSKnA095/lUl\n372YCOw8yYQaQzlwgm85j+gqWnUeEWmBq8Y1VKUKklj8Q0TOBW4HilT17ynyLUJjPrYFBgBLRGQz\nrv35Uoo7IGPdzy+q6iFV/QT3Ps3JpIZY/JsCPAugqu8AObh3FDKFmI7VmEhl50k9HSYtgE1AT77p\n9MmvleefqNn5+GyG+TcE13F1cqZuw1r5l5D6zsdYtuMFwBxv/lu4anHHDPLvFeBab76fd9JJirdj\nD+rvfLyImp2P78ZdTiqDaiBSQQ6xAAAAlElEQVTYsUCZd3Ld7q27B3f1BafMzwEbgXeBEzPMv9eA\nHcAqb3op07ZhrbwpF4YYt6MAs4D1wF+BKzLMv/7AUk80VgHnp9i/ucA24BCudjAF9xrCVN/2e9jz\n/6+J7GN78tEwjCgyoY/BMIwMw4TBMIwoTBgMw4jChMEwjChMGAzDiMKEwTCMKEwYDMOIwoTBMIwo\n/g/VafrJulFciwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f907dd57e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put mopdel and likelihood into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize axes\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Test points are every 0.01 from 0 to 1 inclusive ## THIS IS SAME AS TRAINING \n",
    "test_x = Variable(torch.linspace(0, 1, 1001)).cuda()\n",
    "# Make predictions from model output Gaussian warped through Bernoulli likelihood\n",
    "predictions = likelihood(model(test_x))\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    # Black stars for trainng data\n",
    "    ax.plot(train_x.data.cpu().numpy(), train_y.data.cpu().numpy(), 'k*')\n",
    "    # Based of prediction probability label -1 or 1\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    # Plot test predictions as blue line\n",
    "    ax.plot(test_x.data.cpu().numpy(), pred_labels.data.cpu().numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Call plot\n",
    "ax_plot(observed_ax, predictions, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
