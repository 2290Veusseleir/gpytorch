{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "%set_env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from scipy.io import loadmat\n",
    "\n",
    "dataset = 'yacht'\n",
    "\n",
    "data = torch.Tensor(loadmat(f'/home/jake.gardner/data/{dataset}.mat')['data'])\n",
    "\n",
    "subsample = 1.0\n",
    "data = data[:int(subsample * data.shape[0]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([277, 6]) torch.Size([31, 6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "N = data.shape[0]\n",
    "# make train/val/test\n",
    "n_train = int(0.9 * N)\n",
    "train_x, train_y = data[:n_train, :-1], data[:n_train, -1]\n",
    "test_x, test_y = data[n_train:, :-1], data[n_train:, -1]\n",
    "\n",
    "# normalize features\n",
    "mean = train_x.mean(dim=-2, keepdim=True)\n",
    "std = train_x.std(dim=-2, keepdim=True) + 1e-6 # prevent dividing by 0\n",
    "train_x = (train_x - mean) / std\n",
    "test_x = (test_x - mean) / std\n",
    "\n",
    "# normalize labels\n",
    "# mean, std = train_y.mean(),train_y.std()\n",
    "# train_y = (train_y - mean) / std\n",
    "# test_y = (test_y - mean) / std\n",
    "\n",
    "# make continguous\n",
    "train_x, train_y = train_x.contiguous(), train_y.contiguous()\n",
    "test_x, test_y = test_x.contiguous(), test_y.contiguous()\n",
    "\n",
    "output_device = torch.device('cuda:0')\n",
    "\n",
    "train_x, train_y = train_x.to(output_device), train_y.to(output_device)\n",
    "test_x, test_y = test_x.to(output_device), test_y.to(output_device)\n",
    "\n",
    "print(train_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExactGPModel(\n",
      "  (likelihood): GaussianLikelihood(\n",
      "    (noise_covar): HomoskedasticNoise(\n",
      "      (raw_noise_constraint): Positive()\n",
      "    )\n",
      "  )\n",
      "  (mean_module): ZeroMean()\n",
      "  (covar_module): ScaleKernel(\n",
      "    (base_kernel): RFNSSpectralDeltaKernel(\n",
      "      (raw_lengthscale_constraint): Positive()\n",
      "      (raw_Z_constraint): Positive()\n",
      "    )\n",
      "    (raw_outputscale_constraint): Positive()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from gpytorch.kernels.spectral_autoregressive_flow_kernel import (\n",
    "    RFNSSpectralDeltaKernel,\n",
    ")\n",
    "\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel, ProductStructureKernel\n",
    "from gpytorch.kernels.keops import MaternKernel as KMaternKernel\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "#         self.mean_module = gpytorch.means.LinearMean(train_x.size(-1))\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        base_covar_module = RFNSSpectralDeltaKernel(\n",
    "            num_dims=train_x.size(-1),\n",
    "            num_deltas=1024,\n",
    "            ard_num_dims=train_x.size(-1),\n",
    "        )# , num_dims=train_x.size(-1)\n",
    "        base_covar_module.initialize_from_data(train_x, train_y)\n",
    "#         base_covar_module = MaternKernel(ard_num_dims=train_x.size(-1))\n",
    "        self.covar_module = ScaleKernel(base_covar_module)\n",
    "#         self.covar_module = KMaternKernel(nu=2.5, ard_num_dims=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Positive()).cuda()\n",
    "model = ExactGPModel(train_x, train_y, likelihood).cuda()\n",
    "# likelihood.register_prior(\"noise_prior\", gpytorch.priors.HorseshoePrior(1.), \"noise\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([277, 6])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/750 - Loss: 2.82\n",
      "Iter 50/750 - Loss: 1.32\n",
      "Iter 100/750 - Loss: 1.07\n",
      "Iter 150/750 - Loss: 0.91\n",
      "Iter 200/750 - Loss: 0.73\n",
      "Iter 250/750 - Loss: 0.53\n",
      "Iter 300/750 - Loss: 0.34\n",
      "Iter 350/750 - Loss: 0.14\n",
      "Iter 400/750 - Loss: -0.05\n",
      "Iter 450/750 - Loss: -0.23\n",
      "Iter 500/750 - Loss: -0.37\n",
      "Iter 550/750 - Loss: -0.39\n",
      "Iter 600/750 - Loss: -0.41\n",
      "Iter 650/750 - Loss: -0.42\n",
      "Iter 700/750 - Loss: -0.43\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[500], gamma=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "with gpytorch.settings.max_cholesky_size(5000), gpytorch.settings.cg_tolerance(0.001):\n",
    "    training_iter = 750\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if i % 50 == 0:\n",
    "            print(f'Iter {i}/{training_iter} - Loss: {loss.item():.2f}')\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.fit import fit_gpytorch_model\n",
    "with gpytorch.settings.max_cholesky_size(5000):\n",
    "    fit_gpytorch_model(mll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with the model\n",
    "\n",
    "In the next cell, we make predictions with the model. To do this, we simply put the model and likelihood in eval mode, and call both modules on the test data.\n",
    "\n",
    "Just as a user defined GP model returns a `MultivariateNormal` containing the prior mean and covariance from forward, a trained GP model in eval mode returns a `MultivariateNormal` containing the posterior mean and covariance. Thus, getting the predictive mean and variance, and then sampling functions from the GP at the given test points could be accomplished with calls like:\n",
    "\n",
    "```python\n",
    "f_preds = model(test_x)\n",
    "y_preds = likelihood(model(test_x))\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_var = f_preds.variance\n",
    "f_covar = f_preds.covariance_matrix\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size(1000,))\n",
    "```\n",
    "\n",
    "The `gpytorch.settings.fast_pred_var` context is not needed, but here we are giving a preview of using one of our cool features, getting faster predictive distributions using [LOVE](https://arxiv.org/abs/1803.06058)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.829\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "with gpytorch.settings.max_cholesky_size(5000), gpytorch.settings.skip_posterior_variances(), gpytorch.settings.fast_computations(False):\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    print(f'{loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.skip_posterior_variances(), gpytorch.settings.max_cholesky_size(5000):\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model fit\n",
    "\n",
    "In the next cell, we plot the mean and confidence region of the Gaussian process model. The `confidence_region` method is a helper method that returns 2 standard deviations above and below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0740, device='cuda:0')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.mean(torch.pow(observed_pred.mean - test_y, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake.gardner/git/gpytorch/gpytorch/models/exact_gp.py:282: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.max_cholesky_size(5000):\n",
    "    observed_pred = likelihood(model(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, device='cuda:0')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.mean(torch.pow(observed_pred.mean - train_y, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gpytorch.lazy.non_lazy_tensor.NonLazyTensor at 0x7f8370d57ba8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-6f570fd13103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
