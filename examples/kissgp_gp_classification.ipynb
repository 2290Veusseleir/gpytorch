{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use a `GridInducingVariationalGP` module. This classification module is designed for when the inputs of the function you're modeling are one-dimensional.\n",
    "\n",
    "The use of inducing points allows for scaling up the training data by making computational complexity linear instead of cubic.\n",
    "\n",
    "In this example, weâ€™re modeling a function that is periodically labeled cycling every 1/8 (think of a square wave with period 1/4)\n",
    "\n",
    "This notebook doesn't use cuda, in general we recommend GPU use if possible and most of our notebooks utilize cuda as well.\n"
    "\n"
    "Kernel interpolation for scalable structured Gaussian processes (KISS-GP) was introduced in Wilson & Nickisch (ICML 2015):\n",
    "http://proceedings.mlr.press/v37/wilson15.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# The train data points are spaced every 1/25 between 0 and 1 inclusive\n",
    "train_x = Variable(torch.linspace(0, 1, 26))\n",
    "# Use the sign function (-1 if value <0, 1 if value>0) to assign\n",
    "# periodic labels to the data\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (8 * math.pi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import BernoulliLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model to classify, we use a GridInducingVariationalGP which exploits\n",
    "# grid structure (the x data points are linspace)\n",
    "# to get fast predictive distributions\n",
    "class GPClassificationModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPClassificationModel, self).__init__(grid_size=32, grid_bounds=[(0, 1)])\n",
    "        # Near-zero constant mean\n",
    "        self.mean_module = ConstantMean(constant_bounds=[-1e-5,1e-5])\n",
    "        # RBF kernel as universal approximator\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        # Register RBF lengthscale as hyperparameter\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-5,6))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Calc predictive mean (zero)\n",
    "        mean_x = self.mean_module(x)\n",
    "        # Calc predictive covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        # Make predictive distribution from predictive mean and covariance\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "# Initialize model\n",
    "model = GPClassificationModel()\n",
    "# Use Bernoulli Likelihood (warps via normal CDF to (0,1))\n",
    "likelihood = BernoulliLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 5175.440   log_lengthscale: 0.000\n",
      "Iter 2/200 - Loss: 3964.343   log_lengthscale: -0.100\n",
      "Iter 3/200 - Loss: 2691.397   log_lengthscale: -0.197\n",
      "Iter 4/200 - Loss: 2000.065   log_lengthscale: -0.294\n",
      "Iter 5/200 - Loss: 1526.649   log_lengthscale: -0.393\n",
      "Iter 6/200 - Loss: 1087.136   log_lengthscale: -0.493\n",
      "Iter 7/200 - Loss: 726.850   log_lengthscale: -0.592\n",
      "Iter 8/200 - Loss: 484.078   log_lengthscale: -0.691\n",
      "Iter 9/200 - Loss: 337.686   log_lengthscale: -0.790\n",
      "Iter 10/200 - Loss: 248.389   log_lengthscale: -0.889\n",
      "Iter 11/200 - Loss: 203.194   log_lengthscale: -0.989\n",
      "Iter 12/200 - Loss: 167.665   log_lengthscale: -1.087\n",
      "Iter 13/200 - Loss: 138.769   log_lengthscale: -1.183\n",
      "Iter 14/200 - Loss: 139.608   log_lengthscale: -1.278\n",
      "Iter 15/200 - Loss: 137.127   log_lengthscale: -1.370\n",
      "Iter 16/200 - Loss: 132.433   log_lengthscale: -1.461\n",
      "Iter 17/200 - Loss: 119.795   log_lengthscale: -1.550\n",
      "Iter 18/200 - Loss: 105.533   log_lengthscale: -1.638\n",
      "Iter 19/200 - Loss: 106.290   log_lengthscale: -1.724\n",
      "Iter 20/200 - Loss: 73.494   log_lengthscale: -1.808\n",
      "Iter 21/200 - Loss: 87.107   log_lengthscale: -1.889\n",
      "Iter 22/200 - Loss: 82.330   log_lengthscale: -1.969\n",
      "Iter 23/200 - Loss: 68.608   log_lengthscale: -2.048\n",
      "Iter 24/200 - Loss: 54.699   log_lengthscale: -2.125\n",
      "Iter 25/200 - Loss: 40.407   log_lengthscale: -2.200\n",
      "Iter 26/200 - Loss: 30.781   log_lengthscale: -2.273\n",
      "Iter 27/200 - Loss: 23.468   log_lengthscale: -2.342\n",
      "Iter 28/200 - Loss: 22.951   log_lengthscale: -2.407\n",
      "Iter 29/200 - Loss: 19.611   log_lengthscale: -2.469\n",
      "Iter 30/200 - Loss: 21.820   log_lengthscale: -2.528\n",
      "Iter 31/200 - Loss: 17.753   log_lengthscale: -2.584\n",
      "Iter 32/200 - Loss: 15.287   log_lengthscale: -2.637\n",
      "Iter 33/200 - Loss: 11.762   log_lengthscale: -2.687\n",
      "Iter 34/200 - Loss: 9.640   log_lengthscale: -2.734\n",
      "Iter 35/200 - Loss: 9.013   log_lengthscale: -2.778\n",
      "Iter 36/200 - Loss: 8.414   log_lengthscale: -2.819\n",
      "Iter 37/200 - Loss: 7.860   log_lengthscale: -2.857\n",
      "Iter 38/200 - Loss: 9.127   log_lengthscale: -2.893\n",
      "Iter 39/200 - Loss: 6.949   log_lengthscale: -2.927\n",
      "Iter 40/200 - Loss: 6.564   log_lengthscale: -2.958\n",
      "Iter 41/200 - Loss: 7.266   log_lengthscale: -2.987\n",
      "Iter 42/200 - Loss: 5.939   log_lengthscale: -3.014\n",
      "Iter 43/200 - Loss: 5.698   log_lengthscale: -3.040\n",
      "Iter 44/200 - Loss: 4.933   log_lengthscale: -3.063\n",
      "Iter 45/200 - Loss: 5.182   log_lengthscale: -3.085\n",
      "Iter 46/200 - Loss: 5.050   log_lengthscale: -3.105\n",
      "Iter 47/200 - Loss: 4.405   log_lengthscale: -3.124\n",
      "Iter 48/200 - Loss: 4.478   log_lengthscale: -3.141\n",
      "Iter 49/200 - Loss: 4.289   log_lengthscale: -3.157\n",
      "Iter 50/200 - Loss: 4.402   log_lengthscale: -3.172\n",
      "Iter 51/200 - Loss: 4.482   log_lengthscale: -3.185\n",
      "Iter 52/200 - Loss: 3.844   log_lengthscale: -3.198\n",
      "Iter 53/200 - Loss: 3.656   log_lengthscale: -3.209\n",
      "Iter 54/200 - Loss: 3.669   log_lengthscale: -3.219\n",
      "Iter 55/200 - Loss: 3.780   log_lengthscale: -3.229\n",
      "Iter 56/200 - Loss: 4.149   log_lengthscale: -3.238\n",
      "Iter 57/200 - Loss: 3.865   log_lengthscale: -3.246\n",
      "Iter 58/200 - Loss: 3.888   log_lengthscale: -3.253\n",
      "Iter 59/200 - Loss: 3.859   log_lengthscale: -3.260\n",
      "Iter 60/200 - Loss: 3.142   log_lengthscale: -3.266\n",
      "Iter 61/200 - Loss: 3.881   log_lengthscale: -3.271\n",
      "Iter 62/200 - Loss: 3.028   log_lengthscale: -3.276\n",
      "Iter 63/200 - Loss: 3.998   log_lengthscale: -3.281\n",
      "Iter 64/200 - Loss: 3.557   log_lengthscale: -3.285\n",
      "Iter 65/200 - Loss: 3.711   log_lengthscale: -3.289\n",
      "Iter 66/200 - Loss: 3.430   log_lengthscale: -3.292\n",
      "Iter 67/200 - Loss: 3.732   log_lengthscale: -3.296\n",
      "Iter 68/200 - Loss: 3.717   log_lengthscale: -3.298\n",
      "Iter 69/200 - Loss: 3.090   log_lengthscale: -3.301\n",
      "Iter 70/200 - Loss: 3.401   log_lengthscale: -3.303\n",
      "Iter 71/200 - Loss: 3.255   log_lengthscale: -3.305\n",
      "Iter 72/200 - Loss: 3.575   log_lengthscale: -3.307\n",
      "Iter 73/200 - Loss: 3.437   log_lengthscale: -3.309\n",
      "Iter 74/200 - Loss: 3.335   log_lengthscale: -3.311\n",
      "Iter 75/200 - Loss: 3.369   log_lengthscale: -3.312\n",
      "Iter 76/200 - Loss: 3.114   log_lengthscale: -3.314\n",
      "Iter 77/200 - Loss: 2.907   log_lengthscale: -3.315\n",
      "Iter 78/200 - Loss: 3.623   log_lengthscale: -3.316\n",
      "Iter 79/200 - Loss: 3.550   log_lengthscale: -3.317\n",
      "Iter 80/200 - Loss: 3.630   log_lengthscale: -3.318\n",
      "Iter 81/200 - Loss: 3.558   log_lengthscale: -3.319\n",
      "Iter 82/200 - Loss: 3.232   log_lengthscale: -3.319\n",
      "Iter 83/200 - Loss: 3.228   log_lengthscale: -3.320\n",
      "Iter 84/200 - Loss: 3.290   log_lengthscale: -3.320\n",
      "Iter 85/200 - Loss: 3.428   log_lengthscale: -3.321\n",
      "Iter 86/200 - Loss: 3.089   log_lengthscale: -3.321\n",
      "Iter 87/200 - Loss: 3.265   log_lengthscale: -3.322\n",
      "Iter 88/200 - Loss: 3.327   log_lengthscale: -3.322\n",
      "Iter 89/200 - Loss: 3.009   log_lengthscale: -3.323\n",
      "Iter 90/200 - Loss: 3.646   log_lengthscale: -3.323\n",
      "Iter 91/200 - Loss: 3.016   log_lengthscale: -3.323\n",
      "Iter 92/200 - Loss: 3.215   log_lengthscale: -3.323\n",
      "Iter 93/200 - Loss: 3.295   log_lengthscale: -3.324\n",
      "Iter 94/200 - Loss: 3.022   log_lengthscale: -3.324\n",
      "Iter 95/200 - Loss: 3.457   log_lengthscale: -3.324\n",
      "Iter 96/200 - Loss: 3.239   log_lengthscale: -3.324\n",
      "Iter 97/200 - Loss: 3.626   log_lengthscale: -3.325\n",
      "Iter 98/200 - Loss: 3.194   log_lengthscale: -3.325\n",
      "Iter 99/200 - Loss: 2.998   log_lengthscale: -3.325\n",
      "Iter 100/200 - Loss: 2.736   log_lengthscale: -3.325\n",
      "Iter 101/200 - Loss: 3.971   log_lengthscale: -3.325\n",
      "Iter 102/200 - Loss: 3.189   log_lengthscale: -3.325\n",
      "Iter 103/200 - Loss: 3.493   log_lengthscale: -3.325\n",
      "Iter 104/200 - Loss: 3.187   log_lengthscale: -3.325\n",
      "Iter 105/200 - Loss: 3.386   log_lengthscale: -3.325\n",
      "Iter 106/200 - Loss: 3.109   log_lengthscale: -3.325\n",
      "Iter 107/200 - Loss: 3.225   log_lengthscale: -3.326\n",
      "Iter 108/200 - Loss: 3.018   log_lengthscale: -3.326\n",
      "Iter 109/200 - Loss: 3.318   log_lengthscale: -3.326\n",
      "Iter 110/200 - Loss: 3.203   log_lengthscale: -3.326\n",
      "Iter 111/200 - Loss: 3.119   log_lengthscale: -3.326\n",
      "Iter 112/200 - Loss: 3.412   log_lengthscale: -3.326\n",
      "Iter 113/200 - Loss: 2.953   log_lengthscale: -3.326\n",
      "Iter 114/200 - Loss: 3.027   log_lengthscale: -3.326\n",
      "Iter 115/200 - Loss: 3.096   log_lengthscale: -3.326\n",
      "Iter 116/200 - Loss: 3.049   log_lengthscale: -3.326\n",
      "Iter 117/200 - Loss: 2.785   log_lengthscale: -3.325\n",
      "Iter 118/200 - Loss: 3.147   log_lengthscale: -3.325\n",
      "Iter 119/200 - Loss: 2.902   log_lengthscale: -3.325\n",
      "Iter 120/200 - Loss: 3.166   log_lengthscale: -3.325\n",
      "Iter 121/200 - Loss: 2.920   log_lengthscale: -3.325\n",
      "Iter 122/200 - Loss: 3.888   log_lengthscale: -3.326\n",
      "Iter 123/200 - Loss: 3.174   log_lengthscale: -3.326\n",
      "Iter 124/200 - Loss: 3.104   log_lengthscale: -3.326\n",
      "Iter 125/200 - Loss: 2.963   log_lengthscale: -3.327\n",
      "Iter 126/200 - Loss: 3.470   log_lengthscale: -3.327\n",
      "Iter 127/200 - Loss: 2.815   log_lengthscale: -3.327\n",
      "Iter 128/200 - Loss: 3.158   log_lengthscale: -3.327\n",
      "Iter 129/200 - Loss: 2.923   log_lengthscale: -3.328\n",
      "Iter 130/200 - Loss: 3.067   log_lengthscale: -3.328\n",
      "Iter 131/200 - Loss: 2.846   log_lengthscale: -3.328\n",
      "Iter 132/200 - Loss: 2.759   log_lengthscale: -3.329\n",
      "Iter 133/200 - Loss: 2.980   log_lengthscale: -3.329\n",
      "Iter 134/200 - Loss: 3.192   log_lengthscale: -3.329\n",
      "Iter 135/200 - Loss: 3.443   log_lengthscale: -3.329\n",
      "Iter 136/200 - Loss: 2.766   log_lengthscale: -3.329\n",
      "Iter 137/200 - Loss: 2.921   log_lengthscale: -3.329\n",
      "Iter 138/200 - Loss: 3.110   log_lengthscale: -3.329\n",
      "Iter 139/200 - Loss: 3.160   log_lengthscale: -3.329\n",
      "Iter 140/200 - Loss: 3.101   log_lengthscale: -3.329\n",
      "Iter 141/200 - Loss: 2.936   log_lengthscale: -3.329\n",
      "Iter 142/200 - Loss: 3.143   log_lengthscale: -3.329\n",
      "Iter 143/200 - Loss: 3.069   log_lengthscale: -3.330\n",
      "Iter 144/200 - Loss: 2.885   log_lengthscale: -3.330\n",
      "Iter 145/200 - Loss: 3.307   log_lengthscale: -3.330\n",
      "Iter 146/200 - Loss: 3.257   log_lengthscale: -3.331\n",
      "Iter 147/200 - Loss: 3.060   log_lengthscale: -3.331\n",
      "Iter 148/200 - Loss: 3.344   log_lengthscale: -3.332\n",
      "Iter 149/200 - Loss: 2.983   log_lengthscale: -3.332\n",
      "Iter 150/200 - Loss: 2.844   log_lengthscale: -3.333\n",
      "Iter 151/200 - Loss: 3.340   log_lengthscale: -3.333\n",
      "Iter 152/200 - Loss: 3.125   log_lengthscale: -3.334\n",
      "Iter 153/200 - Loss: 3.270   log_lengthscale: -3.335\n",
      "Iter 154/200 - Loss: 3.151   log_lengthscale: -3.335\n",
      "Iter 155/200 - Loss: 3.083   log_lengthscale: -3.336\n",
      "Iter 156/200 - Loss: 2.952   log_lengthscale: -3.336\n",
      "Iter 157/200 - Loss: 3.056   log_lengthscale: -3.336\n",
      "Iter 158/200 - Loss: 2.705   log_lengthscale: -3.336\n",
      "Iter 159/200 - Loss: 3.110   log_lengthscale: -3.337\n",
      "Iter 160/200 - Loss: 3.271   log_lengthscale: -3.337\n",
      "Iter 161/200 - Loss: 2.720   log_lengthscale: -3.337\n",
      "Iter 162/200 - Loss: 2.887   log_lengthscale: -3.337\n",
      "Iter 163/200 - Loss: 2.659   log_lengthscale: -3.337\n",
      "Iter 164/200 - Loss: 2.997   log_lengthscale: -3.338\n",
      "Iter 165/200 - Loss: 2.716   log_lengthscale: -3.338\n",
      "Iter 166/200 - Loss: 3.018   log_lengthscale: -3.339\n",
      "Iter 167/200 - Loss: 3.071   log_lengthscale: -3.339\n",
      "Iter 168/200 - Loss: 3.603   log_lengthscale: -3.340\n",
      "Iter 169/200 - Loss: 3.373   log_lengthscale: -3.341\n",
      "Iter 170/200 - Loss: 3.206   log_lengthscale: -3.341\n",
      "Iter 171/200 - Loss: 3.195   log_lengthscale: -3.342\n",
      "Iter 172/200 - Loss: 2.878   log_lengthscale: -3.343\n",
      "Iter 173/200 - Loss: 3.147   log_lengthscale: -3.343\n",
      "Iter 174/200 - Loss: 2.930   log_lengthscale: -3.344\n",
      "Iter 175/200 - Loss: 3.161   log_lengthscale: -3.344\n",
      "Iter 176/200 - Loss: 3.001   log_lengthscale: -3.345\n",
      "Iter 177/200 - Loss: 3.324   log_lengthscale: -3.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 178/200 - Loss: 2.928   log_lengthscale: -3.346\n",
      "Iter 179/200 - Loss: 2.991   log_lengthscale: -3.347\n",
      "Iter 180/200 - Loss: 3.308   log_lengthscale: -3.347\n",
      "Iter 181/200 - Loss: 3.015   log_lengthscale: -3.348\n",
      "Iter 182/200 - Loss: 2.968   log_lengthscale: -3.349\n",
      "Iter 183/200 - Loss: 3.077   log_lengthscale: -3.350\n",
      "Iter 184/200 - Loss: 3.122   log_lengthscale: -3.351\n",
      "Iter 185/200 - Loss: 3.252   log_lengthscale: -3.352\n",
      "Iter 186/200 - Loss: 3.318   log_lengthscale: -3.354\n",
      "Iter 187/200 - Loss: 2.842   log_lengthscale: -3.355\n",
      "Iter 188/200 - Loss: 3.075   log_lengthscale: -3.356\n",
      "Iter 189/200 - Loss: 3.095   log_lengthscale: -3.356\n",
      "Iter 190/200 - Loss: 2.631   log_lengthscale: -3.357\n",
      "Iter 191/200 - Loss: 3.351   log_lengthscale: -3.358\n",
      "Iter 192/200 - Loss: 3.363   log_lengthscale: -3.358\n",
      "Iter 193/200 - Loss: 3.143   log_lengthscale: -3.359\n",
      "Iter 194/200 - Loss: 3.268   log_lengthscale: -3.360\n",
      "Iter 195/200 - Loss: 2.863   log_lengthscale: -3.360\n",
      "Iter 196/200 - Loss: 3.009   log_lengthscale: -3.361\n",
      "Iter 197/200 - Loss: 3.135   log_lengthscale: -3.362\n",
      "Iter 198/200 - Loss: 3.043   log_lengthscale: -3.363\n",
      "Iter 199/200 - Loss: 2.952   log_lengthscale: -3.363\n",
      "Iter 200/200 - Loss: 2.980   log_lengthscale: -3.364\n",
      "CPU times: user 1.88 s, sys: 8 ms, total: 1.89 s\n",
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    # BernoulliLikelihood has no parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# n_data refers to the amount of training data\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=len(train_y))\n",
    "\n",
    "def train():\n",
    "    num_iter = 200\n",
    "    for i in range(num_iter):\n",
    "        # Zero gradients out for new iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calculate loss\n",
    "        loss = -mll(output, train_y)\n",
    "        # Calc gradients\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f   log_lengthscale: %.3f' % (\n",
    "            i + 1, num_iter, loss.data[0],\n",
    "            model.covar_module.base_kernel_module.log_lengthscale.data.squeeze()[0],\n",
    "        ))\n",
    "        optimizer.step()\n",
    "        \n",
    "# Get clock time\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86f51130b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set model and likelihood into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize axes\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Test points are every 0.01 from 0 to 1 inclusive \n",
    "test_x = Variable(torch.linspace(0, 1, 101))\n",
    "# Make predictions from model output Gaussian warped through Bernoulli likelihood\n",
    "predictions = likelihood(model(test_x))\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    # Black stars for trainng data\n",
    "    ax.plot(train_x.data.numpy(), train_y.data.numpy(), 'k*')\n",
    "    # Based of prediction probability label -1 or 1\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    # Plot test predictions as blue line\n",
    "    ax.plot(test_x.data.numpy(), pred_labels.data.numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "# Call plot\n",
    "ax_plot(observed_ax, predictions, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
