{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use a `GridInducingVariationalGP` module. This classification module is designed for when the inputs of the function you're modeling are one-dimensional.\n",
    "\n",
    "The use of inducing points allows for scaling up the training data by making computational complexity linear instead of cubic.\n",
    "\n",
    "In this example, weâ€™re modeling a function that is periodically labeled cycling every 1/8 (think of a square wave with period 1/4)\n",
    "\n",
    "This notebook doesn't use cuda, in general we recommend GPU use if possible and most of our notebooks utilize cuda as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# The train data points are spaced every 1/25 between 0 and 1 inclusive\n",
    "train_x = Variable(torch.linspace(0, 1, 26))\n",
    "# Use the sign function (-1 if value <0, 1 if value>0) to assign\n",
    "# periodic labels to the data\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (8 * math.pi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import BernoulliLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a model to classify, we use a GridInducingVariationalGP which exploits\n",
    "# grid structure (the x data points are linspace)\n",
    "# to get fast predictive distributions\n",
    "class GPClassificationModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPClassificationModel, self).__init__(grid_size=32, grid_bounds=[(0, 1)])\n",
    "        # Near-zero constant mean\n",
    "        self.mean_module = ConstantMean(constant_bounds=[-1e-5,1e-5])\n",
    "        # RBF kernel as universal approximator\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        # Register RBF lengthscale as hyperparameter\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-5,6))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Calc predictive mean (zero)\n",
    "        mean_x = self.mean_module(x)\n",
    "        # Calc predictive covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        # Make predictive distribution from predictive mean and covariance\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "# Initialize model\n",
    "model = GPClassificationModel()\n",
    "# Use Bernoulli Likelihood (warps via normal CDF to (0,1))\n",
    "likelihood = BernoulliLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 5353.406   log_lengthscale: 0.000\n",
      "Iter 2/200 - Loss: 3988.173   log_lengthscale: -0.100\n",
      "Iter 3/200 - Loss: 2732.286   log_lengthscale: -0.199\n",
      "Iter 4/200 - Loss: 2080.948   log_lengthscale: -0.290\n",
      "Iter 5/200 - Loss: 1521.180   log_lengthscale: -0.381\n",
      "Iter 6/200 - Loss: 1079.421   log_lengthscale: -0.475\n",
      "Iter 7/200 - Loss: 660.612   log_lengthscale: -0.571\n",
      "Iter 8/200 - Loss: 507.943   log_lengthscale: -0.666\n",
      "Iter 9/200 - Loss: 296.518   log_lengthscale: -0.760\n",
      "Iter 10/200 - Loss: 239.947   log_lengthscale: -0.852\n",
      "Iter 11/200 - Loss: 233.900   log_lengthscale: -0.942\n",
      "Iter 12/200 - Loss: 183.201   log_lengthscale: -1.034\n",
      "Iter 13/200 - Loss: 185.246   log_lengthscale: -1.126\n",
      "Iter 14/200 - Loss: 173.481   log_lengthscale: -1.214\n",
      "Iter 15/200 - Loss: 173.254   log_lengthscale: -1.299\n",
      "Iter 16/200 - Loss: 134.377   log_lengthscale: -1.385\n",
      "Iter 17/200 - Loss: 96.272   log_lengthscale: -1.472\n",
      "Iter 18/200 - Loss: 113.721   log_lengthscale: -1.555\n",
      "Iter 19/200 - Loss: 114.197   log_lengthscale: -1.633\n",
      "Iter 20/200 - Loss: 98.571   log_lengthscale: -1.707\n",
      "Iter 21/200 - Loss: 61.199   log_lengthscale: -1.775\n",
      "Iter 22/200 - Loss: 47.356   log_lengthscale: -1.843\n",
      "Iter 23/200 - Loss: 31.922   log_lengthscale: -1.912\n",
      "Iter 24/200 - Loss: 34.845   log_lengthscale: -1.976\n",
      "Iter 25/200 - Loss: 38.455   log_lengthscale: -2.038\n",
      "Iter 26/200 - Loss: 23.313   log_lengthscale: -2.094\n",
      "Iter 27/200 - Loss: 23.799   log_lengthscale: -2.148\n",
      "Iter 28/200 - Loss: 42.305   log_lengthscale: -2.202\n",
      "Iter 29/200 - Loss: 24.189   log_lengthscale: -2.252\n",
      "Iter 30/200 - Loss: 7.654   log_lengthscale: -2.298\n",
      "Iter 31/200 - Loss: 6.912   log_lengthscale: -2.342\n",
      "Iter 32/200 - Loss: 13.122   log_lengthscale: -2.389\n",
      "Iter 33/200 - Loss: 9.279   log_lengthscale: -2.429\n",
      "Iter 34/200 - Loss: 6.247   log_lengthscale: -2.467\n",
      "Iter 35/200 - Loss: 24.506   log_lengthscale: -2.504\n",
      "Iter 36/200 - Loss: 6.077   log_lengthscale: -2.540\n",
      "Iter 37/200 - Loss: 4.889   log_lengthscale: -2.571\n",
      "Iter 38/200 - Loss: 5.528   log_lengthscale: -2.595\n",
      "Iter 39/200 - Loss: 3.113   log_lengthscale: -2.625\n",
      "Iter 40/200 - Loss: 8.830   log_lengthscale: -2.647\n",
      "Iter 41/200 - Loss: 9.960   log_lengthscale: -2.669\n",
      "Iter 42/200 - Loss: 3.459   log_lengthscale: -2.691\n",
      "Iter 43/200 - Loss: 3.372   log_lengthscale: -2.712\n",
      "Iter 44/200 - Loss: 3.666   log_lengthscale: -2.730\n",
      "Iter 45/200 - Loss: 3.466   log_lengthscale: -2.750\n",
      "Iter 46/200 - Loss: 4.563   log_lengthscale: -2.767\n",
      "Iter 47/200 - Loss: 3.952   log_lengthscale: -2.782\n",
      "Iter 48/200 - Loss: 1.519   log_lengthscale: -2.795\n",
      "Iter 49/200 - Loss: 3.059   log_lengthscale: -2.806\n",
      "Iter 50/200 - Loss: 2.320   log_lengthscale: -2.816\n",
      "Iter 51/200 - Loss: 3.153   log_lengthscale: -2.824\n",
      "Iter 52/200 - Loss: 3.256   log_lengthscale: -2.830\n",
      "Iter 53/200 - Loss: 2.809   log_lengthscale: -2.837\n",
      "Iter 54/200 - Loss: 2.929   log_lengthscale: -2.844\n",
      "Iter 55/200 - Loss: 2.547   log_lengthscale: -2.849\n",
      "Iter 56/200 - Loss: 4.290   log_lengthscale: -2.854\n",
      "Iter 57/200 - Loss: 1.313   log_lengthscale: -2.856\n",
      "Iter 58/200 - Loss: 2.176   log_lengthscale: -2.860\n",
      "Iter 59/200 - Loss: 2.278   log_lengthscale: -2.864\n",
      "Iter 60/200 - Loss: 2.144   log_lengthscale: -2.867\n",
      "Iter 61/200 - Loss: 3.442   log_lengthscale: -2.870\n",
      "Iter 62/200 - Loss: 2.277   log_lengthscale: -2.875\n",
      "Iter 63/200 - Loss: 1.252   log_lengthscale: -2.880\n",
      "Iter 64/200 - Loss: 2.057   log_lengthscale: -2.885\n",
      "Iter 65/200 - Loss: 2.956   log_lengthscale: -2.890\n",
      "Iter 66/200 - Loss: 2.418   log_lengthscale: -2.895\n",
      "Iter 67/200 - Loss: 1.432   log_lengthscale: -2.898\n",
      "Iter 68/200 - Loss: 1.937   log_lengthscale: -2.902\n",
      "Iter 69/200 - Loss: 2.217   log_lengthscale: -2.905\n",
      "Iter 70/200 - Loss: 2.222   log_lengthscale: -2.909\n",
      "Iter 71/200 - Loss: 1.293   log_lengthscale: -2.912\n",
      "Iter 72/200 - Loss: 0.799   log_lengthscale: -2.914\n",
      "Iter 73/200 - Loss: 1.904   log_lengthscale: -2.913\n",
      "Iter 74/200 - Loss: 2.334   log_lengthscale: -2.913\n",
      "Iter 75/200 - Loss: 1.997   log_lengthscale: -2.913\n",
      "Iter 76/200 - Loss: 1.700   log_lengthscale: -2.913\n",
      "Iter 77/200 - Loss: 1.940   log_lengthscale: -2.913\n",
      "Iter 78/200 - Loss: 2.037   log_lengthscale: -2.913\n",
      "Iter 79/200 - Loss: 3.469   log_lengthscale: -2.914\n",
      "Iter 80/200 - Loss: 1.605   log_lengthscale: -2.916\n",
      "Iter 81/200 - Loss: 1.518   log_lengthscale: -2.919\n",
      "Iter 82/200 - Loss: 1.797   log_lengthscale: -2.920\n",
      "Iter 83/200 - Loss: 1.600   log_lengthscale: -2.921\n",
      "Iter 84/200 - Loss: 2.208   log_lengthscale: -2.921\n",
      "Iter 85/200 - Loss: 1.623   log_lengthscale: -2.921\n",
      "Iter 86/200 - Loss: 2.599   log_lengthscale: -2.922\n",
      "Iter 87/200 - Loss: 2.079   log_lengthscale: -2.923\n",
      "Iter 88/200 - Loss: 0.834   log_lengthscale: -2.922\n",
      "Iter 89/200 - Loss: 1.209   log_lengthscale: -2.920\n",
      "Iter 90/200 - Loss: 1.517   log_lengthscale: -2.919\n",
      "Iter 91/200 - Loss: 2.164   log_lengthscale: -2.918\n",
      "Iter 92/200 - Loss: 1.304   log_lengthscale: -2.916\n",
      "Iter 93/200 - Loss: 0.725   log_lengthscale: -2.915\n",
      "Iter 94/200 - Loss: 1.554   log_lengthscale: -2.914\n",
      "Iter 95/200 - Loss: 2.155   log_lengthscale: -2.914\n",
      "Iter 96/200 - Loss: 1.749   log_lengthscale: -2.912\n",
      "Iter 97/200 - Loss: 1.770   log_lengthscale: -2.910\n",
      "Iter 98/200 - Loss: 1.558   log_lengthscale: -2.910\n",
      "Iter 99/200 - Loss: 1.645   log_lengthscale: -2.910\n",
      "Iter 100/200 - Loss: 1.760   log_lengthscale: -2.910\n",
      "Iter 101/200 - Loss: 1.818   log_lengthscale: -2.909\n",
      "Iter 102/200 - Loss: 1.738   log_lengthscale: -2.907\n",
      "Iter 103/200 - Loss: 2.186   log_lengthscale: -2.902\n",
      "Iter 104/200 - Loss: 1.452   log_lengthscale: -2.898\n",
      "Iter 105/200 - Loss: 1.239   log_lengthscale: -2.894\n",
      "Iter 106/200 - Loss: 0.995   log_lengthscale: -2.888\n",
      "Iter 107/200 - Loss: 1.328   log_lengthscale: -2.883\n",
      "Iter 108/200 - Loss: 2.348   log_lengthscale: -2.878\n",
      "Iter 109/200 - Loss: 1.257   log_lengthscale: -2.873\n",
      "Iter 110/200 - Loss: 1.832   log_lengthscale: -2.869\n",
      "Iter 111/200 - Loss: 2.943   log_lengthscale: -2.864\n",
      "Iter 112/200 - Loss: 1.640   log_lengthscale: -2.858\n",
      "Iter 113/200 - Loss: 1.686   log_lengthscale: -2.853\n",
      "Iter 114/200 - Loss: 0.915   log_lengthscale: -2.848\n",
      "Iter 115/200 - Loss: 1.779   log_lengthscale: -2.844\n",
      "Iter 116/200 - Loss: 1.850   log_lengthscale: -2.841\n",
      "Iter 117/200 - Loss: 1.344   log_lengthscale: -2.838\n",
      "Iter 118/200 - Loss: 1.686   log_lengthscale: -2.835\n",
      "Iter 119/200 - Loss: 1.797   log_lengthscale: -2.831\n",
      "Iter 120/200 - Loss: 1.234   log_lengthscale: -2.826\n",
      "Iter 121/200 - Loss: 1.857   log_lengthscale: -2.823\n",
      "Iter 122/200 - Loss: 1.778   log_lengthscale: -2.819\n",
      "Iter 123/200 - Loss: 1.629   log_lengthscale: -2.816\n",
      "Iter 124/200 - Loss: 2.196   log_lengthscale: -2.813\n",
      "Iter 125/200 - Loss: 2.559   log_lengthscale: -2.811\n",
      "Iter 126/200 - Loss: 1.350   log_lengthscale: -2.807\n",
      "Iter 127/200 - Loss: 2.135   log_lengthscale: -2.805\n",
      "Iter 128/200 - Loss: 3.152   log_lengthscale: -2.803\n",
      "Iter 129/200 - Loss: 1.257   log_lengthscale: -2.803\n",
      "Iter 130/200 - Loss: 1.180   log_lengthscale: -2.804\n",
      "Iter 131/200 - Loss: 0.919   log_lengthscale: -2.805\n",
      "Iter 132/200 - Loss: 2.330   log_lengthscale: -2.804\n",
      "Iter 133/200 - Loss: 2.407   log_lengthscale: -2.799\n",
      "Iter 134/200 - Loss: 2.783   log_lengthscale: -2.794\n",
      "Iter 135/200 - Loss: 1.937   log_lengthscale: -2.789\n",
      "Iter 136/200 - Loss: 1.510   log_lengthscale: -2.784\n",
      "Iter 137/200 - Loss: 2.319   log_lengthscale: -2.782\n",
      "Iter 138/200 - Loss: 2.760   log_lengthscale: -2.781\n",
      "Iter 139/200 - Loss: 2.647   log_lengthscale: -2.782\n",
      "Iter 140/200 - Loss: 0.979   log_lengthscale: -2.781\n",
      "Iter 141/200 - Loss: 2.455   log_lengthscale: -2.780\n",
      "Iter 142/200 - Loss: 1.477   log_lengthscale: -2.782\n",
      "Iter 143/200 - Loss: 1.162   log_lengthscale: -2.783\n",
      "Iter 144/200 - Loss: 2.311   log_lengthscale: -2.785\n",
      "Iter 145/200 - Loss: 1.488   log_lengthscale: -2.785\n",
      "Iter 146/200 - Loss: 3.107   log_lengthscale: -2.788\n",
      "Iter 147/200 - Loss: 1.855   log_lengthscale: -2.786\n",
      "Iter 148/200 - Loss: 2.178   log_lengthscale: -2.781\n",
      "Iter 149/200 - Loss: 2.178   log_lengthscale: -2.777\n",
      "Iter 150/200 - Loss: 3.626   log_lengthscale: -2.775\n",
      "Iter 151/200 - Loss: 1.956   log_lengthscale: -2.772\n",
      "Iter 152/200 - Loss: 2.148   log_lengthscale: -2.768\n",
      "Iter 153/200 - Loss: 3.034   log_lengthscale: -2.762\n",
      "Iter 154/200 - Loss: 1.674   log_lengthscale: -2.757\n",
      "Iter 155/200 - Loss: 2.004   log_lengthscale: -2.750\n",
      "Iter 156/200 - Loss: 2.360   log_lengthscale: -2.744\n",
      "Iter 157/200 - Loss: 1.290   log_lengthscale: -2.736\n",
      "Iter 158/200 - Loss: 1.566   log_lengthscale: -2.732\n",
      "Iter 159/200 - Loss: 2.224   log_lengthscale: -2.732\n",
      "Iter 160/200 - Loss: 2.670   log_lengthscale: -2.733\n",
      "Iter 161/200 - Loss: 2.090   log_lengthscale: -2.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 162/200 - Loss: 3.067   log_lengthscale: -2.736\n",
      "Iter 163/200 - Loss: 1.229   log_lengthscale: -2.736\n",
      "Iter 164/200 - Loss: 1.653   log_lengthscale: -2.735\n",
      "Iter 165/200 - Loss: 1.064   log_lengthscale: -2.735\n",
      "Iter 166/200 - Loss: 1.641   log_lengthscale: -2.736\n",
      "Iter 167/200 - Loss: 1.484   log_lengthscale: -2.736\n",
      "Iter 168/200 - Loss: 1.806   log_lengthscale: -2.735\n",
      "Iter 169/200 - Loss: 3.237   log_lengthscale: -2.735\n",
      "Iter 170/200 - Loss: 2.212   log_lengthscale: -2.736\n",
      "Iter 171/200 - Loss: 1.153   log_lengthscale: -2.736\n",
      "Iter 172/200 - Loss: 3.400   log_lengthscale: -2.736\n",
      "Iter 173/200 - Loss: 3.314   log_lengthscale: -2.737\n",
      "Iter 174/200 - Loss: 1.526   log_lengthscale: -2.739\n",
      "Iter 175/200 - Loss: 1.623   log_lengthscale: -2.742\n",
      "Iter 176/200 - Loss: 1.716   log_lengthscale: -2.744\n",
      "Iter 177/200 - Loss: 3.058   log_lengthscale: -2.746\n",
      "Iter 178/200 - Loss: 1.522   log_lengthscale: -2.750\n",
      "Iter 179/200 - Loss: 1.782   log_lengthscale: -2.754\n",
      "Iter 180/200 - Loss: 1.886   log_lengthscale: -2.757\n",
      "Iter 181/200 - Loss: 1.421   log_lengthscale: -2.759\n",
      "Iter 182/200 - Loss: 1.830   log_lengthscale: -2.762\n",
      "Iter 183/200 - Loss: 1.873   log_lengthscale: -2.769\n",
      "Iter 184/200 - Loss: 2.112   log_lengthscale: -2.775\n",
      "Iter 185/200 - Loss: 1.264   log_lengthscale: -2.781\n",
      "Iter 186/200 - Loss: 1.653   log_lengthscale: -2.786\n",
      "Iter 187/200 - Loss: 1.563   log_lengthscale: -2.789\n",
      "Iter 188/200 - Loss: 1.791   log_lengthscale: -2.791\n",
      "Iter 189/200 - Loss: 1.413   log_lengthscale: -2.792\n",
      "Iter 190/200 - Loss: 2.977   log_lengthscale: -2.791\n",
      "Iter 191/200 - Loss: 2.278   log_lengthscale: -2.789\n",
      "Iter 192/200 - Loss: 2.311   log_lengthscale: -2.787\n",
      "Iter 193/200 - Loss: 2.488   log_lengthscale: -2.785\n",
      "Iter 194/200 - Loss: 1.577   log_lengthscale: -2.785\n",
      "Iter 195/200 - Loss: 1.292   log_lengthscale: -2.783\n",
      "Iter 196/200 - Loss: 0.600   log_lengthscale: -2.781\n",
      "Iter 197/200 - Loss: 0.936   log_lengthscale: -2.780\n",
      "Iter 198/200 - Loss: 1.385   log_lengthscale: -2.778\n",
      "Iter 199/200 - Loss: 1.726   log_lengthscale: -2.777\n",
      "Iter 200/200 - Loss: 0.815   log_lengthscale: -2.774\n",
      "CPU times: user 6.56 s, sys: 28 ms, total: 6.58 s\n",
      "Wall time: 6.57 s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "optimizer.n_iter = 0\n",
    "\n",
    "def train():\n",
    "    num_iter = 200\n",
    "    for i in range(num_iter):\n",
    "        # Zero gradients out for new iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calculate loss\n",
    "        loss = -model.marginal_log_likelihood(likelihood, output, train_y)\n",
    "        # Calc gradients\n",
    "        loss.backward()\n",
    "        optimizer.n_iter += 1\n",
    "        print('Iter %d/%d - Loss: %.3f   log_lengthscale: %.3f' % (\n",
    "            i + 1, num_iter, loss.data[0],\n",
    "            model.covar_module.base_kernel_module.log_lengthscale.data.squeeze()[0],\n",
    "        ))\n",
    "        optimizer.step()\n",
    "# Get clock time\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f026c9e0cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set model and likelihood into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize axes\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Test points are every 0.01 from 0 to 1 inclusive \n",
    "test_x = Variable(torch.linspace(0, 1, 101))\n",
    "# Make predictions from model output Gaussian warped through Bernoulli likelihood\n",
    "predictions = likelihood(model(test_x))\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    # Black stars for trainng data\n",
    "    ax.plot(train_x.data.numpy(), train_y.data.numpy(), 'k*')\n",
    "    # Based of prediction probability label -1 or 1\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    # Plot test predictions as blue line\n",
    "    ax.plot(test_x.data.numpy(), pred_labels.data.numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "# Call plot\n",
    "ax_plot(observed_ax, predictions, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
