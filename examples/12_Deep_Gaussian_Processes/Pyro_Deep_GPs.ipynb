{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "from gpytorch.models.pyro_deep_gp import AbstractPyroHiddenGPLayer, AbstractPyroDeepGP\n",
    "from gpytorch.variational import PyroExactVariationalStrategy, PyroSamplingVariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO, Trace_ELBO\n",
    "from pyro import optim\n",
    "import pyro\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 21 15:25:22 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:02:00.0  On |                  N/A |\r\n",
      "| 30%   46C    P5    37W / 250W |   1893MiB / 10988MiB |      4%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:03:00.0 Off |                  N/A |\r\n",
      "| 29%   33C    P8    20W / 250W |     11MiB / 10989MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1803      G   /usr/lib/xorg/Xorg                           571MiB |\r\n",
      "|    0      3647      G   compiz                                       252MiB |\r\n",
      "|    0      4398      G   ...AAGAAAAAAAAAAAAAIAAAAAAAAAAgAAAAAAAAA -   486MiB |\r\n",
      "|    0     14774      G   ...-token=4E3C744CC6A0CE73FFB07C1B1EA98318   579MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bayesian_benchmarks\n",
    "from bayesian_benchmarks.data import get_regression_data\n",
    "\n",
    "\n",
    "\n",
    "class ToyHiddenGPLayer(AbstractPyroHiddenGPLayer):\n",
    "    def __init__(self, input_dims, output_dims, name=\"\", inducing_points=50):\n",
    "        if type(inducing_points) == int:\n",
    "            inducing_points = torch.randn(output_dims, inducing_points, input_dims)\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(-2),\n",
    "            batch_size=output_dims\n",
    "        )\n",
    "\n",
    "        variational_strategy = PyroSamplingVariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy, input_dims, output_dims, True, name)\n",
    "\n",
    "        batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            MaternKernel(nu=2.5, batch_shape=batch_shape, ard_num_dims=input_dims),\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Double inheritance\n",
    "class ToyDeepGP(AbstractPyroDeepGP):\n",
    "    def __init__(self, input_dims, output_dims, total_num_data, hidden_gp_layers, likelihood, name=\"\", inducing_points=50):\n",
    "        inducing_points = torch.randn(output_dims, inducing_points, input_dims)\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(-2),\n",
    "            batch_size=output_dims\n",
    "        )\n",
    "\n",
    "        variational_strategy = PyroSamplingVariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            variational_strategy,\n",
    "            input_dims,\n",
    "            output_dims,\n",
    "            total_num_data,\n",
    "            hidden_gp_layers,\n",
    "            likelihood,\n",
    "            name\n",
    "        )\n",
    "\n",
    "        batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            MaternKernel(nu=2.5, batch_shape=batch_shape, ard_num_dims=input_dims),\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train = 14939   N_test = 1660\n",
      "cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "dataset='wilson_elevators'\n",
    "dataset = get_regression_data(dataset)\n",
    "N_train = dataset.X_train.shape[0]\n",
    "N_test = dataset.X_test.shape[0]\n",
    "D_X = dataset.D + 1\n",
    "train_x, train_y = torch.tensor(dataset.X_train).float().cuda(), torch.tensor(dataset.Y_train[:, 0]).float().cuda()\n",
    "test_x, test_y = torch.tensor(dataset.X_test).float().cuda(), torch.tensor(dataset.Y_test[:, 0]).float().cuda()\n",
    "print(\"N_train = %d   N_test = %d\" % (N_train, N_test))\n",
    "\n",
    "pyro.set_rng_seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "hidden_layer_width = 2\n",
    "num_inducing = 50\n",
    "inducing_points = (train_x[torch.randperm(N_train)[0:num_inducing], :])\n",
    "inducing_points = inducing_points.clone().data.cpu().numpy()\n",
    "inducing_points = torch.tensor(kmeans2(train_x.data.cpu().numpy(), inducing_points, minit='matrix')[0])\n",
    "inducing_points = inducing_points.unsqueeze(0).expand((hidden_layer_width,) + inducing_points.shape)\n",
    "inducing_points = inducing_points.to(device=train_x.device, dtype=train_x.dtype)\n",
    "\n",
    "print(train_x.device, test_x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = GaussianLikelihood().cuda()\n",
    "\n",
    "hidden_gp = ToyHiddenGPLayer(\n",
    "    train_x.size(-1),\n",
    "    hidden_layer_width,\n",
    "    name=\"layer1\",\n",
    "    inducing_points=inducing_points\n",
    ").to(device=train_x.device, dtype=train_x.dtype)\n",
    "deep_gp = ToyDeepGP(hidden_layer_width, 1, train_x.size(-2), [hidden_gp], likelihood, name=\"output_layer\",\n",
    "                    inducing_points=num_inducing).to(device=train_x.device, dtype=train_x.dtype)\n",
    "\n",
    "hidden_gp.variational_strategy.variational_distribution.variational_mean.data = \\\n",
    "    0.2 * torch.randn(hidden_gp.variational_strategy.variational_distribution.variational_mean.shape, device=train_x.device, dtype=train_x.dtype)\n",
    "deep_gp.variational_strategy.variational_distribution.variational_mean.data = \\\n",
    "    0.2 * torch.randn(deep_gp.variational_strategy.variational_distribution.variational_mean.shape, device=train_x.device, dtype=train_x.dtype)\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=80, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam({\"lr\": 0.03, \"betas\": (0.90, 0.999)})\n",
    "\n",
    "deep_gp.annealing = 0.1\n",
    "hidden_gp.annealing = 0.1\n",
    "\n",
    "USE_NF = True\n",
    "\n",
    "\n",
    "# different settings for u/f sampling versus f sampling (u marginalized out)\n",
    "deep_gp.EXACT = hidden_gp.EXACT = False\n",
    "num_particles = 4 if deep_gp.EXACT else 32\n",
    "annealing_epoch = 0 if deep_gp.EXACT else 100\n",
    "use_nf_epoch = 0 if USE_NF else 999999\n",
    "n_epochs = 300 if deep_gp.EXACT else 500\n",
    "\n",
    "elbo = TraceMeanField_ELBO(num_particles=num_particles, vectorize_particles=True, max_plate_nesting=1)\n",
    "svi = SVI(deep_gp.model, deep_gp.guide, optimizer, elbo)\n",
    "\n",
    "def ll_rmse(x, y, num_samples=50):\n",
    "    pred = deep_gp(x, num_samples=num_samples)[:, :, 0]\n",
    "    log_prob = torch.distributions.Normal(pred, (-0.5 * deep_gp.log_beta).exp()).log_prob(y)\n",
    "    log_prob = torch.logsumexp(log_prob - math.log(num_samples), dim=0).mean()\n",
    "    rmse = (pred.mean(0) - y).pow(2.0).mean().sqrt().item()\n",
    "    return log_prob, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training in EXACT=False mode with 32 particles\n",
      "[epoch 000] loss: 3.2658  test_ll: -1.496  train_ll: -1.563  test_rmse: 0.966  train_rmse: 1.000  obs_prec: 2.023\n"
     ]
    }
   ],
   "source": [
    "print(\"Beginning training in EXACT=%s mode with %d particles\" % (deep_gp.EXACT, num_particles))\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    if epoch_i == annealing_epoch:\n",
    "        deep_gp.annealing = 1.0\n",
    "        hidden_gp.annealing = 1.0\n",
    "        if epoch_i > 0:\n",
    "            print(\"Turning off KL annealing...\")\n",
    "    \n",
    "    if epoch_i == use_nf_epoch:\n",
    "        hidden_gp.use_nf = True\n",
    "        deep_gp.use_nf = True\n",
    "\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "#         print(list(hidden_gp.named_parameters()))\n",
    "        loss = svi.step(x_batch, y_batch)\n",
    "        epoch_loss = epoch_loss + loss / len(train_loader)\n",
    "    if epoch_i % 5 == 0 or epoch_i == (n_epochs - 1):\n",
    "        train_ll, train_rmse = ll_rmse(train_x, train_y)\n",
    "        test_ll, test_rmse = ll_rmse(test_x, test_y)\n",
    "        precision = pyro.param('log_beta').exp().item()\n",
    "        frmt = \"[epoch %03d] loss: %.4f  test_ll: %.3f  train_ll: %.3f  test_rmse: %.3f  train_rmse: %.3f  obs_prec: %.3f\"\n",
    "        print(frmt % (epoch_i, epoch_loss, test_ll, train_ll, test_rmse, train_rmse, precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 45500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
