{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to perform standard (Kronecker) multitask regression with kernels.IndexKernel.\n",
    "\n",
    "This differs from the [hadamard_multitask_gp_regression example notebook](https://github.com/cornellius-gp/gpytorch/blob/master/examples/hadamard_multitask_gp_regression.ipynb) in one key way:\n",
    "- Here, we assume that we want to learn **all tasks per input**. (The kernel that we learn is expressed as a Kronecker product of an input kernel and a task kernel).\n",
    "- In the other notebook, we assume that we want to learn one tasks per input.  For each input, we specify the task of the input that we care about. (The kernel in that notebook is the Hadamard product of an input kernel and a task kernel).\n",
    "\n",
    "Multitask regression, first introduced in [this paper](https://papers.nips.cc/paper/3189-multi-task-gaussian-process-prediction.pdf) learns similarities in the outputs simultaneously. It's useful when you are performing regression on multiple functions that share the same inputs, especially if they have similarities (such as being sinusodial). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training points are every 0.1 in [0,1] (note that they're the same for both tasks)\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "\n",
    "# y1 function is sin(2*pi*x) with noise N(0, 0.04)\n",
    "train_y1 = torch.sin(train_x.data * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n",
    "# y2 function is cos(2*pi*x) with noise N(0, 0.04)\n",
    "train_y2 = torch.cos(train_x.data * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n",
    "\n",
    "# Create a train_y which interleaves the two\n",
    "train_y = torch.stack([train_y1, train_y2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from gpytorch.kernels import RBFKernel, MultitaskKernel\n",
    "from gpytorch.means import ConstantMean, MultitaskMean\n",
    "from gpytorch.likelihoods import MultitaskGaussianLikelihood\n",
    "from gpytorch.random_variables import MultitaskGaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = MultitaskMean(ConstantMean(), n_tasks=2)\n",
    "        self.data_covar_module = RBFKernel()\n",
    "        self.covar_module = MultitaskKernel(self.data_covar_module, n_tasks=2, rank=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultitaskGaussianRandomVariable(mean_x, covar_x)\n",
    "\n",
    "# Gaussian likelihood is used for regression to give predictive mean+variance\n",
    "# and learn noise\n",
    "likelihood = MultitaskGaussianLikelihood(n_tasks=2)\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 48.059\n",
      "Iter 2/50 - Loss: 43.019\n",
      "Iter 3/50 - Loss: 38.098\n",
      "Iter 4/50 - Loss: 32.897\n",
      "Iter 5/50 - Loss: 27.928\n",
      "Iter 6/50 - Loss: 23.344\n",
      "Iter 7/50 - Loss: 18.607\n",
      "Iter 8/50 - Loss: 13.661\n",
      "Iter 9/50 - Loss: 9.280\n",
      "Iter 10/50 - Loss: 4.566\n",
      "Iter 11/50 - Loss: -0.570\n",
      "Iter 12/50 - Loss: -5.042\n",
      "Iter 13/50 - Loss: -10.119\n",
      "Iter 14/50 - Loss: -14.331\n",
      "Iter 15/50 - Loss: -19.019\n",
      "Iter 16/50 - Loss: -22.370\n",
      "Iter 17/50 - Loss: -26.618\n",
      "Iter 18/50 - Loss: -31.420\n",
      "Iter 19/50 - Loss: -35.941\n",
      "Iter 20/50 - Loss: -39.936\n",
      "Iter 21/50 - Loss: -43.314\n",
      "Iter 22/50 - Loss: -47.282\n",
      "Iter 23/50 - Loss: -51.082\n",
      "Iter 24/50 - Loss: -55.068\n",
      "Iter 25/50 - Loss: -59.269\n",
      "Iter 26/50 - Loss: -62.675\n",
      "Iter 27/50 - Loss: -65.660\n",
      "Iter 28/50 - Loss: -67.841\n",
      "Iter 29/50 - Loss: -70.802\n",
      "Iter 30/50 - Loss: -75.729\n",
      "Iter 31/50 - Loss: -77.334\n",
      "Iter 32/50 - Loss: -80.709\n",
      "Iter 33/50 - Loss: -81.387\n",
      "Iter 34/50 - Loss: -85.560\n",
      "Iter 35/50 - Loss: -86.400\n",
      "Iter 36/50 - Loss: -87.201\n",
      "Iter 37/50 - Loss: -88.868\n",
      "Iter 38/50 - Loss: -89.509\n",
      "Iter 39/50 - Loss: -89.932\n",
      "Iter 40/50 - Loss: -90.942\n",
      "Iter 41/50 - Loss: -88.448\n",
      "Iter 42/50 - Loss: -92.510\n",
      "Iter 43/50 - Loss: -91.196\n",
      "Iter 44/50 - Loss: -91.918\n",
      "Iter 45/50 - Loss: -92.964\n",
      "Iter 46/50 - Loss: -90.384\n",
      "Iter 47/50 - Loss: -90.682\n",
      "Iter 48/50 - Loss: -91.597\n",
      "Iter 49/50 - Loss: -90.941\n",
      "Iter 50/50 - Loss: -88.224\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "n_iter = 50\n",
    "for i in range(n_iter):\n",
    "    # Zero prev backpropped gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Make predictions from training data\n",
    "    # Again, note feeding duplicated x_data and indices indicating which task\n",
    "    output = model(train_x)\n",
    "    # TODO: Fix this view call!!\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, n_iter, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([302])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (200) must match the size of tensor b (100) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-92b2e0e6f46e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mobserved_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Get mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobserved_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Gardn/gpytorch/gpytorch/models/exact_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mn_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mlikelihood\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mprecomputed_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             )\n\u001b[1;32m    127\u001b[0m             predictive_covar, covar_cache = exact_predictive_covar(\n",
      "\u001b[0;32m/mnt/c/Users/Gardn/gpytorch/gpytorch/functions/__init__.py\u001b[0m in \u001b[0;36mexact_predictive_mean\u001b[0;34m(full_covar, full_mean, train_labels, n_train, likelihood, precomputed_cache)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mfull_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonLazyVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfull_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexact_predictive_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecomputed_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/Gardn/gpytorch/gpytorch/lazy/lazy_variable.py\u001b[0m in \u001b[0;36mexact_predictive_mean\u001b[0;34m(self, full_mean, train_labels, n_train, likelihood, precomputed_cache)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mgrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianRandomVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_train_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mtrain_labels_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0mtrain_labels_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels_offset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (200) must match the size of tensor b (100) at non-singleton dimension 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAADGCAYAAADytqj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa67b5aa128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize plots\n",
    "f, (y1_ax, y2_ax) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "# Test points every 0.02 in [0,1]\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    # Get mean\n",
    "    mean = observed_pred.mean()\n",
    "    # Get lower and upper confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "# This contains predictions for both tasks, flattened out\n",
    "# The first half of the predictions is for the first task\n",
    "# The second half is for the second task\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot():\n",
    "\n",
    "    # Plot training data as black stars\n",
    "    y1_ax.plot(train_x.detach().numpy(), train_y1.detach().numpy(), 'k*')\n",
    "    # Predictive mean as blue line\n",
    "    y1_ax.plot(test_x.numpy(), mean[:, 0].numpy(), 'b')\n",
    "    # Shade in confidence \n",
    "    y1_ax.fill_between(test_x.numpy(), lower[:, 0].numpy(), upper[:, 0].numpy(), alpha=0.5)\n",
    "    y1_ax.set_ylim([-3, 3])\n",
    "    y1_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    y1_ax.set_title('Observed Values (Likelihood)')\n",
    "    \n",
    "    # Plot training data as black stars\n",
    "    y2_ax.plot(train_x.detach().numpy(), train_y2.detach().numpy(), 'k*')\n",
    "    # Predictive mean as blue line\n",
    "    y2_ax.plot(test_x.numpy(), mean[:, 1].numpy(), 'b')\n",
    "    # Shade in confidence \n",
    "    y2_ax.fill_between(test_x.numpy(), lower[:, 1].numpy(), upper[:, 1].numpy(), alpha=0.5)\n",
    "    y2_ax.set_ylim([-3, 3])\n",
    "    y2_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    y2_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# Plot both tasks\n",
    "ax_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
