{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Gaussian Processes with Doubly Stochastic VI\n",
    "\n",
    "In this notebook, we provide a GPyTorch implementation of deep Gaussian processes, where training and inference is performed using the method of Salimbeni et al., 2017 (https://arxiv.org/abs/1705.08933) adapted to CG-based inference.\n",
    "\n",
    "We'll be training a simple two layer deep GP on the `elevators` UCI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch.nn import Linear\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution, MeanFieldVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ApproximateGP, GP\n",
    "from gpytorch.mlls import VariationalELBO, AddedLossTerm, PredictiveLogLikelihood\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from scipy.cluster.vq import kmeans2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models.deep_gps import AbstractDeepGPLayer, AbstractDeepGP, DeepLikelihood\n",
    "from gpytorch.models.deep_gps.predictive_deep_gp import AbstractPredictiveDeepGPLayer, AbstractDeepGP, DeepPredictiveGaussianLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `elevators` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a ~400 KB dataset file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.isfile('elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', 'elevators.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "N = data.shape[0]\n",
    "np.random.seed(0)\n",
    "data = data[np.random.permutation(np.arange(N)),:]\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()\n",
    "\n",
    "mean = train_x.mean(dim=-2, keepdim=True)\n",
    "std = train_x.std(dim=-2, keepdim=True) + 1e-6\n",
    "train_x = (train_x - mean) / std\n",
    "test_x = (test_x - mean) / std\n",
    "\n",
    "mean,std = train_y.mean(),train_y.std()\n",
    "train_y = (train_y - mean) / std\n",
    "test_y = (test_y - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining GP layers\n",
    "\n",
    "In GPyTorch, defining a GP involves extending one of our abstract GP models and defining a `forward` method that returns the prior. For deep GPs, things are similar, but there are two abstract GP models that must be overwritten: one for hidden layers and one for the deep GP model itself.\n",
    "\n",
    "In the next cell, we define an example deep GP hidden layer. This looks very similar to every other variational GP you might define. However, there are a few key differences:\n",
    "\n",
    "1. Instead of extending `ApproximateGP`, we extend `AbstractDeepGPLayer`.\n",
    "2. `AbstractDeepGPLayers` need a number of input dimensions, a number of output dimensions, and a number of samples. This is kind of like a linear layer in a standard neural network -- `input_dims` defines how many inputs this hidden layer will expect, and `output_dims` defines how many hidden GPs to create outputs for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3\n",
    "\n",
    "class ToyDeepGPHiddenLayer(AbstractPredictiveDeepGPLayer):\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, inducing_points=None, mean_type='constant'):\n",
    "        if inducing_points is None:\n",
    "            if output_dims is None:\n",
    "                inducing_points = torch.randn(num_inducing, input_dims)\n",
    "            else:\n",
    "                inducing_points = torch.randn(output_dims, num_inducing, input_dims)\n",
    "\n",
    "        variational_distribution = MeanFieldVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=torch.Size([output_dims]) if output_dims is not None else torch.Size([])\n",
    "        )\n",
    "\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims, Q)\n",
    "        \n",
    "        if mean_type == 'constant':\n",
    "            self.mean_module = ConstantMean()\n",
    "        else:\n",
    "            self.mean_module = LinearMean(input_dims)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_size=output_dims, ard_num_dims=input_dims),\n",
    "            batch_size=output_dims, ard_num_dims=None\n",
    "        )\n",
    "        \n",
    "        # self.linear_layer = Linear(input_dims, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def __call__(self, x, *other_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Overriding __call__ isn't strictly necessary, but it lets us add concatenation based skip connections\n",
    "        easily. For example, hidden_layer2(hidden_layer1_outputs, inputs) will pass the concatenation of the first\n",
    "        hidden layer's outputs and the input data to hidden_layer2.\n",
    "        \"\"\"\n",
    "        if len(other_inputs):\n",
    "            if isinstance(x, gpytorch.distributions.MultitaskMultivariateNormal):\n",
    "                x = x.rsample()\n",
    "\n",
    "            processed_inputs = [\n",
    "                inp.unsqueeze(0).expand(self.num_samples, *inp.shape)\n",
    "                for inp in other_inputs\n",
    "            ]\n",
    "\n",
    "            x = torch.cat([x] + processed_inputs, dim=-1)\n",
    "\n",
    "        return super().__call__(x, are_samples=bool(len(other_inputs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "Now that we've defined a class for our hidden layers and a class for our output layer, we can build our deep GP. To do this, we create a `Module` whose forward is simply responsible for forwarding through the various layers.\n",
    "\n",
    "This also allows for various network connectivities easily. For example calling,\n",
    "```\n",
    "hidden_rep2 = self.second_hidden_layer(hidden_rep1, inputs)\n",
    "```\n",
    "in forward would cause the second hidden layer to use both the output of the first hidden layer and the input data as inputs, concatenating the two together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGP(AbstractDeepGP):\n",
    "    def __init__(self, train_x_shape, inducing_points):\n",
    "        hidden_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=train_x_shape[-1],\n",
    "            output_dims=3,\n",
    "            mean_type='linear',\n",
    "            inducing_points=inducing_points,\n",
    "        )\n",
    "        \n",
    "        # second_layer_inds = hidden_layer(inducing_points.cpu())\n",
    "        \n",
    "        last_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dims,\n",
    "            output_dims=None,\n",
    "            mean_type='constant',\n",
    "            inducing_points=None,  # No real initialization here.\n",
    "        )\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.last_layer = last_layer\n",
    "        self.likelihood = DeepPredictiveGaussianLikelihood(hidden_layer.output_dims, Q)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        hidden_rep1 = self.hidden_layer(inputs)\n",
    "        output = self.last_layer(hidden_rep1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        with gpytorch.settings.fast_computations(log_prob=False, solves=False), torch.no_grad():\n",
    "            mus = []\n",
    "            variances = []\n",
    "            lls = []\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                preds = model.likelihood(model(x_batch))\n",
    "                mus.append(preds.mean)\n",
    "                variances.append(preds.variance)\n",
    "                lls.append(model.likelihood.log_marginal(y_batch, model(x_batch)))\n",
    "        \n",
    "        return torch.cat(mus, dim=-1), torch.cat(variances, dim=-1), torch.cat(lls, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = train_x.size(-2)\n",
    "inducing_points = (train_x[torch.randperm(N_train)[0:128], :]).clone().data.cpu().numpy()\n",
    "inducing_points = torch.tensor(kmeans2(train_x.cpu().numpy(), inducing_points, minit='matrix')[0]).cuda()\n",
    "\n",
    "model = DeepGP(train_x.shape, inducing_points=inducing_points).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "Because deep GPs use some amounts of internal sampling (even in the stochastic variational setting), we need to handle the likelihood in a slightly different way. In the future, we anticipate `DeepLikelihood` being a general wrapper around an arbitrary likelihood once likelihoods become a little more general purpose, but for now we simply define a `DeepGaussianLikelihood` to use for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "The training loop for a deep GP looks similar to a standard GP model with stochastic variational inference, but there are a few differences:\n",
    "\n",
    "1. Because the output of a deep GP is actually num_outputs x num_samples Gaussians rather than a single Gaussian, we need to expand the labels to be num_outputs x num_samples x minibatch_size before calling the ELBO.\n",
    "2. Because deep GPs involve a few added loss terms and normalize slightly differently, we created the `VariationalELBO` above with `combine_terms=False`. This just lets us do the extra normalization we need to make the math work out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/13] - Loss: 1.422 - - Time: 0.633\n",
      "Epoch 1 [1/13] - Loss: 1.422 - - Time: 0.019\n",
      "Epoch 1 [2/13] - Loss: 1.436 - - Time: 0.020\n",
      "Epoch 1 [3/13] - Loss: 1.447 - - Time: 0.020\n",
      "Epoch 1 [4/13] - Loss: 1.440 - - Time: 0.020\n",
      "Epoch 1 [5/13] - Loss: 1.490 - - Time: 0.019\n",
      "Epoch 1 [6/13] - Loss: 1.427 - - Time: 0.020\n",
      "Epoch 1 [7/13] - Loss: 1.444 - - Time: 0.019\n",
      "Epoch 1 [8/13] - Loss: 1.451 - - Time: 0.019\n",
      "Epoch 1 [9/13] - Loss: 1.426 - - Time: 0.019\n",
      "Epoch 1 [10/13] - Loss: 1.407 - - Time: 0.019\n",
      "Epoch 1 [11/13] - Loss: 1.406 - - Time: 0.019\n",
      "Epoch 1 [12/13] - Loss: 1.419 - - Time: 0.018\n",
      "Epoch 2 [0/13] - Loss: 1.418 - - Time: 0.018\n",
      "Epoch 2 [1/13] - Loss: 1.410 - - Time: 0.018\n",
      "Epoch 2 [2/13] - Loss: 1.415 - - Time: 0.018\n",
      "Epoch 2 [3/13] - Loss: 1.397 - - Time: 0.018\n",
      "Epoch 2 [4/13] - Loss: 1.423 - - Time: 0.018\n",
      "Epoch 2 [5/13] - Loss: 1.385 - - Time: 0.019\n",
      "Epoch 2 [6/13] - Loss: 1.424 - - Time: 0.019\n",
      "Epoch 2 [7/13] - Loss: 1.433 - - Time: 0.019\n",
      "Epoch 2 [8/13] - Loss: 1.495 - - Time: 0.017\n",
      "Epoch 2 [9/13] - Loss: 1.345 - - Time: 0.018\n",
      "Epoch 2 [10/13] - Loss: 1.389 - - Time: 0.017\n",
      "Epoch 2 [11/13] - Loss: 1.421 - - Time: 0.017\n",
      "Epoch 2 [12/13] - Loss: 1.427 - - Time: 0.017\n",
      "Epoch 3 [0/13] - Loss: 1.443 - - Time: 0.018\n",
      "Epoch 3 [1/13] - Loss: 1.370 - - Time: 0.018\n",
      "Epoch 3 [2/13] - Loss: 1.394 - - Time: 0.018\n",
      "Epoch 3 [3/13] - Loss: 1.404 - - Time: 0.018\n",
      "Epoch 3 [4/13] - Loss: 1.344 - - Time: 0.018\n",
      "Epoch 3 [5/13] - Loss: 1.397 - - Time: 0.018\n",
      "Epoch 3 [6/13] - Loss: 1.427 - - Time: 0.018\n",
      "Epoch 3 [7/13] - Loss: 1.434 - - Time: 0.019\n",
      "Epoch 3 [8/13] - Loss: 1.416 - - Time: 0.019\n",
      "Epoch 3 [9/13] - Loss: 1.390 - - Time: 0.019\n",
      "Epoch 3 [10/13] - Loss: 1.368 - - Time: 0.020\n",
      "Epoch 3 [11/13] - Loss: 1.372 - - Time: 0.020\n",
      "Epoch 3 [12/13] - Loss: 1.398 - - Time: 0.020\n",
      "Epoch 4 [0/13] - Loss: 1.367 - - Time: 0.020\n",
      "Epoch 4 [1/13] - Loss: 1.378 - - Time: 0.020\n",
      "Epoch 4 [2/13] - Loss: 1.364 - - Time: 0.020\n",
      "Epoch 4 [3/13] - Loss: 1.429 - - Time: 0.019\n",
      "Epoch 4 [4/13] - Loss: 1.400 - - Time: 0.018\n",
      "Epoch 4 [5/13] - Loss: 1.359 - - Time: 0.019\n",
      "Epoch 4 [6/13] - Loss: 1.343 - - Time: 0.018\n",
      "Epoch 4 [7/13] - Loss: 1.414 - - Time: 0.019\n",
      "Epoch 4 [8/13] - Loss: 1.353 - - Time: 0.020\n",
      "Epoch 4 [9/13] - Loss: 1.395 - - Time: 0.022\n",
      "Epoch 4 [10/13] - Loss: 1.369 - - Time: 0.022\n",
      "Epoch 4 [11/13] - Loss: 1.329 - - Time: 0.024\n",
      "Epoch 4 [12/13] - Loss: 1.401 - - Time: 0.021\n",
      "Epoch 5 [0/13] - Loss: 1.379 - - Time: 0.022\n",
      "Epoch 5 [1/13] - Loss: 1.353 - - Time: 0.022\n",
      "Epoch 5 [2/13] - Loss: 1.352 - - Time: 0.022\n",
      "Epoch 5 [3/13] - Loss: 1.382 - - Time: 0.022\n",
      "Epoch 5 [4/13] - Loss: 1.397 - - Time: 0.022\n",
      "Epoch 5 [5/13] - Loss: 1.324 - - Time: 0.024\n",
      "Epoch 5 [6/13] - Loss: 1.345 - - Time: 0.023\n",
      "Epoch 5 [7/13] - Loss: 1.283 - - Time: 0.023\n",
      "Epoch 5 [8/13] - Loss: 1.343 - - Time: 0.024\n",
      "Epoch 5 [9/13] - Loss: 1.286 - - Time: 0.023\n",
      "Epoch 5 [10/13] - Loss: 1.343 - - Time: 0.023\n",
      "Epoch 5 [11/13] - Loss: 1.312 - - Time: 0.026\n",
      "Epoch 5 [12/13] - Loss: 1.304 - - Time: 0.023\n",
      "Epoch 6 [0/13] - Loss: 1.322 - - Time: 0.027\n",
      "Epoch 6 [1/13] - Loss: 1.294 - - Time: 0.026\n",
      "Epoch 6 [2/13] - Loss: 1.283 - - Time: 0.023\n",
      "Epoch 6 [3/13] - Loss: 1.278 - - Time: 0.022\n",
      "Epoch 6 [4/13] - Loss: 1.263 - - Time: 0.026\n",
      "Epoch 6 [5/13] - Loss: 1.242 - - Time: 0.023\n",
      "Epoch 6 [6/13] - Loss: 1.233 - - Time: 0.024\n",
      "Epoch 6 [7/13] - Loss: 1.204 - - Time: 0.022\n",
      "Epoch 6 [8/13] - Loss: 1.284 - - Time: 0.023\n",
      "Epoch 6 [9/13] - Loss: 1.269 - - Time: 0.024\n",
      "Epoch 6 [10/13] - Loss: 1.229 - - Time: 0.025\n",
      "Epoch 6 [11/13] - Loss: 1.210 - - Time: 0.023\n",
      "Epoch 6 [12/13] - Loss: 1.257 - - Time: 0.023\n",
      "Epoch 7 [0/13] - Loss: 1.205 - - Time: 0.025\n",
      "Epoch 7 [1/13] - Loss: 1.128 - - Time: 0.022\n",
      "Epoch 7 [2/13] - Loss: 1.160 - - Time: 0.022\n",
      "Epoch 7 [3/13] - Loss: 1.194 - - Time: 0.024\n",
      "Epoch 7 [4/13] - Loss: 1.112 - - Time: 0.023\n",
      "Epoch 7 [5/13] - Loss: 1.170 - - Time: 0.022\n",
      "Epoch 7 [6/13] - Loss: 1.125 - - Time: 0.025\n",
      "Epoch 7 [7/13] - Loss: 1.114 - - Time: 0.023\n",
      "Epoch 7 [8/13] - Loss: 1.077 - - Time: 0.023\n",
      "Epoch 7 [9/13] - Loss: 1.084 - - Time: 0.024\n",
      "Epoch 7 [10/13] - Loss: 1.120 - - Time: 0.022\n",
      "Epoch 7 [11/13] - Loss: 1.092 - - Time: 0.023\n",
      "Epoch 7 [12/13] - Loss: 1.099 - - Time: 0.028\n",
      "Epoch 8 [0/13] - Loss: 1.033 - - Time: 0.022\n",
      "Epoch 8 [1/13] - Loss: 0.990 - - Time: 0.022\n",
      "Epoch 8 [2/13] - Loss: 1.013 - - Time: 0.025\n",
      "Epoch 8 [3/13] - Loss: 1.002 - - Time: 0.022\n",
      "Epoch 8 [4/13] - Loss: 1.038 - - Time: 0.022\n",
      "Epoch 8 [5/13] - Loss: 1.003 - - Time: 0.024\n",
      "Epoch 8 [6/13] - Loss: 0.960 - - Time: 0.022\n",
      "Epoch 8 [7/13] - Loss: 0.983 - - Time: 0.021\n",
      "Epoch 8 [8/13] - Loss: 0.922 - - Time: 0.026\n",
      "Epoch 8 [9/13] - Loss: 0.929 - - Time: 0.021\n",
      "Epoch 8 [10/13] - Loss: 0.964 - - Time: 0.021\n",
      "Epoch 8 [11/13] - Loss: 0.947 - - Time: 0.021\n",
      "Epoch 8 [12/13] - Loss: 0.897 - - Time: 0.022\n",
      "Epoch 9 [0/13] - Loss: 0.863 - - Time: 0.021\n",
      "Epoch 9 [1/13] - Loss: 0.888 - - Time: 0.021\n",
      "Epoch 9 [2/13] - Loss: 0.846 - - Time: 0.021\n",
      "Epoch 9 [3/13] - Loss: 0.885 - - Time: 0.022\n",
      "Epoch 9 [4/13] - Loss: 0.840 - - Time: 0.021\n",
      "Epoch 9 [5/13] - Loss: 0.843 - - Time: 0.021\n",
      "Epoch 9 [6/13] - Loss: 0.846 - - Time: 0.021\n",
      "Epoch 9 [7/13] - Loss: 0.811 - - Time: 0.023\n",
      "Epoch 9 [8/13] - Loss: 0.813 - - Time: 0.021\n",
      "Epoch 9 [9/13] - Loss: 0.785 - - Time: 0.021\n",
      "Epoch 9 [10/13] - Loss: 0.814 - - Time: 0.021\n",
      "Epoch 9 [11/13] - Loss: 0.789 - - Time: 0.021\n",
      "Epoch 9 [12/13] - Loss: 0.835 - - Time: 0.021\n",
      "Epoch 10 [0/13] - Loss: 0.732 - - Time: 0.021\n",
      "Epoch 10 [1/13] - Loss: 0.775 - - Time: 0.022\n",
      "Epoch 10 [2/13] - Loss: 0.703 - - Time: 0.021\n",
      "Epoch 10 [3/13] - Loss: 0.742 - - Time: 0.021\n",
      "Epoch 10 [4/13] - Loss: 0.765 - - Time: 0.021\n",
      "Epoch 10 [5/13] - Loss: 0.763 - - Time: 0.022\n",
      "Epoch 10 [6/13] - Loss: 0.692 - - Time: 0.021\n",
      "Epoch 10 [7/13] - Loss: 0.696 - - Time: 0.021\n",
      "Epoch 10 [8/13] - Loss: 0.712 - - Time: 0.021\n",
      "Epoch 10 [9/13] - Loss: 0.703 - - Time: 0.021\n",
      "Epoch 10 [10/13] - Loss: 0.680 - - Time: 0.021\n",
      "Epoch 10 [11/13] - Loss: 0.616 - - Time: 0.021\n",
      "Epoch 10 [12/13] - Loss: 0.670 - - Time: 0.020\n",
      "Epoch 11 [0/13] - Loss: 0.621 - - Time: 0.021\n",
      "Epoch 11 [1/13] - Loss: 0.596 - - Time: 0.022\n",
      "Epoch 11 [2/13] - Loss: 0.614 - - Time: 0.021\n",
      "Epoch 11 [3/13] - Loss: 0.614 - - Time: 0.021\n",
      "Epoch 11 [4/13] - Loss: 0.621 - - Time: 0.021\n",
      "Epoch 11 [5/13] - Loss: 0.639 - - Time: 0.021\n",
      "Epoch 11 [6/13] - Loss: 0.585 - - Time: 0.020\n",
      "Epoch 11 [7/13] - Loss: 0.596 - - Time: 0.020\n",
      "Epoch 11 [8/13] - Loss: 0.655 - - Time: 0.020\n",
      "Epoch 11 [9/13] - Loss: 0.556 - - Time: 0.020\n",
      "Epoch 11 [10/13] - Loss: 0.587 - - Time: 0.020\n",
      "Epoch 11 [11/13] - Loss: 0.590 - - Time: 0.020\n",
      "Epoch 11 [12/13] - Loss: 0.565 - - Time: 0.020\n",
      "Epoch 12 [0/13] - Loss: 0.519 - - Time: 0.021\n",
      "Epoch 12 [1/13] - Loss: 0.547 - - Time: 0.021\n",
      "Epoch 12 [2/13] - Loss: 0.563 - - Time: 0.020\n",
      "Epoch 12 [3/13] - Loss: 0.539 - - Time: 0.020\n",
      "Epoch 12 [4/13] - Loss: 0.557 - - Time: 0.021\n",
      "Epoch 12 [5/13] - Loss: 0.509 - - Time: 0.020\n",
      "Epoch 12 [6/13] - Loss: 0.519 - - Time: 0.020\n",
      "Epoch 12 [7/13] - Loss: 0.523 - - Time: 0.020\n",
      "Epoch 12 [8/13] - Loss: 0.482 - - Time: 0.021\n",
      "Epoch 12 [9/13] - Loss: 0.525 - - Time: 0.020\n",
      "Epoch 12 [10/13] - Loss: 0.481 - - Time: 0.021\n",
      "Epoch 12 [11/13] - Loss: 0.526 - - Time: 0.020\n",
      "Epoch 12 [12/13] - Loss: 0.454 - - Time: 0.020\n",
      "Epoch 13 [0/13] - Loss: 0.459 - - Time: 0.021\n",
      "Epoch 13 [1/13] - Loss: 0.437 - - Time: 0.020\n",
      "Epoch 13 [2/13] - Loss: 0.491 - - Time: 0.020\n",
      "Epoch 13 [3/13] - Loss: 0.446 - - Time: 0.021\n",
      "Epoch 13 [4/13] - Loss: 0.438 - - Time: 0.020\n",
      "Epoch 13 [5/13] - Loss: 0.442 - - Time: 0.021\n",
      "Epoch 13 [6/13] - Loss: 0.455 - - Time: 0.021\n",
      "Epoch 13 [7/13] - Loss: 0.498 - - Time: 0.021\n",
      "Epoch 13 [8/13] - Loss: 0.480 - - Time: 0.021\n",
      "Epoch 13 [9/13] - Loss: 0.465 - - Time: 0.021\n",
      "Epoch 13 [10/13] - Loss: 0.477 - - Time: 0.021\n",
      "Epoch 13 [11/13] - Loss: 0.471 - - Time: 0.020\n",
      "Epoch 13 [12/13] - Loss: 0.436 - - Time: 0.020\n",
      "Epoch 14 [0/13] - Loss: 0.409 - - Time: 0.021\n",
      "Epoch 14 [1/13] - Loss: 0.457 - - Time: 0.020\n",
      "Epoch 14 [2/13] - Loss: 0.462 - - Time: 0.021\n",
      "Epoch 14 [3/13] - Loss: 0.454 - - Time: 0.020\n",
      "Epoch 14 [4/13] - Loss: 0.426 - - Time: 0.020\n",
      "Epoch 14 [5/13] - Loss: 0.400 - - Time: 0.020\n",
      "Epoch 14 [6/13] - Loss: 0.428 - - Time: 0.022\n",
      "Epoch 14 [7/13] - Loss: 0.421 - - Time: 0.021\n",
      "Epoch 14 [8/13] - Loss: 0.426 - - Time: 0.021\n",
      "Epoch 14 [9/13] - Loss: 0.422 - - Time: 0.021\n",
      "Epoch 14 [10/13] - Loss: 0.381 - - Time: 0.021\n",
      "Epoch 14 [11/13] - Loss: 0.370 - - Time: 0.021\n",
      "Epoch 14 [12/13] - Loss: 0.427 - - Time: 0.021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 [0/13] - Loss: 0.384 - - Time: 0.021\n",
      "Epoch 15 [1/13] - Loss: 0.381 - - Time: 0.020\n",
      "Epoch 15 [2/13] - Loss: 0.381 - - Time: 0.021\n",
      "Epoch 15 [3/13] - Loss: 0.378 - - Time: 0.020\n",
      "Epoch 15 [4/13] - Loss: 0.397 - - Time: 0.020\n",
      "Epoch 15 [5/13] - Loss: 0.412 - - Time: 0.020\n",
      "Epoch 15 [6/13] - Loss: 0.434 - - Time: 0.020\n",
      "Epoch 15 [7/13] - Loss: 0.367 - - Time: 0.020\n",
      "Epoch 15 [8/13] - Loss: 0.371 - - Time: 0.020\n",
      "Epoch 15 [9/13] - Loss: 0.376 - - Time: 0.025\n",
      "Epoch 15 [10/13] - Loss: 0.372 - - Time: 0.021\n",
      "Epoch 15 [11/13] - Loss: 0.428 - - Time: 0.021\n",
      "Epoch 15 [12/13] - Loss: 0.426 - - Time: 0.020\n",
      "Epoch 16 [0/13] - Loss: 0.359 - - Time: 0.021\n",
      "Epoch 16 [1/13] - Loss: 0.349 - - Time: 0.021\n",
      "Epoch 16 [2/13] - Loss: 0.429 - - Time: 0.021\n",
      "Epoch 16 [3/13] - Loss: 0.404 - - Time: 0.021\n",
      "Epoch 16 [4/13] - Loss: 0.362 - - Time: 0.021\n",
      "Epoch 16 [5/13] - Loss: 0.326 - - Time: 0.021\n",
      "Epoch 16 [6/13] - Loss: 0.352 - - Time: 0.021\n",
      "Epoch 16 [7/13] - Loss: 0.364 - - Time: 0.021\n",
      "Epoch 16 [8/13] - Loss: 0.335 - - Time: 0.021\n",
      "Epoch 16 [9/13] - Loss: 0.383 - - Time: 0.021\n",
      "Epoch 16 [10/13] - Loss: 0.383 - - Time: 0.021\n",
      "Epoch 16 [11/13] - Loss: 0.389 - - Time: 0.022\n",
      "Epoch 16 [12/13] - Loss: 0.432 - - Time: 0.021\n",
      "Epoch 17 [0/13] - Loss: 0.348 - - Time: 0.021\n",
      "Epoch 17 [1/13] - Loss: 0.388 - - Time: 0.021\n",
      "Epoch 17 [2/13] - Loss: 0.334 - - Time: 0.021\n",
      "Epoch 17 [3/13] - Loss: 0.323 - - Time: 0.021\n",
      "Epoch 17 [4/13] - Loss: 0.332 - - Time: 0.021\n",
      "Epoch 17 [5/13] - Loss: 0.362 - - Time: 0.021\n",
      "Epoch 17 [6/13] - Loss: 0.392 - - Time: 0.021\n",
      "Epoch 17 [7/13] - Loss: 0.369 - - Time: 0.021\n",
      "Epoch 17 [8/13] - Loss: 0.383 - - Time: 0.021\n",
      "Epoch 17 [9/13] - Loss: 0.309 - - Time: 0.021\n",
      "Epoch 17 [10/13] - Loss: 0.381 - - Time: 0.021\n",
      "Epoch 17 [11/13] - Loss: 0.376 - - Time: 0.022\n",
      "Epoch 17 [12/13] - Loss: 0.366 - - Time: 0.021\n",
      "Epoch 18 [0/13] - Loss: 0.380 - - Time: 0.021\n",
      "Epoch 18 [1/13] - Loss: 0.332 - - Time: 0.021\n",
      "Epoch 18 [2/13] - Loss: 0.330 - - Time: 0.021\n",
      "Epoch 18 [3/13] - Loss: 0.338 - - Time: 0.021\n",
      "Epoch 18 [4/13] - Loss: 0.310 - - Time: 0.021\n",
      "Epoch 18 [5/13] - Loss: 0.367 - - Time: 0.021\n",
      "Epoch 18 [6/13] - Loss: 0.336 - - Time: 0.021\n",
      "Epoch 18 [7/13] - Loss: 0.352 - - Time: 0.021\n",
      "Epoch 18 [8/13] - Loss: 0.361 - - Time: 0.021\n",
      "Epoch 18 [9/13] - Loss: 0.353 - - Time: 0.021\n",
      "Epoch 18 [10/13] - Loss: 0.333 - - Time: 0.021\n",
      "Epoch 18 [11/13] - Loss: 0.293 - - Time: 0.021\n",
      "Epoch 18 [12/13] - Loss: 0.387 - - Time: 0.021\n",
      "Epoch 19 [0/13] - Loss: 0.317 - - Time: 0.021\n",
      "Epoch 19 [1/13] - Loss: 0.376 - - Time: 0.021\n",
      "Epoch 19 [2/13] - Loss: 0.332 - - Time: 0.021\n",
      "Epoch 19 [3/13] - Loss: 0.354 - - Time: 0.021\n",
      "Epoch 19 [4/13] - Loss: 0.341 - - Time: 0.021\n",
      "Epoch 19 [5/13] - Loss: 0.351 - - Time: 0.021\n",
      "Epoch 19 [6/13] - Loss: 0.315 - - Time: 0.021\n",
      "Epoch 19 [7/13] - Loss: 0.337 - - Time: 0.021\n",
      "Epoch 19 [8/13] - Loss: 0.331 - - Time: 0.021\n",
      "Epoch 19 [9/13] - Loss: 0.320 - - Time: 0.021\n",
      "Epoch 19 [10/13] - Loss: 0.366 - - Time: 0.021\n",
      "Epoch 19 [11/13] - Loss: 0.379 - - Time: 0.020\n",
      "Epoch 19 [12/13] - Loss: 0.316 - - Time: 0.020\n",
      "Epoch 20 [0/13] - Loss: 0.352 - - Time: 0.020\n",
      "Epoch 20 [1/13] - Loss: 0.360 - - Time: 0.020\n",
      "Epoch 20 [2/13] - Loss: 0.336 - - Time: 0.020\n",
      "Epoch 20 [3/13] - Loss: 0.335 - - Time: 0.020\n",
      "Epoch 20 [4/13] - Loss: 0.345 - - Time: 0.022\n",
      "Epoch 20 [5/13] - Loss: 0.306 - - Time: 0.021\n",
      "Epoch 20 [6/13] - Loss: 0.337 - - Time: 0.021\n",
      "Epoch 20 [7/13] - Loss: 0.285 - - Time: 0.021\n",
      "Epoch 20 [8/13] - Loss: 0.319 - - Time: 0.021\n",
      "Epoch 20 [9/13] - Loss: 0.368 - - Time: 0.021\n",
      "Epoch 20 [10/13] - Loss: 0.313 - - Time: 0.021\n",
      "Epoch 20 [11/13] - Loss: 0.287 - - Time: 0.021\n",
      "Epoch 20 [12/13] - Loss: 0.342 - - Time: 0.021\n",
      "Epoch 21 [0/13] - Loss: 0.354 - - Time: 0.020\n",
      "Epoch 21 [1/13] - Loss: 0.300 - - Time: 0.021\n",
      "Epoch 21 [2/13] - Loss: 0.316 - - Time: 0.021\n",
      "Epoch 21 [3/13] - Loss: 0.325 - - Time: 0.021\n",
      "Epoch 21 [4/13] - Loss: 0.280 - - Time: 0.020\n",
      "Epoch 21 [5/13] - Loss: 0.306 - - Time: 0.020\n",
      "Epoch 21 [6/13] - Loss: 0.333 - - Time: 0.020\n",
      "Epoch 21 [7/13] - Loss: 0.324 - - Time: 0.021\n",
      "Epoch 21 [8/13] - Loss: 0.298 - - Time: 0.021\n",
      "Epoch 21 [9/13] - Loss: 0.378 - - Time: 0.021\n",
      "Epoch 21 [10/13] - Loss: 0.333 - - Time: 0.021\n",
      "Epoch 21 [11/13] - Loss: 0.316 - - Time: 0.021\n",
      "Epoch 21 [12/13] - Loss: 0.275 - - Time: 0.020\n",
      "Epoch 22 [0/13] - Loss: 0.340 - - Time: 0.020\n",
      "Epoch 22 [1/13] - Loss: 0.289 - - Time: 0.020\n",
      "Epoch 22 [2/13] - Loss: 0.310 - - Time: 0.021\n",
      "Epoch 22 [3/13] - Loss: 0.307 - - Time: 0.020\n",
      "Epoch 22 [4/13] - Loss: 0.286 - - Time: 0.021\n",
      "Epoch 22 [5/13] - Loss: 0.286 - - Time: 0.020\n",
      "Epoch 22 [6/13] - Loss: 0.321 - - Time: 0.021\n",
      "Epoch 22 [7/13] - Loss: 0.310 - - Time: 0.021\n",
      "Epoch 22 [8/13] - Loss: 0.347 - - Time: 0.021\n",
      "Epoch 22 [9/13] - Loss: 0.324 - - Time: 0.021\n",
      "Epoch 22 [10/13] - Loss: 0.327 - - Time: 0.021\n",
      "Epoch 22 [11/13] - Loss: 0.320 - - Time: 0.021\n",
      "Epoch 22 [12/13] - Loss: 0.287 - - Time: 0.020\n",
      "Epoch 23 [0/13] - Loss: 0.293 - - Time: 0.021\n",
      "Epoch 23 [1/13] - Loss: 0.269 - - Time: 0.021\n",
      "Epoch 23 [2/13] - Loss: 0.342 - - Time: 0.021\n",
      "Epoch 23 [3/13] - Loss: 0.278 - - Time: 0.021\n",
      "Epoch 23 [4/13] - Loss: 0.260 - - Time: 0.021\n",
      "Epoch 23 [5/13] - Loss: 0.348 - - Time: 0.021\n",
      "Epoch 23 [6/13] - Loss: 0.298 - - Time: 0.021\n",
      "Epoch 23 [7/13] - Loss: 0.304 - - Time: 0.021\n",
      "Epoch 23 [8/13] - Loss: 0.323 - - Time: 0.021\n",
      "Epoch 23 [9/13] - Loss: 0.316 - - Time: 0.021\n",
      "Epoch 23 [10/13] - Loss: 0.308 - - Time: 0.021\n",
      "Epoch 23 [11/13] - Loss: 0.303 - - Time: 0.021\n",
      "Epoch 23 [12/13] - Loss: 0.316 - - Time: 0.021\n",
      "Epoch 24 [0/13] - Loss: 0.298 - - Time: 0.021\n",
      "Epoch 24 [1/13] - Loss: 0.304 - - Time: 0.021\n",
      "Epoch 24 [2/13] - Loss: 0.317 - - Time: 0.021\n",
      "Epoch 24 [3/13] - Loss: 0.338 - - Time: 0.021\n",
      "Epoch 24 [4/13] - Loss: 0.307 - - Time: 0.021\n",
      "Epoch 24 [5/13] - Loss: 0.251 - - Time: 0.021\n",
      "Epoch 24 [6/13] - Loss: 0.266 - - Time: 0.021\n",
      "Epoch 24 [7/13] - Loss: 0.290 - - Time: 0.021\n",
      "Epoch 24 [8/13] - Loss: 0.314 - - Time: 0.021\n",
      "Epoch 24 [9/13] - Loss: 0.277 - - Time: 0.021\n",
      "Epoch 24 [10/13] - Loss: 0.280 - - Time: 0.021\n",
      "Epoch 24 [11/13] - Loss: 0.323 - - Time: 0.021\n",
      "Epoch 24 [12/13] - Loss: 0.301 - - Time: 0.021\n",
      "Epoch 25 [0/13] - Loss: 0.303 - - Time: 0.021\n",
      "Epoch 25 [1/13] - Loss: 0.282 - - Time: 0.021\n",
      "Epoch 25 [2/13] - Loss: 0.275 - - Time: 0.021\n",
      "Epoch 25 [3/13] - Loss: 0.302 - - Time: 0.021\n",
      "Epoch 25 [4/13] - Loss: 0.277 - - Time: 0.021\n",
      "Epoch 25 [5/13] - Loss: 0.284 - - Time: 0.021\n",
      "Epoch 25 [6/13] - Loss: 0.252 - - Time: 0.021\n",
      "Epoch 25 [7/13] - Loss: 0.289 - - Time: 0.021\n",
      "Epoch 25 [8/13] - Loss: 0.328 - - Time: 0.021\n",
      "Epoch 25 [9/13] - Loss: 0.281 - - Time: 0.021\n",
      "Epoch 25 [10/13] - Loss: 0.315 - - Time: 0.021\n",
      "Epoch 25 [11/13] - Loss: 0.328 - - Time: 0.020\n",
      "Epoch 25 [12/13] - Loss: 0.280 - - Time: 0.021\n",
      "Epoch 26 [0/13] - Loss: 0.278 - - Time: 0.020\n",
      "Epoch 26 [1/13] - Loss: 0.287 - - Time: 0.024\n",
      "Epoch 26 [2/13] - Loss: 0.241 - - Time: 0.021\n",
      "Epoch 26 [3/13] - Loss: 0.298 - - Time: 0.021\n",
      "Epoch 26 [4/13] - Loss: 0.268 - - Time: 0.020\n",
      "Epoch 26 [5/13] - Loss: 0.333 - - Time: 0.020\n",
      "Epoch 26 [6/13] - Loss: 0.297 - - Time: 0.020\n",
      "Epoch 26 [7/13] - Loss: 0.294 - - Time: 0.021\n",
      "Epoch 26 [8/13] - Loss: 0.281 - - Time: 0.021\n",
      "Epoch 26 [9/13] - Loss: 0.284 - - Time: 0.021\n",
      "Epoch 26 [10/13] - Loss: 0.251 - - Time: 0.021\n",
      "Epoch 26 [11/13] - Loss: 0.286 - - Time: 0.021\n",
      "Epoch 26 [12/13] - Loss: 0.327 - - Time: 0.020\n",
      "Epoch 27 [0/13] - Loss: 0.327 - - Time: 0.020\n",
      "Epoch 27 [1/13] - Loss: 0.257 - - Time: 0.021\n",
      "Epoch 27 [2/13] - Loss: 0.305 - - Time: 0.021\n",
      "Epoch 27 [3/13] - Loss: 0.286 - - Time: 0.021\n",
      "Epoch 27 [4/13] - Loss: 0.298 - - Time: 0.021\n",
      "Epoch 27 [5/13] - Loss: 0.285 - - Time: 0.021\n",
      "Epoch 27 [6/13] - Loss: 0.288 - - Time: 0.021\n",
      "Epoch 27 [7/13] - Loss: 0.257 - - Time: 0.021\n",
      "Epoch 27 [8/13] - Loss: 0.256 - - Time: 0.022\n",
      "Epoch 27 [9/13] - Loss: 0.290 - - Time: 0.021\n",
      "Epoch 27 [10/13] - Loss: 0.262 - - Time: 0.021\n",
      "Epoch 27 [11/13] - Loss: 0.302 - - Time: 0.021\n",
      "Epoch 27 [12/13] - Loss: 0.296 - - Time: 0.021\n",
      "Epoch 28 [0/13] - Loss: 0.265 - - Time: 0.021\n",
      "Epoch 28 [1/13] - Loss: 0.267 - - Time: 0.021\n",
      "Epoch 28 [2/13] - Loss: 0.231 - - Time: 0.021\n",
      "Epoch 28 [3/13] - Loss: 0.278 - - Time: 0.022\n",
      "Epoch 28 [4/13] - Loss: 0.311 - - Time: 0.021\n",
      "Epoch 28 [5/13] - Loss: 0.300 - - Time: 0.021\n",
      "Epoch 28 [6/13] - Loss: 0.281 - - Time: 0.021\n",
      "Epoch 28 [7/13] - Loss: 0.286 - - Time: 0.020\n",
      "Epoch 28 [8/13] - Loss: 0.255 - - Time: 0.020\n",
      "Epoch 28 [9/13] - Loss: 0.284 - - Time: 0.021\n",
      "Epoch 28 [10/13] - Loss: 0.266 - - Time: 0.021\n",
      "Epoch 28 [11/13] - Loss: 0.297 - - Time: 0.021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 [12/13] - Loss: 0.342 - - Time: 0.021\n",
      "Epoch 29 [0/13] - Loss: 0.286 - - Time: 0.021\n",
      "Epoch 29 [1/13] - Loss: 0.283 - - Time: 0.021\n",
      "Epoch 29 [2/13] - Loss: 0.258 - - Time: 0.021\n",
      "Epoch 29 [3/13] - Loss: 0.319 - - Time: 0.021\n",
      "Epoch 29 [4/13] - Loss: 0.296 - - Time: 0.021\n",
      "Epoch 29 [5/13] - Loss: 0.245 - - Time: 0.021\n",
      "Epoch 29 [6/13] - Loss: 0.236 - - Time: 0.021\n",
      "Epoch 29 [7/13] - Loss: 0.288 - - Time: 0.021\n",
      "Epoch 29 [8/13] - Loss: 0.333 - - Time: 0.021\n",
      "Epoch 29 [9/13] - Loss: 0.302 - - Time: 0.021\n",
      "Epoch 29 [10/13] - Loss: 0.274 - - Time: 0.021\n",
      "Epoch 29 [11/13] - Loss: 0.319 - - Time: 0.021\n",
      "Epoch 29 [12/13] - Loss: 0.235 - - Time: 0.021\n",
      "Epoch 30 [0/13] - Loss: 0.299 - - Time: 0.021\n",
      "Epoch 30 [1/13] - Loss: 0.272 - - Time: 0.021\n",
      "Epoch 30 [2/13] - Loss: 0.263 - - Time: 0.021\n",
      "Epoch 30 [3/13] - Loss: 0.249 - - Time: 0.021\n",
      "Epoch 30 [4/13] - Loss: 0.257 - - Time: 0.021\n",
      "Epoch 30 [5/13] - Loss: 0.260 - - Time: 0.021\n",
      "Epoch 30 [6/13] - Loss: 0.258 - - Time: 0.021\n",
      "Epoch 30 [7/13] - Loss: 0.267 - - Time: 0.021\n",
      "Epoch 30 [8/13] - Loss: 0.320 - - Time: 0.021\n",
      "Epoch 30 [9/13] - Loss: 0.276 - - Time: 0.021\n",
      "Epoch 30 [10/13] - Loss: 0.316 - - Time: 0.021\n",
      "Epoch 30 [11/13] - Loss: 0.298 - - Time: 0.021\n",
      "Epoch 30 [12/13] - Loss: 0.279 - - Time: 0.021\n",
      "Epoch 31 [0/13] - Loss: 0.259 - - Time: 0.021\n",
      "Epoch 31 [1/13] - Loss: 0.314 - - Time: 0.021\n",
      "Epoch 31 [2/13] - Loss: 0.260 - - Time: 0.021\n",
      "Epoch 31 [3/13] - Loss: 0.264 - - Time: 0.021\n",
      "Epoch 31 [4/13] - Loss: 0.249 - - Time: 0.021\n",
      "Epoch 31 [5/13] - Loss: 0.263 - - Time: 0.021\n",
      "Epoch 31 [6/13] - Loss: 0.307 - - Time: 0.021\n",
      "Epoch 31 [7/13] - Loss: 0.279 - - Time: 0.021\n",
      "Epoch 31 [8/13] - Loss: 0.249 - - Time: 0.021\n",
      "Epoch 31 [9/13] - Loss: 0.273 - - Time: 0.021\n",
      "Epoch 31 [10/13] - Loss: 0.288 - - Time: 0.021\n",
      "Epoch 31 [11/13] - Loss: 0.281 - - Time: 0.021\n",
      "Epoch 31 [12/13] - Loss: 0.303 - - Time: 0.021\n",
      "Epoch 32 [0/13] - Loss: 0.285 - - Time: 0.021\n",
      "Epoch 32 [1/13] - Loss: 0.244 - - Time: 0.021\n",
      "Epoch 32 [2/13] - Loss: 0.261 - - Time: 0.021\n",
      "Epoch 32 [3/13] - Loss: 0.279 - - Time: 0.021\n",
      "Epoch 32 [4/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 32 [5/13] - Loss: 0.293 - - Time: 0.020\n",
      "Epoch 32 [6/13] - Loss: 0.271 - - Time: 0.021\n",
      "Epoch 32 [7/13] - Loss: 0.274 - - Time: 0.020\n",
      "Epoch 32 [8/13] - Loss: 0.285 - - Time: 0.021\n",
      "Epoch 32 [9/13] - Loss: 0.277 - - Time: 0.021\n",
      "Epoch 32 [10/13] - Loss: 0.259 - - Time: 0.021\n",
      "Epoch 32 [11/13] - Loss: 0.265 - - Time: 0.021\n",
      "Epoch 32 [12/13] - Loss: 0.239 - - Time: 0.023\n",
      "Epoch 33 [0/13] - Loss: 0.230 - - Time: 0.021\n",
      "Epoch 33 [1/13] - Loss: 0.266 - - Time: 0.021\n",
      "Epoch 33 [2/13] - Loss: 0.263 - - Time: 0.021\n",
      "Epoch 33 [3/13] - Loss: 0.275 - - Time: 0.021\n",
      "Epoch 33 [4/13] - Loss: 0.249 - - Time: 0.021\n",
      "Epoch 33 [5/13] - Loss: 0.287 - - Time: 0.021\n",
      "Epoch 33 [6/13] - Loss: 0.254 - - Time: 0.021\n",
      "Epoch 33 [7/13] - Loss: 0.299 - - Time: 0.021\n",
      "Epoch 33 [8/13] - Loss: 0.235 - - Time: 0.021\n",
      "Epoch 33 [9/13] - Loss: 0.306 - - Time: 0.021\n",
      "Epoch 33 [10/13] - Loss: 0.250 - - Time: 0.021\n",
      "Epoch 33 [11/13] - Loss: 0.290 - - Time: 0.021\n",
      "Epoch 33 [12/13] - Loss: 0.266 - - Time: 0.020\n",
      "Epoch 34 [0/13] - Loss: 0.287 - - Time: 0.020\n",
      "Epoch 34 [1/13] - Loss: 0.265 - - Time: 0.021\n",
      "Epoch 34 [2/13] - Loss: 0.222 - - Time: 0.021\n",
      "Epoch 34 [3/13] - Loss: 0.222 - - Time: 0.021\n",
      "Epoch 34 [4/13] - Loss: 0.259 - - Time: 0.021\n",
      "Epoch 34 [5/13] - Loss: 0.266 - - Time: 0.021\n",
      "Epoch 34 [6/13] - Loss: 0.299 - - Time: 0.021\n",
      "Epoch 34 [7/13] - Loss: 0.261 - - Time: 0.021\n",
      "Epoch 34 [8/13] - Loss: 0.245 - - Time: 0.021\n",
      "Epoch 34 [9/13] - Loss: 0.297 - - Time: 0.021\n",
      "Epoch 34 [10/13] - Loss: 0.246 - - Time: 0.021\n",
      "Epoch 34 [11/13] - Loss: 0.268 - - Time: 0.020\n",
      "Epoch 34 [12/13] - Loss: 0.314 - - Time: 0.020\n",
      "Epoch 35 [0/13] - Loss: 0.268 - - Time: 0.020\n",
      "Epoch 35 [1/13] - Loss: 0.265 - - Time: 0.021\n",
      "Epoch 35 [2/13] - Loss: 0.238 - - Time: 0.021\n",
      "Epoch 35 [3/13] - Loss: 0.278 - - Time: 0.021\n",
      "Epoch 35 [4/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 35 [5/13] - Loss: 0.258 - - Time: 0.021\n",
      "Epoch 35 [6/13] - Loss: 0.272 - - Time: 0.020\n",
      "Epoch 35 [7/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 35 [8/13] - Loss: 0.244 - - Time: 0.024\n",
      "Epoch 35 [9/13] - Loss: 0.298 - - Time: 0.021\n",
      "Epoch 35 [10/13] - Loss: 0.242 - - Time: 0.021\n",
      "Epoch 35 [11/13] - Loss: 0.333 - - Time: 0.021\n",
      "Epoch 35 [12/13] - Loss: 0.212 - - Time: 0.020\n",
      "Epoch 36 [0/13] - Loss: 0.286 - - Time: 0.021\n",
      "Epoch 36 [1/13] - Loss: 0.245 - - Time: 0.021\n",
      "Epoch 36 [2/13] - Loss: 0.278 - - Time: 0.021\n",
      "Epoch 36 [3/13] - Loss: 0.268 - - Time: 0.020\n",
      "Epoch 36 [4/13] - Loss: 0.243 - - Time: 0.021\n",
      "Epoch 36 [5/13] - Loss: 0.261 - - Time: 0.021\n",
      "Epoch 36 [6/13] - Loss: 0.306 - - Time: 0.021\n",
      "Epoch 36 [7/13] - Loss: 0.259 - - Time: 0.021\n",
      "Epoch 36 [8/13] - Loss: 0.262 - - Time: 0.020\n",
      "Epoch 36 [9/13] - Loss: 0.267 - - Time: 0.021\n",
      "Epoch 36 [10/13] - Loss: 0.270 - - Time: 0.021\n",
      "Epoch 36 [11/13] - Loss: 0.245 - - Time: 0.021\n",
      "Epoch 36 [12/13] - Loss: 0.269 - - Time: 0.020\n",
      "Epoch 37 [0/13] - Loss: 0.226 - - Time: 0.021\n",
      "Epoch 37 [1/13] - Loss: 0.278 - - Time: 0.020\n",
      "Epoch 37 [2/13] - Loss: 0.267 - - Time: 0.020\n",
      "Epoch 37 [3/13] - Loss: 0.256 - - Time: 0.021\n",
      "Epoch 37 [4/13] - Loss: 0.311 - - Time: 0.020\n",
      "Epoch 37 [5/13] - Loss: 0.240 - - Time: 0.020\n",
      "Epoch 37 [6/13] - Loss: 0.250 - - Time: 0.021\n",
      "Epoch 37 [7/13] - Loss: 0.302 - - Time: 0.021\n",
      "Epoch 37 [8/13] - Loss: 0.251 - - Time: 0.020\n",
      "Epoch 37 [9/13] - Loss: 0.302 - - Time: 0.020\n",
      "Epoch 37 [10/13] - Loss: 0.235 - - Time: 0.021\n",
      "Epoch 37 [11/13] - Loss: 0.255 - - Time: 0.020\n",
      "Epoch 37 [12/13] - Loss: 0.226 - - Time: 0.020\n",
      "Epoch 38 [0/13] - Loss: 0.258 - - Time: 0.021\n",
      "Epoch 38 [1/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 38 [2/13] - Loss: 0.311 - - Time: 0.021\n",
      "Epoch 38 [3/13] - Loss: 0.252 - - Time: 0.021\n",
      "Epoch 38 [4/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 38 [5/13] - Loss: 0.257 - - Time: 0.021\n",
      "Epoch 38 [6/13] - Loss: 0.281 - - Time: 0.020\n",
      "Epoch 38 [7/13] - Loss: 0.297 - - Time: 0.020\n",
      "Epoch 38 [8/13] - Loss: 0.209 - - Time: 0.020\n",
      "Epoch 38 [9/13] - Loss: 0.246 - - Time: 0.020\n",
      "Epoch 38 [10/13] - Loss: 0.303 - - Time: 0.024\n",
      "Epoch 38 [11/13] - Loss: 0.246 - - Time: 0.020\n",
      "Epoch 38 [12/13] - Loss: 0.230 - - Time: 0.020\n",
      "Epoch 39 [0/13] - Loss: 0.268 - - Time: 0.020\n",
      "Epoch 39 [1/13] - Loss: 0.234 - - Time: 0.021\n",
      "Epoch 39 [2/13] - Loss: 0.281 - - Time: 0.021\n",
      "Epoch 39 [3/13] - Loss: 0.250 - - Time: 0.021\n",
      "Epoch 39 [4/13] - Loss: 0.214 - - Time: 0.021\n",
      "Epoch 39 [5/13] - Loss: 0.260 - - Time: 0.020\n",
      "Epoch 39 [6/13] - Loss: 0.252 - - Time: 0.020\n",
      "Epoch 39 [7/13] - Loss: 0.241 - - Time: 0.020\n",
      "Epoch 39 [8/13] - Loss: 0.310 - - Time: 0.020\n",
      "Epoch 39 [9/13] - Loss: 0.281 - - Time: 0.020\n",
      "Epoch 39 [10/13] - Loss: 0.259 - - Time: 0.021\n",
      "Epoch 39 [11/13] - Loss: 0.290 - - Time: 0.020\n",
      "Epoch 39 [12/13] - Loss: 0.233 - - Time: 0.020\n",
      "Epoch 40 [0/13] - Loss: 0.262 - - Time: 0.020\n",
      "Epoch 40 [1/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 40 [2/13] - Loss: 0.250 - - Time: 0.020\n",
      "Epoch 40 [3/13] - Loss: 0.263 - - Time: 0.021\n",
      "Epoch 40 [4/13] - Loss: 0.280 - - Time: 0.020\n",
      "Epoch 40 [5/13] - Loss: 0.264 - - Time: 0.020\n",
      "Epoch 40 [6/13] - Loss: 0.230 - - Time: 0.020\n",
      "Epoch 40 [7/13] - Loss: 0.269 - - Time: 0.020\n",
      "Epoch 40 [8/13] - Loss: 0.218 - - Time: 0.021\n",
      "Epoch 40 [9/13] - Loss: 0.251 - - Time: 0.020\n",
      "Epoch 40 [10/13] - Loss: 0.258 - - Time: 0.020\n",
      "Epoch 40 [11/13] - Loss: 0.257 - - Time: 0.020\n",
      "Epoch 40 [12/13] - Loss: 0.217 - - Time: 0.021\n",
      "Epoch 41 [0/13] - Loss: 0.246 - - Time: 0.020\n",
      "Epoch 41 [1/13] - Loss: 0.261 - - Time: 0.020\n",
      "Epoch 41 [2/13] - Loss: 0.222 - - Time: 0.020\n",
      "Epoch 41 [3/13] - Loss: 0.206 - - Time: 0.021\n",
      "Epoch 41 [4/13] - Loss: 0.262 - - Time: 0.020\n",
      "Epoch 41 [5/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 41 [6/13] - Loss: 0.258 - - Time: 0.020\n",
      "Epoch 41 [7/13] - Loss: 0.237 - - Time: 0.020\n",
      "Epoch 41 [8/13] - Loss: 0.265 - - Time: 0.020\n",
      "Epoch 41 [9/13] - Loss: 0.314 - - Time: 0.021\n",
      "Epoch 41 [10/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 41 [11/13] - Loss: 0.267 - - Time: 0.020\n",
      "Epoch 41 [12/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 42 [0/13] - Loss: 0.253 - - Time: 0.020\n",
      "Epoch 42 [1/13] - Loss: 0.224 - - Time: 0.020\n",
      "Epoch 42 [2/13] - Loss: 0.227 - - Time: 0.020\n",
      "Epoch 42 [3/13] - Loss: 0.222 - - Time: 0.020\n",
      "Epoch 42 [4/13] - Loss: 0.279 - - Time: 0.020\n",
      "Epoch 42 [5/13] - Loss: 0.211 - - Time: 0.020\n",
      "Epoch 42 [6/13] - Loss: 0.247 - - Time: 0.020\n",
      "Epoch 42 [7/13] - Loss: 0.275 - - Time: 0.020\n",
      "Epoch 42 [8/13] - Loss: 0.257 - - Time: 0.021\n",
      "Epoch 42 [9/13] - Loss: 0.255 - - Time: 0.021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 [10/13] - Loss: 0.306 - - Time: 0.021\n",
      "Epoch 42 [11/13] - Loss: 0.244 - - Time: 0.020\n",
      "Epoch 42 [12/13] - Loss: 0.277 - - Time: 0.020\n",
      "Epoch 43 [0/13] - Loss: 0.280 - - Time: 0.020\n",
      "Epoch 43 [1/13] - Loss: 0.248 - - Time: 0.020\n",
      "Epoch 43 [2/13] - Loss: 0.286 - - Time: 0.020\n",
      "Epoch 43 [3/13] - Loss: 0.265 - - Time: 0.020\n",
      "Epoch 43 [4/13] - Loss: 0.250 - - Time: 0.020\n",
      "Epoch 43 [5/13] - Loss: 0.251 - - Time: 0.021\n",
      "Epoch 43 [6/13] - Loss: 0.223 - - Time: 0.020\n",
      "Epoch 43 [7/13] - Loss: 0.269 - - Time: 0.020\n",
      "Epoch 43 [8/13] - Loss: 0.223 - - Time: 0.020\n",
      "Epoch 43 [9/13] - Loss: 0.212 - - Time: 0.020\n",
      "Epoch 43 [10/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 43 [11/13] - Loss: 0.237 - - Time: 0.020\n",
      "Epoch 43 [12/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 44 [0/13] - Loss: 0.249 - - Time: 0.020\n",
      "Epoch 44 [1/13] - Loss: 0.222 - - Time: 0.020\n",
      "Epoch 44 [2/13] - Loss: 0.272 - - Time: 0.020\n",
      "Epoch 44 [3/13] - Loss: 0.242 - - Time: 0.020\n",
      "Epoch 44 [4/13] - Loss: 0.209 - - Time: 0.021\n",
      "Epoch 44 [5/13] - Loss: 0.256 - - Time: 0.020\n",
      "Epoch 44 [6/13] - Loss: 0.230 - - Time: 0.020\n",
      "Epoch 44 [7/13] - Loss: 0.245 - - Time: 0.020\n",
      "Epoch 44 [8/13] - Loss: 0.251 - - Time: 0.020\n",
      "Epoch 44 [9/13] - Loss: 0.221 - - Time: 0.020\n",
      "Epoch 44 [10/13] - Loss: 0.249 - - Time: 0.020\n",
      "Epoch 44 [11/13] - Loss: 0.284 - - Time: 0.020\n",
      "Epoch 44 [12/13] - Loss: 0.289 - - Time: 0.020\n",
      "Epoch 45 [0/13] - Loss: 0.222 - - Time: 0.020\n",
      "Epoch 45 [1/13] - Loss: 0.246 - - Time: 0.020\n",
      "Epoch 45 [2/13] - Loss: 0.242 - - Time: 0.020\n",
      "Epoch 45 [3/13] - Loss: 0.238 - - Time: 0.020\n",
      "Epoch 45 [4/13] - Loss: 0.218 - - Time: 0.024\n",
      "Epoch 45 [5/13] - Loss: 0.268 - - Time: 0.020\n",
      "Epoch 45 [6/13] - Loss: 0.267 - - Time: 0.020\n",
      "Epoch 45 [7/13] - Loss: 0.255 - - Time: 0.021\n",
      "Epoch 45 [8/13] - Loss: 0.250 - - Time: 0.020\n",
      "Epoch 45 [9/13] - Loss: 0.245 - - Time: 0.020\n",
      "Epoch 45 [10/13] - Loss: 0.295 - - Time: 0.020\n",
      "Epoch 45 [11/13] - Loss: 0.280 - - Time: 0.020\n",
      "Epoch 45 [12/13] - Loss: 0.242 - - Time: 0.020\n",
      "Epoch 46 [0/13] - Loss: 0.248 - - Time: 0.020\n",
      "Epoch 46 [1/13] - Loss: 0.288 - - Time: 0.021\n",
      "Epoch 46 [2/13] - Loss: 0.209 - - Time: 0.020\n",
      "Epoch 46 [3/13] - Loss: 0.242 - - Time: 0.020\n",
      "Epoch 46 [4/13] - Loss: 0.227 - - Time: 0.020\n",
      "Epoch 46 [5/13] - Loss: 0.244 - - Time: 0.020\n",
      "Epoch 46 [6/13] - Loss: 0.284 - - Time: 0.020\n",
      "Epoch 46 [7/13] - Loss: 0.255 - - Time: 0.020\n",
      "Epoch 46 [8/13] - Loss: 0.289 - - Time: 0.020\n",
      "Epoch 46 [9/13] - Loss: 0.240 - - Time: 0.020\n",
      "Epoch 46 [10/13] - Loss: 0.258 - - Time: 0.020\n",
      "Epoch 46 [11/13] - Loss: 0.264 - - Time: 0.020\n",
      "Epoch 46 [12/13] - Loss: 0.210 - - Time: 0.020\n",
      "Epoch 47 [0/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 47 [1/13] - Loss: 0.319 - - Time: 0.020\n",
      "Epoch 47 [2/13] - Loss: 0.244 - - Time: 0.020\n",
      "Epoch 47 [3/13] - Loss: 0.222 - - Time: 0.020\n",
      "Epoch 47 [4/13] - Loss: 0.251 - - Time: 0.020\n",
      "Epoch 47 [5/13] - Loss: 0.227 - - Time: 0.020\n",
      "Epoch 47 [6/13] - Loss: 0.249 - - Time: 0.020\n",
      "Epoch 47 [7/13] - Loss: 0.265 - - Time: 0.020\n",
      "Epoch 47 [8/13] - Loss: 0.216 - - Time: 0.020\n",
      "Epoch 47 [9/13] - Loss: 0.227 - - Time: 0.020\n",
      "Epoch 47 [10/13] - Loss: 0.261 - - Time: 0.020\n",
      "Epoch 47 [11/13] - Loss: 0.224 - - Time: 0.020\n",
      "Epoch 47 [12/13] - Loss: 0.271 - - Time: 0.020\n",
      "Epoch 48 [0/13] - Loss: 0.226 - - Time: 0.021\n",
      "Epoch 48 [1/13] - Loss: 0.243 - - Time: 0.020\n",
      "Epoch 48 [2/13] - Loss: 0.241 - - Time: 0.020\n",
      "Epoch 48 [3/13] - Loss: 0.258 - - Time: 0.021\n",
      "Epoch 48 [4/13] - Loss: 0.245 - - Time: 0.020\n",
      "Epoch 48 [5/13] - Loss: 0.276 - - Time: 0.020\n",
      "Epoch 48 [6/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 48 [7/13] - Loss: 0.250 - - Time: 0.020\n",
      "Epoch 48 [8/13] - Loss: 0.229 - - Time: 0.020\n",
      "Epoch 48 [9/13] - Loss: 0.258 - - Time: 0.020\n",
      "Epoch 48 [10/13] - Loss: 0.234 - - Time: 0.020\n",
      "Epoch 48 [11/13] - Loss: 0.232 - - Time: 0.020\n",
      "Epoch 48 [12/13] - Loss: 0.265 - - Time: 0.020\n",
      "Epoch 49 [0/13] - Loss: 0.231 - - Time: 0.020\n",
      "Epoch 49 [1/13] - Loss: 0.220 - - Time: 0.020\n",
      "Epoch 49 [2/13] - Loss: 0.207 - - Time: 0.020\n",
      "Epoch 49 [3/13] - Loss: 0.263 - - Time: 0.020\n",
      "Epoch 49 [4/13] - Loss: 0.232 - - Time: 0.020\n",
      "Epoch 49 [5/13] - Loss: 0.271 - - Time: 0.020\n",
      "Epoch 49 [6/13] - Loss: 0.191 - - Time: 0.020\n",
      "Epoch 49 [7/13] - Loss: 0.233 - - Time: 0.021\n",
      "Epoch 49 [8/13] - Loss: 0.221 - - Time: 0.020\n",
      "Epoch 49 [9/13] - Loss: 0.252 - - Time: 0.020\n",
      "Epoch 49 [10/13] - Loss: 0.272 - - Time: 0.020\n",
      "Epoch 49 [11/13] - Loss: 0.270 - - Time: 0.020\n",
      "Epoch 49 [12/13] - Loss: 0.264 - - Time: 0.020\n",
      "Epoch 50 [0/13] - Loss: 0.221 - - Time: 0.020\n",
      "Epoch 50 [1/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 50 [2/13] - Loss: 0.194 - - Time: 0.021\n",
      "Epoch 50 [3/13] - Loss: 0.243 - - Time: 0.020\n",
      "Epoch 50 [4/13] - Loss: 0.217 - - Time: 0.020\n",
      "Epoch 50 [5/13] - Loss: 0.207 - - Time: 0.020\n",
      "Epoch 50 [6/13] - Loss: 0.271 - - Time: 0.020\n",
      "Epoch 50 [7/13] - Loss: 0.269 - - Time: 0.020\n",
      "Epoch 50 [8/13] - Loss: 0.209 - - Time: 0.020\n",
      "Epoch 50 [9/13] - Loss: 0.194 - - Time: 0.020\n",
      "Epoch 50 [10/13] - Loss: 0.261 - - Time: 0.021\n",
      "Epoch 50 [11/13] - Loss: 0.262 - - Time: 0.021\n",
      "Epoch 50 [12/13] - Loss: 0.284 - - Time: 0.020\n",
      "Epoch 51 [0/13] - Loss: 0.242 - - Time: 0.020\n",
      "Epoch 51 [1/13] - Loss: 0.235 - - Time: 0.020\n",
      "Epoch 51 [2/13] - Loss: 0.244 - - Time: 0.020\n",
      "Epoch 51 [3/13] - Loss: 0.185 - - Time: 0.020\n",
      "Epoch 51 [4/13] - Loss: 0.237 - - Time: 0.020\n",
      "Epoch 51 [5/13] - Loss: 0.234 - - Time: 0.020\n",
      "Epoch 51 [6/13] - Loss: 0.266 - - Time: 0.020\n",
      "Epoch 51 [7/13] - Loss: 0.244 - - Time: 0.020\n",
      "Epoch 51 [8/13] - Loss: 0.230 - - Time: 0.020\n",
      "Epoch 51 [9/13] - Loss: 0.237 - - Time: 0.020\n",
      "Epoch 51 [10/13] - Loss: 0.261 - - Time: 0.020\n",
      "Epoch 51 [11/13] - Loss: 0.255 - - Time: 0.020\n",
      "Epoch 51 [12/13] - Loss: 0.221 - - Time: 0.020\n",
      "Epoch 52 [0/13] - Loss: 0.195 - - Time: 0.020\n",
      "Epoch 52 [1/13] - Loss: 0.257 - - Time: 0.020\n",
      "Epoch 52 [2/13] - Loss: 0.239 - - Time: 0.020\n",
      "Epoch 52 [3/13] - Loss: 0.274 - - Time: 0.020\n",
      "Epoch 52 [4/13] - Loss: 0.226 - - Time: 0.020\n",
      "Epoch 52 [5/13] - Loss: 0.227 - - Time: 0.020\n",
      "Epoch 52 [6/13] - Loss: 0.273 - - Time: 0.020\n",
      "Epoch 52 [7/13] - Loss: 0.239 - - Time: 0.020\n",
      "Epoch 52 [8/13] - Loss: 0.244 - - Time: 0.021\n",
      "Epoch 52 [9/13] - Loss: 0.202 - - Time: 0.020\n",
      "Epoch 52 [10/13] - Loss: 0.232 - - Time: 0.020\n",
      "Epoch 52 [11/13] - Loss: 0.261 - - Time: 0.020\n",
      "Epoch 52 [12/13] - Loss: 0.248 - - Time: 0.021\n",
      "Epoch 53 [0/13] - Loss: 0.205 - - Time: 0.020\n",
      "Epoch 53 [1/13] - Loss: 0.217 - - Time: 0.020\n",
      "Epoch 53 [2/13] - Loss: 0.241 - - Time: 0.020\n",
      "Epoch 53 [3/13] - Loss: 0.236 - - Time: 0.020\n",
      "Epoch 53 [4/13] - Loss: 0.230 - - Time: 0.021\n",
      "Epoch 53 [5/13] - Loss: 0.267 - - Time: 0.020\n",
      "Epoch 53 [6/13] - Loss: 0.237 - - Time: 0.020\n",
      "Epoch 53 [7/13] - Loss: 0.234 - - Time: 0.020\n",
      "Epoch 53 [8/13] - Loss: 0.226 - - Time: 0.020\n",
      "Epoch 53 [9/13] - Loss: 0.236 - - Time: 0.020\n",
      "Epoch 53 [10/13] - Loss: 0.236 - - Time: 0.020\n",
      "Epoch 53 [11/13] - Loss: 0.228 - - Time: 0.020\n",
      "Epoch 53 [12/13] - Loss: 0.262 - - Time: 0.020\n",
      "Epoch 54 [0/13] - Loss: 0.228 - - Time: 0.020\n",
      "Epoch 54 [1/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 54 [2/13] - Loss: 0.255 - - Time: 0.020\n",
      "Epoch 54 [3/13] - Loss: 0.220 - - Time: 0.020\n",
      "Epoch 54 [4/13] - Loss: 0.247 - - Time: 0.020\n",
      "Epoch 54 [5/13] - Loss: 0.211 - - Time: 0.020\n",
      "Epoch 54 [6/13] - Loss: 0.286 - - Time: 0.020\n",
      "Epoch 54 [7/13] - Loss: 0.220 - - Time: 0.020\n",
      "Epoch 54 [8/13] - Loss: 0.238 - - Time: 0.020\n",
      "Epoch 54 [9/13] - Loss: 0.252 - - Time: 0.020\n",
      "Epoch 54 [10/13] - Loss: 0.230 - - Time: 0.022\n",
      "Epoch 54 [11/13] - Loss: 0.213 - - Time: 0.020\n",
      "Epoch 54 [12/13] - Loss: 0.252 - - Time: 0.020\n",
      "Epoch 55 [0/13] - Loss: 0.239 - - Time: 0.020\n",
      "Epoch 55 [1/13] - Loss: 0.231 - - Time: 0.021\n",
      "Epoch 55 [2/13] - Loss: 0.249 - - Time: 0.020\n",
      "Epoch 55 [3/13] - Loss: 0.257 - - Time: 0.020\n",
      "Epoch 55 [4/13] - Loss: 0.241 - - Time: 0.021\n",
      "Epoch 55 [5/13] - Loss: 0.293 - - Time: 0.020\n",
      "Epoch 55 [6/13] - Loss: 0.186 - - Time: 0.020\n",
      "Epoch 55 [7/13] - Loss: 0.275 - - Time: 0.020\n",
      "Epoch 55 [8/13] - Loss: 0.240 - - Time: 0.020\n",
      "Epoch 55 [9/13] - Loss: 0.205 - - Time: 0.020\n",
      "Epoch 55 [10/13] - Loss: 0.236 - - Time: 0.020\n",
      "Epoch 55 [11/13] - Loss: 0.216 - - Time: 0.020\n",
      "Epoch 55 [12/13] - Loss: 0.251 - - Time: 0.020\n",
      "Epoch 56 [0/13] - Loss: 0.243 - - Time: 0.021\n",
      "Epoch 56 [1/13] - Loss: 0.233 - - Time: 0.020\n",
      "Epoch 56 [2/13] - Loss: 0.195 - - Time: 0.020\n",
      "Epoch 56 [3/13] - Loss: 0.214 - - Time: 0.020\n",
      "Epoch 56 [4/13] - Loss: 0.224 - - Time: 0.020\n",
      "Epoch 56 [5/13] - Loss: 0.271 - - Time: 0.020\n",
      "Epoch 56 [6/13] - Loss: 0.236 - - Time: 0.020\n",
      "Epoch 56 [7/13] - Loss: 0.260 - - Time: 0.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 [8/13] - Loss: 0.198 - - Time: 0.020\n",
      "Epoch 56 [9/13] - Loss: 0.254 - - Time: 0.020\n",
      "Epoch 56 [10/13] - Loss: 0.233 - - Time: 0.021\n",
      "Epoch 56 [11/13] - Loss: 0.244 - - Time: 0.020\n",
      "Epoch 56 [12/13] - Loss: 0.227 - - Time: 0.020\n",
      "Epoch 57 [0/13] - Loss: 0.228 - - Time: 0.020\n",
      "Epoch 57 [1/13] - Loss: 0.166 - - Time: 0.020\n",
      "Epoch 57 [2/13] - Loss: 0.195 - - Time: 0.020\n",
      "Epoch 57 [3/13] - Loss: 0.240 - - Time: 0.021\n",
      "Epoch 57 [4/13] - Loss: 0.251 - - Time: 0.020\n",
      "Epoch 57 [5/13] - Loss: 0.224 - - Time: 0.020\n",
      "Epoch 57 [6/13] - Loss: 0.243 - - Time: 0.020\n",
      "Epoch 57 [7/13] - Loss: 0.253 - - Time: 0.020\n",
      "Epoch 57 [8/13] - Loss: 0.213 - - Time: 0.020\n",
      "Epoch 57 [9/13] - Loss: 0.274 - - Time: 0.020\n",
      "Epoch 57 [10/13] - Loss: 0.269 - - Time: 0.020\n",
      "Epoch 57 [11/13] - Loss: 0.241 - - Time: 0.020\n",
      "Epoch 57 [12/13] - Loss: 0.250 - - Time: 0.020\n",
      "Epoch 58 [0/13] - Loss: 0.193 - - Time: 0.020\n",
      "Epoch 58 [1/13] - Loss: 0.240 - - Time: 0.020\n",
      "Epoch 58 [2/13] - Loss: 0.257 - - Time: 0.021\n",
      "Epoch 58 [3/13] - Loss: 0.200 - - Time: 0.020\n",
      "Epoch 58 [4/13] - Loss: 0.235 - - Time: 0.020\n",
      "Epoch 58 [5/13] - Loss: 0.249 - - Time: 0.020\n",
      "Epoch 58 [6/13] - Loss: 0.213 - - Time: 0.021\n",
      "Epoch 58 [7/13] - Loss: 0.265 - - Time: 0.020\n",
      "Epoch 58 [8/13] - Loss: 0.242 - - Time: 0.020\n",
      "Epoch 58 [9/13] - Loss: 0.259 - - Time: 0.020\n",
      "Epoch 58 [10/13] - Loss: 0.220 - - Time: 0.022\n",
      "Epoch 58 [11/13] - Loss: 0.242 - - Time: 0.020\n",
      "Epoch 58 [12/13] - Loss: 0.250 - - Time: 0.020\n",
      "Epoch 59 [0/13] - Loss: 0.249 - - Time: 0.020\n",
      "Epoch 59 [1/13] - Loss: 0.238 - - Time: 0.020\n",
      "Epoch 59 [2/13] - Loss: 0.212 - - Time: 0.020\n",
      "Epoch 59 [3/13] - Loss: 0.226 - - Time: 0.020\n",
      "Epoch 59 [4/13] - Loss: 0.218 - - Time: 0.020\n",
      "Epoch 59 [5/13] - Loss: 0.240 - - Time: 0.021\n",
      "Epoch 59 [6/13] - Loss: 0.258 - - Time: 0.020\n",
      "Epoch 59 [7/13] - Loss: 0.264 - - Time: 0.020\n",
      "Epoch 59 [8/13] - Loss: 0.234 - - Time: 0.020\n",
      "Epoch 59 [9/13] - Loss: 0.276 - - Time: 0.020\n",
      "Epoch 59 [10/13] - Loss: 0.191 - - Time: 0.020\n",
      "Epoch 59 [11/13] - Loss: 0.180 - - Time: 0.020\n",
      "Epoch 59 [12/13] - Loss: 0.248 - - Time: 0.020\n",
      "Epoch 60 [0/13] - Loss: 0.218 - - Time: 0.020\n",
      "Epoch 60 [1/13] - Loss: 0.240 - - Time: 0.020\n",
      "Epoch 60 [2/13] - Loss: 0.241 - - Time: 0.020\n",
      "Epoch 60 [3/13] - Loss: 0.238 - - Time: 0.020\n",
      "Epoch 60 [4/13] - Loss: 0.279 - - Time: 0.020\n",
      "Epoch 60 [5/13] - Loss: 0.229 - - Time: 0.021\n",
      "Epoch 60 [6/13] - Loss: 0.252 - - Time: 0.020\n",
      "Epoch 60 [7/13] - Loss: 0.193 - - Time: 0.021\n",
      "Epoch 60 [8/13] - Loss: 0.195 - - Time: 0.020\n",
      "Epoch 60 [9/13] - Loss: 0.189 - - Time: 0.020\n",
      "Epoch 60 [10/13] - Loss: 0.253 - - Time: 0.020\n",
      "Epoch 60 [11/13] - Loss: 0.247 - - Time: 0.020\n",
      "Epoch 60 [12/13] - Loss: 0.220 - - Time: 0.020\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "], lr=0.01)\n",
    "mll = PredictiveLogLikelihood(model.likelihood, model, train_x.shape[-2], beta=0.3)\n",
    "\n",
    "import time\n",
    "\n",
    "with gpytorch.settings.fast_computations(log_prob=False, solves=False):\n",
    "    for i in range(num_epochs):\n",
    "        for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f - - Time: %.3f' % (i + 1, minibatch_i, len(train_loader), loss.item(), time.time() - start_time))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and get an RMSE\n",
    "\n",
    "The output distribution of a deep GP in this framework is actually a mixture of `num_samples` Gaussians for each output. We get predictions the same way with all GPyTorch models, but we do currently need to do some reshaping to get the means and variances in a reasonable form.\n",
    "\n",
    "SVGP gets an RMSE of around 0.41 after 60 epochs of training, so overall getting an RMSE of 0.35 out of a 2 layer deep GP without much tuning involved is pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import math\n",
    "\n",
    "model.eval()\n",
    "predictive_means, predictive_variances, lls = model.predict(test_loader)\n",
    "\n",
    "# rmse = torch.mean(torch.pow(predictive_means.mean(0) - test_y, 2)).sqrt()\n",
    "# print(rmse)\n",
    "# print(lls.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2690, device='cuda:0')\n",
      "tensor([-10.9496, -10.0149], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(lls.mean())\n",
    "print(lls[lls < -10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.likelihood.quad_weight_grid.exp().sum())\n",
    "# print(predictive_means * model.likelihood.quad_weight_grid.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wa(vec):\n",
    "#     return (vec * model.likelihood.quad_weight_grid.unsqueeze(-1).exp()).sum(0)\n",
    "\n",
    "# denom = (wa(predictive_variances) + wa(predictive_means.pow(2)) - wa(predictive_means)**2)\n",
    "# num = wa(predictive_variances)\n",
    "\n",
    "# print((num / denom).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.2743e-02, 1.7496e-02, 1.1253e-02, 1.9547e-02, 1.1099e-01, 2.6383e-02,\n",
      "        5.5161e-04, 4.2894e-03, 3.2499e-03, 4.4174e-02, 4.6009e-02, 2.6631e-03,\n",
      "        6.7975e-03, 4.2767e-01, 1.1693e-01, 6.8025e-04, 3.3794e-03, 7.5509e-02,\n",
      "        6.6699e-04, 9.5833e-03, 1.6967e-03, 6.6611e-04, 1.2993e-02, 1.8322e-02,\n",
      "        1.2514e-04, 1.4710e-03, 4.1513e-03], device='cuda:0',\n",
      "       grad_fn=<ExpBackward>)\n",
      "Parameter containing:\n",
      "tensor(0.2634, device='cuda:0', requires_grad=True)\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(model.likelihood.quad_weight_grid.exp())\n",
    "print(model.last_layer.fudge)\n",
    "print(math.sqrt(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3480, device='cuda:0', grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "sq_errors = torch.pow((predictive_means * model.likelihood.quad_weight_grid.unsqueeze(-1).exp()).sum(0) - test_y, 2)\n",
    "# sq_errors = torch.pow(predictive_means.mean(0) - test_y, 2)\n",
    "print(sq_errors.mean().sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, 1.0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMnUlEQVR4nO3cPXIbSZoG4Lc3+gAUJoIHkIz2NdnmWEvdQB27F1jqBlJ7irZWkj+GeIQe3kD0xtwMGeONIbXPiObgBloDxV4sBJIffkgUoOeJUChYWRCzUFX5Kisr87svX74EAO7yb7uuAAD7QWAAUCIwACgRGACUCAwASgQGACXf77oCq/r555+9Bwywhjdv3ny3yef3LjCS5Jdfftl1Fe7N5eVljo+Pd12Ne+P49tshH98hH1uSvH79euN/wyMpAEoEBgAlAgOAEoEBQInAAKBEYABQIjAAKBEYAJQIDABK9nKmN3C4fnh9ce+/45+/nNz77zhEehgAlAgMAEoEBgAlAgOAEoEBQInAAKBEYABQIjAAKBEYAJQIDABKBAYAJQIDgBKBAUCJwACgxPLmwDfHEurr0cMAoERgAFAiMAAoKY1htNaeJ/mx9/5qyfbHSc6TXCU5TXLee/88t8/LJJ+TTJKk93628G/cWg7AONzaw2itnQwN+oskR0t2mSR5m+RTkt+SfF4Ii7fDtvMhCJ4MIVMqB2A8bu1h9N4vkly01v6U5YGRJI+STOaDYs7pQq/kQ5JXmfVIKuUAjMTGr9X23qdJpovbW2tPl+x+leSkUg7AuGwcGK2108wa+kmSo977u6FoMmyfNx0+c3RX+RBEAIzEpm9JXST5dWEM4nQouw6FedcBMSmUAzAiG/UwloxbfMhsEPwsSx5T5f+C4KpQfqPLy8sVarlfptPD7lg5vv126Me3TYfYTq0dGMNjpX8leTT3+Gia2Wu2yazRXxwoP0pm4x6ttVvLb/vdx8fH61Z7Lzi+/eb4SA7ze9r0kdS7hcb9cWZzKtJ7/5ivexGTzB5j3VkOwLisHRhDUPy+sPmnzF6LvXa2MK/iWZL3K5QDMBK3PpIaXn09SfI8yaS19inJxdA7SGYN/svMegpPkrzvvf8xh6L3/qq19nJuRvinVcoBGI+7Ju59TPIxybsbyqc3lc3ts1E5AONg8UEASgQGACUCA4ASgQFAicAAoERgAFAiMAAoERgAlAgMAEoEBgAlAgOAEoEBQInAAKBEYABQIjAAKBEYAJQIDABKBAYAJQIDgBKBAUDJ97uuALA//vLXf+y6CuyQHgYAJQIDgBKBAUCJwACgRGAAUCIwACgRGACUCAwASgQGACUCA4ASgQFAicAAoERgAFAiMAAoERgAlAgMAEoEBgAlAgOAEoEBQInAAKBEYABQIjAAKBEYAJQIDABKBAYAJQIDgBKBAUDJ95WdWmvPk/zYe3+1pOxlks9JJknSez/bZjkA43BrD6O1djI06C+SHC0pf5vkc+/9fGjonwzhspVyAMbj1sDovV/03t8l+XjDLqe99/O5nz9kFi7bKgdgJNYew2itPV2y+SrJyTbKARiXTQa9J5k18POmSdJaO9pCOQAjsklgXDf6864DYLKFcgBGZJPAmC7Zdt3QX22hHIARKb1We4OrfP3m1FGS9N6nrbWNym/7xZeXl+vWefSm01sPfe85Pr4Vh9hOrR0YvfePrbXFu2OS5GIb5bc5Pj5er9J7wvHtt0M/PmoO8TrYdKb32cK8iWdJ3m+xHICRuLWHMbz6epLkeZJJa+1Tkove+8ck6b2/aq29HBr9x0k+zc+r2LQcqPnh9Z0dc9jYrYExBMPHJO9u2efGsm2UAzAOFh8EoERgAFAiMAAoERgAlAgMAEoEBgAlAgOAEoEBQInAAKBEYABQIjAAKBEYAJQIDABKBAYAJQIDgBKBAUCJwACgRGAAUCIwACgRGACUCAwASgQGACUCA4ASgQFAicAAoERgAFAiMAAoERgAlAgMAEoEBgAlAgOAEoEBQInAAKBEYABQIjAAKBEYAJR8v+sKwKH74fXFrqsAW6GHAUCJwACgRGAAUCIwACgRGACUCAwASgQGACUCA4ASE/f4pplUB3V6GACUCAwASgQGACUCA4CSjQe9W2vPkzxOcp7kKslpkvPe++e5fV4m+ZxkkiS997OFf+PWcgB2bxs9jEmSt0k+JfktyeeFsHg7bDsfguDJEDKlcgDGYVuPpB4ledJ7f9R7P18oO13Y9iHJixXKARiBrczD6L1Pk0wXt7fWni7Z/SrJSaUcgPHYSmC01k4za+gnSY567++Gosmwfd50+MzRXeVDEAEwAtsIjIskV9eNe2vtfWvtdBiPuA6FedcBMSmULw2My8vLLVR7nKbTw87IQz8+uHaI7dTGgTE/wD34kNkg+FmWN/jXAXFVKF/q+Ph4xVruF8cH++8vf/3Hvf+Of/7ysE/vNxr0bq0dtda+DI+Xrk0ze802mTX6RwsfO0r+GPe4qxyAkdjGW1LvFhr3x5nNqUjv/WO+7kVMMnuMdWc5AOOxUWAMQfH7wuafkrya+/lsYV7FsyTvVygHYAS2Meh9NszUniZ5kuT9/LyK3vur1trLuRnhn1YpB2ActjHoPU3y7o59NioHYPcsPghAicAAoERgAFAiMAAo2cpaUnAffnhtOg6MiR4GACUCA4ASgQFAicAAoERgAFAiMAAoERgAlAgMAEoEBgAlAgOAEoEBQIm1pFiLdZ7g26OHAUCJwACgRGAAUCIwACgRGACUCAwASgQGACXmYRwgcySA+6CHAUCJwACgRGAAUCIwACgx6P3ADEgD+0oPA4ASgQFAicAAoERgAFAiMAAoERgAlAgMAEoEBgAlAgOAEoEBQInAAKDEWlJzrPMEcDM9DABKBAYAJQIDgBKBAUDJXg56G5wGeHh6GACUjKKH0Vp7meRzkkmS9N7PdlsjABbtvIfRWnub5HPv/XwIiiettee7rhcA/9/OAyPJae/9fO7nD0le7KoyACy308BorT1dsvkqyclD1wWA2+26hzHJLCDmTZOktXb08NUB4Ca7DoyjDAPdc64DZHE7ADu067ekpku2XQfFYs/jD/+Zv99PbQD2yOvXD9sW7jowrjLrZcw7SpLe+7IwyZs3b76770oB8LWdPpLqvX/M172MSRJTuQFGZtdjGElytjDv4lmS97uqDADLfffly5dd12F+pvfjJFMzvQHGZxSBAcBmhic1P/beXxX2XWs5pl0PejOonuxhv8dJzjN7aeA0yXnv/fP913J9D3Exs7lVv/t9Oler1HWf7rPW2kmSp5k9zr+zfsNyTP9zvcJGa+1ta+35woobS+1FYAwnOkl+zOxA3xX235eLeKWTndkxvR3+TJP81xgv4msPeTHvyqE0RKt+9/t0rtao697cZ733iyQXrbU/5eu3Tpc5XfiP24ckrzK7Jm81hkHvW7XW3vfe3w1/fkryH3MBsmz/vVrMsPd+MQTgxxU+9ijJk977ozHenPPWOL69WltsjevtuiH6lOS34bNjaYhW/e736VytU9e9uc+qNl2OadQ9jGF5kMXXbt9ndsPd1MtYOz33xTBHZek8lX22p2uLrXO9PUoyGVFQrPzd79O5WreuB3qf3boc003z366NOjAyO7iXQy9j/uZa2u3ap4t4E62108yOa5Lk6K5HdHtko4v5oR1YQ7Tqd79P52qtuh7ofXbXcky3nrdRP5IaQuLPC2HxLDdP7PsWFjO8SPLrwiOQ011Xakv2bW2xta631tppa+358PeNj1cf2Krf/T6dq3Xqeqj32VrLMV0bew/jejZ4kj9uwpMkf75h943Sc1vuCqdN/ve15DHGh8we0T3YwP49Ht9GF/MOrHO9XSS5uv6OWmvvW2unI3gxY9Xvfp/O1cp1HcN9dk9WXo5p3k4CY4MG529J/v2WZ787v4iHAc9nd+wzrbxeuuRzR0n+leTR3Hc0zeytmwdxn8eXDS/mbVnh+jykhmjV734U56popbqO4T67L733j621tZdjevDAWLfBGd5GeTvf41hi5xfx8DbFfQ6wv1s4lsepvY67Ffd5fJtezNuw4vV5MA3Rqt/9GM5V1Zp13el9tk2ttcdJns696XW28EpxeTmmBw+MdRqc4Sb+MLxvnNba02XBsU8XcdX8ye69T1trvy/s8lNmb+XspW1ezNuwyvV5gA3Rrd/92M7VisrHtm/32fDyxUmS50kmrbVPSS7m2siTzOp/niS991ettZdzc4I+VV8bHv3SIMPEr8dJfh02TZK8uO6BLF7EyybozP88NnMn+0Vmx/bfmTvZw0DbT733Z8PPR5lN9pomeZIRH1uy+vEN2/ZmbbG7rrcl1+fL+bdtWmsfkrwfyzm87bs/gHNVPrZ9u88eyqgDY64Lv+h8mMS39xcx+09DxLdi1IEBwHiMeh4GAOMhMAAoERgAlAgMAEoEBgAlAgOAEoEBQInAAKDkfwHfl9loFHzN/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lls[sq_errors <1.].cpu(), bins=40)\n",
    "plt.xlim([-2, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = []\n",
    "sigmas = []\n",
    "for x_batch, y_batch in test_loader:\n",
    "    preds = model.likelihood(model(x_batch))\n",
    "    mus.append(preds.mean)\n",
    "    sigmas.append(preds.variance.sqrt())\n",
    "\n",
    "mus = torch.cat(mus, -1)\n",
    "sigmas = torch.cat(sigmas, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 3320])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = torch.mean(torch.pow(mus.mean(0) - test_y, 2)).sqrt()\n",
    "test_lls = torch.distributions.Normal(mus, sigmas).log_prob(test_y).logsumexp(0) - math.log(predictive_means.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
