{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import pyro\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('song.mat'):\n",
    "    print('Downloading \\'song\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/mg91x4c0muatanp/song.mat?dl=1', 'song.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('song.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())                  \n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))     \n",
    "        self.add_module('relu2', torch.nn.ReLU())                  \n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import PyroVariationalGP\n",
    "from gpytorch.variational import VariationalDistribution, GridInterpolationVariationalStrategy\n",
    "class PyroSVDKLGridInterpModel(PyroVariationalGP):\n",
    "    def __init__(self, likelihood, grid_size=20, grid_bounds=[(-1, 1), (-1, 1)], name_prefix=\"svdkl_grid_example\"):\n",
    "        variational_distribution = VariationalDistribution(num_inducing_points=20*20)\n",
    "        variational_strategy = GridInterpolationVariationalStrategy(self,\n",
    "                                                                    grid_size=grid_size,\n",
    "                                                                    grid_bounds=grid_bounds,\n",
    "                                                                    variational_distribution=variational_distribution)\n",
    "        super(PyroSVDKLGridInterpModel, self).__init__(variational_strategy, likelihood, name_prefix=name_prefix)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "            log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1., sigma=0.1, log_transform=True)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, likelihood, feature_extractor, num_features, grid_bounds=(-1., 1.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = PyroSVDKLGridInterpModel(likelihood)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def features(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = self.gp_layer(self.features(x))\n",
    "        return res\n",
    "    \n",
    "    def guide(self, x, y):\n",
    "        self.gp_layer.guide(self.features(x), y)\n",
    "    \n",
    "    def model(self, x, y):\n",
    "        pyro.module(self.gp_layer.name_prefix + \".feature_extractor\", self.feature_extractor)\n",
    "        self.gp_layer.model(self.features(x), y)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "model = DKLModel(likelihood, feature_extractor, num_features=num_features).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyro import optim\n",
    "from pyro import infer\n",
    "\n",
    "optimizer = optim.Adam({\"lr\": 0.1})\n",
    "\n",
    "elbo = infer.Trace_ELBO(num_particles=20, vectorize_particles=True)\n",
    "svi = infer.SVI(model.model, model.guide, optimizer, elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2336019.675583496\n",
      "loss 4158620.735316162\n",
      "loss 4067251.380131836\n",
      "loss 2096783.6699572755\n",
      "loss 1210297.9459643555\n",
      "loss 942903.0973748779\n",
      "loss 574982.4999612427\n",
      "loss 506037.5636941528\n",
      "loss 402567.0265090942\n",
      "loss 301674.42257751466\n",
      "loss 274601.9205041504\n",
      "loss 241471.38258026124\n",
      "loss 237175.26527923584\n",
      "loss 209330.1889819336\n",
      "loss 172374.87983551025\n",
      "loss 179163.2836514282\n",
      "loss 159603.58023773192\n",
      "loss 142606.30208221436\n",
      "loss 121732.56744659424\n",
      "loss 113145.68934265137\n",
      "loss 105781.6995727539\n",
      "loss 100264.59629302978\n",
      "loss 91100.60944732666\n",
      "loss 85801.01454498291\n",
      "loss 77707.99836334228\n",
      "loss 70148.3449899292\n",
      "loss 68224.59989624024\n",
      "loss 63423.49473144531\n",
      "loss 59349.29501953125\n",
      "loss 54972.7710055542\n",
      "loss 52792.60175476074\n",
      "loss 49289.64760681152\n",
      "loss 45982.17471954346\n",
      "loss 42778.187420349124\n",
      "loss 40101.62431213379\n",
      "loss 39797.46937896728\n",
      "loss 35628.33073272705\n",
      "loss 33586.76700012207\n",
      "loss 33186.84608520508\n",
      "loss 30147.970809020997\n",
      "loss 28411.82062713623\n",
      "loss 27577.87855895996\n",
      "loss 25600.490164489747\n",
      "loss 23700.075010681154\n",
      "loss 22748.167987060548\n",
      "loss 20941.013014221193\n",
      "loss 20106.986676940916\n",
      "loss 18910.8872744751\n",
      "loss 17673.955521850585\n",
      "loss 17076.1254296875\n",
      "loss 15706.422606201171\n",
      "loss 15054.926806945801\n",
      "loss 14714.206233825684\n",
      "loss 13907.340183410644\n",
      "loss 13308.68043029785\n",
      "loss 12715.886946411132\n",
      "loss 12014.324410095214\n",
      "loss 11269.119545288086\n",
      "loss 10708.167243652344\n",
      "loss 10377.53843170166\n",
      "loss 9830.760045166016\n",
      "loss 9507.777417907715\n",
      "loss 8963.790901184082\n",
      "loss 8552.128233032226\n",
      "loss 8119.137301025391\n",
      "loss 7840.294801635742\n",
      "loss 7753.95702545166\n",
      "loss 7230.965321350098\n",
      "loss 6976.098403015137\n",
      "loss 6682.3424429321285\n",
      "loss 6546.502846374512\n",
      "loss 6391.886244506836\n",
      "loss 6048.3487231445315\n",
      "loss 5844.617374877929\n",
      "loss 5598.686479797363\n",
      "loss 5304.8828399658205\n",
      "loss 5282.531183776856\n",
      "loss 5022.893774108887\n",
      "loss 4901.749856567383\n",
      "loss 4698.563680114746\n",
      "loss 4511.992737426758\n",
      "loss 4568.947349243164\n",
      "loss 4467.172702636719\n",
      "loss 4126.302682189941\n",
      "loss 4002.6226934814454\n",
      "loss 3918.7964474487303\n",
      "loss 3701.9104141235352\n",
      "loss 3642.797882080078\n",
      "loss 3609.12862121582\n",
      "loss 3597.087104187012\n",
      "loss 3378.370553588867\n",
      "loss 3398.8694744873046\n",
      "loss 3334.4362341308592\n",
      "loss 3228.4176095581056\n",
      "loss 3051.094033508301\n",
      "loss 3054.636485290527\n",
      "loss 2897.4252911376952\n",
      "loss 2843.1133935546877\n",
      "loss 2808.34068359375\n",
      "loss 2656.042035217285\n",
      "loss 2658.7438479614257\n",
      "loss 2620.3419522094728\n",
      "loss 2499.4641986083984\n",
      "loss 2508.2198190307618\n",
      "loss 2448.277326965332\n",
      "loss 2367.2151684570313\n",
      "loss 2312.6885440063475\n",
      "loss 2309.617781677246\n",
      "loss 2265.5846279907228\n",
      "loss 2149.059816894531\n",
      "loss 2158.4832656860353\n",
      "loss 2116.995061035156\n",
      "loss 2066.480510559082\n",
      "loss 2028.303454284668\n",
      "loss 1965.138609313965\n",
      "loss 1939.3720471191407\n",
      "loss 1905.7431668090821\n",
      "loss 1900.998637084961\n",
      "loss 1798.5923184204103\n",
      "loss 1847.5579364013672\n",
      "loss 1808.9619497680665\n",
      "loss 1774.6270126342774\n",
      "loss 1720.7933905029297\n",
      "loss 1710.9170050048829\n",
      "loss 1629.8021551513673\n",
      "loss 1661.260998840332\n",
      "loss 1642.1609927368163\n",
      "loss 1565.369256286621\n",
      "loss 1549.6674377441407\n",
      "loss 1507.6880700683594\n",
      "loss 1518.2746252441407\n",
      "loss 1462.5860034179686\n",
      "loss 1449.395715637207\n",
      "loss 1475.5242221069336\n",
      "loss 1411.7536587524414\n",
      "loss 1421.4762329101563\n",
      "loss 1379.0319366455078\n",
      "loss 1385.7249325561525\n",
      "loss 1358.7114987182617\n",
      "loss 1337.8316696166992\n",
      "loss 1326.1220538330078\n",
      "loss 1312.8939538574218\n",
      "loss 1322.7793927001953\n",
      "loss 1245.235226135254\n",
      "loss 1260.742981262207\n",
      "loss 1249.5790322875976\n",
      "loss 1228.685608215332\n",
      "loss 1191.3724743652344\n",
      "loss 1176.6044894409179\n",
      "loss 1200.5645852661132\n",
      "loss 1161.9064761352538\n",
      "loss 1159.4076171875\n",
      "loss 1136.8021817016602\n",
      "loss 1128.4764987182616\n",
      "loss 1137.2597357177735\n",
      "loss 1106.936590576172\n",
      "loss 1077.374103088379\n",
      "loss 1077.7135198974609\n",
      "loss 1048.0785095214844\n",
      "loss 1036.7945764160156\n",
      "loss 1067.9996990966797\n",
      "loss 1049.8257168579103\n",
      "loss 1051.5865283203125\n",
      "loss 1011.6087658691406\n",
      "loss 1012.6304656982422\n",
      "loss 990.0259262084961\n",
      "loss 986.1751849365235\n",
      "loss 1008.9162701416016\n",
      "loss 983.3970993041992\n",
      "loss 953.8196917724609\n",
      "loss 935.5529223632813\n",
      "loss 925.312787475586\n",
      "loss 948.1520349121093\n",
      "loss 903.3423886108399\n",
      "loss 906.1627398681641\n",
      "loss 908.985103149414\n",
      "loss 877.8464666748047\n",
      "loss 899.0774774169922\n",
      "loss 897.7486474609375\n",
      "loss 891.9707077026367\n",
      "loss 890.6028335571289\n",
      "loss 871.7041772460938\n",
      "loss 858.5854721069336\n",
      "loss 857.1739572143555\n",
      "loss 849.2660162353516\n",
      "loss 846.3107025146485\n",
      "loss 850.2321600341797\n",
      "loss 847.8297341918945\n",
      "loss 819.5418371582032\n",
      "loss 811.0381469726562\n",
      "loss 796.2708779907226\n",
      "loss 776.8227947998047\n",
      "loss 775.2178295898437\n",
      "loss 813.1460827636719\n",
      "loss 789.1812829589844\n",
      "loss 781.5785354614258\n",
      "loss 765.2618188476563\n",
      "loss 779.7034375\n",
      "loss 753.4673330688477\n",
      "loss 767.581928100586\n",
      "loss 761.8075680541992\n",
      "loss 754.3606488037109\n",
      "loss 756.3021099853515\n",
      "loss 723.0378924560547\n",
      "loss 741.8574020385743\n",
      "loss 729.7083493041993\n",
      "loss 733.8340631103515\n",
      "loss 737.2269854736328\n",
      "loss 722.5536407470703\n",
      "loss 738.2567828369141\n",
      "loss 716.3313037109375\n",
      "loss 694.0873803710938\n",
      "loss 715.0518203735352\n",
      "loss 697.7639636230468\n",
      "loss 694.0957006835938\n",
      "loss 678.3854248046875\n",
      "loss 672.1731567382812\n",
      "loss 698.3837127685547\n",
      "loss 666.222077331543\n",
      "loss 684.5894903564454\n",
      "loss 657.0405911254883\n",
      "loss 687.0974353027344\n",
      "loss 637.5847915649414\n",
      "loss 685.9866781616211\n",
      "loss 672.8447106933594\n",
      "loss 646.1982913208008\n",
      "loss 651.4550024414062\n",
      "loss 647.3070239257812\n",
      "loss 633.2159063720703\n",
      "loss 626.3912487792969\n",
      "loss 622.9267572021485\n",
      "loss 642.6947030639649\n",
      "loss 620.1712475585938\n",
      "loss 629.8014590454102\n",
      "loss 618.9422448730469\n",
      "loss 617.1937576293946\n",
      "loss 607.2555603027344\n",
      "loss 592.6443325805664\n",
      "loss 584.6638482666016\n",
      "loss 606.6000061035156\n",
      "loss 601.9066729736328\n",
      "loss 576.9106530761719\n",
      "loss 588.8077490234375\n",
      "loss 605.8127249145508\n",
      "loss 608.3875839233399\n",
      "loss 556.809194946289\n",
      "loss 585.4238500976562\n",
      "loss 612.1871224975586\n",
      "loss 563.5536285400391\n",
      "loss 565.7160858154297\n",
      "loss 567.0431411743164\n",
      "loss 589.5552212524414\n",
      "loss 555.9177752685547\n",
      "loss 578.4107702636719\n",
      "loss 589.6581628417969\n",
      "loss 572.4038537597656\n",
      "loss 564.6062045288086\n",
      "loss 560.2741464233399\n",
      "loss 567.6196362304687\n",
      "loss 559.1973773193359\n",
      "loss 530.7283895874024\n",
      "loss 563.5472741699218\n",
      "loss 543.8157342529297\n",
      "loss 555.9603604125977\n",
      "loss 536.9036413574219\n",
      "loss 563.7124243164062\n",
      "loss 555.1092385864258\n",
      "loss 546.7930068969727\n",
      "loss 531.119384765625\n",
      "loss 545.0225964355469\n",
      "loss 516.6423831176758\n",
      "loss 537.5721520996094\n",
      "loss 539.2905715942383\n",
      "loss 518.493581237793\n",
      "loss 526.849382019043\n",
      "loss 532.7910327148437\n",
      "loss 535.7210525512695\n",
      "loss 505.20067321777344\n",
      "loss 519.83380859375\n",
      "loss 508.57444458007814\n",
      "loss 496.73158142089846\n",
      "loss 521.1353598022461\n",
      "loss 520.3287255859375\n",
      "loss 489.00310089111326\n",
      "loss 504.0483993530273\n",
      "loss 514.8687005615234\n",
      "loss 494.76345092773437\n",
      "loss 503.19952880859375\n",
      "loss 490.80713531494143\n",
      "loss 498.0227767944336\n",
      "loss 491.6718432617188\n",
      "loss 485.68727416992186\n",
      "loss 514.0437371826172\n",
      "loss 478.81846282958986\n",
      "loss 482.1640872192383\n",
      "loss 487.14052337646484\n",
      "loss 483.215341796875\n",
      "loss 488.6182681274414\n",
      "loss 437.26619323730466\n",
      "loss 452.1267172241211\n",
      "loss 484.8208038330078\n",
      "loss 440.0383020019531\n",
      "loss 457.2684503173828\n",
      "loss 477.8702474975586\n",
      "loss 460.567141418457\n",
      "loss 482.2486846923828\n",
      "loss 465.63912109375\n",
      "loss 460.0026217651367\n",
      "loss 446.2028698730469\n",
      "loss 460.7048934936523\n",
      "loss 459.6910650634766\n",
      "loss 444.0411846923828\n",
      "loss 474.9279052734375\n",
      "loss 434.27176055908205\n",
      "loss 433.706376953125\n",
      "loss 448.42716888427736\n",
      "loss 434.95615051269533\n",
      "loss 457.6140383911133\n",
      "loss 446.1708639526367\n",
      "loss 475.96806243896486\n",
      "loss 432.7316061401367\n",
      "loss 447.1730041503906\n",
      "loss 426.91945220947264\n",
      "loss 435.8695999145508\n",
      "loss 447.3664868164062\n",
      "loss 427.2037622070313\n",
      "loss 449.2981817626953\n",
      "loss 444.88608703613284\n",
      "loss 425.2986346435547\n",
      "loss 431.4847979736328\n",
      "loss 402.2129672241211\n",
      "loss 423.89550323486327\n",
      "loss 424.7943408203125\n",
      "loss 455.8258444213867\n",
      "loss 434.8630123901367\n",
      "loss 423.7710366821289\n",
      "loss 409.261838684082\n",
      "loss 426.57880920410156\n",
      "loss 413.2534686279297\n",
      "loss 432.00723754882813\n",
      "loss 443.4572772216797\n",
      "loss 418.81716186523437\n",
      "loss 387.4620181274414\n",
      "loss 414.1096517944336\n",
      "loss 439.1388998413086\n",
      "loss 402.1377203369141\n",
      "loss 410.04961334228517\n",
      "loss 429.40364227294924\n",
      "loss 400.91278198242185\n",
      "loss 410.6116873168945\n",
      "loss 392.4872573852539\n",
      "loss 413.59210815429685\n",
      "loss 388.50529846191404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 414.0151388549805\n",
      "loss 407.7424078369141\n",
      "loss 413.65128021240236\n",
      "loss 407.16767517089846\n",
      "loss 406.84657897949216\n",
      "loss 415.9659326171875\n",
      "loss 384.5176733398437\n",
      "loss 402.68549713134763\n",
      "loss 401.52425354003907\n",
      "loss 390.20977294921875\n",
      "loss 401.2391409301758\n",
      "loss 355.9790972900391\n",
      "loss 374.8517300415039\n",
      "loss 383.7428646850586\n",
      "loss 396.0201455688477\n",
      "loss 390.69847991943357\n",
      "loss 422.64212005615235\n",
      "loss 384.98068267822265\n",
      "loss 385.9450004577637\n",
      "loss 413.06124298095705\n",
      "loss 396.34627548217776\n",
      "loss 387.88541091918944\n",
      "loss 393.20414276123046\n",
      "loss 379.7134860229492\n",
      "loss 397.4904901123047\n",
      "loss 371.2942074584961\n",
      "loss 398.20728881835936\n",
      "loss 361.9511828613281\n",
      "loss 379.1823669433594\n",
      "loss 397.4315545654297\n",
      "loss 379.1300286865234\n",
      "loss 362.81505096435546\n",
      "loss 374.9832525634766\n",
      "loss 369.4448498535156\n",
      "loss 352.765442199707\n",
      "loss 377.3762457275391\n",
      "loss 381.14376159667967\n",
      "loss 388.97028106689453\n",
      "loss 353.4455297851562\n",
      "loss 361.4295022583008\n",
      "loss 385.1436657714844\n",
      "loss 352.46007385253904\n",
      "loss 373.1099575805664\n",
      "loss 382.92870025634767\n",
      "loss 367.3096862792969\n",
      "loss 359.9640222167969\n",
      "loss 372.2612603759766\n",
      "loss 376.32662994384765\n",
      "loss 373.6950909423828\n",
      "loss 362.0739892578125\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        loss = svi.step(x_batch, y_batch)\n",
    "        print('Epoch {} [{} / {}]'.format(i, minibatch_i, len(train_loader)), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.5311331152915955\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
