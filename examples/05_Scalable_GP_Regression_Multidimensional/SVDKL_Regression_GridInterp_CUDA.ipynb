{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA) (w/ KISS-GP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use Deep Kernel Learning with SKI and stochastic variational regression to rapidly train using minibatches on the `song` UCI dataset, which has hundreds of thousands of training examples.\n",
    "\n",
    "Stochastic variational inference has several major advantages over the standard regression setting. Most notably, the ELBO used for optimization decomposes in such a way that stochastic gradient descent techniques can be used. See https://arxiv.org/pdf/1411.2005.pdf and https://arxiv.org/pdf/1611.00336.pdf for more technical details of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `3droad` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~136 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2028, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Feature Extractor\n",
    "\n",
    "Next, we define the neural network feature extractor used to define the deep kernel. In this case, we use a fully connected network with the architecture `d -> 1000 -> 500 -> 50 -> 2`, as described in the original DKL paper. All of the code below uses standard PyTorch implementations of neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('bn1', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 1000))\n",
    "        self.add_module('bn2', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu2', torch.nn.ReLU())                       \n",
    "        self.add_module('linear3', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('bn3', torch.nn.BatchNorm1d(500))\n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('bn4', torch.nn.BatchNorm1d(50))\n",
    "        self.add_module('relu4', torch.nn.ReLU())                  \n",
    "        self.add_module('linear5', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Regression Layer\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. To use grid interpolation for variational inference, we'll be using a `GridInterpolationVariationalStrategy`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions.\n",
    "\n",
    "See the CIFAR example for using an `AbstractVariationalGP` with an `AdditiveGridInterpolationVariationalStrategy`, which additionally assumes the kernel decomposes additively, which is a strong modelling assumption but allows us to use many more output features from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, GridInterpolationVariationalStrategy\n",
    "class GPRegressionLayer(AbstractVariationalGP):\n",
    "    def __init__(self, grid_size=32, grid_bounds=[(-1, 1), (-1, 1)]):\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=grid_size*grid_size)\n",
    "        variational_strategy = GridInterpolationVariationalStrategy(self,\n",
    "                                                                    grid_size=grid_size,\n",
    "                                                                    grid_bounds=grid_bounds,\n",
    "                                                                    variational_distribution=variational_distribution)\n",
    "        super(GPRegressionLayer, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "            log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1., sigma=0.1, log_transform=True)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Model\n",
    "\n",
    "With the feature extractor and GP regression layer defined, we can now define our full model. To do this, we simply create a module whose `forward()` method passes the data first through the feature extractor, and then through the GP regression layer.\n",
    "\n",
    "The only other interesting feature of the model below is that we use a helper function, `scale_to_bounds`, to ensure that the features extracted by the neural network fit within the grid bounds used for SKI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_features, grid_bounds=(-1., 1.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GPRegressionLayer()\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "\n",
    "model = DKLModel(feature_extractor, num_features=num_features).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the DKL model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalELBO`), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/172] - Loss: 176.594 [-176.593, 0.000, -0.000]\n",
      "Epoch 1 [1/172] - Loss: 152.574 [-152.070, 0.505, -0.000]\n",
      "Epoch 1 [2/172] - Loss: 151.459 [-150.812, 0.647, -0.000]\n",
      "Epoch 1 [3/172] - Loss: 145.079 [-144.706, 0.373, -0.000]\n",
      "Epoch 1 [4/172] - Loss: 129.085 [-128.564, 0.521, -0.000]\n",
      "Epoch 1 [5/172] - Loss: 103.243 [-102.748, 0.494, -0.000]\n",
      "Epoch 1 [6/172] - Loss: 94.733 [-94.339, 0.394, -0.000]\n",
      "Epoch 1 [7/172] - Loss: 96.624 [-96.357, 0.267, -0.000]\n",
      "Epoch 1 [8/172] - Loss: 80.956 [-80.752, 0.204, -0.000]\n",
      "Epoch 1 [9/172] - Loss: 69.646 [-69.420, 0.226, -0.000]\n",
      "Epoch 1 [10/172] - Loss: 68.924 [-68.710, 0.214, -0.000]\n",
      "Epoch 1 [11/172] - Loss: 64.165 [-63.946, 0.219, -0.000]\n",
      "Epoch 1 [12/172] - Loss: 58.513 [-58.327, 0.186, -0.000]\n",
      "Epoch 1 [13/172] - Loss: 52.696 [-52.519, 0.176, -0.000]\n",
      "Epoch 1 [14/172] - Loss: 49.839 [-49.701, 0.138, -0.000]\n",
      "Epoch 1 [15/172] - Loss: 48.205 [-48.083, 0.122, -0.000]\n",
      "Epoch 1 [16/172] - Loss: 42.446 [-42.348, 0.097, -0.000]\n",
      "Epoch 1 [17/172] - Loss: 38.538 [-38.462, 0.076, -0.000]\n",
      "Epoch 1 [18/172] - Loss: 35.160 [-35.095, 0.065, -0.000]\n",
      "Epoch 1 [19/172] - Loss: 34.755 [-34.697, 0.058, -0.000]\n",
      "Epoch 1 [20/172] - Loss: 32.115 [-32.064, 0.050, -0.000]\n",
      "Epoch 1 [21/172] - Loss: 32.093 [-32.049, 0.044, -0.000]\n",
      "Epoch 1 [22/172] - Loss: 28.964 [-28.925, 0.039, -0.000]\n",
      "Epoch 1 [23/172] - Loss: 27.591 [-27.557, 0.034, -0.000]\n",
      "Epoch 1 [24/172] - Loss: 26.980 [-26.950, 0.030, -0.000]\n",
      "Epoch 1 [25/172] - Loss: 26.175 [-26.150, 0.025, -0.000]\n",
      "Epoch 1 [26/172] - Loss: 23.854 [-23.833, 0.020, -0.000]\n",
      "Epoch 1 [27/172] - Loss: 22.334 [-22.318, 0.016, -0.000]\n",
      "Epoch 1 [28/172] - Loss: 21.607 [-21.594, 0.013, -0.000]\n",
      "Epoch 1 [29/172] - Loss: 20.977 [-20.966, 0.010, -0.000]\n",
      "Epoch 1 [30/172] - Loss: 19.440 [-19.432, 0.009, -0.000]\n",
      "Epoch 1 [31/172] - Loss: 18.184 [-18.176, 0.008, -0.000]\n",
      "Epoch 1 [32/172] - Loss: 19.714 [-19.707, 0.007, -0.000]\n",
      "Epoch 1 [33/172] - Loss: 17.027 [-17.020, 0.006, -0.000]\n",
      "Epoch 1 [34/172] - Loss: 17.163 [-17.156, 0.006, -0.000]\n",
      "Epoch 1 [35/172] - Loss: 16.306 [-16.300, 0.006, -0.000]\n",
      "Epoch 1 [36/172] - Loss: 15.897 [-15.891, 0.006, -0.000]\n",
      "Epoch 1 [37/172] - Loss: 17.481 [-17.475, 0.006, -0.000]\n",
      "Epoch 1 [38/172] - Loss: 14.529 [-14.523, 0.005, -0.000]\n",
      "Epoch 1 [39/172] - Loss: 14.901 [-14.896, 0.005, -0.000]\n",
      "Epoch 1 [40/172] - Loss: 13.603 [-13.598, 0.005, -0.000]\n",
      "Epoch 1 [41/172] - Loss: 14.313 [-14.307, 0.005, -0.000]\n",
      "Epoch 1 [42/172] - Loss: 13.424 [-13.419, 0.005, -0.000]\n",
      "Epoch 1 [43/172] - Loss: 12.590 [-12.585, 0.005, -0.000]\n",
      "Epoch 1 [44/172] - Loss: 13.314 [-13.309, 0.005, -0.000]\n",
      "Epoch 1 [45/172] - Loss: 12.883 [-12.878, 0.005, -0.000]\n",
      "Epoch 1 [46/172] - Loss: 12.452 [-12.447, 0.005, -0.000]\n",
      "Epoch 1 [47/172] - Loss: 12.289 [-12.284, 0.005, -0.000]\n",
      "Epoch 1 [48/172] - Loss: 11.989 [-11.985, 0.005, -0.000]\n",
      "Epoch 1 [49/172] - Loss: 11.960 [-11.955, 0.005, -0.000]\n",
      "Epoch 1 [50/172] - Loss: 11.397 [-11.393, 0.005, -0.000]\n",
      "Epoch 1 [51/172] - Loss: 11.031 [-11.026, 0.005, -0.000]\n",
      "Epoch 1 [52/172] - Loss: 10.832 [-10.828, 0.005, -0.000]\n",
      "Epoch 1 [53/172] - Loss: 11.045 [-11.040, 0.005, -0.000]\n",
      "Epoch 1 [54/172] - Loss: 10.913 [-10.908, 0.005, -0.000]\n",
      "Epoch 1 [55/172] - Loss: 10.253 [-10.249, 0.005, -0.000]\n",
      "Epoch 1 [56/172] - Loss: 10.293 [-10.289, 0.004, -0.000]\n",
      "Epoch 1 [57/172] - Loss: 9.484 [-9.479, 0.004, -0.000]\n",
      "Epoch 1 [58/172] - Loss: 9.223 [-9.218, 0.004, -0.000]\n",
      "Epoch 1 [59/172] - Loss: 9.364 [-9.360, 0.004, -0.000]\n",
      "Epoch 1 [60/172] - Loss: 9.284 [-9.280, 0.004, -0.000]\n",
      "Epoch 1 [61/172] - Loss: 8.866 [-8.862, 0.004, -0.000]\n",
      "Epoch 1 [62/172] - Loss: 8.735 [-8.731, 0.004, -0.000]\n",
      "Epoch 1 [63/172] - Loss: 8.982 [-8.978, 0.004, -0.000]\n",
      "Epoch 1 [64/172] - Loss: 8.531 [-8.527, 0.004, -0.000]\n",
      "Epoch 1 [65/172] - Loss: 8.102 [-8.098, 0.004, -0.000]\n",
      "Epoch 1 [66/172] - Loss: 8.483 [-8.479, 0.004, -0.000]\n",
      "Epoch 1 [67/172] - Loss: 8.592 [-8.588, 0.004, -0.000]\n",
      "Epoch 1 [68/172] - Loss: 8.241 [-8.236, 0.004, -0.000]\n",
      "Epoch 1 [69/172] - Loss: 8.856 [-8.852, 0.004, -0.000]\n",
      "Epoch 1 [70/172] - Loss: 8.044 [-8.040, 0.004, -0.000]\n",
      "Epoch 1 [71/172] - Loss: 7.958 [-7.954, 0.004, -0.000]\n",
      "Epoch 1 [72/172] - Loss: 8.470 [-8.466, 0.004, -0.000]\n",
      "Epoch 1 [73/172] - Loss: 8.297 [-8.293, 0.004, -0.000]\n",
      "Epoch 1 [74/172] - Loss: 7.873 [-7.869, 0.004, -0.000]\n",
      "Epoch 1 [75/172] - Loss: 7.995 [-7.991, 0.004, -0.000]\n",
      "Epoch 1 [76/172] - Loss: 7.884 [-7.880, 0.004, -0.000]\n",
      "Epoch 1 [77/172] - Loss: 7.970 [-7.966, 0.004, -0.000]\n",
      "Epoch 1 [78/172] - Loss: 7.600 [-7.596, 0.004, -0.000]\n",
      "Epoch 1 [79/172] - Loss: 7.305 [-7.301, 0.004, -0.000]\n",
      "Epoch 1 [80/172] - Loss: 7.473 [-7.469, 0.004, -0.000]\n",
      "Epoch 1 [81/172] - Loss: 7.331 [-7.327, 0.004, -0.000]\n",
      "Epoch 1 [82/172] - Loss: 7.347 [-7.344, 0.004, -0.000]\n",
      "Epoch 1 [83/172] - Loss: 6.993 [-6.990, 0.004, -0.000]\n",
      "Epoch 1 [84/172] - Loss: 7.125 [-7.122, 0.004, -0.000]\n",
      "Epoch 1 [85/172] - Loss: 7.326 [-7.322, 0.004, -0.000]\n",
      "Epoch 1 [86/172] - Loss: 6.714 [-6.711, 0.004, -0.000]\n",
      "Epoch 1 [87/172] - Loss: 7.232 [-7.228, 0.004, -0.000]\n",
      "Epoch 1 [88/172] - Loss: 7.209 [-7.206, 0.004, -0.000]\n",
      "Epoch 1 [89/172] - Loss: 6.722 [-6.718, 0.004, -0.000]\n",
      "Epoch 1 [90/172] - Loss: 6.602 [-6.599, 0.003, -0.000]\n",
      "Epoch 1 [91/172] - Loss: 6.699 [-6.696, 0.003, -0.000]\n",
      "Epoch 1 [92/172] - Loss: 6.421 [-6.418, 0.003, -0.000]\n",
      "Epoch 1 [93/172] - Loss: 6.390 [-6.386, 0.003, -0.000]\n",
      "Epoch 1 [94/172] - Loss: 6.931 [-6.927, 0.003, -0.000]\n",
      "Epoch 1 [95/172] - Loss: 6.902 [-6.899, 0.003, -0.000]\n",
      "Epoch 1 [96/172] - Loss: 6.336 [-6.333, 0.003, -0.000]\n",
      "Epoch 1 [97/172] - Loss: 6.335 [-6.331, 0.003, -0.000]\n",
      "Epoch 1 [98/172] - Loss: 6.527 [-6.523, 0.003, -0.000]\n",
      "Epoch 1 [99/172] - Loss: 6.408 [-6.404, 0.003, -0.000]\n",
      "Epoch 1 [100/172] - Loss: 6.251 [-6.248, 0.003, -0.000]\n",
      "Epoch 1 [101/172] - Loss: 6.324 [-6.321, 0.003, -0.000]\n",
      "Epoch 1 [102/172] - Loss: 6.577 [-6.574, 0.003, -0.000]\n",
      "Epoch 1 [103/172] - Loss: 6.577 [-6.574, 0.003, -0.000]\n",
      "Epoch 1 [104/172] - Loss: 6.163 [-6.160, 0.003, -0.000]\n",
      "Epoch 1 [105/172] - Loss: 6.145 [-6.142, 0.003, -0.000]\n",
      "Epoch 1 [106/172] - Loss: 6.351 [-6.348, 0.003, -0.000]\n",
      "Epoch 1 [107/172] - Loss: 6.304 [-6.301, 0.003, -0.000]\n",
      "Epoch 1 [108/172] - Loss: 6.219 [-6.216, 0.003, -0.000]\n",
      "Epoch 1 [109/172] - Loss: 5.909 [-5.906, 0.003, -0.000]\n",
      "Epoch 1 [110/172] - Loss: 5.930 [-5.927, 0.003, -0.000]\n",
      "Epoch 1 [111/172] - Loss: 6.066 [-6.063, 0.003, -0.000]\n",
      "Epoch 1 [112/172] - Loss: 5.784 [-5.781, 0.003, -0.000]\n",
      "Epoch 1 [113/172] - Loss: 6.272 [-6.268, 0.003, -0.000]\n",
      "Epoch 1 [114/172] - Loss: 5.847 [-5.844, 0.003, -0.000]\n",
      "Epoch 1 [115/172] - Loss: 5.867 [-5.864, 0.003, -0.000]\n",
      "Epoch 1 [116/172] - Loss: 6.266 [-6.263, 0.003, -0.000]\n",
      "Epoch 1 [117/172] - Loss: 6.320 [-6.317, 0.003, -0.000]\n",
      "Epoch 1 [118/172] - Loss: 5.873 [-5.870, 0.003, -0.000]\n",
      "Epoch 1 [119/172] - Loss: 5.799 [-5.796, 0.003, -0.000]\n",
      "Epoch 1 [120/172] - Loss: 5.572 [-5.570, 0.003, -0.000]\n",
      "Epoch 1 [121/172] - Loss: 5.787 [-5.784, 0.003, -0.000]\n",
      "Epoch 1 [122/172] - Loss: 5.718 [-5.716, 0.003, -0.000]\n",
      "Epoch 1 [123/172] - Loss: 5.524 [-5.521, 0.003, -0.000]\n",
      "Epoch 1 [124/172] - Loss: 5.737 [-5.734, 0.003, -0.000]\n",
      "Epoch 1 [125/172] - Loss: 5.591 [-5.588, 0.003, -0.000]\n",
      "Epoch 1 [126/172] - Loss: 5.889 [-5.886, 0.003, -0.000]\n",
      "Epoch 1 [127/172] - Loss: 5.412 [-5.409, 0.003, -0.000]\n",
      "Epoch 1 [128/172] - Loss: 5.466 [-5.463, 0.003, -0.000]\n",
      "Epoch 1 [129/172] - Loss: 5.417 [-5.414, 0.003, -0.000]\n",
      "Epoch 1 [130/172] - Loss: 5.838 [-5.836, 0.003, -0.000]\n",
      "Epoch 1 [131/172] - Loss: 5.720 [-5.718, 0.003, -0.000]\n",
      "Epoch 1 [132/172] - Loss: 5.790 [-5.787, 0.003, -0.000]\n",
      "Epoch 1 [133/172] - Loss: 5.484 [-5.481, 0.003, -0.000]\n",
      "Epoch 1 [134/172] - Loss: 5.498 [-5.495, 0.003, -0.000]\n",
      "Epoch 1 [135/172] - Loss: 5.419 [-5.416, 0.003, -0.000]\n",
      "Epoch 1 [136/172] - Loss: 5.343 [-5.340, 0.003, -0.000]\n",
      "Epoch 1 [137/172] - Loss: 5.314 [-5.311, 0.003, -0.000]\n",
      "Epoch 1 [138/172] - Loss: 5.325 [-5.322, 0.003, -0.000]\n",
      "Epoch 1 [139/172] - Loss: 5.426 [-5.423, 0.003, -0.000]\n",
      "Epoch 1 [140/172] - Loss: 5.195 [-5.192, 0.003, -0.000]\n",
      "Epoch 1 [141/172] - Loss: 5.298 [-5.296, 0.003, -0.000]\n",
      "Epoch 1 [142/172] - Loss: 5.438 [-5.435, 0.003, -0.000]\n",
      "Epoch 1 [143/172] - Loss: 5.161 [-5.159, 0.003, -0.000]\n",
      "Epoch 1 [144/172] - Loss: 5.306 [-5.303, 0.003, -0.000]\n",
      "Epoch 1 [145/172] - Loss: 5.071 [-5.069, 0.003, -0.000]\n",
      "Epoch 1 [146/172] - Loss: 5.208 [-5.205, 0.003, -0.000]\n",
      "Epoch 1 [147/172] - Loss: 5.253 [-5.251, 0.003, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [148/172] - Loss: 5.267 [-5.265, 0.003, -0.000]\n",
      "Epoch 1 [149/172] - Loss: 5.167 [-5.165, 0.003, -0.000]\n",
      "Epoch 1 [150/172] - Loss: 5.099 [-5.096, 0.003, -0.000]\n",
      "Epoch 1 [151/172] - Loss: 5.061 [-5.059, 0.003, -0.000]\n",
      "Epoch 1 [152/172] - Loss: 5.291 [-5.288, 0.003, -0.000]\n",
      "Epoch 1 [153/172] - Loss: 5.101 [-5.098, 0.003, -0.000]\n",
      "Epoch 1 [154/172] - Loss: 5.121 [-5.119, 0.003, -0.000]\n",
      "Epoch 1 [155/172] - Loss: 4.892 [-4.889, 0.003, -0.000]\n",
      "Epoch 1 [156/172] - Loss: 4.994 [-4.992, 0.003, -0.000]\n",
      "Epoch 1 [157/172] - Loss: 5.229 [-5.227, 0.002, -0.000]\n",
      "Epoch 1 [158/172] - Loss: 4.926 [-4.923, 0.003, -0.000]\n",
      "Epoch 1 [159/172] - Loss: 5.028 [-5.026, 0.003, -0.000]\n",
      "Epoch 1 [160/172] - Loss: 4.975 [-4.973, 0.002, -0.000]\n",
      "Epoch 1 [161/172] - Loss: 5.083 [-5.081, 0.002, -0.000]\n",
      "Epoch 1 [162/172] - Loss: 5.236 [-5.234, 0.002, -0.000]\n",
      "Epoch 1 [163/172] - Loss: 5.302 [-5.299, 0.002, -0.000]\n",
      "Epoch 1 [164/172] - Loss: 5.162 [-5.159, 0.002, -0.000]\n",
      "Epoch 1 [165/172] - Loss: 5.098 [-5.096, 0.002, -0.000]\n",
      "Epoch 1 [166/172] - Loss: 4.853 [-4.851, 0.002, -0.000]\n",
      "Epoch 1 [167/172] - Loss: 5.016 [-5.013, 0.002, -0.000]\n",
      "Epoch 1 [168/172] - Loss: 4.948 [-4.946, 0.002, -0.000]\n",
      "Epoch 1 [169/172] - Loss: 4.898 [-4.896, 0.002, -0.000]\n",
      "Epoch 1 [170/172] - Loss: 4.949 [-4.947, 0.002, -0.000]\n",
      "Epoch 1 [171/172] - Loss: 4.893 [-4.890, 0.002, -0.000]\n",
      "Epoch 2 [0/172] - Loss: 4.932 [-4.930, 0.002, -0.000]\n",
      "Epoch 2 [1/172] - Loss: 4.979 [-4.976, 0.002, -0.000]\n",
      "Epoch 2 [2/172] - Loss: 4.918 [-4.916, 0.002, -0.000]\n",
      "Epoch 2 [3/172] - Loss: 4.932 [-4.930, 0.002, -0.000]\n",
      "Epoch 2 [4/172] - Loss: 4.699 [-4.697, 0.002, -0.000]\n",
      "Epoch 2 [5/172] - Loss: 4.917 [-4.915, 0.002, -0.000]\n",
      "Epoch 2 [6/172] - Loss: 5.175 [-5.173, 0.002, -0.000]\n",
      "Epoch 2 [7/172] - Loss: 4.821 [-4.818, 0.002, -0.000]\n",
      "Epoch 2 [8/172] - Loss: 4.837 [-4.835, 0.002, -0.000]\n",
      "Epoch 2 [9/172] - Loss: 4.803 [-4.800, 0.002, -0.000]\n",
      "Epoch 2 [10/172] - Loss: 4.912 [-4.910, 0.002, -0.000]\n",
      "Epoch 2 [11/172] - Loss: 5.018 [-5.015, 0.002, -0.000]\n",
      "Epoch 2 [12/172] - Loss: 4.788 [-4.786, 0.002, -0.000]\n",
      "Epoch 2 [13/172] - Loss: 4.752 [-4.750, 0.002, -0.000]\n",
      "Epoch 2 [14/172] - Loss: 4.801 [-4.799, 0.002, -0.000]\n",
      "Epoch 2 [15/172] - Loss: 4.614 [-4.612, 0.002, -0.000]\n",
      "Epoch 2 [16/172] - Loss: 4.990 [-4.987, 0.002, -0.000]\n",
      "Epoch 2 [17/172] - Loss: 4.887 [-4.884, 0.002, -0.000]\n",
      "Epoch 2 [18/172] - Loss: 4.747 [-4.745, 0.002, -0.000]\n",
      "Epoch 2 [19/172] - Loss: 4.748 [-4.746, 0.002, -0.000]\n",
      "Epoch 2 [20/172] - Loss: 4.694 [-4.692, 0.002, -0.000]\n",
      "Epoch 2 [21/172] - Loss: 4.795 [-4.792, 0.002, -0.000]\n",
      "Epoch 2 [22/172] - Loss: 4.736 [-4.733, 0.002, -0.000]\n",
      "Epoch 2 [23/172] - Loss: 4.830 [-4.828, 0.002, -0.000]\n",
      "Epoch 2 [24/172] - Loss: 4.873 [-4.871, 0.002, -0.000]\n",
      "Epoch 2 [25/172] - Loss: 4.809 [-4.807, 0.002, -0.000]\n",
      "Epoch 2 [26/172] - Loss: 4.991 [-4.989, 0.002, -0.000]\n",
      "Epoch 2 [27/172] - Loss: 4.846 [-4.843, 0.002, -0.000]\n",
      "Epoch 2 [28/172] - Loss: 4.760 [-4.758, 0.002, -0.000]\n",
      "Epoch 2 [29/172] - Loss: 4.574 [-4.572, 0.002, -0.000]\n",
      "Epoch 2 [30/172] - Loss: 4.789 [-4.787, 0.002, -0.000]\n",
      "Epoch 2 [31/172] - Loss: 4.659 [-4.657, 0.002, -0.000]\n",
      "Epoch 2 [32/172] - Loss: 4.866 [-4.863, 0.002, -0.000]\n",
      "Epoch 2 [33/172] - Loss: 4.768 [-4.766, 0.002, -0.000]\n",
      "Epoch 2 [34/172] - Loss: 4.801 [-4.799, 0.002, -0.000]\n",
      "Epoch 2 [35/172] - Loss: 4.816 [-4.814, 0.002, -0.000]\n",
      "Epoch 2 [36/172] - Loss: 4.637 [-4.635, 0.002, -0.000]\n",
      "Epoch 2 [37/172] - Loss: 4.660 [-4.658, 0.002, -0.000]\n",
      "Epoch 2 [38/172] - Loss: 4.537 [-4.535, 0.002, -0.000]\n",
      "Epoch 2 [39/172] - Loss: 4.666 [-4.664, 0.002, -0.000]\n",
      "Epoch 2 [40/172] - Loss: 4.608 [-4.606, 0.002, -0.000]\n",
      "Epoch 2 [41/172] - Loss: 4.796 [-4.794, 0.002, -0.000]\n",
      "Epoch 2 [42/172] - Loss: 4.816 [-4.814, 0.002, -0.000]\n",
      "Epoch 2 [43/172] - Loss: 4.626 [-4.623, 0.002, -0.000]\n",
      "Epoch 2 [44/172] - Loss: 4.648 [-4.646, 0.002, -0.000]\n",
      "Epoch 2 [45/172] - Loss: 4.578 [-4.576, 0.002, -0.000]\n",
      "Epoch 2 [46/172] - Loss: 4.723 [-4.721, 0.002, -0.000]\n",
      "Epoch 2 [47/172] - Loss: 4.515 [-4.513, 0.002, -0.000]\n",
      "Epoch 2 [48/172] - Loss: 4.636 [-4.634, 0.002, -0.000]\n",
      "Epoch 2 [49/172] - Loss: 4.432 [-4.430, 0.002, -0.000]\n",
      "Epoch 2 [50/172] - Loss: 4.493 [-4.491, 0.002, -0.000]\n",
      "Epoch 2 [51/172] - Loss: 4.541 [-4.539, 0.002, -0.000]\n",
      "Epoch 2 [52/172] - Loss: 4.477 [-4.475, 0.002, -0.000]\n",
      "Epoch 2 [53/172] - Loss: 4.517 [-4.515, 0.002, -0.000]\n",
      "Epoch 2 [54/172] - Loss: 4.541 [-4.538, 0.002, -0.000]\n",
      "Epoch 2 [55/172] - Loss: 4.546 [-4.544, 0.002, -0.000]\n",
      "Epoch 2 [56/172] - Loss: 4.643 [-4.641, 0.002, -0.000]\n",
      "Epoch 2 [57/172] - Loss: 4.593 [-4.591, 0.002, -0.000]\n",
      "Epoch 2 [58/172] - Loss: 4.444 [-4.442, 0.002, -0.000]\n",
      "Epoch 2 [59/172] - Loss: 4.446 [-4.444, 0.002, -0.000]\n",
      "Epoch 2 [60/172] - Loss: 4.521 [-4.519, 0.002, -0.000]\n",
      "Epoch 2 [61/172] - Loss: 4.548 [-4.545, 0.002, -0.000]\n",
      "Epoch 2 [62/172] - Loss: 4.529 [-4.527, 0.002, -0.000]\n",
      "Epoch 2 [63/172] - Loss: 4.522 [-4.520, 0.002, -0.000]\n",
      "Epoch 2 [64/172] - Loss: 4.534 [-4.532, 0.002, -0.000]\n",
      "Epoch 2 [65/172] - Loss: 4.399 [-4.397, 0.002, -0.000]\n",
      "Epoch 2 [66/172] - Loss: 4.642 [-4.640, 0.002, -0.000]\n",
      "Epoch 2 [67/172] - Loss: 4.482 [-4.480, 0.002, -0.000]\n",
      "Epoch 2 [68/172] - Loss: 4.451 [-4.449, 0.002, -0.000]\n",
      "Epoch 2 [69/172] - Loss: 4.486 [-4.484, 0.002, -0.000]\n",
      "Epoch 2 [70/172] - Loss: 4.394 [-4.392, 0.002, -0.000]\n",
      "Epoch 2 [71/172] - Loss: 4.450 [-4.448, 0.002, -0.000]\n",
      "Epoch 2 [72/172] - Loss: 4.509 [-4.507, 0.002, -0.000]\n",
      "Epoch 2 [73/172] - Loss: 4.468 [-4.466, 0.002, -0.000]\n",
      "Epoch 2 [74/172] - Loss: 4.660 [-4.658, 0.002, -0.000]\n",
      "Epoch 2 [75/172] - Loss: 4.513 [-4.511, 0.002, -0.000]\n",
      "Epoch 2 [76/172] - Loss: 4.587 [-4.585, 0.002, -0.000]\n",
      "Epoch 2 [77/172] - Loss: 4.482 [-4.480, 0.002, -0.000]\n",
      "Epoch 2 [78/172] - Loss: 4.425 [-4.423, 0.002, -0.000]\n",
      "Epoch 2 [79/172] - Loss: 4.394 [-4.392, 0.002, -0.000]\n",
      "Epoch 2 [80/172] - Loss: 4.528 [-4.526, 0.002, -0.000]\n",
      "Epoch 2 [81/172] - Loss: 4.467 [-4.465, 0.002, -0.000]\n",
      "Epoch 2 [82/172] - Loss: 4.360 [-4.358, 0.002, -0.000]\n",
      "Epoch 2 [83/172] - Loss: 4.455 [-4.453, 0.002, -0.000]\n",
      "Epoch 2 [84/172] - Loss: 4.540 [-4.538, 0.002, -0.000]\n",
      "Epoch 2 [85/172] - Loss: 4.395 [-4.393, 0.002, -0.000]\n",
      "Epoch 2 [86/172] - Loss: 4.434 [-4.432, 0.002, -0.000]\n",
      "Epoch 2 [87/172] - Loss: 4.403 [-4.401, 0.002, -0.000]\n",
      "Epoch 2 [88/172] - Loss: 4.477 [-4.475, 0.002, -0.000]\n",
      "Epoch 2 [89/172] - Loss: 4.392 [-4.390, 0.002, -0.000]\n",
      "Epoch 2 [90/172] - Loss: 4.581 [-4.579, 0.002, -0.000]\n",
      "Epoch 2 [91/172] - Loss: 4.345 [-4.343, 0.002, -0.000]\n",
      "Epoch 2 [92/172] - Loss: 4.558 [-4.556, 0.002, -0.000]\n",
      "Epoch 2 [93/172] - Loss: 4.511 [-4.509, 0.002, -0.000]\n",
      "Epoch 2 [94/172] - Loss: 4.643 [-4.641, 0.002, -0.000]\n",
      "Epoch 2 [95/172] - Loss: 4.393 [-4.391, 0.002, -0.000]\n",
      "Epoch 2 [96/172] - Loss: 4.385 [-4.383, 0.002, -0.000]\n",
      "Epoch 2 [97/172] - Loss: 4.493 [-4.491, 0.002, -0.000]\n",
      "Epoch 2 [98/172] - Loss: 4.409 [-4.407, 0.002, -0.000]\n",
      "Epoch 2 [99/172] - Loss: 4.453 [-4.451, 0.002, -0.000]\n",
      "Epoch 2 [100/172] - Loss: 4.473 [-4.471, 0.002, -0.000]\n",
      "Epoch 2 [101/172] - Loss: 4.413 [-4.411, 0.002, -0.000]\n",
      "Epoch 2 [102/172] - Loss: 4.407 [-4.405, 0.002, -0.000]\n",
      "Epoch 2 [103/172] - Loss: 4.368 [-4.366, 0.002, -0.000]\n",
      "Epoch 2 [104/172] - Loss: 4.461 [-4.459, 0.002, -0.000]\n",
      "Epoch 2 [105/172] - Loss: 4.337 [-4.336, 0.002, -0.000]\n",
      "Epoch 2 [106/172] - Loss: 4.348 [-4.346, 0.002, -0.000]\n",
      "Epoch 2 [107/172] - Loss: 4.416 [-4.414, 0.002, -0.000]\n",
      "Epoch 2 [108/172] - Loss: 4.373 [-4.371, 0.002, -0.000]\n",
      "Epoch 2 [109/172] - Loss: 4.261 [-4.260, 0.002, -0.000]\n",
      "Epoch 2 [110/172] - Loss: 4.481 [-4.479, 0.002, -0.000]\n",
      "Epoch 2 [111/172] - Loss: 4.434 [-4.432, 0.002, -0.000]\n",
      "Epoch 2 [112/172] - Loss: 4.358 [-4.357, 0.002, -0.000]\n",
      "Epoch 2 [113/172] - Loss: 4.338 [-4.336, 0.002, -0.000]\n",
      "Epoch 2 [114/172] - Loss: 4.415 [-4.413, 0.002, -0.000]\n",
      "Epoch 2 [115/172] - Loss: 4.317 [-4.315, 0.002, -0.000]\n",
      "Epoch 2 [116/172] - Loss: 4.411 [-4.409, 0.002, -0.000]\n",
      "Epoch 2 [117/172] - Loss: 4.197 [-4.195, 0.002, -0.000]\n",
      "Epoch 2 [118/172] - Loss: 4.305 [-4.304, 0.002, -0.000]\n",
      "Epoch 2 [119/172] - Loss: 4.327 [-4.326, 0.002, -0.000]\n",
      "Epoch 2 [120/172] - Loss: 4.346 [-4.344, 0.002, -0.000]\n",
      "Epoch 2 [121/172] - Loss: 4.387 [-4.385, 0.002, -0.000]\n",
      "Epoch 2 [122/172] - Loss: 4.342 [-4.340, 0.002, -0.000]\n",
      "Epoch 2 [123/172] - Loss: 4.271 [-4.269, 0.002, -0.000]\n",
      "Epoch 2 [124/172] - Loss: 4.337 [-4.335, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [125/172] - Loss: 4.161 [-4.159, 0.002, -0.000]\n",
      "Epoch 2 [126/172] - Loss: 4.300 [-4.299, 0.002, -0.000]\n",
      "Epoch 2 [127/172] - Loss: 4.352 [-4.350, 0.002, -0.000]\n",
      "Epoch 2 [128/172] - Loss: 4.249 [-4.248, 0.002, -0.000]\n",
      "Epoch 2 [129/172] - Loss: 4.311 [-4.309, 0.002, -0.000]\n",
      "Epoch 2 [130/172] - Loss: 4.268 [-4.266, 0.002, -0.000]\n",
      "Epoch 2 [131/172] - Loss: 4.286 [-4.284, 0.002, -0.000]\n",
      "Epoch 2 [132/172] - Loss: 4.346 [-4.344, 0.002, -0.000]\n",
      "Epoch 2 [133/172] - Loss: 4.267 [-4.265, 0.002, -0.000]\n",
      "Epoch 2 [134/172] - Loss: 4.237 [-4.235, 0.002, -0.000]\n",
      "Epoch 2 [135/172] - Loss: 4.275 [-4.273, 0.002, -0.000]\n",
      "Epoch 2 [136/172] - Loss: 4.207 [-4.205, 0.002, -0.000]\n",
      "Epoch 2 [137/172] - Loss: 4.341 [-4.340, 0.002, -0.000]\n",
      "Epoch 2 [138/172] - Loss: 4.300 [-4.299, 0.002, -0.000]\n",
      "Epoch 2 [139/172] - Loss: 4.266 [-4.264, 0.002, -0.000]\n",
      "Epoch 2 [140/172] - Loss: 4.249 [-4.247, 0.002, -0.000]\n",
      "Epoch 2 [141/172] - Loss: 4.284 [-4.282, 0.002, -0.000]\n",
      "Epoch 2 [142/172] - Loss: 4.171 [-4.169, 0.002, -0.000]\n",
      "Epoch 2 [143/172] - Loss: 4.267 [-4.265, 0.002, -0.000]\n",
      "Epoch 2 [144/172] - Loss: 4.310 [-4.308, 0.002, -0.000]\n",
      "Epoch 2 [145/172] - Loss: 4.223 [-4.222, 0.002, -0.000]\n",
      "Epoch 2 [146/172] - Loss: 4.276 [-4.275, 0.002, -0.000]\n",
      "Epoch 2 [147/172] - Loss: 4.446 [-4.444, 0.002, -0.000]\n",
      "Epoch 2 [148/172] - Loss: 4.341 [-4.339, 0.002, -0.000]\n",
      "Epoch 2 [149/172] - Loss: 4.306 [-4.304, 0.002, -0.000]\n",
      "Epoch 2 [150/172] - Loss: 4.328 [-4.326, 0.002, -0.000]\n",
      "Epoch 2 [151/172] - Loss: 4.254 [-4.252, 0.002, -0.000]\n",
      "Epoch 2 [152/172] - Loss: 4.206 [-4.204, 0.002, -0.000]\n",
      "Epoch 2 [153/172] - Loss: 4.265 [-4.263, 0.002, -0.000]\n",
      "Epoch 2 [154/172] - Loss: 4.270 [-4.268, 0.002, -0.000]\n",
      "Epoch 2 [155/172] - Loss: 4.322 [-4.320, 0.002, -0.000]\n",
      "Epoch 2 [156/172] - Loss: 4.251 [-4.249, 0.002, -0.000]\n",
      "Epoch 2 [157/172] - Loss: 4.272 [-4.270, 0.002, -0.000]\n",
      "Epoch 2 [158/172] - Loss: 4.202 [-4.200, 0.002, -0.000]\n",
      "Epoch 2 [159/172] - Loss: 4.314 [-4.312, 0.002, -0.000]\n",
      "Epoch 2 [160/172] - Loss: 4.212 [-4.211, 0.002, -0.000]\n",
      "Epoch 2 [161/172] - Loss: 4.253 [-4.251, 0.002, -0.000]\n",
      "Epoch 2 [162/172] - Loss: 4.242 [-4.240, 0.002, -0.000]\n",
      "Epoch 2 [163/172] - Loss: 4.286 [-4.284, 0.002, -0.000]\n",
      "Epoch 2 [164/172] - Loss: 4.232 [-4.230, 0.002, -0.000]\n",
      "Epoch 2 [165/172] - Loss: 4.324 [-4.323, 0.002, -0.000]\n",
      "Epoch 2 [166/172] - Loss: 4.300 [-4.298, 0.002, -0.000]\n",
      "Epoch 2 [167/172] - Loss: 4.365 [-4.363, 0.002, -0.000]\n",
      "Epoch 2 [168/172] - Loss: 4.252 [-4.251, 0.002, -0.000]\n",
      "Epoch 2 [169/172] - Loss: 4.351 [-4.349, 0.002, -0.000]\n",
      "Epoch 2 [170/172] - Loss: 4.169 [-4.167, 0.002, -0.000]\n",
      "Epoch 2 [171/172] - Loss: 4.299 [-4.297, 0.002, -0.000]\n",
      "Epoch 3 [0/172] - Loss: 4.218 [-4.216, 0.002, -0.000]\n",
      "Epoch 3 [1/172] - Loss: 4.266 [-4.264, 0.002, -0.000]\n",
      "Epoch 3 [2/172] - Loss: 4.112 [-4.110, 0.002, -0.000]\n",
      "Epoch 3 [3/172] - Loss: 4.234 [-4.232, 0.002, -0.000]\n",
      "Epoch 3 [4/172] - Loss: 4.216 [-4.214, 0.002, -0.000]\n",
      "Epoch 3 [5/172] - Loss: 4.189 [-4.187, 0.002, -0.000]\n",
      "Epoch 3 [6/172] - Loss: 4.340 [-4.339, 0.002, -0.000]\n",
      "Epoch 3 [7/172] - Loss: 4.211 [-4.209, 0.002, -0.000]\n",
      "Epoch 3 [8/172] - Loss: 4.211 [-4.210, 0.002, -0.000]\n",
      "Epoch 3 [9/172] - Loss: 4.233 [-4.231, 0.002, -0.000]\n",
      "Epoch 3 [10/172] - Loss: 4.360 [-4.358, 0.002, -0.000]\n",
      "Epoch 3 [11/172] - Loss: 4.304 [-4.302, 0.002, -0.000]\n",
      "Epoch 3 [12/172] - Loss: 4.307 [-4.305, 0.002, -0.000]\n",
      "Epoch 3 [13/172] - Loss: 4.296 [-4.295, 0.002, -0.000]\n",
      "Epoch 3 [14/172] - Loss: 4.149 [-4.147, 0.002, -0.000]\n",
      "Epoch 3 [15/172] - Loss: 4.269 [-4.267, 0.002, -0.000]\n",
      "Epoch 3 [16/172] - Loss: 4.212 [-4.211, 0.002, -0.000]\n",
      "Epoch 3 [17/172] - Loss: 4.240 [-4.238, 0.002, -0.000]\n",
      "Epoch 3 [18/172] - Loss: 4.156 [-4.155, 0.002, -0.000]\n",
      "Epoch 3 [19/172] - Loss: 4.234 [-4.232, 0.002, -0.000]\n",
      "Epoch 3 [20/172] - Loss: 4.248 [-4.246, 0.002, -0.000]\n",
      "Epoch 3 [21/172] - Loss: 4.205 [-4.204, 0.002, -0.000]\n",
      "Epoch 3 [22/172] - Loss: 4.122 [-4.121, 0.002, -0.000]\n",
      "Epoch 3 [23/172] - Loss: 4.247 [-4.245, 0.002, -0.000]\n",
      "Epoch 3 [24/172] - Loss: 4.182 [-4.180, 0.002, -0.000]\n",
      "Epoch 3 [25/172] - Loss: 4.215 [-4.214, 0.002, -0.000]\n",
      "Epoch 3 [26/172] - Loss: 4.132 [-4.131, 0.002, -0.000]\n",
      "Epoch 3 [27/172] - Loss: 4.141 [-4.139, 0.002, -0.000]\n",
      "Epoch 3 [28/172] - Loss: 4.137 [-4.135, 0.002, -0.000]\n",
      "Epoch 3 [29/172] - Loss: 4.156 [-4.154, 0.002, -0.000]\n",
      "Epoch 3 [30/172] - Loss: 4.190 [-4.188, 0.002, -0.000]\n",
      "Epoch 3 [31/172] - Loss: 4.160 [-4.158, 0.002, -0.000]\n",
      "Epoch 3 [32/172] - Loss: 4.047 [-4.045, 0.002, -0.000]\n",
      "Epoch 3 [33/172] - Loss: 4.147 [-4.145, 0.002, -0.000]\n",
      "Epoch 3 [34/172] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [35/172] - Loss: 4.119 [-4.118, 0.002, -0.000]\n",
      "Epoch 3 [36/172] - Loss: 4.196 [-4.194, 0.002, -0.000]\n",
      "Epoch 3 [37/172] - Loss: 4.201 [-4.199, 0.002, -0.000]\n",
      "Epoch 3 [38/172] - Loss: 4.222 [-4.221, 0.002, -0.000]\n",
      "Epoch 3 [39/172] - Loss: 4.191 [-4.189, 0.002, -0.000]\n",
      "Epoch 3 [40/172] - Loss: 4.097 [-4.095, 0.002, -0.000]\n",
      "Epoch 3 [41/172] - Loss: 4.131 [-4.129, 0.002, -0.000]\n",
      "Epoch 3 [42/172] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [43/172] - Loss: 4.173 [-4.171, 0.002, -0.000]\n",
      "Epoch 3 [44/172] - Loss: 4.257 [-4.256, 0.002, -0.000]\n",
      "Epoch 3 [45/172] - Loss: 4.081 [-4.079, 0.002, -0.000]\n",
      "Epoch 3 [46/172] - Loss: 4.183 [-4.181, 0.002, -0.000]\n",
      "Epoch 3 [47/172] - Loss: 4.296 [-4.295, 0.002, -0.000]\n",
      "Epoch 3 [48/172] - Loss: 4.151 [-4.150, 0.002, -0.000]\n",
      "Epoch 3 [49/172] - Loss: 4.230 [-4.228, 0.002, -0.000]\n",
      "Epoch 3 [50/172] - Loss: 4.173 [-4.172, 0.002, -0.000]\n",
      "Epoch 3 [51/172] - Loss: 4.215 [-4.213, 0.002, -0.000]\n",
      "Epoch 3 [52/172] - Loss: 4.103 [-4.101, 0.002, -0.000]\n",
      "Epoch 3 [53/172] - Loss: 4.221 [-4.220, 0.002, -0.000]\n",
      "Epoch 3 [54/172] - Loss: 4.106 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [55/172] - Loss: 4.178 [-4.177, 0.002, -0.000]\n",
      "Epoch 3 [56/172] - Loss: 4.081 [-4.080, 0.002, -0.000]\n",
      "Epoch 3 [57/172] - Loss: 4.241 [-4.240, 0.002, -0.000]\n",
      "Epoch 3 [58/172] - Loss: 4.164 [-4.162, 0.002, -0.000]\n",
      "Epoch 3 [59/172] - Loss: 4.122 [-4.121, 0.002, -0.000]\n",
      "Epoch 3 [60/172] - Loss: 4.124 [-4.122, 0.002, -0.000]\n",
      "Epoch 3 [61/172] - Loss: 4.139 [-4.138, 0.002, -0.000]\n",
      "Epoch 3 [62/172] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [63/172] - Loss: 4.133 [-4.131, 0.002, -0.000]\n",
      "Epoch 3 [64/172] - Loss: 4.131 [-4.130, 0.002, -0.000]\n",
      "Epoch 3 [65/172] - Loss: 4.203 [-4.201, 0.002, -0.000]\n",
      "Epoch 3 [66/172] - Loss: 4.178 [-4.176, 0.002, -0.000]\n",
      "Epoch 3 [67/172] - Loss: 4.180 [-4.178, 0.002, -0.000]\n",
      "Epoch 3 [68/172] - Loss: 4.224 [-4.222, 0.002, -0.000]\n",
      "Epoch 3 [69/172] - Loss: 4.206 [-4.205, 0.002, -0.000]\n",
      "Epoch 3 [70/172] - Loss: 4.122 [-4.121, 0.002, -0.000]\n",
      "Epoch 3 [71/172] - Loss: 4.215 [-4.213, 0.002, -0.000]\n",
      "Epoch 3 [72/172] - Loss: 4.154 [-4.152, 0.002, -0.000]\n",
      "Epoch 3 [73/172] - Loss: 4.134 [-4.133, 0.002, -0.000]\n",
      "Epoch 3 [74/172] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [75/172] - Loss: 4.172 [-4.171, 0.002, -0.000]\n",
      "Epoch 3 [76/172] - Loss: 4.122 [-4.121, 0.002, -0.000]\n",
      "Epoch 3 [77/172] - Loss: 4.110 [-4.108, 0.002, -0.000]\n",
      "Epoch 3 [78/172] - Loss: 4.066 [-4.065, 0.002, -0.000]\n",
      "Epoch 3 [79/172] - Loss: 4.143 [-4.141, 0.002, -0.000]\n",
      "Epoch 3 [80/172] - Loss: 4.172 [-4.170, 0.002, -0.000]\n",
      "Epoch 3 [81/172] - Loss: 4.115 [-4.113, 0.002, -0.000]\n",
      "Epoch 3 [82/172] - Loss: 4.254 [-4.253, 0.002, -0.000]\n",
      "Epoch 3 [83/172] - Loss: 4.084 [-4.082, 0.002, -0.000]\n",
      "Epoch 3 [84/172] - Loss: 4.109 [-4.107, 0.002, -0.000]\n",
      "Epoch 3 [85/172] - Loss: 4.170 [-4.169, 0.002, -0.000]\n",
      "Epoch 3 [86/172] - Loss: 4.181 [-4.179, 0.002, -0.000]\n",
      "Epoch 3 [87/172] - Loss: 4.100 [-4.098, 0.002, -0.000]\n",
      "Epoch 3 [88/172] - Loss: 4.176 [-4.174, 0.002, -0.000]\n",
      "Epoch 3 [89/172] - Loss: 4.117 [-4.116, 0.002, -0.000]\n",
      "Epoch 3 [90/172] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [91/172] - Loss: 4.120 [-4.118, 0.002, -0.000]\n",
      "Epoch 3 [92/172] - Loss: 4.135 [-4.133, 0.002, -0.000]\n",
      "Epoch 3 [93/172] - Loss: 4.144 [-4.142, 0.002, -0.000]\n",
      "Epoch 3 [94/172] - Loss: 4.141 [-4.140, 0.002, -0.000]\n",
      "Epoch 3 [95/172] - Loss: 4.096 [-4.095, 0.002, -0.000]\n",
      "Epoch 3 [96/172] - Loss: 4.153 [-4.151, 0.002, -0.000]\n",
      "Epoch 3 [97/172] - Loss: 4.167 [-4.165, 0.002, -0.000]\n",
      "Epoch 3 [98/172] - Loss: 4.200 [-4.198, 0.002, -0.000]\n",
      "Epoch 3 [99/172] - Loss: 4.095 [-4.093, 0.002, -0.000]\n",
      "Epoch 3 [100/172] - Loss: 4.113 [-4.111, 0.002, -0.000]\n",
      "Epoch 3 [101/172] - Loss: 4.093 [-4.091, 0.002, -0.000]\n",
      "Epoch 3 [102/172] - Loss: 4.148 [-4.146, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [103/172] - Loss: 4.125 [-4.124, 0.002, -0.000]\n",
      "Epoch 3 [104/172] - Loss: 4.100 [-4.098, 0.002, -0.000]\n",
      "Epoch 3 [105/172] - Loss: 4.225 [-4.224, 0.002, -0.000]\n",
      "Epoch 3 [106/172] - Loss: 4.151 [-4.150, 0.002, -0.000]\n",
      "Epoch 3 [107/172] - Loss: 4.116 [-4.114, 0.002, -0.000]\n",
      "Epoch 3 [108/172] - Loss: 4.116 [-4.114, 0.002, -0.000]\n",
      "Epoch 3 [109/172] - Loss: 4.056 [-4.054, 0.002, -0.000]\n",
      "Epoch 3 [110/172] - Loss: 4.107 [-4.106, 0.002, -0.000]\n",
      "Epoch 3 [111/172] - Loss: 4.082 [-4.080, 0.002, -0.000]\n",
      "Epoch 3 [112/172] - Loss: 4.147 [-4.145, 0.002, -0.000]\n",
      "Epoch 3 [113/172] - Loss: 4.177 [-4.175, 0.002, -0.000]\n",
      "Epoch 3 [114/172] - Loss: 4.081 [-4.079, 0.002, -0.000]\n",
      "Epoch 3 [115/172] - Loss: 4.158 [-4.156, 0.002, -0.000]\n",
      "Epoch 3 [116/172] - Loss: 4.088 [-4.086, 0.002, -0.000]\n",
      "Epoch 3 [117/172] - Loss: 4.114 [-4.112, 0.001, -0.000]\n",
      "Epoch 3 [118/172] - Loss: 4.124 [-4.122, 0.002, -0.000]\n",
      "Epoch 3 [119/172] - Loss: 4.199 [-4.197, 0.002, -0.000]\n",
      "Epoch 3 [120/172] - Loss: 4.247 [-4.246, 0.002, -0.000]\n",
      "Epoch 3 [121/172] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [122/172] - Loss: 4.131 [-4.129, 0.002, -0.000]\n",
      "Epoch 3 [123/172] - Loss: 4.118 [-4.117, 0.002, -0.000]\n",
      "Epoch 3 [124/172] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [125/172] - Loss: 4.122 [-4.120, 0.002, -0.000]\n",
      "Epoch 3 [126/172] - Loss: 4.113 [-4.112, 0.002, -0.000]\n",
      "Epoch 3 [127/172] - Loss: 4.104 [-4.102, 0.002, -0.000]\n",
      "Epoch 3 [128/172] - Loss: 4.083 [-4.082, 0.002, -0.000]\n",
      "Epoch 3 [129/172] - Loss: 4.192 [-4.190, 0.002, -0.000]\n",
      "Epoch 3 [130/172] - Loss: 4.072 [-4.070, 0.002, -0.000]\n",
      "Epoch 3 [131/172] - Loss: 4.124 [-4.123, 0.002, -0.000]\n",
      "Epoch 3 [132/172] - Loss: 4.105 [-4.103, 0.002, -0.000]\n",
      "Epoch 3 [133/172] - Loss: 4.098 [-4.097, 0.002, -0.000]\n",
      "Epoch 3 [134/172] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 3 [135/172] - Loss: 4.054 [-4.053, 0.002, -0.000]\n",
      "Epoch 3 [136/172] - Loss: 4.066 [-4.064, 0.002, -0.000]\n",
      "Epoch 3 [137/172] - Loss: 4.071 [-4.069, 0.002, -0.000]\n",
      "Epoch 3 [138/172] - Loss: 4.026 [-4.025, 0.002, -0.000]\n",
      "Epoch 3 [139/172] - Loss: 4.109 [-4.107, 0.002, -0.000]\n",
      "Epoch 3 [140/172] - Loss: 4.160 [-4.159, 0.002, -0.000]\n",
      "Epoch 3 [141/172] - Loss: 4.083 [-4.082, 0.002, -0.000]\n",
      "Epoch 3 [142/172] - Loss: 4.099 [-4.098, 0.002, -0.000]\n",
      "Epoch 3 [143/172] - Loss: 4.111 [-4.109, 0.002, -0.000]\n",
      "Epoch 3 [144/172] - Loss: 4.135 [-4.133, 0.001, -0.000]\n",
      "Epoch 3 [145/172] - Loss: 4.085 [-4.083, 0.002, -0.000]\n",
      "Epoch 3 [146/172] - Loss: 4.067 [-4.065, 0.002, -0.000]\n",
      "Epoch 3 [147/172] - Loss: 4.114 [-4.112, 0.002, -0.000]\n",
      "Epoch 3 [148/172] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [149/172] - Loss: 4.071 [-4.070, 0.001, -0.000]\n",
      "Epoch 3 [150/172] - Loss: 4.044 [-4.043, 0.002, -0.000]\n",
      "Epoch 3 [151/172] - Loss: 4.141 [-4.139, 0.001, -0.000]\n",
      "Epoch 3 [152/172] - Loss: 4.147 [-4.146, 0.002, -0.000]\n",
      "Epoch 3 [153/172] - Loss: 4.083 [-4.082, 0.002, -0.000]\n",
      "Epoch 3 [154/172] - Loss: 4.129 [-4.127, 0.001, -0.000]\n",
      "Epoch 3 [155/172] - Loss: 4.127 [-4.125, 0.002, -0.000]\n",
      "Epoch 3 [156/172] - Loss: 4.089 [-4.087, 0.001, -0.000]\n",
      "Epoch 3 [157/172] - Loss: 4.157 [-4.155, 0.002, -0.000]\n",
      "Epoch 3 [158/172] - Loss: 4.076 [-4.074, 0.002, -0.000]\n",
      "Epoch 3 [159/172] - Loss: 4.115 [-4.113, 0.001, -0.000]\n",
      "Epoch 3 [160/172] - Loss: 4.104 [-4.102, 0.002, -0.000]\n",
      "Epoch 3 [161/172] - Loss: 4.120 [-4.118, 0.001, -0.000]\n",
      "Epoch 3 [162/172] - Loss: 4.059 [-4.057, 0.002, -0.000]\n",
      "Epoch 3 [163/172] - Loss: 4.058 [-4.056, 0.002, -0.000]\n",
      "Epoch 3 [164/172] - Loss: 4.066 [-4.065, 0.001, -0.000]\n",
      "Epoch 3 [165/172] - Loss: 4.076 [-4.074, 0.001, -0.000]\n",
      "Epoch 3 [166/172] - Loss: 4.143 [-4.141, 0.001, -0.000]\n",
      "Epoch 3 [167/172] - Loss: 4.139 [-4.137, 0.002, -0.000]\n",
      "Epoch 3 [168/172] - Loss: 4.126 [-4.125, 0.001, -0.000]\n",
      "Epoch 3 [169/172] - Loss: 4.136 [-4.134, 0.002, -0.000]\n",
      "Epoch 3 [170/172] - Loss: 4.094 [-4.092, 0.001, -0.000]\n",
      "Epoch 3 [171/172] - Loss: 4.096 [-4.094, 0.001, -0.000]\n",
      "Epoch 4 [0/172] - Loss: 4.077 [-4.076, 0.001, -0.000]\n",
      "Epoch 4 [1/172] - Loss: 4.105 [-4.103, 0.001, -0.000]\n",
      "Epoch 4 [2/172] - Loss: 4.130 [-4.129, 0.001, -0.000]\n",
      "Epoch 4 [3/172] - Loss: 4.023 [-4.021, 0.001, -0.000]\n",
      "Epoch 4 [4/172] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 4 [5/172] - Loss: 4.037 [-4.035, 0.002, -0.000]\n",
      "Epoch 4 [6/172] - Loss: 4.109 [-4.108, 0.002, -0.000]\n",
      "Epoch 4 [7/172] - Loss: 4.034 [-4.032, 0.002, -0.000]\n",
      "Epoch 4 [8/172] - Loss: 4.078 [-4.077, 0.001, -0.000]\n",
      "Epoch 4 [9/172] - Loss: 4.052 [-4.051, 0.002, -0.000]\n",
      "Epoch 4 [10/172] - Loss: 4.065 [-4.064, 0.001, -0.000]\n",
      "Epoch 4 [11/172] - Loss: 4.055 [-4.054, 0.002, -0.000]\n",
      "Epoch 4 [12/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 4 [13/172] - Loss: 4.029 [-4.028, 0.002, -0.000]\n",
      "Epoch 4 [14/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 4 [15/172] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 4 [16/172] - Loss: 4.056 [-4.054, 0.001, -0.000]\n",
      "Epoch 4 [17/172] - Loss: 4.071 [-4.069, 0.001, -0.000]\n",
      "Epoch 4 [18/172] - Loss: 4.120 [-4.118, 0.001, -0.000]\n",
      "Epoch 4 [19/172] - Loss: 4.011 [-4.010, 0.001, -0.000]\n",
      "Epoch 4 [20/172] - Loss: 4.000 [-3.999, 0.001, -0.000]\n",
      "Epoch 4 [21/172] - Loss: 4.075 [-4.074, 0.002, -0.000]\n",
      "Epoch 4 [22/172] - Loss: 4.079 [-4.078, 0.001, -0.000]\n",
      "Epoch 4 [23/172] - Loss: 4.079 [-4.077, 0.001, -0.000]\n",
      "Epoch 4 [24/172] - Loss: 4.046 [-4.044, 0.001, -0.000]\n",
      "Epoch 4 [25/172] - Loss: 4.024 [-4.022, 0.001, -0.000]\n",
      "Epoch 4 [26/172] - Loss: 4.026 [-4.024, 0.001, -0.000]\n",
      "Epoch 4 [27/172] - Loss: 4.002 [-4.000, 0.001, -0.000]\n",
      "Epoch 4 [28/172] - Loss: 4.016 [-4.015, 0.002, -0.000]\n",
      "Epoch 4 [29/172] - Loss: 4.030 [-4.028, 0.001, -0.000]\n",
      "Epoch 4 [30/172] - Loss: 4.053 [-4.052, 0.001, -0.000]\n",
      "Epoch 4 [31/172] - Loss: 4.002 [-4.001, 0.001, -0.000]\n",
      "Epoch 4 [32/172] - Loss: 4.053 [-4.051, 0.001, -0.000]\n",
      "Epoch 4 [33/172] - Loss: 4.057 [-4.056, 0.002, -0.000]\n",
      "Epoch 4 [34/172] - Loss: 4.095 [-4.093, 0.001, -0.000]\n",
      "Epoch 4 [35/172] - Loss: 4.045 [-4.044, 0.001, -0.000]\n",
      "Epoch 4 [36/172] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 4 [37/172] - Loss: 4.020 [-4.019, 0.001, -0.000]\n",
      "Epoch 4 [38/172] - Loss: 4.043 [-4.041, 0.001, -0.000]\n",
      "Epoch 4 [39/172] - Loss: 4.109 [-4.108, 0.001, -0.000]\n",
      "Epoch 4 [40/172] - Loss: 4.060 [-4.059, 0.001, -0.000]\n",
      "Epoch 4 [41/172] - Loss: 4.008 [-4.007, 0.001, -0.000]\n",
      "Epoch 4 [42/172] - Loss: 4.044 [-4.042, 0.001, -0.000]\n",
      "Epoch 4 [43/172] - Loss: 4.011 [-4.010, 0.001, -0.000]\n",
      "Epoch 4 [44/172] - Loss: 4.025 [-4.024, 0.001, -0.000]\n",
      "Epoch 4 [45/172] - Loss: 4.060 [-4.058, 0.001, -0.000]\n",
      "Epoch 4 [46/172] - Loss: 4.071 [-4.070, 0.001, -0.000]\n",
      "Epoch 4 [47/172] - Loss: 4.124 [-4.123, 0.002, -0.000]\n",
      "Epoch 4 [48/172] - Loss: 4.019 [-4.018, 0.001, -0.000]\n",
      "Epoch 4 [49/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 4 [50/172] - Loss: 4.042 [-4.040, 0.002, -0.000]\n",
      "Epoch 4 [51/172] - Loss: 4.095 [-4.093, 0.001, -0.000]\n",
      "Epoch 4 [52/172] - Loss: 4.047 [-4.045, 0.001, -0.000]\n",
      "Epoch 4 [53/172] - Loss: 4.030 [-4.028, 0.001, -0.000]\n",
      "Epoch 4 [54/172] - Loss: 3.973 [-3.972, 0.001, -0.000]\n",
      "Epoch 4 [55/172] - Loss: 4.087 [-4.086, 0.001, -0.000]\n",
      "Epoch 4 [56/172] - Loss: 4.045 [-4.044, 0.001, -0.000]\n",
      "Epoch 4 [57/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 4 [58/172] - Loss: 4.021 [-4.020, 0.002, -0.000]\n",
      "Epoch 4 [59/172] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 4 [60/172] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 4 [61/172] - Loss: 4.110 [-4.109, 0.001, -0.000]\n",
      "Epoch 4 [62/172] - Loss: 4.028 [-4.027, 0.002, -0.000]\n",
      "Epoch 4 [63/172] - Loss: 3.978 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [64/172] - Loss: 3.967 [-3.965, 0.001, -0.000]\n",
      "Epoch 4 [65/172] - Loss: 3.965 [-3.964, 0.002, -0.000]\n",
      "Epoch 4 [66/172] - Loss: 4.001 [-4.000, 0.001, -0.000]\n",
      "Epoch 4 [67/172] - Loss: 4.038 [-4.036, 0.002, -0.000]\n",
      "Epoch 4 [68/172] - Loss: 4.026 [-4.025, 0.002, -0.000]\n",
      "Epoch 4 [69/172] - Loss: 4.032 [-4.031, 0.002, -0.000]\n",
      "Epoch 4 [70/172] - Loss: 4.027 [-4.025, 0.002, -0.000]\n",
      "Epoch 4 [71/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 4 [72/172] - Loss: 4.081 [-4.079, 0.001, -0.000]\n",
      "Epoch 4 [73/172] - Loss: 4.037 [-4.036, 0.001, -0.000]\n",
      "Epoch 4 [74/172] - Loss: 4.043 [-4.041, 0.002, -0.000]\n",
      "Epoch 4 [75/172] - Loss: 3.996 [-3.994, 0.001, -0.000]\n",
      "Epoch 4 [76/172] - Loss: 4.075 [-4.073, 0.001, -0.000]\n",
      "Epoch 4 [77/172] - Loss: 4.081 [-4.079, 0.001, -0.000]\n",
      "Epoch 4 [78/172] - Loss: 4.057 [-4.055, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [79/172] - Loss: 4.051 [-4.050, 0.001, -0.000]\n",
      "Epoch 4 [80/172] - Loss: 4.049 [-4.048, 0.002, -0.000]\n",
      "Epoch 4 [81/172] - Loss: 4.068 [-4.067, 0.001, -0.000]\n",
      "Epoch 4 [82/172] - Loss: 4.070 [-4.068, 0.002, -0.000]\n",
      "Epoch 4 [83/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 4 [84/172] - Loss: 4.062 [-4.060, 0.001, -0.000]\n",
      "Epoch 4 [85/172] - Loss: 4.043 [-4.042, 0.001, -0.000]\n",
      "Epoch 4 [86/172] - Loss: 4.027 [-4.025, 0.001, -0.000]\n",
      "Epoch 4 [87/172] - Loss: 3.994 [-3.992, 0.001, -0.000]\n",
      "Epoch 4 [88/172] - Loss: 4.060 [-4.059, 0.002, -0.000]\n",
      "Epoch 4 [89/172] - Loss: 3.987 [-3.986, 0.001, -0.000]\n",
      "Epoch 4 [90/172] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 4 [91/172] - Loss: 3.990 [-3.989, 0.001, -0.000]\n",
      "Epoch 4 [92/172] - Loss: 3.996 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [93/172] - Loss: 4.094 [-4.093, 0.001, -0.000]\n",
      "Epoch 4 [94/172] - Loss: 3.989 [-3.987, 0.001, -0.000]\n",
      "Epoch 4 [95/172] - Loss: 4.036 [-4.034, 0.001, -0.000]\n",
      "Epoch 4 [96/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 4 [97/172] - Loss: 3.973 [-3.972, 0.001, -0.000]\n",
      "Epoch 4 [98/172] - Loss: 4.033 [-4.031, 0.001, -0.000]\n",
      "Epoch 4 [99/172] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 4 [100/172] - Loss: 4.079 [-4.078, 0.001, -0.000]\n",
      "Epoch 4 [101/172] - Loss: 4.021 [-4.019, 0.001, -0.000]\n",
      "Epoch 4 [102/172] - Loss: 4.016 [-4.014, 0.001, -0.000]\n",
      "Epoch 4 [103/172] - Loss: 4.043 [-4.041, 0.002, -0.000]\n",
      "Epoch 4 [104/172] - Loss: 4.066 [-4.065, 0.001, -0.000]\n",
      "Epoch 4 [105/172] - Loss: 4.053 [-4.051, 0.001, -0.000]\n",
      "Epoch 4 [106/172] - Loss: 4.051 [-4.049, 0.001, -0.000]\n",
      "Epoch 4 [107/172] - Loss: 3.994 [-3.993, 0.001, -0.000]\n",
      "Epoch 4 [108/172] - Loss: 3.944 [-3.943, 0.001, -0.000]\n",
      "Epoch 4 [109/172] - Loss: 4.034 [-4.032, 0.001, -0.000]\n",
      "Epoch 4 [110/172] - Loss: 4.048 [-4.047, 0.001, -0.000]\n",
      "Epoch 4 [111/172] - Loss: 4.000 [-3.999, 0.002, -0.000]\n",
      "Epoch 4 [112/172] - Loss: 3.963 [-3.962, 0.001, -0.000]\n",
      "Epoch 4 [113/172] - Loss: 3.990 [-3.989, 0.001, -0.000]\n",
      "Epoch 4 [114/172] - Loss: 4.085 [-4.084, 0.001, -0.000]\n",
      "Epoch 4 [115/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 4 [116/172] - Loss: 4.018 [-4.016, 0.001, -0.000]\n",
      "Epoch 4 [117/172] - Loss: 4.099 [-4.097, 0.001, -0.000]\n",
      "Epoch 4 [118/172] - Loss: 4.057 [-4.055, 0.001, -0.000]\n",
      "Epoch 4 [119/172] - Loss: 4.017 [-4.015, 0.001, -0.000]\n",
      "Epoch 4 [120/172] - Loss: 4.027 [-4.026, 0.001, -0.000]\n",
      "Epoch 4 [121/172] - Loss: 4.053 [-4.051, 0.001, -0.000]\n",
      "Epoch 4 [122/172] - Loss: 4.006 [-4.004, 0.001, -0.000]\n",
      "Epoch 4 [123/172] - Loss: 4.039 [-4.037, 0.002, -0.000]\n",
      "Epoch 4 [124/172] - Loss: 4.047 [-4.045, 0.001, -0.000]\n",
      "Epoch 4 [125/172] - Loss: 4.021 [-4.020, 0.001, -0.000]\n",
      "Epoch 4 [126/172] - Loss: 4.117 [-4.115, 0.001, -0.000]\n",
      "Epoch 4 [127/172] - Loss: 4.033 [-4.032, 0.001, -0.000]\n",
      "Epoch 4 [128/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 4 [129/172] - Loss: 4.096 [-4.095, 0.001, -0.000]\n",
      "Epoch 4 [130/172] - Loss: 4.018 [-4.017, 0.001, -0.000]\n",
      "Epoch 4 [131/172] - Loss: 4.083 [-4.081, 0.001, -0.000]\n",
      "Epoch 4 [132/172] - Loss: 3.970 [-3.968, 0.001, -0.000]\n",
      "Epoch 4 [133/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 4 [134/172] - Loss: 3.979 [-3.977, 0.001, -0.000]\n",
      "Epoch 4 [135/172] - Loss: 4.071 [-4.069, 0.001, -0.000]\n",
      "Epoch 4 [136/172] - Loss: 4.061 [-4.060, 0.001, -0.000]\n",
      "Epoch 4 [137/172] - Loss: 4.035 [-4.033, 0.001, -0.000]\n",
      "Epoch 4 [138/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 4 [139/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 4 [140/172] - Loss: 4.078 [-4.076, 0.001, -0.000]\n",
      "Epoch 4 [141/172] - Loss: 3.992 [-3.991, 0.001, -0.000]\n",
      "Epoch 4 [142/172] - Loss: 4.032 [-4.030, 0.001, -0.000]\n",
      "Epoch 4 [143/172] - Loss: 4.023 [-4.022, 0.001, -0.000]\n",
      "Epoch 4 [144/172] - Loss: 4.051 [-4.049, 0.002, -0.000]\n",
      "Epoch 4 [145/172] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 4 [146/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 4 [147/172] - Loss: 4.081 [-4.080, 0.001, -0.000]\n",
      "Epoch 4 [148/172] - Loss: 4.038 [-4.036, 0.001, -0.000]\n",
      "Epoch 4 [149/172] - Loss: 4.018 [-4.017, 0.002, -0.000]\n",
      "Epoch 4 [150/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 4 [151/172] - Loss: 4.006 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [152/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 4 [153/172] - Loss: 4.044 [-4.043, 0.001, -0.000]\n",
      "Epoch 4 [154/172] - Loss: 4.040 [-4.038, 0.001, -0.000]\n",
      "Epoch 4 [155/172] - Loss: 4.029 [-4.028, 0.001, -0.000]\n",
      "Epoch 4 [156/172] - Loss: 4.041 [-4.039, 0.001, -0.000]\n",
      "Epoch 4 [157/172] - Loss: 4.011 [-4.010, 0.001, -0.000]\n",
      "Epoch 4 [158/172] - Loss: 4.022 [-4.020, 0.001, -0.000]\n",
      "Epoch 4 [159/172] - Loss: 4.065 [-4.064, 0.001, -0.000]\n",
      "Epoch 4 [160/172] - Loss: 3.972 [-3.970, 0.001, -0.000]\n",
      "Epoch 4 [161/172] - Loss: 4.029 [-4.028, 0.001, -0.000]\n",
      "Epoch 4 [162/172] - Loss: 3.999 [-3.997, 0.001, -0.000]\n",
      "Epoch 4 [163/172] - Loss: 4.046 [-4.044, 0.001, -0.000]\n",
      "Epoch 4 [164/172] - Loss: 4.091 [-4.089, 0.001, -0.000]\n",
      "Epoch 4 [165/172] - Loss: 4.008 [-4.006, 0.001, -0.000]\n",
      "Epoch 4 [166/172] - Loss: 4.005 [-4.003, 0.001, -0.000]\n",
      "Epoch 4 [167/172] - Loss: 4.019 [-4.017, 0.001, -0.000]\n",
      "Epoch 4 [168/172] - Loss: 3.970 [-3.969, 0.001, -0.000]\n",
      "Epoch 4 [169/172] - Loss: 4.053 [-4.052, 0.001, -0.000]\n",
      "Epoch 4 [170/172] - Loss: 4.054 [-4.053, 0.001, -0.000]\n",
      "Epoch 4 [171/172] - Loss: 4.013 [-4.011, 0.001, -0.000]\n",
      "Epoch 5 [0/172] - Loss: 4.040 [-4.039, 0.001, -0.000]\n",
      "Epoch 5 [1/172] - Loss: 4.046 [-4.045, 0.001, -0.000]\n",
      "Epoch 5 [2/172] - Loss: 3.984 [-3.983, 0.001, -0.000]\n",
      "Epoch 5 [3/172] - Loss: 4.012 [-4.010, 0.001, -0.000]\n",
      "Epoch 5 [4/172] - Loss: 4.061 [-4.060, 0.001, -0.000]\n",
      "Epoch 5 [5/172] - Loss: 4.023 [-4.021, 0.001, -0.000]\n",
      "Epoch 5 [6/172] - Loss: 4.034 [-4.032, 0.002, -0.000]\n",
      "Epoch 5 [7/172] - Loss: 4.029 [-4.027, 0.001, -0.000]\n",
      "Epoch 5 [8/172] - Loss: 3.947 [-3.945, 0.001, -0.000]\n",
      "Epoch 5 [9/172] - Loss: 4.043 [-4.041, 0.001, -0.000]\n",
      "Epoch 5 [10/172] - Loss: 4.034 [-4.032, 0.001, -0.000]\n",
      "Epoch 5 [11/172] - Loss: 4.015 [-4.013, 0.001, -0.000]\n",
      "Epoch 5 [12/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 5 [13/172] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 5 [14/172] - Loss: 4.016 [-4.015, 0.001, -0.000]\n",
      "Epoch 5 [15/172] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 5 [16/172] - Loss: 4.024 [-4.023, 0.001, -0.000]\n",
      "Epoch 5 [17/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 5 [18/172] - Loss: 4.016 [-4.014, 0.001, -0.000]\n",
      "Epoch 5 [19/172] - Loss: 4.017 [-4.015, 0.001, -0.000]\n",
      "Epoch 5 [20/172] - Loss: 4.050 [-4.049, 0.002, -0.000]\n",
      "Epoch 5 [21/172] - Loss: 4.052 [-4.051, 0.001, -0.000]\n",
      "Epoch 5 [22/172] - Loss: 4.139 [-4.138, 0.002, -0.000]\n",
      "Epoch 5 [23/172] - Loss: 3.987 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [24/172] - Loss: 4.024 [-4.022, 0.001, -0.000]\n",
      "Epoch 5 [25/172] - Loss: 4.033 [-4.032, 0.002, -0.000]\n",
      "Epoch 5 [26/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 5 [27/172] - Loss: 4.040 [-4.038, 0.001, -0.000]\n",
      "Epoch 5 [28/172] - Loss: 4.069 [-4.068, 0.001, -0.000]\n",
      "Epoch 5 [29/172] - Loss: 4.038 [-4.037, 0.001, -0.000]\n",
      "Epoch 5 [30/172] - Loss: 3.975 [-3.974, 0.001, -0.000]\n",
      "Epoch 5 [31/172] - Loss: 4.005 [-4.003, 0.001, -0.000]\n",
      "Epoch 5 [32/172] - Loss: 3.987 [-3.986, 0.001, -0.000]\n",
      "Epoch 5 [33/172] - Loss: 4.008 [-4.007, 0.002, -0.000]\n",
      "Epoch 5 [34/172] - Loss: 4.067 [-4.065, 0.001, -0.000]\n",
      "Epoch 5 [35/172] - Loss: 4.074 [-4.073, 0.001, -0.000]\n",
      "Epoch 5 [36/172] - Loss: 4.104 [-4.102, 0.001, -0.000]\n",
      "Epoch 5 [37/172] - Loss: 3.966 [-3.964, 0.001, -0.000]\n",
      "Epoch 5 [38/172] - Loss: 4.025 [-4.024, 0.001, -0.000]\n",
      "Epoch 5 [39/172] - Loss: 4.062 [-4.061, 0.001, -0.000]\n",
      "Epoch 5 [40/172] - Loss: 4.006 [-4.005, 0.001, -0.000]\n",
      "Epoch 5 [41/172] - Loss: 4.067 [-4.065, 0.001, -0.000]\n",
      "Epoch 5 [42/172] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 5 [43/172] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [44/172] - Loss: 4.080 [-4.079, 0.001, -0.000]\n",
      "Epoch 5 [45/172] - Loss: 4.027 [-4.026, 0.001, -0.000]\n",
      "Epoch 5 [46/172] - Loss: 4.016 [-4.014, 0.001, -0.000]\n",
      "Epoch 5 [47/172] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [48/172] - Loss: 3.994 [-3.993, 0.001, -0.000]\n",
      "Epoch 5 [49/172] - Loss: 4.021 [-4.020, 0.001, -0.000]\n",
      "Epoch 5 [50/172] - Loss: 4.030 [-4.029, 0.001, -0.000]\n",
      "Epoch 5 [51/172] - Loss: 4.008 [-4.006, 0.001, -0.000]\n",
      "Epoch 5 [52/172] - Loss: 4.036 [-4.035, 0.002, -0.000]\n",
      "Epoch 5 [53/172] - Loss: 4.029 [-4.027, 0.001, -0.000]\n",
      "Epoch 5 [54/172] - Loss: 3.986 [-3.985, 0.001, -0.000]\n",
      "Epoch 5 [55/172] - Loss: 4.022 [-4.020, 0.001, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [56/172] - Loss: 3.938 [-3.937, 0.001, -0.000]\n",
      "Epoch 5 [57/172] - Loss: 4.047 [-4.045, 0.002, -0.000]\n",
      "Epoch 5 [58/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 5 [59/172] - Loss: 4.012 [-4.011, 0.001, -0.000]\n",
      "Epoch 5 [60/172] - Loss: 4.035 [-4.034, 0.001, -0.000]\n",
      "Epoch 5 [61/172] - Loss: 4.043 [-4.041, 0.001, -0.000]\n",
      "Epoch 5 [62/172] - Loss: 3.966 [-3.965, 0.001, -0.000]\n",
      "Epoch 5 [63/172] - Loss: 4.078 [-4.076, 0.001, -0.000]\n",
      "Epoch 5 [64/172] - Loss: 3.998 [-3.996, 0.001, -0.000]\n",
      "Epoch 5 [65/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 5 [66/172] - Loss: 4.044 [-4.042, 0.001, -0.000]\n",
      "Epoch 5 [67/172] - Loss: 4.013 [-4.012, 0.001, -0.000]\n",
      "Epoch 5 [68/172] - Loss: 3.999 [-3.997, 0.001, -0.000]\n",
      "Epoch 5 [69/172] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [70/172] - Loss: 4.027 [-4.026, 0.001, -0.000]\n",
      "Epoch 5 [71/172] - Loss: 3.997 [-3.996, 0.001, -0.000]\n",
      "Epoch 5 [72/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 5 [73/172] - Loss: 4.051 [-4.050, 0.001, -0.000]\n",
      "Epoch 5 [74/172] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [75/172] - Loss: 3.970 [-3.969, 0.001, -0.000]\n",
      "Epoch 5 [76/172] - Loss: 4.080 [-4.078, 0.002, -0.000]\n",
      "Epoch 5 [77/172] - Loss: 4.029 [-4.028, 0.001, -0.000]\n",
      "Epoch 5 [78/172] - Loss: 4.045 [-4.044, 0.001, -0.000]\n",
      "Epoch 5 [79/172] - Loss: 4.007 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [80/172] - Loss: 4.035 [-4.033, 0.001, -0.000]\n",
      "Epoch 5 [81/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 5 [82/172] - Loss: 4.022 [-4.021, 0.001, -0.000]\n",
      "Epoch 5 [83/172] - Loss: 4.035 [-4.034, 0.002, -0.000]\n",
      "Epoch 5 [84/172] - Loss: 4.040 [-4.039, 0.001, -0.000]\n",
      "Epoch 5 [85/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 5 [86/172] - Loss: 4.004 [-4.002, 0.002, -0.000]\n",
      "Epoch 5 [87/172] - Loss: 4.113 [-4.112, 0.002, -0.000]\n",
      "Epoch 5 [88/172] - Loss: 4.038 [-4.037, 0.001, -0.000]\n",
      "Epoch 5 [89/172] - Loss: 4.045 [-4.043, 0.002, -0.000]\n",
      "Epoch 5 [90/172] - Loss: 4.059 [-4.057, 0.001, -0.000]\n",
      "Epoch 5 [91/172] - Loss: 3.987 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [92/172] - Loss: 4.012 [-4.010, 0.001, -0.000]\n",
      "Epoch 5 [93/172] - Loss: 4.019 [-4.017, 0.001, -0.000]\n",
      "Epoch 5 [94/172] - Loss: 4.065 [-4.064, 0.001, -0.000]\n",
      "Epoch 5 [95/172] - Loss: 4.036 [-4.034, 0.001, -0.000]\n",
      "Epoch 5 [96/172] - Loss: 4.010 [-4.009, 0.001, -0.000]\n",
      "Epoch 5 [97/172] - Loss: 4.000 [-3.999, 0.001, -0.000]\n",
      "Epoch 5 [98/172] - Loss: 3.992 [-3.991, 0.001, -0.000]\n",
      "Epoch 5 [99/172] - Loss: 4.059 [-4.058, 0.001, -0.000]\n",
      "Epoch 5 [100/172] - Loss: 3.974 [-3.973, 0.001, -0.000]\n",
      "Epoch 5 [101/172] - Loss: 4.038 [-4.036, 0.001, -0.000]\n",
      "Epoch 5 [102/172] - Loss: 4.023 [-4.022, 0.001, -0.000]\n",
      "Epoch 5 [103/172] - Loss: 4.036 [-4.034, 0.001, -0.000]\n",
      "Epoch 5 [104/172] - Loss: 4.017 [-4.016, 0.001, -0.000]\n",
      "Epoch 5 [105/172] - Loss: 4.017 [-4.016, 0.001, -0.000]\n",
      "Epoch 5 [106/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 5 [107/172] - Loss: 3.974 [-3.973, 0.001, -0.000]\n",
      "Epoch 5 [108/172] - Loss: 4.008 [-4.007, 0.002, -0.000]\n",
      "Epoch 5 [109/172] - Loss: 4.044 [-4.043, 0.001, -0.000]\n",
      "Epoch 5 [110/172] - Loss: 4.047 [-4.045, 0.001, -0.000]\n",
      "Epoch 5 [111/172] - Loss: 4.006 [-4.004, 0.001, -0.000]\n",
      "Epoch 5 [112/172] - Loss: 3.980 [-3.979, 0.001, -0.000]\n",
      "Epoch 5 [113/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 5 [114/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 5 [115/172] - Loss: 3.981 [-3.980, 0.001, -0.000]\n",
      "Epoch 5 [116/172] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 5 [117/172] - Loss: 4.015 [-4.013, 0.002, -0.000]\n",
      "Epoch 5 [118/172] - Loss: 4.073 [-4.071, 0.001, -0.000]\n",
      "Epoch 5 [119/172] - Loss: 3.980 [-3.979, 0.001, -0.000]\n",
      "Epoch 5 [120/172] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 5 [121/172] - Loss: 4.046 [-4.044, 0.001, -0.000]\n",
      "Epoch 5 [122/172] - Loss: 4.073 [-4.072, 0.001, -0.000]\n",
      "Epoch 5 [123/172] - Loss: 4.028 [-4.027, 0.001, -0.000]\n",
      "Epoch 5 [124/172] - Loss: 3.990 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [125/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [126/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 5 [127/172] - Loss: 4.031 [-4.029, 0.001, -0.000]\n",
      "Epoch 5 [128/172] - Loss: 4.089 [-4.087, 0.001, -0.000]\n",
      "Epoch 5 [129/172] - Loss: 4.038 [-4.036, 0.001, -0.000]\n",
      "Epoch 5 [130/172] - Loss: 4.061 [-4.060, 0.001, -0.000]\n",
      "Epoch 5 [131/172] - Loss: 4.023 [-4.022, 0.001, -0.000]\n",
      "Epoch 5 [132/172] - Loss: 4.052 [-4.050, 0.002, -0.000]\n",
      "Epoch 5 [133/172] - Loss: 3.980 [-3.979, 0.001, -0.000]\n",
      "Epoch 5 [134/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 5 [135/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 5 [136/172] - Loss: 4.023 [-4.022, 0.001, -0.000]\n",
      "Epoch 5 [137/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 5 [138/172] - Loss: 4.012 [-4.010, 0.001, -0.000]\n",
      "Epoch 5 [139/172] - Loss: 4.012 [-4.011, 0.002, -0.000]\n",
      "Epoch 5 [140/172] - Loss: 3.945 [-3.944, 0.001, -0.000]\n",
      "Epoch 5 [141/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 5 [142/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 5 [143/172] - Loss: 3.986 [-3.985, 0.001, -0.000]\n",
      "Epoch 5 [144/172] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 5 [145/172] - Loss: 4.058 [-4.056, 0.001, -0.000]\n",
      "Epoch 5 [146/172] - Loss: 4.004 [-4.003, 0.001, -0.000]\n",
      "Epoch 5 [147/172] - Loss: 4.026 [-4.024, 0.002, -0.000]\n",
      "Epoch 5 [148/172] - Loss: 3.967 [-3.965, 0.001, -0.000]\n",
      "Epoch 5 [149/172] - Loss: 4.002 [-4.001, 0.001, -0.000]\n",
      "Epoch 5 [150/172] - Loss: 4.045 [-4.043, 0.002, -0.000]\n",
      "Epoch 5 [151/172] - Loss: 4.093 [-4.092, 0.001, -0.000]\n",
      "Epoch 5 [152/172] - Loss: 3.978 [-3.976, 0.001, -0.000]\n",
      "Epoch 5 [153/172] - Loss: 3.958 [-3.957, 0.001, -0.000]\n",
      "Epoch 5 [154/172] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 5 [155/172] - Loss: 4.066 [-4.064, 0.001, -0.000]\n",
      "Epoch 5 [156/172] - Loss: 3.975 [-3.973, 0.001, -0.000]\n",
      "Epoch 5 [157/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [158/172] - Loss: 4.031 [-4.029, 0.001, -0.000]\n",
      "Epoch 5 [159/172] - Loss: 4.019 [-4.018, 0.001, -0.000]\n",
      "Epoch 5 [160/172] - Loss: 4.022 [-4.021, 0.001, -0.000]\n",
      "Epoch 5 [161/172] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 5 [162/172] - Loss: 4.008 [-4.007, 0.001, -0.000]\n",
      "Epoch 5 [163/172] - Loss: 4.014 [-4.013, 0.001, -0.000]\n",
      "Epoch 5 [164/172] - Loss: 4.006 [-4.005, 0.001, -0.000]\n",
      "Epoch 5 [165/172] - Loss: 4.033 [-4.032, 0.001, -0.000]\n",
      "Epoch 5 [166/172] - Loss: 4.034 [-4.033, 0.001, -0.000]\n",
      "Epoch 5 [167/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 5 [168/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 5 [169/172] - Loss: 4.022 [-4.021, 0.001, -0.000]\n",
      "Epoch 5 [170/172] - Loss: 4.078 [-4.076, 0.001, -0.000]\n",
      "Epoch 5 [171/172] - Loss: 4.103 [-4.102, 0.002, -0.000]\n",
      "Epoch 6 [0/172] - Loss: 4.096 [-4.095, 0.001, -0.000]\n",
      "Epoch 6 [1/172] - Loss: 4.006 [-4.005, 0.002, -0.000]\n",
      "Epoch 6 [2/172] - Loss: 4.006 [-4.004, 0.001, -0.000]\n",
      "Epoch 6 [3/172] - Loss: 3.968 [-3.966, 0.001, -0.000]\n",
      "Epoch 6 [4/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 6 [5/172] - Loss: 4.068 [-4.067, 0.001, -0.000]\n",
      "Epoch 6 [6/172] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 6 [7/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 6 [8/172] - Loss: 3.996 [-3.994, 0.001, -0.000]\n",
      "Epoch 6 [9/172] - Loss: 4.046 [-4.045, 0.001, -0.000]\n",
      "Epoch 6 [10/172] - Loss: 4.008 [-4.007, 0.001, -0.000]\n",
      "Epoch 6 [11/172] - Loss: 3.981 [-3.980, 0.002, -0.000]\n",
      "Epoch 6 [12/172] - Loss: 4.065 [-4.064, 0.001, -0.000]\n",
      "Epoch 6 [13/172] - Loss: 4.028 [-4.026, 0.001, -0.000]\n",
      "Epoch 6 [14/172] - Loss: 4.013 [-4.011, 0.001, -0.000]\n",
      "Epoch 6 [15/172] - Loss: 3.999 [-3.997, 0.001, -0.000]\n",
      "Epoch 6 [16/172] - Loss: 3.984 [-3.983, 0.001, -0.000]\n",
      "Epoch 6 [17/172] - Loss: 3.959 [-3.958, 0.001, -0.000]\n",
      "Epoch 6 [18/172] - Loss: 3.952 [-3.950, 0.001, -0.000]\n",
      "Epoch 6 [19/172] - Loss: 4.003 [-4.002, 0.001, -0.000]\n",
      "Epoch 6 [20/172] - Loss: 4.022 [-4.020, 0.001, -0.000]\n",
      "Epoch 6 [21/172] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 6 [22/172] - Loss: 4.063 [-4.062, 0.001, -0.000]\n",
      "Epoch 6 [23/172] - Loss: 3.961 [-3.959, 0.001, -0.000]\n",
      "Epoch 6 [24/172] - Loss: 3.986 [-3.984, 0.001, -0.000]\n",
      "Epoch 6 [25/172] - Loss: 4.013 [-4.012, 0.001, -0.000]\n",
      "Epoch 6 [26/172] - Loss: 3.997 [-3.995, 0.001, -0.000]\n",
      "Epoch 6 [27/172] - Loss: 3.971 [-3.970, 0.001, -0.000]\n",
      "Epoch 6 [28/172] - Loss: 4.014 [-4.013, 0.001, -0.000]\n",
      "Epoch 6 [29/172] - Loss: 3.952 [-3.951, 0.001, -0.000]\n",
      "Epoch 6 [30/172] - Loss: 3.963 [-3.962, 0.001, -0.000]\n",
      "Epoch 6 [31/172] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [32/172] - Loss: 4.017 [-4.015, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [33/172] - Loss: 4.025 [-4.023, 0.001, -0.000]\n",
      "Epoch 6 [34/172] - Loss: 3.961 [-3.960, 0.001, -0.000]\n",
      "Epoch 6 [35/172] - Loss: 4.097 [-4.096, 0.001, -0.000]\n",
      "Epoch 6 [36/172] - Loss: 4.028 [-4.026, 0.001, -0.000]\n",
      "Epoch 6 [37/172] - Loss: 4.013 [-4.012, 0.001, -0.000]\n",
      "Epoch 6 [38/172] - Loss: 3.978 [-3.977, 0.001, -0.000]\n",
      "Epoch 6 [39/172] - Loss: 4.039 [-4.038, 0.001, -0.000]\n",
      "Epoch 6 [40/172] - Loss: 3.983 [-3.981, 0.001, -0.000]\n",
      "Epoch 6 [41/172] - Loss: 4.011 [-4.010, 0.001, -0.000]\n",
      "Epoch 6 [42/172] - Loss: 3.999 [-3.998, 0.002, -0.000]\n",
      "Epoch 6 [43/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 6 [44/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 6 [45/172] - Loss: 3.966 [-3.964, 0.001, -0.000]\n",
      "Epoch 6 [46/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 6 [47/172] - Loss: 4.000 [-3.999, 0.001, -0.000]\n",
      "Epoch 6 [48/172] - Loss: 4.072 [-4.071, 0.001, -0.000]\n",
      "Epoch 6 [49/172] - Loss: 4.010 [-4.008, 0.001, -0.000]\n",
      "Epoch 6 [50/172] - Loss: 4.008 [-4.007, 0.001, -0.000]\n",
      "Epoch 6 [51/172] - Loss: 4.017 [-4.016, 0.001, -0.000]\n",
      "Epoch 6 [52/172] - Loss: 3.986 [-3.985, 0.002, -0.000]\n",
      "Epoch 6 [53/172] - Loss: 3.941 [-3.939, 0.001, -0.000]\n",
      "Epoch 6 [54/172] - Loss: 4.045 [-4.044, 0.001, -0.000]\n",
      "Epoch 6 [55/172] - Loss: 4.031 [-4.029, 0.001, -0.000]\n",
      "Epoch 6 [56/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 6 [57/172] - Loss: 3.932 [-3.930, 0.001, -0.000]\n",
      "Epoch 6 [58/172] - Loss: 4.004 [-4.003, 0.002, -0.000]\n",
      "Epoch 6 [59/172] - Loss: 4.073 [-4.071, 0.001, -0.000]\n",
      "Epoch 6 [60/172] - Loss: 3.989 [-3.988, 0.001, -0.000]\n",
      "Epoch 6 [61/172] - Loss: 4.030 [-4.028, 0.001, -0.000]\n",
      "Epoch 6 [62/172] - Loss: 3.976 [-3.974, 0.001, -0.000]\n",
      "Epoch 6 [63/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 6 [64/172] - Loss: 4.039 [-4.038, 0.001, -0.000]\n",
      "Epoch 6 [65/172] - Loss: 3.988 [-3.986, 0.001, -0.000]\n",
      "Epoch 6 [66/172] - Loss: 3.982 [-3.980, 0.001, -0.000]\n",
      "Epoch 6 [67/172] - Loss: 4.043 [-4.041, 0.001, -0.000]\n",
      "Epoch 6 [68/172] - Loss: 3.999 [-3.997, 0.001, -0.000]\n",
      "Epoch 6 [69/172] - Loss: 4.001 [-3.999, 0.001, -0.000]\n",
      "Epoch 6 [70/172] - Loss: 3.979 [-3.978, 0.001, -0.000]\n",
      "Epoch 6 [71/172] - Loss: 3.944 [-3.942, 0.001, -0.000]\n",
      "Epoch 6 [72/172] - Loss: 4.016 [-4.014, 0.001, -0.000]\n",
      "Epoch 6 [73/172] - Loss: 3.981 [-3.979, 0.001, -0.000]\n",
      "Epoch 6 [74/172] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 6 [75/172] - Loss: 3.987 [-3.986, 0.001, -0.000]\n",
      "Epoch 6 [76/172] - Loss: 4.038 [-4.037, 0.001, -0.000]\n",
      "Epoch 6 [77/172] - Loss: 3.969 [-3.967, 0.001, -0.000]\n",
      "Epoch 6 [78/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 6 [79/172] - Loss: 4.070 [-4.068, 0.001, -0.000]\n",
      "Epoch 6 [80/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 6 [81/172] - Loss: 4.005 [-4.003, 0.001, -0.000]\n",
      "Epoch 6 [82/172] - Loss: 4.001 [-4.000, 0.001, -0.000]\n",
      "Epoch 6 [83/172] - Loss: 4.063 [-4.061, 0.001, -0.000]\n",
      "Epoch 6 [84/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 6 [85/172] - Loss: 3.922 [-3.921, 0.001, -0.000]\n",
      "Epoch 6 [86/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 6 [87/172] - Loss: 4.029 [-4.028, 0.001, -0.000]\n",
      "Epoch 6 [88/172] - Loss: 4.020 [-4.018, 0.001, -0.000]\n",
      "Epoch 6 [89/172] - Loss: 3.959 [-3.958, 0.001, -0.000]\n",
      "Epoch 6 [90/172] - Loss: 3.985 [-3.984, 0.001, -0.000]\n",
      "Epoch 6 [91/172] - Loss: 3.975 [-3.973, 0.001, -0.000]\n",
      "Epoch 6 [92/172] - Loss: 3.988 [-3.986, 0.001, -0.000]\n",
      "Epoch 6 [93/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 6 [94/172] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 6 [95/172] - Loss: 4.002 [-4.001, 0.001, -0.000]\n",
      "Epoch 6 [96/172] - Loss: 3.980 [-3.979, 0.001, -0.000]\n",
      "Epoch 6 [97/172] - Loss: 4.070 [-4.068, 0.001, -0.000]\n",
      "Epoch 6 [98/172] - Loss: 4.025 [-4.023, 0.001, -0.000]\n",
      "Epoch 6 [99/172] - Loss: 3.955 [-3.954, 0.001, -0.000]\n",
      "Epoch 6 [100/172] - Loss: 3.967 [-3.966, 0.002, -0.000]\n",
      "Epoch 6 [101/172] - Loss: 3.975 [-3.973, 0.001, -0.000]\n",
      "Epoch 6 [102/172] - Loss: 4.040 [-4.039, 0.001, -0.000]\n",
      "Epoch 6 [103/172] - Loss: 4.017 [-4.015, 0.001, -0.000]\n",
      "Epoch 6 [104/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 6 [105/172] - Loss: 3.993 [-3.992, 0.001, -0.000]\n",
      "Epoch 6 [106/172] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 6 [107/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 6 [108/172] - Loss: 4.008 [-4.007, 0.001, -0.000]\n",
      "Epoch 6 [109/172] - Loss: 3.950 [-3.949, 0.001, -0.000]\n",
      "Epoch 6 [110/172] - Loss: 4.010 [-4.009, 0.001, -0.000]\n",
      "Epoch 6 [111/172] - Loss: 4.015 [-4.014, 0.002, -0.000]\n",
      "Epoch 6 [112/172] - Loss: 4.017 [-4.015, 0.001, -0.000]\n",
      "Epoch 6 [113/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 6 [114/172] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 6 [115/172] - Loss: 4.031 [-4.030, 0.001, -0.000]\n",
      "Epoch 6 [116/172] - Loss: 4.024 [-4.023, 0.001, -0.000]\n",
      "Epoch 6 [117/172] - Loss: 4.065 [-4.064, 0.001, -0.000]\n",
      "Epoch 6 [118/172] - Loss: 3.975 [-3.973, 0.001, -0.000]\n",
      "Epoch 6 [119/172] - Loss: 4.018 [-4.016, 0.001, -0.000]\n",
      "Epoch 6 [120/172] - Loss: 4.039 [-4.038, 0.001, -0.000]\n",
      "Epoch 6 [121/172] - Loss: 3.964 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [122/172] - Loss: 3.991 [-3.990, 0.001, -0.000]\n",
      "Epoch 6 [123/172] - Loss: 3.972 [-3.971, 0.001, -0.000]\n",
      "Epoch 6 [124/172] - Loss: 3.971 [-3.970, 0.001, -0.000]\n",
      "Epoch 6 [125/172] - Loss: 3.955 [-3.953, 0.001, -0.000]\n",
      "Epoch 6 [126/172] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 6 [127/172] - Loss: 4.033 [-4.032, 0.001, -0.000]\n",
      "Epoch 6 [128/172] - Loss: 4.046 [-4.044, 0.001, -0.000]\n",
      "Epoch 6 [129/172] - Loss: 4.060 [-4.059, 0.001, -0.000]\n",
      "Epoch 6 [130/172] - Loss: 3.979 [-3.978, 0.001, -0.000]\n",
      "Epoch 6 [131/172] - Loss: 4.038 [-4.036, 0.001, -0.000]\n",
      "Epoch 6 [132/172] - Loss: 3.984 [-3.983, 0.001, -0.000]\n",
      "Epoch 6 [133/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 6 [134/172] - Loss: 4.102 [-4.100, 0.002, -0.000]\n",
      "Epoch 6 [135/172] - Loss: 4.028 [-4.026, 0.001, -0.000]\n",
      "Epoch 6 [136/172] - Loss: 3.971 [-3.969, 0.001, -0.000]\n",
      "Epoch 6 [137/172] - Loss: 3.954 [-3.952, 0.001, -0.000]\n",
      "Epoch 6 [138/172] - Loss: 4.062 [-4.060, 0.002, -0.000]\n",
      "Epoch 6 [139/172] - Loss: 4.017 [-4.016, 0.001, -0.000]\n",
      "Epoch 6 [140/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 6 [141/172] - Loss: 4.021 [-4.019, 0.001, -0.000]\n",
      "Epoch 6 [142/172] - Loss: 4.028 [-4.027, 0.001, -0.000]\n",
      "Epoch 6 [143/172] - Loss: 4.031 [-4.030, 0.001, -0.000]\n",
      "Epoch 6 [144/172] - Loss: 3.991 [-3.990, 0.001, -0.000]\n",
      "Epoch 6 [145/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 6 [146/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 6 [147/172] - Loss: 4.023 [-4.022, 0.001, -0.000]\n",
      "Epoch 6 [148/172] - Loss: 3.955 [-3.954, 0.001, -0.000]\n",
      "Epoch 6 [149/172] - Loss: 4.011 [-4.009, 0.001, -0.000]\n",
      "Epoch 6 [150/172] - Loss: 3.997 [-3.996, 0.001, -0.000]\n",
      "Epoch 6 [151/172] - Loss: 4.054 [-4.052, 0.001, -0.000]\n",
      "Epoch 6 [152/172] - Loss: 3.964 [-3.963, 0.001, -0.000]\n",
      "Epoch 6 [153/172] - Loss: 3.959 [-3.957, 0.001, -0.000]\n",
      "Epoch 6 [154/172] - Loss: 4.011 [-4.009, 0.001, -0.000]\n",
      "Epoch 6 [155/172] - Loss: 4.023 [-4.021, 0.001, -0.000]\n",
      "Epoch 6 [156/172] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 6 [157/172] - Loss: 4.033 [-4.032, 0.001, -0.000]\n",
      "Epoch 6 [158/172] - Loss: 4.059 [-4.058, 0.001, -0.000]\n",
      "Epoch 6 [159/172] - Loss: 4.003 [-4.001, 0.001, -0.000]\n",
      "Epoch 6 [160/172] - Loss: 3.996 [-3.994, 0.001, -0.000]\n",
      "Epoch 6 [161/172] - Loss: 4.044 [-4.043, 0.001, -0.000]\n",
      "Epoch 6 [162/172] - Loss: 3.966 [-3.964, 0.001, -0.000]\n",
      "Epoch 6 [163/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 6 [164/172] - Loss: 3.994 [-3.993, 0.001, -0.000]\n",
      "Epoch 6 [165/172] - Loss: 3.971 [-3.970, 0.001, -0.000]\n",
      "Epoch 6 [166/172] - Loss: 4.020 [-4.019, 0.001, -0.000]\n",
      "Epoch 6 [167/172] - Loss: 3.969 [-3.968, 0.001, -0.000]\n",
      "Epoch 6 [168/172] - Loss: 3.944 [-3.942, 0.001, -0.000]\n",
      "Epoch 6 [169/172] - Loss: 3.983 [-3.981, 0.001, -0.000]\n",
      "Epoch 6 [170/172] - Loss: 3.977 [-3.975, 0.001, -0.000]\n",
      "Epoch 6 [171/172] - Loss: 4.001 [-4.000, 0.001, -0.000]\n",
      "Epoch 7 [0/172] - Loss: 4.050 [-4.048, 0.001, -0.000]\n",
      "Epoch 7 [1/172] - Loss: 3.994 [-3.993, 0.001, -0.000]\n",
      "Epoch 7 [2/172] - Loss: 3.953 [-3.951, 0.001, -0.000]\n",
      "Epoch 7 [3/172] - Loss: 4.016 [-4.015, 0.001, -0.000]\n",
      "Epoch 7 [4/172] - Loss: 3.993 [-3.991, 0.001, -0.000]\n",
      "Epoch 7 [5/172] - Loss: 3.982 [-3.981, 0.001, -0.000]\n",
      "Epoch 7 [6/172] - Loss: 3.965 [-3.964, 0.001, -0.000]\n",
      "Epoch 7 [7/172] - Loss: 3.986 [-3.984, 0.001, -0.000]\n",
      "Epoch 7 [8/172] - Loss: 4.016 [-4.015, 0.001, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [9/172] - Loss: 3.994 [-3.992, 0.001, -0.000]\n",
      "Epoch 7 [10/172] - Loss: 3.999 [-3.997, 0.001, -0.000]\n",
      "Epoch 7 [11/172] - Loss: 3.977 [-3.976, 0.001, -0.000]\n",
      "Epoch 7 [12/172] - Loss: 3.984 [-3.982, 0.001, -0.000]\n",
      "Epoch 7 [13/172] - Loss: 4.013 [-4.011, 0.001, -0.000]\n",
      "Epoch 7 [14/172] - Loss: 3.937 [-3.936, 0.001, -0.000]\n",
      "Epoch 7 [15/172] - Loss: 3.962 [-3.961, 0.002, -0.000]\n",
      "Epoch 7 [16/172] - Loss: 3.989 [-3.988, 0.001, -0.000]\n",
      "Epoch 7 [17/172] - Loss: 4.020 [-4.018, 0.001, -0.000]\n",
      "Epoch 7 [18/172] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 7 [19/172] - Loss: 4.032 [-4.031, 0.001, -0.000]\n",
      "Epoch 7 [20/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 7 [21/172] - Loss: 3.990 [-3.989, 0.002, -0.000]\n",
      "Epoch 7 [22/172] - Loss: 4.019 [-4.018, 0.001, -0.000]\n",
      "Epoch 7 [23/172] - Loss: 4.034 [-4.032, 0.002, -0.000]\n",
      "Epoch 7 [24/172] - Loss: 4.011 [-4.009, 0.001, -0.000]\n",
      "Epoch 7 [25/172] - Loss: 3.997 [-3.996, 0.001, -0.000]\n",
      "Epoch 7 [26/172] - Loss: 4.000 [-3.999, 0.001, -0.000]\n",
      "Epoch 7 [27/172] - Loss: 3.926 [-3.924, 0.001, -0.000]\n",
      "Epoch 7 [28/172] - Loss: 4.040 [-4.038, 0.001, -0.000]\n",
      "Epoch 7 [29/172] - Loss: 4.003 [-4.002, 0.001, -0.000]\n",
      "Epoch 7 [30/172] - Loss: 3.975 [-3.974, 0.001, -0.000]\n",
      "Epoch 7 [31/172] - Loss: 3.982 [-3.981, 0.001, -0.000]\n",
      "Epoch 7 [32/172] - Loss: 4.010 [-4.008, 0.001, -0.000]\n",
      "Epoch 7 [33/172] - Loss: 4.039 [-4.038, 0.001, -0.000]\n",
      "Epoch 7 [34/172] - Loss: 4.032 [-4.030, 0.001, -0.000]\n",
      "Epoch 7 [35/172] - Loss: 3.968 [-3.967, 0.002, -0.000]\n",
      "Epoch 7 [36/172] - Loss: 3.987 [-3.986, 0.001, -0.000]\n",
      "Epoch 7 [37/172] - Loss: 4.043 [-4.041, 0.001, -0.000]\n",
      "Epoch 7 [38/172] - Loss: 3.962 [-3.960, 0.001, -0.000]\n",
      "Epoch 7 [39/172] - Loss: 3.988 [-3.987, 0.001, -0.000]\n",
      "Epoch 7 [40/172] - Loss: 4.023 [-4.021, 0.001, -0.000]\n",
      "Epoch 7 [41/172] - Loss: 4.037 [-4.035, 0.001, -0.000]\n",
      "Epoch 7 [42/172] - Loss: 4.082 [-4.080, 0.001, -0.000]\n",
      "Epoch 7 [43/172] - Loss: 3.987 [-3.986, 0.002, -0.000]\n",
      "Epoch 7 [44/172] - Loss: 4.026 [-4.025, 0.001, -0.000]\n",
      "Epoch 7 [45/172] - Loss: 3.975 [-3.974, 0.001, -0.000]\n",
      "Epoch 7 [46/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 7 [47/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 7 [48/172] - Loss: 3.992 [-3.991, 0.001, -0.000]\n",
      "Epoch 7 [49/172] - Loss: 3.974 [-3.973, 0.001, -0.000]\n",
      "Epoch 7 [50/172] - Loss: 3.984 [-3.982, 0.001, -0.000]\n",
      "Epoch 7 [51/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 7 [52/172] - Loss: 4.073 [-4.071, 0.001, -0.000]\n",
      "Epoch 7 [53/172] - Loss: 3.989 [-3.987, 0.001, -0.000]\n",
      "Epoch 7 [54/172] - Loss: 3.969 [-3.968, 0.002, -0.000]\n",
      "Epoch 7 [55/172] - Loss: 3.981 [-3.980, 0.001, -0.000]\n",
      "Epoch 7 [56/172] - Loss: 4.005 [-4.003, 0.001, -0.000]\n",
      "Epoch 7 [57/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 7 [58/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 7 [59/172] - Loss: 4.011 [-4.009, 0.001, -0.000]\n",
      "Epoch 7 [60/172] - Loss: 3.974 [-3.973, 0.001, -0.000]\n",
      "Epoch 7 [61/172] - Loss: 4.004 [-4.003, 0.001, -0.000]\n",
      "Epoch 7 [62/172] - Loss: 4.010 [-4.008, 0.001, -0.000]\n",
      "Epoch 7 [63/172] - Loss: 4.002 [-4.000, 0.001, -0.000]\n",
      "Epoch 7 [64/172] - Loss: 4.036 [-4.034, 0.001, -0.000]\n",
      "Epoch 7 [65/172] - Loss: 3.957 [-3.955, 0.001, -0.000]\n",
      "Epoch 7 [66/172] - Loss: 4.031 [-4.030, 0.001, -0.000]\n",
      "Epoch 7 [67/172] - Loss: 4.039 [-4.038, 0.001, -0.000]\n",
      "Epoch 7 [68/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 7 [69/172] - Loss: 3.993 [-3.991, 0.001, -0.000]\n",
      "Epoch 7 [70/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 7 [71/172] - Loss: 3.981 [-3.980, 0.001, -0.000]\n",
      "Epoch 7 [72/172] - Loss: 4.022 [-4.021, 0.001, -0.000]\n",
      "Epoch 7 [73/172] - Loss: 4.001 [-3.999, 0.001, -0.000]\n",
      "Epoch 7 [74/172] - Loss: 4.020 [-4.018, 0.001, -0.000]\n",
      "Epoch 7 [75/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 7 [76/172] - Loss: 4.006 [-4.005, 0.001, -0.000]\n",
      "Epoch 7 [77/172] - Loss: 4.000 [-3.998, 0.001, -0.000]\n",
      "Epoch 7 [78/172] - Loss: 4.000 [-3.999, 0.001, -0.000]\n",
      "Epoch 7 [79/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 7 [80/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 7 [81/172] - Loss: 3.967 [-3.965, 0.001, -0.000]\n",
      "Epoch 7 [82/172] - Loss: 4.050 [-4.048, 0.001, -0.000]\n",
      "Epoch 7 [83/172] - Loss: 3.991 [-3.990, 0.001, -0.000]\n",
      "Epoch 7 [84/172] - Loss: 3.966 [-3.965, 0.001, -0.000]\n",
      "Epoch 7 [85/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 7 [86/172] - Loss: 4.008 [-4.006, 0.001, -0.000]\n",
      "Epoch 7 [87/172] - Loss: 4.011 [-4.009, 0.001, -0.000]\n",
      "Epoch 7 [88/172] - Loss: 4.017 [-4.016, 0.001, -0.000]\n",
      "Epoch 7 [89/172] - Loss: 4.046 [-4.045, 0.001, -0.000]\n",
      "Epoch 7 [90/172] - Loss: 3.951 [-3.950, 0.002, -0.000]\n",
      "Epoch 7 [91/172] - Loss: 3.999 [-3.997, 0.001, -0.000]\n",
      "Epoch 7 [92/172] - Loss: 4.033 [-4.032, 0.001, -0.000]\n",
      "Epoch 7 [93/172] - Loss: 4.004 [-4.003, 0.001, -0.000]\n",
      "Epoch 7 [94/172] - Loss: 3.990 [-3.988, 0.001, -0.000]\n",
      "Epoch 7 [95/172] - Loss: 4.074 [-4.073, 0.001, -0.000]\n",
      "Epoch 7 [96/172] - Loss: 4.035 [-4.034, 0.001, -0.000]\n",
      "Epoch 7 [97/172] - Loss: 3.983 [-3.982, 0.001, -0.000]\n",
      "Epoch 7 [98/172] - Loss: 3.948 [-3.946, 0.001, -0.000]\n",
      "Epoch 7 [99/172] - Loss: 3.975 [-3.973, 0.001, -0.000]\n",
      "Epoch 7 [100/172] - Loss: 3.965 [-3.963, 0.001, -0.000]\n",
      "Epoch 7 [101/172] - Loss: 4.001 [-4.000, 0.001, -0.000]\n",
      "Epoch 7 [102/172] - Loss: 4.036 [-4.035, 0.001, -0.000]\n",
      "Epoch 7 [103/172] - Loss: 3.969 [-3.968, 0.001, -0.000]\n",
      "Epoch 7 [104/172] - Loss: 4.016 [-4.014, 0.001, -0.000]\n",
      "Epoch 7 [105/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 7 [106/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 7 [107/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 7 [108/172] - Loss: 4.039 [-4.037, 0.001, -0.000]\n",
      "Epoch 7 [109/172] - Loss: 3.951 [-3.950, 0.001, -0.000]\n",
      "Epoch 7 [110/172] - Loss: 3.994 [-3.992, 0.001, -0.000]\n",
      "Epoch 7 [111/172] - Loss: 4.043 [-4.042, 0.001, -0.000]\n",
      "Epoch 7 [112/172] - Loss: 4.037 [-4.035, 0.001, -0.000]\n",
      "Epoch 7 [113/172] - Loss: 3.990 [-3.989, 0.001, -0.000]\n",
      "Epoch 7 [114/172] - Loss: 4.034 [-4.032, 0.001, -0.000]\n",
      "Epoch 7 [115/172] - Loss: 4.067 [-4.065, 0.001, -0.000]\n",
      "Epoch 7 [116/172] - Loss: 4.032 [-4.031, 0.001, -0.000]\n",
      "Epoch 7 [117/172] - Loss: 3.983 [-3.981, 0.001, -0.000]\n",
      "Epoch 7 [118/172] - Loss: 4.040 [-4.039, 0.001, -0.000]\n",
      "Epoch 7 [119/172] - Loss: 3.970 [-3.969, 0.001, -0.000]\n",
      "Epoch 7 [120/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 7 [121/172] - Loss: 4.029 [-4.027, 0.001, -0.000]\n",
      "Epoch 7 [122/172] - Loss: 3.978 [-3.976, 0.001, -0.000]\n",
      "Epoch 7 [123/172] - Loss: 3.991 [-3.990, 0.001, -0.000]\n",
      "Epoch 7 [124/172] - Loss: 3.986 [-3.984, 0.001, -0.000]\n",
      "Epoch 7 [125/172] - Loss: 4.025 [-4.023, 0.001, -0.000]\n",
      "Epoch 7 [126/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 7 [127/172] - Loss: 3.979 [-3.978, 0.001, -0.000]\n",
      "Epoch 7 [128/172] - Loss: 4.001 [-3.999, 0.001, -0.000]\n",
      "Epoch 7 [129/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 7 [130/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 7 [131/172] - Loss: 4.029 [-4.027, 0.001, -0.000]\n",
      "Epoch 7 [132/172] - Loss: 4.022 [-4.021, 0.001, -0.000]\n",
      "Epoch 7 [133/172] - Loss: 4.048 [-4.047, 0.001, -0.000]\n",
      "Epoch 7 [134/172] - Loss: 4.041 [-4.039, 0.001, -0.000]\n",
      "Epoch 7 [135/172] - Loss: 3.988 [-3.987, 0.001, -0.000]\n",
      "Epoch 7 [136/172] - Loss: 3.994 [-3.992, 0.001, -0.000]\n",
      "Epoch 7 [137/172] - Loss: 3.996 [-3.994, 0.001, -0.000]\n",
      "Epoch 7 [138/172] - Loss: 3.979 [-3.978, 0.001, -0.000]\n",
      "Epoch 7 [139/172] - Loss: 3.983 [-3.982, 0.001, -0.000]\n",
      "Epoch 7 [140/172] - Loss: 3.988 [-3.986, 0.001, -0.000]\n",
      "Epoch 7 [141/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 7 [142/172] - Loss: 3.972 [-3.971, 0.001, -0.000]\n",
      "Epoch 7 [143/172] - Loss: 3.988 [-3.987, 0.001, -0.000]\n",
      "Epoch 7 [144/172] - Loss: 3.954 [-3.953, 0.001, -0.000]\n",
      "Epoch 7 [145/172] - Loss: 4.027 [-4.026, 0.001, -0.000]\n",
      "Epoch 7 [146/172] - Loss: 3.956 [-3.955, 0.001, -0.000]\n",
      "Epoch 7 [147/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 7 [148/172] - Loss: 3.974 [-3.972, 0.001, -0.000]\n",
      "Epoch 7 [149/172] - Loss: 3.964 [-3.963, 0.001, -0.000]\n",
      "Epoch 7 [150/172] - Loss: 3.953 [-3.952, 0.001, -0.000]\n",
      "Epoch 7 [151/172] - Loss: 3.977 [-3.976, 0.001, -0.000]\n",
      "Epoch 7 [152/172] - Loss: 3.969 [-3.968, 0.001, -0.000]\n",
      "Epoch 7 [153/172] - Loss: 3.983 [-3.982, 0.001, -0.000]\n",
      "Epoch 7 [154/172] - Loss: 4.010 [-4.009, 0.001, -0.000]\n",
      "Epoch 7 [155/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 7 [156/172] - Loss: 3.982 [-3.980, 0.001, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [157/172] - Loss: 4.021 [-4.019, 0.001, -0.000]\n",
      "Epoch 7 [158/172] - Loss: 3.959 [-3.957, 0.001, -0.000]\n",
      "Epoch 7 [159/172] - Loss: 4.020 [-4.018, 0.001, -0.000]\n",
      "Epoch 7 [160/172] - Loss: 4.016 [-4.014, 0.001, -0.000]\n",
      "Epoch 7 [161/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 7 [162/172] - Loss: 3.974 [-3.973, 0.001, -0.000]\n",
      "Epoch 7 [163/172] - Loss: 3.961 [-3.960, 0.001, -0.000]\n",
      "Epoch 7 [164/172] - Loss: 3.990 [-3.988, 0.001, -0.000]\n",
      "Epoch 7 [165/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 7 [166/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 7 [167/172] - Loss: 3.982 [-3.981, 0.001, -0.000]\n",
      "Epoch 7 [168/172] - Loss: 3.989 [-3.988, 0.002, -0.000]\n",
      "Epoch 7 [169/172] - Loss: 3.918 [-3.917, 0.001, -0.000]\n",
      "Epoch 7 [170/172] - Loss: 3.970 [-3.969, 0.002, -0.000]\n",
      "Epoch 7 [171/172] - Loss: 4.001 [-3.999, 0.001, -0.000]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 6 epochs of training in this tutorial\n",
    "num_epochs = 7\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Because the grid is relatively small, we turn off the Toeplitz matrix multiplication and just perform them directly\n",
    "        # We find this to be more efficient when the grid is very small.\n",
    "        with gpytorch.settings.use_toeplitz(False):\n",
    "            output = model(x_batch)\n",
    "            log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "            loss = -(log_lik - kl_div + log_prior)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader), loss.item(), log_lik.item(), kl_div.item(), log_prior.item()))\n",
    "\n",
    "        # The actual optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`) using the standard SKI testing code, with no acceleration or precomputation. Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although our other tutorials demonstrate how to do this (for example, see the CIFAR tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 9.33540153503418\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
