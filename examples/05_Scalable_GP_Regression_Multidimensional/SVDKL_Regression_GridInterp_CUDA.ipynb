{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA) (w/ KISS-GP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use Deep Kernel Learning with SKI stochastic variational regression to rapidly train using minibatches on the `song` UCI dataset, which has over 500,000 training examples in 90 dimensions. \n",
    "\n",
    "Stochastic variational inference has several major advantages over the standard regression setting. Most notably, the ELBO used for optimization decomposes in such a way that stochastic gradient descent techniques can be used. See https://arxiv.org/pdf/1411.2005.pdf and https://arxiv.org/pdf/1611.00336.pdf for more technical details of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `3droad` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~340 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jrg365/data/uci/3droad/3droad.mat\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/data/uci/3droad/3droad.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2028, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Feature Extractor\n",
    "\n",
    "Next, we define the neural network feature extractor used to define the deep kernel. In this case, we use a fully connected network with the architecture `d -> 1000 -> 500 -> 50 -> 2`, as described in the original DKL paper. All of the code below uses standard PyTorch implementations of neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('bn1', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 1000))\n",
    "        self.add_module('bn2', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu2', torch.nn.ReLU())                       \n",
    "        self.add_module('linear3', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('bn3', torch.nn.BatchNorm1d(500))\n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('bn4', torch.nn.BatchNorm1d(50))\n",
    "        self.add_module('relu4', torch.nn.ReLU())                  \n",
    "        self.add_module('linear5', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Regression Layer\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. To use grid interpolation for variational inference, we'll be using a `GridInterpolationVariationalStrategy`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions.\n",
    "\n",
    "See the CIFAR example for using an `AbstractVariationalGP` with an `AdditiveGridInterpolationVariationalStrategy`, which additionally assumes the kernel decomposes additively, which is a strong modelling assumption but allows us to use many more output features from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, GridInterpolationVariationalStrategy\n",
    "class GPRegressionLayer(AbstractVariationalGP):\n",
    "    def __init__(self, grid_size=32, grid_bounds=[(-1, 1), (-1, 1)]):\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=grid_size*grid_size)\n",
    "        variational_strategy = GridInterpolationVariationalStrategy(self,\n",
    "                                                                    grid_size=grid_size,\n",
    "                                                                    grid_bounds=grid_bounds,\n",
    "                                                                    variational_distribution=variational_distribution)\n",
    "        super(GPRegressionLayer, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "            log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1., sigma=0.1, log_transform=True)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Model\n",
    "\n",
    "With the feature extractor and GP regression layer defined, we can now define our full model. To do this, we simply create a module whose `forward()` method passes the data first through the feature extractor, and then through the GP regression layer.\n",
    "\n",
    "The only other interesting feature of the model below is that we use a helper function, `scale_to_bounds`, to ensure that the features extracted by the neural network fit within the grid bounds used for SKI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_features, grid_bounds=(-1., 1.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GPRegressionLayer()\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "\n",
    "model = DKLModel(feature_extractor, num_features=num_features).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the DKL model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalELBO`), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/172] - Loss: 171.907 [-171.786, 0.121, -0.000]\n",
      "Epoch 1 [1/172] - Loss: 158.461 [-157.545, 0.916, -0.000]\n",
      "Epoch 1 [2/172] - Loss: 145.601 [-144.634, 0.967, -0.000]\n",
      "Epoch 1 [3/172] - Loss: 126.168 [-125.540, 0.628, -0.000]\n",
      "Epoch 1 [4/172] - Loss: 120.279 [-119.786, 0.494, -0.000]\n",
      "Epoch 1 [5/172] - Loss: 112.098 [-111.550, 0.548, -0.000]\n",
      "Epoch 1 [6/172] - Loss: 100.376 [-99.964, 0.412, -0.000]\n",
      "Epoch 1 [7/172] - Loss: 90.687 [-90.449, 0.238, -0.000]\n",
      "Epoch 1 [8/172] - Loss: 91.281 [-91.098, 0.183, -0.000]\n",
      "Epoch 1 [9/172] - Loss: 74.134 [-73.978, 0.157, -0.000]\n",
      "Epoch 1 [10/172] - Loss: 65.697 [-65.568, 0.129, -0.000]\n",
      "Epoch 1 [11/172] - Loss: 60.717 [-60.583, 0.134, -0.000]\n",
      "Epoch 1 [12/172] - Loss: 61.200 [-61.065, 0.135, -0.000]\n",
      "Epoch 1 [13/172] - Loss: 53.973 [-53.855, 0.118, -0.000]\n",
      "Epoch 1 [14/172] - Loss: 54.097 [-53.984, 0.113, -0.000]\n",
      "Epoch 1 [15/172] - Loss: 46.051 [-45.938, 0.114, -0.000]\n",
      "Epoch 1 [16/172] - Loss: 42.500 [-42.404, 0.096, -0.000]\n",
      "Epoch 1 [17/172] - Loss: 39.498 [-39.407, 0.091, -0.000]\n",
      "Epoch 1 [18/172] - Loss: 38.016 [-37.931, 0.085, -0.000]\n",
      "Epoch 1 [19/172] - Loss: 36.050 [-35.972, 0.077, -0.000]\n",
      "Epoch 1 [20/172] - Loss: 32.187 [-32.113, 0.074, -0.000]\n",
      "Epoch 1 [21/172] - Loss: 29.599 [-29.533, 0.066, -0.000]\n",
      "Epoch 1 [22/172] - Loss: 27.186 [-27.125, 0.061, -0.000]\n",
      "Epoch 1 [23/172] - Loss: 26.192 [-26.136, 0.055, -0.000]\n",
      "Epoch 1 [24/172] - Loss: 25.763 [-25.713, 0.050, -0.000]\n",
      "Epoch 1 [25/172] - Loss: 22.869 [-22.823, 0.046, -0.000]\n",
      "Epoch 1 [26/172] - Loss: 23.872 [-23.832, 0.041, -0.000]\n",
      "Epoch 1 [27/172] - Loss: 20.903 [-20.866, 0.037, -0.000]\n",
      "Epoch 1 [28/172] - Loss: 20.693 [-20.660, 0.033, -0.000]\n",
      "Epoch 1 [29/172] - Loss: 21.199 [-21.169, 0.030, -0.000]\n",
      "Epoch 1 [30/172] - Loss: 18.774 [-18.747, 0.027, -0.000]\n",
      "Epoch 1 [31/172] - Loss: 18.121 [-18.096, 0.024, -0.000]\n",
      "Epoch 1 [32/172] - Loss: 17.756 [-17.735, 0.022, -0.000]\n",
      "Epoch 1 [33/172] - Loss: 17.380 [-17.360, 0.020, -0.000]\n",
      "Epoch 1 [34/172] - Loss: 16.323 [-16.305, 0.018, -0.000]\n",
      "Epoch 1 [35/172] - Loss: 17.617 [-17.601, 0.016, -0.000]\n",
      "Epoch 1 [36/172] - Loss: 16.097 [-16.082, 0.015, -0.000]\n",
      "Epoch 1 [37/172] - Loss: 14.488 [-14.475, 0.013, -0.000]\n",
      "Epoch 1 [38/172] - Loss: 14.372 [-14.360, 0.012, -0.000]\n",
      "Epoch 1 [39/172] - Loss: 14.351 [-14.340, 0.011, -0.000]\n",
      "Epoch 1 [40/172] - Loss: 13.851 [-13.840, 0.010, -0.000]\n",
      "Epoch 1 [41/172] - Loss: 14.191 [-14.181, 0.009, -0.000]\n",
      "Epoch 1 [42/172] - Loss: 12.376 [-12.368, 0.009, -0.000]\n",
      "Epoch 1 [43/172] - Loss: 13.334 [-13.326, 0.008, -0.000]\n",
      "Epoch 1 [44/172] - Loss: 12.156 [-12.148, 0.008, -0.000]\n",
      "Epoch 1 [45/172] - Loss: 12.110 [-12.103, 0.007, -0.000]\n",
      "Epoch 1 [46/172] - Loss: 11.873 [-11.867, 0.007, -0.000]\n",
      "Epoch 1 [47/172] - Loss: 11.522 [-11.516, 0.006, -0.000]\n",
      "Epoch 1 [48/172] - Loss: 10.627 [-10.621, 0.006, -0.000]\n",
      "Epoch 1 [49/172] - Loss: 10.916 [-10.910, 0.006, -0.000]\n",
      "Epoch 1 [50/172] - Loss: 11.261 [-11.256, 0.006, -0.000]\n",
      "Epoch 1 [51/172] - Loss: 11.444 [-11.438, 0.005, -0.000]\n",
      "Epoch 1 [52/172] - Loss: 11.184 [-11.179, 0.005, -0.000]\n",
      "Epoch 1 [53/172] - Loss: 10.682 [-10.677, 0.005, -0.000]\n",
      "Epoch 1 [54/172] - Loss: 10.655 [-10.650, 0.005, -0.000]\n",
      "Epoch 1 [55/172] - Loss: 9.661 [-9.656, 0.005, -0.000]\n",
      "Epoch 1 [56/172] - Loss: 9.580 [-9.575, 0.005, -0.000]\n",
      "Epoch 1 [57/172] - Loss: 9.627 [-9.622, 0.005, -0.000]\n",
      "Epoch 1 [58/172] - Loss: 9.650 [-9.645, 0.005, -0.000]\n",
      "Epoch 1 [59/172] - Loss: 8.990 [-8.986, 0.005, -0.000]\n",
      "Epoch 1 [60/172] - Loss: 9.060 [-9.056, 0.005, -0.000]\n",
      "Epoch 1 [61/172] - Loss: 9.180 [-9.175, 0.005, -0.000]\n",
      "Epoch 1 [62/172] - Loss: 8.576 [-8.572, 0.004, -0.000]\n",
      "Epoch 1 [63/172] - Loss: 9.130 [-9.125, 0.004, -0.000]\n",
      "Epoch 1 [64/172] - Loss: 8.649 [-8.645, 0.004, -0.000]\n",
      "Epoch 1 [65/172] - Loss: 8.755 [-8.751, 0.004, -0.000]\n",
      "Epoch 1 [66/172] - Loss: 8.120 [-8.116, 0.004, -0.000]\n",
      "Epoch 1 [67/172] - Loss: 7.620 [-7.616, 0.004, -0.000]\n",
      "Epoch 1 [68/172] - Loss: 8.438 [-8.434, 0.004, -0.000]\n",
      "Epoch 1 [69/172] - Loss: 8.072 [-8.068, 0.004, -0.000]\n",
      "Epoch 1 [70/172] - Loss: 8.256 [-8.252, 0.004, -0.000]\n",
      "Epoch 1 [71/172] - Loss: 8.115 [-8.111, 0.004, -0.000]\n",
      "Epoch 1 [72/172] - Loss: 7.981 [-7.977, 0.004, -0.000]\n",
      "Epoch 1 [73/172] - Loss: 8.267 [-8.263, 0.004, -0.000]\n",
      "Epoch 1 [74/172] - Loss: 7.673 [-7.669, 0.004, -0.000]\n",
      "Epoch 1 [75/172] - Loss: 7.569 [-7.565, 0.004, -0.000]\n",
      "Epoch 1 [76/172] - Loss: 7.918 [-7.914, 0.004, -0.000]\n",
      "Epoch 1 [77/172] - Loss: 8.037 [-8.033, 0.004, -0.000]\n",
      "Epoch 1 [78/172] - Loss: 7.451 [-7.447, 0.004, -0.000]\n",
      "Epoch 1 [79/172] - Loss: 7.297 [-7.293, 0.004, -0.000]\n",
      "Epoch 1 [80/172] - Loss: 7.197 [-7.194, 0.004, -0.000]\n",
      "Epoch 1 [81/172] - Loss: 7.157 [-7.153, 0.004, -0.000]\n",
      "Epoch 1 [82/172] - Loss: 6.831 [-6.828, 0.004, -0.000]\n",
      "Epoch 1 [83/172] - Loss: 6.948 [-6.944, 0.004, -0.000]\n",
      "Epoch 1 [84/172] - Loss: 7.045 [-7.041, 0.004, -0.000]\n",
      "Epoch 1 [85/172] - Loss: 6.888 [-6.885, 0.004, -0.000]\n",
      "Epoch 1 [86/172] - Loss: 7.321 [-7.317, 0.004, -0.000]\n",
      "Epoch 1 [87/172] - Loss: 7.373 [-7.370, 0.004, -0.000]\n",
      "Epoch 1 [88/172] - Loss: 7.134 [-7.130, 0.004, -0.000]\n",
      "Epoch 1 [89/172] - Loss: 7.295 [-7.291, 0.004, -0.000]\n",
      "Epoch 1 [90/172] - Loss: 7.056 [-7.052, 0.003, -0.000]\n",
      "Epoch 1 [91/172] - Loss: 6.856 [-6.852, 0.004, -0.000]\n",
      "Epoch 1 [92/172] - Loss: 6.671 [-6.667, 0.004, -0.000]\n",
      "Epoch 1 [93/172] - Loss: 6.815 [-6.811, 0.004, -0.000]\n",
      "Epoch 1 [94/172] - Loss: 6.973 [-6.970, 0.003, -0.000]\n",
      "Epoch 1 [95/172] - Loss: 6.558 [-6.555, 0.003, -0.000]\n",
      "Epoch 1 [96/172] - Loss: 6.760 [-6.757, 0.003, -0.000]\n",
      "Epoch 1 [97/172] - Loss: 6.583 [-6.580, 0.003, -0.000]\n",
      "Epoch 1 [98/172] - Loss: 6.728 [-6.724, 0.003, -0.000]\n",
      "Epoch 1 [99/172] - Loss: 6.403 [-6.399, 0.003, -0.000]\n",
      "Epoch 1 [100/172] - Loss: 6.407 [-6.404, 0.003, -0.000]\n",
      "Epoch 1 [101/172] - Loss: 5.970 [-5.966, 0.003, -0.000]\n",
      "Epoch 1 [102/172] - Loss: 6.190 [-6.186, 0.003, -0.000]\n",
      "Epoch 1 [103/172] - Loss: 6.259 [-6.255, 0.003, -0.000]\n",
      "Epoch 1 [104/172] - Loss: 5.910 [-5.907, 0.003, -0.000]\n",
      "Epoch 1 [105/172] - Loss: 6.120 [-6.117, 0.003, -0.000]\n",
      "Epoch 1 [106/172] - Loss: 6.170 [-6.166, 0.003, -0.000]\n",
      "Epoch 1 [107/172] - Loss: 5.975 [-5.972, 0.003, -0.000]\n",
      "Epoch 1 [108/172] - Loss: 6.001 [-5.998, 0.003, -0.000]\n",
      "Epoch 1 [109/172] - Loss: 5.794 [-5.791, 0.003, -0.000]\n",
      "Epoch 1 [110/172] - Loss: 6.059 [-6.056, 0.003, -0.000]\n",
      "Epoch 1 [111/172] - Loss: 6.054 [-6.051, 0.003, -0.000]\n",
      "Epoch 1 [112/172] - Loss: 6.056 [-6.053, 0.003, -0.000]\n",
      "Epoch 1 [113/172] - Loss: 5.937 [-5.934, 0.003, -0.000]\n",
      "Epoch 1 [114/172] - Loss: 5.987 [-5.984, 0.003, -0.000]\n",
      "Epoch 1 [115/172] - Loss: 6.143 [-6.140, 0.003, -0.000]\n",
      "Epoch 1 [116/172] - Loss: 6.032 [-6.028, 0.003, -0.000]\n",
      "Epoch 1 [117/172] - Loss: 6.108 [-6.104, 0.003, -0.000]\n",
      "Epoch 1 [118/172] - Loss: 5.524 [-5.521, 0.003, -0.000]\n",
      "Epoch 1 [119/172] - Loss: 5.901 [-5.898, 0.003, -0.000]\n",
      "Epoch 1 [120/172] - Loss: 5.688 [-5.685, 0.003, -0.000]\n",
      "Epoch 1 [121/172] - Loss: 5.571 [-5.568, 0.003, -0.000]\n",
      "Epoch 1 [122/172] - Loss: 5.672 [-5.669, 0.003, -0.000]\n",
      "Epoch 1 [123/172] - Loss: 5.806 [-5.803, 0.003, -0.000]\n",
      "Epoch 1 [124/172] - Loss: 5.651 [-5.648, 0.003, -0.000]\n",
      "Epoch 1 [125/172] - Loss: 5.792 [-5.789, 0.003, -0.000]\n",
      "Epoch 1 [126/172] - Loss: 6.045 [-6.042, 0.003, -0.000]\n",
      "Epoch 1 [127/172] - Loss: 5.902 [-5.899, 0.003, -0.000]\n",
      "Epoch 1 [128/172] - Loss: 5.623 [-5.620, 0.003, -0.000]\n",
      "Epoch 1 [129/172] - Loss: 5.812 [-5.809, 0.003, -0.000]\n",
      "Epoch 1 [130/172] - Loss: 5.600 [-5.597, 0.003, -0.000]\n",
      "Epoch 1 [131/172] - Loss: 5.496 [-5.493, 0.003, -0.000]\n",
      "Epoch 1 [132/172] - Loss: 5.659 [-5.656, 0.003, -0.000]\n",
      "Epoch 1 [133/172] - Loss: 5.452 [-5.449, 0.003, -0.000]\n",
      "Epoch 1 [134/172] - Loss: 5.520 [-5.517, 0.003, -0.000]\n",
      "Epoch 1 [135/172] - Loss: 5.462 [-5.459, 0.003, -0.000]\n",
      "Epoch 1 [136/172] - Loss: 5.337 [-5.334, 0.003, -0.000]\n",
      "Epoch 1 [137/172] - Loss: 5.434 [-5.431, 0.003, -0.000]\n",
      "Epoch 1 [138/172] - Loss: 5.527 [-5.524, 0.003, -0.000]\n",
      "Epoch 1 [139/172] - Loss: 5.193 [-5.190, 0.003, -0.000]\n",
      "Epoch 1 [140/172] - Loss: 5.472 [-5.469, 0.003, -0.000]\n",
      "Epoch 1 [141/172] - Loss: 5.513 [-5.511, 0.003, -0.000]\n",
      "Epoch 1 [142/172] - Loss: 5.347 [-5.345, 0.003, -0.000]\n",
      "Epoch 1 [143/172] - Loss: 5.247 [-5.244, 0.003, -0.000]\n",
      "Epoch 1 [144/172] - Loss: 5.270 [-5.267, 0.003, -0.000]\n",
      "Epoch 1 [145/172] - Loss: 5.211 [-5.208, 0.003, -0.000]\n",
      "Epoch 1 [146/172] - Loss: 5.080 [-5.077, 0.003, -0.000]\n",
      "Epoch 1 [147/172] - Loss: 5.183 [-5.180, 0.003, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [148/172] - Loss: 5.327 [-5.324, 0.003, -0.000]\n",
      "Epoch 1 [149/172] - Loss: 5.029 [-5.026, 0.003, -0.000]\n",
      "Epoch 1 [150/172] - Loss: 5.265 [-5.263, 0.003, -0.000]\n",
      "Epoch 1 [151/172] - Loss: 5.076 [-5.073, 0.003, -0.000]\n",
      "Epoch 1 [152/172] - Loss: 5.124 [-5.121, 0.003, -0.000]\n",
      "Epoch 1 [153/172] - Loss: 5.118 [-5.115, 0.003, -0.000]\n",
      "Epoch 1 [154/172] - Loss: 5.024 [-5.022, 0.003, -0.000]\n",
      "Epoch 1 [155/172] - Loss: 5.109 [-5.106, 0.003, -0.000]\n",
      "Epoch 1 [156/172] - Loss: 5.134 [-5.131, 0.003, -0.000]\n",
      "Epoch 1 [157/172] - Loss: 5.177 [-5.174, 0.003, -0.000]\n",
      "Epoch 1 [158/172] - Loss: 4.959 [-4.956, 0.003, -0.000]\n",
      "Epoch 1 [159/172] - Loss: 4.741 [-4.739, 0.003, -0.000]\n",
      "Epoch 1 [160/172] - Loss: 5.186 [-5.183, 0.003, -0.000]\n",
      "Epoch 1 [161/172] - Loss: 4.922 [-4.919, 0.003, -0.000]\n",
      "Epoch 1 [162/172] - Loss: 4.857 [-4.855, 0.003, -0.000]\n",
      "Epoch 1 [163/172] - Loss: 5.135 [-5.133, 0.003, -0.000]\n",
      "Epoch 1 [164/172] - Loss: 5.087 [-5.084, 0.003, -0.000]\n",
      "Epoch 1 [165/172] - Loss: 4.894 [-4.891, 0.003, -0.000]\n",
      "Epoch 1 [166/172] - Loss: 4.958 [-4.956, 0.003, -0.000]\n",
      "Epoch 1 [167/172] - Loss: 4.974 [-4.971, 0.003, -0.000]\n",
      "Epoch 1 [168/172] - Loss: 4.862 [-4.859, 0.003, -0.000]\n",
      "Epoch 1 [169/172] - Loss: 4.878 [-4.875, 0.003, -0.000]\n",
      "Epoch 1 [170/172] - Loss: 4.921 [-4.918, 0.003, -0.000]\n",
      "Epoch 1 [171/172] - Loss: 5.043 [-5.041, 0.003, -0.000]\n",
      "Epoch 2 [0/172] - Loss: 5.179 [-5.176, 0.003, -0.000]\n",
      "Epoch 2 [1/172] - Loss: 4.958 [-4.955, 0.003, -0.000]\n",
      "Epoch 2 [2/172] - Loss: 4.850 [-4.847, 0.003, -0.000]\n",
      "Epoch 2 [3/172] - Loss: 4.945 [-4.943, 0.003, -0.000]\n",
      "Epoch 2 [4/172] - Loss: 4.622 [-4.619, 0.003, -0.000]\n",
      "Epoch 2 [5/172] - Loss: 4.787 [-4.784, 0.003, -0.000]\n",
      "Epoch 2 [6/172] - Loss: 4.815 [-4.813, 0.003, -0.000]\n",
      "Epoch 2 [7/172] - Loss: 4.827 [-4.824, 0.003, -0.000]\n",
      "Epoch 2 [8/172] - Loss: 4.926 [-4.924, 0.003, -0.000]\n",
      "Epoch 2 [9/172] - Loss: 4.745 [-4.742, 0.003, -0.000]\n",
      "Epoch 2 [10/172] - Loss: 4.733 [-4.731, 0.003, -0.000]\n",
      "Epoch 2 [11/172] - Loss: 4.725 [-4.722, 0.003, -0.000]\n",
      "Epoch 2 [12/172] - Loss: 5.038 [-5.036, 0.003, -0.000]\n",
      "Epoch 2 [13/172] - Loss: 4.796 [-4.793, 0.003, -0.000]\n",
      "Epoch 2 [14/172] - Loss: 4.729 [-4.727, 0.002, -0.000]\n",
      "Epoch 2 [15/172] - Loss: 4.740 [-4.738, 0.003, -0.000]\n",
      "Epoch 2 [16/172] - Loss: 4.846 [-4.844, 0.002, -0.000]\n",
      "Epoch 2 [17/172] - Loss: 4.782 [-4.779, 0.003, -0.000]\n",
      "Epoch 2 [18/172] - Loss: 4.743 [-4.741, 0.003, -0.000]\n",
      "Epoch 2 [19/172] - Loss: 4.685 [-4.682, 0.002, -0.000]\n",
      "Epoch 2 [20/172] - Loss: 4.740 [-4.738, 0.003, -0.000]\n",
      "Epoch 2 [21/172] - Loss: 4.601 [-4.598, 0.002, -0.000]\n",
      "Epoch 2 [22/172] - Loss: 4.706 [-4.703, 0.003, -0.000]\n",
      "Epoch 2 [23/172] - Loss: 4.742 [-4.739, 0.003, -0.000]\n",
      "Epoch 2 [24/172] - Loss: 4.791 [-4.789, 0.003, -0.000]\n",
      "Epoch 2 [25/172] - Loss: 4.656 [-4.654, 0.003, -0.000]\n",
      "Epoch 2 [26/172] - Loss: 4.504 [-4.501, 0.003, -0.000]\n",
      "Epoch 2 [27/172] - Loss: 4.749 [-4.746, 0.002, -0.000]\n",
      "Epoch 2 [28/172] - Loss: 4.694 [-4.691, 0.002, -0.000]\n",
      "Epoch 2 [29/172] - Loss: 4.621 [-4.618, 0.002, -0.000]\n",
      "Epoch 2 [30/172] - Loss: 4.600 [-4.597, 0.002, -0.000]\n",
      "Epoch 2 [31/172] - Loss: 4.680 [-4.677, 0.002, -0.000]\n",
      "Epoch 2 [32/172] - Loss: 4.639 [-4.637, 0.002, -0.000]\n",
      "Epoch 2 [33/172] - Loss: 4.374 [-4.371, 0.002, -0.000]\n",
      "Epoch 2 [34/172] - Loss: 4.673 [-4.671, 0.002, -0.000]\n",
      "Epoch 2 [35/172] - Loss: 4.634 [-4.631, 0.002, -0.000]\n",
      "Epoch 2 [36/172] - Loss: 4.596 [-4.594, 0.002, -0.000]\n",
      "Epoch 2 [37/172] - Loss: 4.508 [-4.506, 0.002, -0.000]\n",
      "Epoch 2 [38/172] - Loss: 4.666 [-4.664, 0.002, -0.000]\n",
      "Epoch 2 [39/172] - Loss: 4.530 [-4.528, 0.002, -0.000]\n",
      "Epoch 2 [40/172] - Loss: 4.600 [-4.598, 0.002, -0.000]\n",
      "Epoch 2 [41/172] - Loss: 4.716 [-4.714, 0.002, -0.000]\n",
      "Epoch 2 [42/172] - Loss: 4.651 [-4.648, 0.002, -0.000]\n",
      "Epoch 2 [43/172] - Loss: 4.579 [-4.576, 0.002, -0.000]\n",
      "Epoch 2 [44/172] - Loss: 4.690 [-4.688, 0.002, -0.000]\n",
      "Epoch 2 [45/172] - Loss: 4.576 [-4.573, 0.002, -0.000]\n",
      "Epoch 2 [46/172] - Loss: 4.581 [-4.579, 0.002, -0.000]\n",
      "Epoch 2 [47/172] - Loss: 4.673 [-4.670, 0.002, -0.000]\n",
      "Epoch 2 [48/172] - Loss: 4.422 [-4.419, 0.002, -0.000]\n",
      "Epoch 2 [49/172] - Loss: 4.477 [-4.474, 0.002, -0.000]\n",
      "Epoch 2 [50/172] - Loss: 4.736 [-4.733, 0.002, -0.000]\n",
      "Epoch 2 [51/172] - Loss: 4.575 [-4.573, 0.002, -0.000]\n",
      "Epoch 2 [52/172] - Loss: 4.507 [-4.505, 0.002, -0.000]\n",
      "Epoch 2 [53/172] - Loss: 4.447 [-4.445, 0.002, -0.000]\n",
      "Epoch 2 [54/172] - Loss: 4.559 [-4.557, 0.002, -0.000]\n",
      "Epoch 2 [55/172] - Loss: 4.487 [-4.485, 0.002, -0.000]\n",
      "Epoch 2 [56/172] - Loss: 4.490 [-4.488, 0.002, -0.000]\n",
      "Epoch 2 [57/172] - Loss: 4.476 [-4.474, 0.002, -0.000]\n",
      "Epoch 2 [58/172] - Loss: 4.658 [-4.656, 0.002, -0.000]\n",
      "Epoch 2 [59/172] - Loss: 4.689 [-4.687, 0.002, -0.000]\n",
      "Epoch 2 [60/172] - Loss: 4.406 [-4.404, 0.002, -0.000]\n",
      "Epoch 2 [61/172] - Loss: 4.477 [-4.475, 0.002, -0.000]\n",
      "Epoch 2 [62/172] - Loss: 4.333 [-4.331, 0.002, -0.000]\n",
      "Epoch 2 [63/172] - Loss: 4.525 [-4.522, 0.002, -0.000]\n",
      "Epoch 2 [64/172] - Loss: 4.392 [-4.389, 0.002, -0.000]\n",
      "Epoch 2 [65/172] - Loss: 4.427 [-4.425, 0.002, -0.000]\n",
      "Epoch 2 [66/172] - Loss: 4.420 [-4.417, 0.002, -0.000]\n",
      "Epoch 2 [67/172] - Loss: 4.501 [-4.498, 0.002, -0.000]\n",
      "Epoch 2 [68/172] - Loss: 4.504 [-4.502, 0.002, -0.000]\n",
      "Epoch 2 [69/172] - Loss: 4.538 [-4.535, 0.002, -0.000]\n",
      "Epoch 2 [70/172] - Loss: 4.386 [-4.384, 0.002, -0.000]\n",
      "Epoch 2 [71/172] - Loss: 4.411 [-4.409, 0.002, -0.000]\n",
      "Epoch 2 [72/172] - Loss: 4.421 [-4.419, 0.002, -0.000]\n",
      "Epoch 2 [73/172] - Loss: 4.434 [-4.431, 0.002, -0.000]\n",
      "Epoch 2 [74/172] - Loss: 4.531 [-4.528, 0.002, -0.000]\n",
      "Epoch 2 [75/172] - Loss: 4.423 [-4.421, 0.002, -0.000]\n",
      "Epoch 2 [76/172] - Loss: 4.334 [-4.331, 0.002, -0.000]\n",
      "Epoch 2 [77/172] - Loss: 4.391 [-4.389, 0.002, -0.000]\n",
      "Epoch 2 [78/172] - Loss: 4.368 [-4.366, 0.002, -0.000]\n",
      "Epoch 2 [79/172] - Loss: 4.408 [-4.406, 0.002, -0.000]\n",
      "Epoch 2 [80/172] - Loss: 4.309 [-4.307, 0.002, -0.000]\n",
      "Epoch 2 [81/172] - Loss: 4.278 [-4.276, 0.002, -0.000]\n",
      "Epoch 2 [82/172] - Loss: 4.435 [-4.433, 0.002, -0.000]\n",
      "Epoch 2 [83/172] - Loss: 4.418 [-4.416, 0.002, -0.000]\n",
      "Epoch 2 [84/172] - Loss: 4.384 [-4.381, 0.002, -0.000]\n",
      "Epoch 2 [85/172] - Loss: 4.362 [-4.359, 0.002, -0.000]\n",
      "Epoch 2 [86/172] - Loss: 4.503 [-4.501, 0.002, -0.000]\n",
      "Epoch 2 [87/172] - Loss: 4.432 [-4.429, 0.002, -0.000]\n",
      "Epoch 2 [88/172] - Loss: 4.419 [-4.416, 0.002, -0.000]\n",
      "Epoch 2 [89/172] - Loss: 4.328 [-4.326, 0.002, -0.000]\n",
      "Epoch 2 [90/172] - Loss: 4.250 [-4.248, 0.002, -0.000]\n",
      "Epoch 2 [91/172] - Loss: 4.305 [-4.303, 0.002, -0.000]\n",
      "Epoch 2 [92/172] - Loss: 4.353 [-4.351, 0.002, -0.000]\n",
      "Epoch 2 [93/172] - Loss: 4.241 [-4.239, 0.002, -0.000]\n",
      "Epoch 2 [94/172] - Loss: 4.234 [-4.231, 0.002, -0.000]\n",
      "Epoch 2 [95/172] - Loss: 4.246 [-4.244, 0.002, -0.000]\n",
      "Epoch 2 [96/172] - Loss: 4.259 [-4.257, 0.002, -0.000]\n",
      "Epoch 2 [97/172] - Loss: 4.320 [-4.318, 0.002, -0.000]\n",
      "Epoch 2 [98/172] - Loss: 4.267 [-4.265, 0.002, -0.000]\n",
      "Epoch 2 [99/172] - Loss: 4.368 [-4.365, 0.002, -0.000]\n",
      "Epoch 2 [100/172] - Loss: 4.273 [-4.271, 0.002, -0.000]\n",
      "Epoch 2 [101/172] - Loss: 4.358 [-4.356, 0.002, -0.000]\n",
      "Epoch 2 [102/172] - Loss: 4.285 [-4.283, 0.002, -0.000]\n",
      "Epoch 2 [103/172] - Loss: 4.311 [-4.309, 0.002, -0.000]\n",
      "Epoch 2 [104/172] - Loss: 4.242 [-4.240, 0.002, -0.000]\n",
      "Epoch 2 [105/172] - Loss: 4.202 [-4.200, 0.002, -0.000]\n",
      "Epoch 2 [106/172] - Loss: 4.370 [-4.368, 0.002, -0.000]\n",
      "Epoch 2 [107/172] - Loss: 4.366 [-4.364, 0.002, -0.000]\n",
      "Epoch 2 [108/172] - Loss: 4.331 [-4.329, 0.002, -0.000]\n",
      "Epoch 2 [109/172] - Loss: 4.350 [-4.348, 0.002, -0.000]\n",
      "Epoch 2 [110/172] - Loss: 4.246 [-4.243, 0.002, -0.000]\n",
      "Epoch 2 [111/172] - Loss: 4.332 [-4.330, 0.002, -0.000]\n",
      "Epoch 2 [112/172] - Loss: 4.285 [-4.283, 0.002, -0.000]\n",
      "Epoch 2 [113/172] - Loss: 4.373 [-4.371, 0.002, -0.000]\n",
      "Epoch 2 [114/172] - Loss: 4.386 [-4.384, 0.002, -0.000]\n",
      "Epoch 2 [115/172] - Loss: 4.382 [-4.380, 0.002, -0.000]\n",
      "Epoch 2 [116/172] - Loss: 4.358 [-4.355, 0.002, -0.000]\n",
      "Epoch 2 [117/172] - Loss: 4.349 [-4.347, 0.002, -0.000]\n",
      "Epoch 2 [118/172] - Loss: 4.492 [-4.490, 0.002, -0.000]\n",
      "Epoch 2 [119/172] - Loss: 4.348 [-4.346, 0.002, -0.000]\n",
      "Epoch 2 [120/172] - Loss: 4.200 [-4.198, 0.002, -0.000]\n",
      "Epoch 2 [121/172] - Loss: 4.372 [-4.370, 0.002, -0.000]\n",
      "Epoch 2 [122/172] - Loss: 4.191 [-4.189, 0.002, -0.000]\n",
      "Epoch 2 [123/172] - Loss: 4.248 [-4.246, 0.002, -0.000]\n",
      "Epoch 2 [124/172] - Loss: 4.201 [-4.199, 0.002, -0.000]\n",
      "Epoch 2 [125/172] - Loss: 4.280 [-4.278, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [126/172] - Loss: 4.242 [-4.240, 0.002, -0.000]\n",
      "Epoch 2 [127/172] - Loss: 4.342 [-4.340, 0.002, -0.000]\n",
      "Epoch 2 [128/172] - Loss: 4.495 [-4.493, 0.002, -0.000]\n",
      "Epoch 2 [129/172] - Loss: 4.381 [-4.379, 0.002, -0.000]\n",
      "Epoch 2 [130/172] - Loss: 4.305 [-4.303, 0.002, -0.000]\n",
      "Epoch 2 [131/172] - Loss: 4.373 [-4.371, 0.002, -0.000]\n",
      "Epoch 2 [132/172] - Loss: 4.221 [-4.219, 0.002, -0.000]\n",
      "Epoch 2 [133/172] - Loss: 4.321 [-4.319, 0.002, -0.000]\n",
      "Epoch 2 [134/172] - Loss: 4.547 [-4.545, 0.002, -0.000]\n",
      "Epoch 2 [135/172] - Loss: 4.413 [-4.411, 0.002, -0.000]\n",
      "Epoch 2 [136/172] - Loss: 4.332 [-4.330, 0.002, -0.000]\n",
      "Epoch 2 [137/172] - Loss: 4.321 [-4.319, 0.002, -0.000]\n",
      "Epoch 2 [138/172] - Loss: 4.526 [-4.524, 0.002, -0.000]\n",
      "Epoch 2 [139/172] - Loss: 4.277 [-4.275, 0.002, -0.000]\n",
      "Epoch 2 [140/172] - Loss: 4.327 [-4.325, 0.002, -0.000]\n",
      "Epoch 2 [141/172] - Loss: 4.235 [-4.232, 0.002, -0.000]\n",
      "Epoch 2 [142/172] - Loss: 4.447 [-4.444, 0.002, -0.000]\n",
      "Epoch 2 [143/172] - Loss: 4.581 [-4.579, 0.002, -0.000]\n",
      "Epoch 2 [144/172] - Loss: 4.617 [-4.615, 0.002, -0.000]\n",
      "Epoch 2 [145/172] - Loss: 4.661 [-4.659, 0.002, -0.000]\n",
      "Epoch 2 [146/172] - Loss: 4.576 [-4.574, 0.002, -0.000]\n",
      "Epoch 2 [147/172] - Loss: 4.499 [-4.497, 0.002, -0.000]\n",
      "Epoch 2 [148/172] - Loss: 4.450 [-4.448, 0.002, -0.000]\n",
      "Epoch 2 [149/172] - Loss: 4.449 [-4.447, 0.002, -0.000]\n",
      "Epoch 2 [150/172] - Loss: 4.461 [-4.459, 0.002, -0.000]\n",
      "Epoch 2 [151/172] - Loss: 4.383 [-4.381, 0.002, -0.000]\n",
      "Epoch 2 [152/172] - Loss: 4.423 [-4.421, 0.002, -0.000]\n",
      "Epoch 2 [153/172] - Loss: 4.344 [-4.342, 0.002, -0.000]\n",
      "Epoch 2 [154/172] - Loss: 4.312 [-4.310, 0.002, -0.000]\n",
      "Epoch 2 [155/172] - Loss: 4.261 [-4.259, 0.002, -0.000]\n",
      "Epoch 2 [156/172] - Loss: 4.248 [-4.246, 0.002, -0.000]\n",
      "Epoch 2 [157/172] - Loss: 4.186 [-4.184, 0.002, -0.000]\n",
      "Epoch 2 [158/172] - Loss: 4.266 [-4.264, 0.002, -0.000]\n",
      "Epoch 2 [159/172] - Loss: 4.193 [-4.191, 0.002, -0.000]\n",
      "Epoch 2 [160/172] - Loss: 4.263 [-4.261, 0.002, -0.000]\n",
      "Epoch 2 [161/172] - Loss: 4.287 [-4.285, 0.002, -0.000]\n",
      "Epoch 2 [162/172] - Loss: 4.181 [-4.179, 0.002, -0.000]\n",
      "Epoch 2 [163/172] - Loss: 4.221 [-4.219, 0.002, -0.000]\n",
      "Epoch 2 [164/172] - Loss: 4.191 [-4.189, 0.002, -0.000]\n",
      "Epoch 2 [165/172] - Loss: 4.359 [-4.357, 0.002, -0.000]\n",
      "Epoch 2 [166/172] - Loss: 4.338 [-4.336, 0.002, -0.000]\n",
      "Epoch 2 [167/172] - Loss: 4.198 [-4.196, 0.002, -0.000]\n",
      "Epoch 2 [168/172] - Loss: 4.275 [-4.273, 0.002, -0.000]\n",
      "Epoch 2 [169/172] - Loss: 4.382 [-4.380, 0.002, -0.000]\n",
      "Epoch 2 [170/172] - Loss: 4.372 [-4.371, 0.002, -0.000]\n",
      "Epoch 2 [171/172] - Loss: 4.218 [-4.216, 0.002, -0.000]\n",
      "Epoch 3 [0/172] - Loss: 4.201 [-4.199, 0.002, -0.000]\n",
      "Epoch 3 [1/172] - Loss: 4.158 [-4.156, 0.002, -0.000]\n",
      "Epoch 3 [2/172] - Loss: 4.240 [-4.238, 0.002, -0.000]\n",
      "Epoch 3 [3/172] - Loss: 4.135 [-4.133, 0.002, -0.000]\n",
      "Epoch 3 [4/172] - Loss: 4.222 [-4.220, 0.002, -0.000]\n",
      "Epoch 3 [5/172] - Loss: 4.128 [-4.127, 0.002, -0.000]\n",
      "Epoch 3 [6/172] - Loss: 4.223 [-4.221, 0.002, -0.000]\n",
      "Epoch 3 [7/172] - Loss: 4.156 [-4.154, 0.002, -0.000]\n",
      "Epoch 3 [8/172] - Loss: 4.162 [-4.160, 0.002, -0.000]\n",
      "Epoch 3 [9/172] - Loss: 4.192 [-4.190, 0.002, -0.000]\n",
      "Epoch 3 [10/172] - Loss: 4.225 [-4.223, 0.002, -0.000]\n",
      "Epoch 3 [11/172] - Loss: 4.301 [-4.299, 0.002, -0.000]\n",
      "Epoch 3 [12/172] - Loss: 4.258 [-4.256, 0.002, -0.000]\n",
      "Epoch 3 [13/172] - Loss: 4.193 [-4.191, 0.002, -0.000]\n",
      "Epoch 3 [14/172] - Loss: 4.286 [-4.284, 0.002, -0.000]\n",
      "Epoch 3 [15/172] - Loss: 4.230 [-4.228, 0.002, -0.000]\n",
      "Epoch 3 [16/172] - Loss: 4.152 [-4.150, 0.002, -0.000]\n",
      "Epoch 3 [17/172] - Loss: 4.180 [-4.178, 0.002, -0.000]\n",
      "Epoch 3 [18/172] - Loss: 4.247 [-4.246, 0.002, -0.000]\n",
      "Epoch 3 [19/172] - Loss: 4.250 [-4.248, 0.002, -0.000]\n",
      "Epoch 3 [20/172] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [21/172] - Loss: 4.298 [-4.296, 0.002, -0.000]\n",
      "Epoch 3 [22/172] - Loss: 4.188 [-4.186, 0.002, -0.000]\n",
      "Epoch 3 [23/172] - Loss: 4.186 [-4.184, 0.002, -0.000]\n",
      "Epoch 3 [24/172] - Loss: 4.176 [-4.174, 0.002, -0.000]\n",
      "Epoch 3 [25/172] - Loss: 4.248 [-4.246, 0.002, -0.000]\n",
      "Epoch 3 [26/172] - Loss: 4.254 [-4.253, 0.002, -0.000]\n",
      "Epoch 3 [27/172] - Loss: 4.114 [-4.112, 0.002, -0.000]\n",
      "Epoch 3 [28/172] - Loss: 4.192 [-4.190, 0.002, -0.000]\n",
      "Epoch 3 [29/172] - Loss: 4.255 [-4.253, 0.002, -0.000]\n",
      "Epoch 3 [30/172] - Loss: 4.127 [-4.125, 0.002, -0.000]\n",
      "Epoch 3 [31/172] - Loss: 4.049 [-4.047, 0.002, -0.000]\n",
      "Epoch 3 [32/172] - Loss: 4.053 [-4.052, 0.002, -0.000]\n",
      "Epoch 3 [33/172] - Loss: 4.169 [-4.167, 0.002, -0.000]\n",
      "Epoch 3 [34/172] - Loss: 4.135 [-4.133, 0.002, -0.000]\n",
      "Epoch 3 [35/172] - Loss: 4.157 [-4.155, 0.002, -0.000]\n",
      "Epoch 3 [36/172] - Loss: 4.098 [-4.097, 0.002, -0.000]\n",
      "Epoch 3 [37/172] - Loss: 4.167 [-4.165, 0.002, -0.000]\n",
      "Epoch 3 [38/172] - Loss: 4.202 [-4.201, 0.002, -0.000]\n",
      "Epoch 3 [39/172] - Loss: 4.134 [-4.133, 0.002, -0.000]\n",
      "Epoch 3 [40/172] - Loss: 4.082 [-4.080, 0.002, -0.000]\n",
      "Epoch 3 [41/172] - Loss: 4.198 [-4.197, 0.002, -0.000]\n",
      "Epoch 3 [42/172] - Loss: 4.111 [-4.109, 0.002, -0.000]\n",
      "Epoch 3 [43/172] - Loss: 4.093 [-4.091, 0.002, -0.000]\n",
      "Epoch 3 [44/172] - Loss: 4.136 [-4.134, 0.002, -0.000]\n",
      "Epoch 3 [45/172] - Loss: 4.060 [-4.059, 0.002, -0.000]\n",
      "Epoch 3 [46/172] - Loss: 4.075 [-4.073, 0.002, -0.000]\n",
      "Epoch 3 [47/172] - Loss: 4.195 [-4.193, 0.002, -0.000]\n",
      "Epoch 3 [48/172] - Loss: 4.110 [-4.108, 0.002, -0.000]\n",
      "Epoch 3 [49/172] - Loss: 4.182 [-4.180, 0.002, -0.000]\n",
      "Epoch 3 [50/172] - Loss: 4.091 [-4.089, 0.002, -0.000]\n",
      "Epoch 3 [51/172] - Loss: 4.134 [-4.132, 0.002, -0.000]\n",
      "Epoch 3 [52/172] - Loss: 4.185 [-4.184, 0.002, -0.000]\n",
      "Epoch 3 [53/172] - Loss: 4.101 [-4.099, 0.002, -0.000]\n",
      "Epoch 3 [54/172] - Loss: 4.201 [-4.199, 0.002, -0.000]\n",
      "Epoch 3 [55/172] - Loss: 4.249 [-4.247, 0.002, -0.000]\n",
      "Epoch 3 [56/172] - Loss: 4.183 [-4.181, 0.002, -0.000]\n",
      "Epoch 3 [57/172] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 3 [58/172] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [59/172] - Loss: 4.089 [-4.087, 0.002, -0.000]\n",
      "Epoch 3 [60/172] - Loss: 4.283 [-4.281, 0.002, -0.000]\n",
      "Epoch 3 [61/172] - Loss: 4.304 [-4.302, 0.002, -0.000]\n",
      "Epoch 3 [62/172] - Loss: 4.308 [-4.307, 0.002, -0.000]\n",
      "Epoch 3 [63/172] - Loss: 4.178 [-4.176, 0.002, -0.000]\n",
      "Epoch 3 [64/172] - Loss: 4.168 [-4.166, 0.002, -0.000]\n",
      "Epoch 3 [65/172] - Loss: 4.253 [-4.252, 0.002, -0.000]\n",
      "Epoch 3 [66/172] - Loss: 4.128 [-4.126, 0.002, -0.000]\n",
      "Epoch 3 [67/172] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [68/172] - Loss: 4.166 [-4.164, 0.002, -0.000]\n",
      "Epoch 3 [69/172] - Loss: 4.160 [-4.159, 0.002, -0.000]\n",
      "Epoch 3 [70/172] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [71/172] - Loss: 4.175 [-4.173, 0.002, -0.000]\n",
      "Epoch 3 [72/172] - Loss: 4.147 [-4.145, 0.002, -0.000]\n",
      "Epoch 3 [73/172] - Loss: 4.192 [-4.190, 0.002, -0.000]\n",
      "Epoch 3 [74/172] - Loss: 4.128 [-4.126, 0.002, -0.000]\n",
      "Epoch 3 [75/172] - Loss: 4.054 [-4.052, 0.002, -0.000]\n",
      "Epoch 3 [76/172] - Loss: 4.170 [-4.168, 0.002, -0.000]\n",
      "Epoch 3 [77/172] - Loss: 4.144 [-4.142, 0.002, -0.000]\n",
      "Epoch 3 [78/172] - Loss: 4.262 [-4.260, 0.002, -0.000]\n",
      "Epoch 3 [79/172] - Loss: 4.291 [-4.290, 0.002, -0.000]\n",
      "Epoch 3 [80/172] - Loss: 4.155 [-4.153, 0.002, -0.000]\n",
      "Epoch 3 [81/172] - Loss: 4.142 [-4.140, 0.002, -0.000]\n",
      "Epoch 3 [82/172] - Loss: 4.317 [-4.315, 0.002, -0.000]\n",
      "Epoch 3 [83/172] - Loss: 4.173 [-4.172, 0.002, -0.000]\n",
      "Epoch 3 [84/172] - Loss: 4.147 [-4.145, 0.002, -0.000]\n",
      "Epoch 3 [85/172] - Loss: 4.284 [-4.282, 0.002, -0.000]\n",
      "Epoch 3 [86/172] - Loss: 4.260 [-4.258, 0.002, -0.000]\n",
      "Epoch 3 [87/172] - Loss: 4.185 [-4.184, 0.002, -0.000]\n",
      "Epoch 3 [88/172] - Loss: 4.160 [-4.159, 0.002, -0.000]\n",
      "Epoch 3 [89/172] - Loss: 4.083 [-4.082, 0.002, -0.000]\n",
      "Epoch 3 [90/172] - Loss: 4.088 [-4.087, 0.002, -0.000]\n",
      "Epoch 3 [91/172] - Loss: 4.122 [-4.120, 0.002, -0.000]\n",
      "Epoch 3 [92/172] - Loss: 4.119 [-4.117, 0.002, -0.000]\n",
      "Epoch 3 [93/172] - Loss: 4.194 [-4.192, 0.002, -0.000]\n",
      "Epoch 3 [94/172] - Loss: 4.126 [-4.124, 0.002, -0.000]\n",
      "Epoch 3 [95/172] - Loss: 4.061 [-4.059, 0.002, -0.000]\n",
      "Epoch 3 [96/172] - Loss: 4.163 [-4.162, 0.002, -0.000]\n",
      "Epoch 3 [97/172] - Loss: 4.083 [-4.081, 0.002, -0.000]\n",
      "Epoch 3 [98/172] - Loss: 4.166 [-4.164, 0.002, -0.000]\n",
      "Epoch 3 [99/172] - Loss: 4.186 [-4.185, 0.002, -0.000]\n",
      "Epoch 3 [100/172] - Loss: 4.258 [-4.256, 0.002, -0.000]\n",
      "Epoch 3 [101/172] - Loss: 4.163 [-4.162, 0.002, -0.000]\n",
      "Epoch 3 [102/172] - Loss: 4.140 [-4.138, 0.002, -0.000]\n",
      "Epoch 3 [103/172] - Loss: 4.115 [-4.113, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [104/172] - Loss: 4.096 [-4.095, 0.002, -0.000]\n",
      "Epoch 3 [105/172] - Loss: 4.053 [-4.052, 0.002, -0.000]\n",
      "Epoch 3 [106/172] - Loss: 4.092 [-4.090, 0.002, -0.000]\n",
      "Epoch 3 [107/172] - Loss: 4.075 [-4.073, 0.002, -0.000]\n",
      "Epoch 3 [108/172] - Loss: 4.077 [-4.075, 0.002, -0.000]\n",
      "Epoch 3 [109/172] - Loss: 4.183 [-4.181, 0.002, -0.000]\n",
      "Epoch 3 [110/172] - Loss: 4.105 [-4.103, 0.002, -0.000]\n",
      "Epoch 3 [111/172] - Loss: 4.142 [-4.140, 0.002, -0.000]\n",
      "Epoch 3 [112/172] - Loss: 4.090 [-4.089, 0.002, -0.000]\n",
      "Epoch 3 [113/172] - Loss: 4.085 [-4.083, 0.002, -0.000]\n",
      "Epoch 3 [114/172] - Loss: 4.084 [-4.082, 0.002, -0.000]\n",
      "Epoch 3 [115/172] - Loss: 4.122 [-4.120, 0.002, -0.000]\n",
      "Epoch 3 [116/172] - Loss: 4.158 [-4.156, 0.002, -0.000]\n",
      "Epoch 3 [117/172] - Loss: 4.082 [-4.081, 0.002, -0.000]\n",
      "Epoch 3 [118/172] - Loss: 4.111 [-4.109, 0.002, -0.000]\n",
      "Epoch 3 [119/172] - Loss: 4.122 [-4.120, 0.002, -0.000]\n",
      "Epoch 3 [120/172] - Loss: 4.197 [-4.195, 0.002, -0.000]\n",
      "Epoch 3 [121/172] - Loss: 4.061 [-4.059, 0.002, -0.000]\n",
      "Epoch 3 [122/172] - Loss: 4.019 [-4.018, 0.002, -0.000]\n",
      "Epoch 3 [123/172] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 3 [124/172] - Loss: 4.041 [-4.039, 0.002, -0.000]\n",
      "Epoch 3 [125/172] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [126/172] - Loss: 4.126 [-4.124, 0.002, -0.000]\n",
      "Epoch 3 [127/172] - Loss: 4.041 [-4.040, 0.002, -0.000]\n",
      "Epoch 3 [128/172] - Loss: 4.093 [-4.091, 0.001, -0.000]\n",
      "Epoch 3 [129/172] - Loss: 4.033 [-4.031, 0.002, -0.000]\n",
      "Epoch 3 [130/172] - Loss: 4.102 [-4.100, 0.002, -0.000]\n",
      "Epoch 3 [131/172] - Loss: 4.103 [-4.101, 0.002, -0.000]\n",
      "Epoch 3 [132/172] - Loss: 4.107 [-4.106, 0.002, -0.000]\n",
      "Epoch 3 [133/172] - Loss: 4.111 [-4.109, 0.002, -0.000]\n",
      "Epoch 3 [134/172] - Loss: 4.092 [-4.090, 0.002, -0.000]\n",
      "Epoch 3 [135/172] - Loss: 4.086 [-4.085, 0.002, -0.000]\n",
      "Epoch 3 [136/172] - Loss: 4.052 [-4.050, 0.002, -0.000]\n",
      "Epoch 3 [137/172] - Loss: 4.098 [-4.096, 0.002, -0.000]\n",
      "Epoch 3 [138/172] - Loss: 4.040 [-4.039, 0.002, -0.000]\n",
      "Epoch 3 [139/172] - Loss: 4.043 [-4.041, 0.002, -0.000]\n",
      "Epoch 3 [140/172] - Loss: 4.044 [-4.042, 0.002, -0.000]\n",
      "Epoch 3 [141/172] - Loss: 4.048 [-4.047, 0.002, -0.000]\n",
      "Epoch 3 [142/172] - Loss: 4.144 [-4.142, 0.002, -0.000]\n",
      "Epoch 3 [143/172] - Loss: 4.072 [-4.071, 0.002, -0.000]\n",
      "Epoch 3 [144/172] - Loss: 4.067 [-4.066, 0.002, -0.000]\n",
      "Epoch 3 [145/172] - Loss: 4.106 [-4.105, 0.002, -0.000]\n",
      "Epoch 3 [146/172] - Loss: 4.140 [-4.138, 0.002, -0.000]\n",
      "Epoch 3 [147/172] - Loss: 4.094 [-4.092, 0.002, -0.000]\n",
      "Epoch 3 [148/172] - Loss: 4.093 [-4.091, 0.002, -0.000]\n",
      "Epoch 3 [149/172] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 3 [150/172] - Loss: 4.091 [-4.089, 0.002, -0.000]\n",
      "Epoch 3 [151/172] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 3 [152/172] - Loss: 4.102 [-4.100, 0.002, -0.000]\n",
      "Epoch 3 [153/172] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [154/172] - Loss: 4.107 [-4.105, 0.001, -0.000]\n",
      "Epoch 3 [155/172] - Loss: 4.084 [-4.083, 0.001, -0.000]\n",
      "Epoch 3 [156/172] - Loss: 4.131 [-4.129, 0.002, -0.000]\n",
      "Epoch 3 [157/172] - Loss: 4.128 [-4.126, 0.002, -0.000]\n",
      "Epoch 3 [158/172] - Loss: 4.102 [-4.101, 0.002, -0.000]\n",
      "Epoch 3 [159/172] - Loss: 4.095 [-4.093, 0.002, -0.000]\n",
      "Epoch 3 [160/172] - Loss: 4.073 [-4.072, 0.001, -0.000]\n",
      "Epoch 3 [161/172] - Loss: 4.050 [-4.049, 0.002, -0.000]\n",
      "Epoch 3 [162/172] - Loss: 4.079 [-4.077, 0.002, -0.000]\n",
      "Epoch 3 [163/172] - Loss: 4.030 [-4.028, 0.002, -0.000]\n",
      "Epoch 3 [164/172] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 3 [165/172] - Loss: 4.192 [-4.190, 0.002, -0.000]\n",
      "Epoch 3 [166/172] - Loss: 4.134 [-4.133, 0.002, -0.000]\n",
      "Epoch 3 [167/172] - Loss: 4.329 [-4.328, 0.001, -0.000]\n",
      "Epoch 3 [168/172] - Loss: 4.121 [-4.120, 0.002, -0.000]\n",
      "Epoch 3 [169/172] - Loss: 4.255 [-4.254, 0.001, -0.000]\n",
      "Epoch 3 [170/172] - Loss: 4.234 [-4.233, 0.002, -0.000]\n",
      "Epoch 3 [171/172] - Loss: 4.165 [-4.163, 0.002, -0.000]\n",
      "Epoch 4 [0/172] - Loss: 4.033 [-4.032, 0.001, -0.000]\n",
      "Epoch 4 [1/172] - Loss: 4.146 [-4.145, 0.002, -0.000]\n",
      "Epoch 4 [2/172] - Loss: 4.151 [-4.149, 0.002, -0.000]\n",
      "Epoch 4 [3/172] - Loss: 4.109 [-4.107, 0.002, -0.000]\n",
      "Epoch 4 [4/172] - Loss: 4.046 [-4.045, 0.002, -0.000]\n",
      "Epoch 4 [5/172] - Loss: 4.095 [-4.093, 0.002, -0.000]\n",
      "Epoch 4 [6/172] - Loss: 4.084 [-4.082, 0.001, -0.000]\n",
      "Epoch 4 [7/172] - Loss: 4.068 [-4.067, 0.002, -0.000]\n",
      "Epoch 4 [8/172] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 4 [9/172] - Loss: 4.039 [-4.037, 0.002, -0.000]\n",
      "Epoch 4 [10/172] - Loss: 4.029 [-4.028, 0.001, -0.000]\n",
      "Epoch 4 [11/172] - Loss: 4.069 [-4.068, 0.001, -0.000]\n",
      "Epoch 4 [12/172] - Loss: 4.084 [-4.083, 0.001, -0.000]\n",
      "Epoch 4 [13/172] - Loss: 4.072 [-4.070, 0.002, -0.000]\n",
      "Epoch 4 [14/172] - Loss: 4.116 [-4.115, 0.002, -0.000]\n",
      "Epoch 4 [15/172] - Loss: 4.061 [-4.059, 0.001, -0.000]\n",
      "Epoch 4 [16/172] - Loss: 4.059 [-4.058, 0.001, -0.000]\n",
      "Epoch 4 [17/172] - Loss: 4.131 [-4.129, 0.001, -0.000]\n",
      "Epoch 4 [18/172] - Loss: 4.091 [-4.089, 0.002, -0.000]\n",
      "Epoch 4 [19/172] - Loss: 4.121 [-4.119, 0.002, -0.000]\n",
      "Epoch 4 [20/172] - Loss: 4.105 [-4.104, 0.002, -0.000]\n",
      "Epoch 4 [21/172] - Loss: 4.043 [-4.041, 0.002, -0.000]\n",
      "Epoch 4 [22/172] - Loss: 4.033 [-4.031, 0.002, -0.000]\n",
      "Epoch 4 [23/172] - Loss: 4.058 [-4.056, 0.002, -0.000]\n",
      "Epoch 4 [24/172] - Loss: 4.096 [-4.095, 0.002, -0.000]\n",
      "Epoch 4 [25/172] - Loss: 4.040 [-4.039, 0.002, -0.000]\n",
      "Epoch 4 [26/172] - Loss: 4.046 [-4.045, 0.001, -0.000]\n",
      "Epoch 4 [27/172] - Loss: 4.046 [-4.045, 0.002, -0.000]\n",
      "Epoch 4 [28/172] - Loss: 3.985 [-3.984, 0.001, -0.000]\n",
      "Epoch 4 [29/172] - Loss: 4.050 [-4.048, 0.001, -0.000]\n",
      "Epoch 4 [30/172] - Loss: 4.013 [-4.012, 0.001, -0.000]\n",
      "Epoch 4 [31/172] - Loss: 4.067 [-4.065, 0.002, -0.000]\n",
      "Epoch 4 [32/172] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 4 [33/172] - Loss: 4.054 [-4.053, 0.002, -0.000]\n",
      "Epoch 4 [34/172] - Loss: 3.986 [-3.984, 0.001, -0.000]\n",
      "Epoch 4 [35/172] - Loss: 3.998 [-3.997, 0.002, -0.000]\n",
      "Epoch 4 [36/172] - Loss: 4.054 [-4.052, 0.001, -0.000]\n",
      "Epoch 4 [37/172] - Loss: 4.058 [-4.056, 0.002, -0.000]\n",
      "Epoch 4 [38/172] - Loss: 4.035 [-4.034, 0.001, -0.000]\n",
      "Epoch 4 [39/172] - Loss: 4.057 [-4.056, 0.002, -0.000]\n",
      "Epoch 4 [40/172] - Loss: 4.011 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [41/172] - Loss: 4.052 [-4.051, 0.002, -0.000]\n",
      "Epoch 4 [42/172] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 4 [43/172] - Loss: 3.999 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [44/172] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 4 [45/172] - Loss: 4.045 [-4.043, 0.002, -0.000]\n",
      "Epoch 4 [46/172] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 4 [47/172] - Loss: 4.054 [-4.052, 0.002, -0.000]\n",
      "Epoch 4 [48/172] - Loss: 4.083 [-4.081, 0.002, -0.000]\n",
      "Epoch 4 [49/172] - Loss: 4.054 [-4.052, 0.001, -0.000]\n",
      "Epoch 4 [50/172] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [51/172] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [52/172] - Loss: 3.963 [-3.961, 0.001, -0.000]\n",
      "Epoch 4 [53/172] - Loss: 4.023 [-4.021, 0.001, -0.000]\n",
      "Epoch 4 [54/172] - Loss: 4.030 [-4.029, 0.002, -0.000]\n",
      "Epoch 4 [55/172] - Loss: 4.061 [-4.059, 0.002, -0.000]\n",
      "Epoch 4 [56/172] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 4 [57/172] - Loss: 4.020 [-4.019, 0.002, -0.000]\n",
      "Epoch 4 [58/172] - Loss: 4.080 [-4.078, 0.001, -0.000]\n",
      "Epoch 4 [59/172] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 4 [60/172] - Loss: 4.002 [-4.000, 0.001, -0.000]\n",
      "Epoch 4 [61/172] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [62/172] - Loss: 4.022 [-4.021, 0.001, -0.000]\n",
      "Epoch 4 [63/172] - Loss: 4.011 [-4.009, 0.001, -0.000]\n",
      "Epoch 4 [64/172] - Loss: 4.042 [-4.041, 0.001, -0.000]\n",
      "Epoch 4 [65/172] - Loss: 4.030 [-4.028, 0.002, -0.000]\n",
      "Epoch 4 [66/172] - Loss: 4.041 [-4.040, 0.002, -0.000]\n",
      "Epoch 4 [67/172] - Loss: 4.015 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [68/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 4 [69/172] - Loss: 4.087 [-4.085, 0.001, -0.000]\n",
      "Epoch 4 [70/172] - Loss: 4.003 [-4.002, 0.001, -0.000]\n",
      "Epoch 4 [71/172] - Loss: 4.012 [-4.010, 0.001, -0.000]\n",
      "Epoch 4 [72/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 4 [73/172] - Loss: 4.031 [-4.030, 0.001, -0.000]\n",
      "Epoch 4 [74/172] - Loss: 4.066 [-4.064, 0.002, -0.000]\n",
      "Epoch 4 [75/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 4 [76/172] - Loss: 4.068 [-4.066, 0.002, -0.000]\n",
      "Epoch 4 [77/172] - Loss: 4.023 [-4.022, 0.002, -0.000]\n",
      "Epoch 4 [78/172] - Loss: 4.028 [-4.026, 0.001, -0.000]\n",
      "Epoch 4 [79/172] - Loss: 4.037 [-4.035, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [80/172] - Loss: 4.021 [-4.020, 0.002, -0.000]\n",
      "Epoch 4 [81/172] - Loss: 3.998 [-3.997, 0.002, -0.000]\n",
      "Epoch 4 [82/172] - Loss: 4.083 [-4.081, 0.002, -0.000]\n",
      "Epoch 4 [83/172] - Loss: 4.012 [-4.011, 0.001, -0.000]\n",
      "Epoch 4 [84/172] - Loss: 4.071 [-4.070, 0.001, -0.000]\n",
      "Epoch 4 [85/172] - Loss: 4.037 [-4.035, 0.001, -0.000]\n",
      "Epoch 4 [86/172] - Loss: 4.071 [-4.070, 0.001, -0.000]\n",
      "Epoch 4 [87/172] - Loss: 4.042 [-4.040, 0.002, -0.000]\n",
      "Epoch 4 [88/172] - Loss: 4.032 [-4.030, 0.001, -0.000]\n",
      "Epoch 4 [89/172] - Loss: 4.006 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [90/172] - Loss: 3.982 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [91/172] - Loss: 3.991 [-3.990, 0.001, -0.000]\n",
      "Epoch 4 [92/172] - Loss: 4.050 [-4.048, 0.001, -0.000]\n",
      "Epoch 4 [93/172] - Loss: 4.010 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [94/172] - Loss: 4.000 [-3.999, 0.002, -0.000]\n",
      "Epoch 4 [95/172] - Loss: 3.972 [-3.970, 0.001, -0.000]\n",
      "Epoch 4 [96/172] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [97/172] - Loss: 4.010 [-4.008, 0.001, -0.000]\n",
      "Epoch 4 [98/172] - Loss: 4.073 [-4.071, 0.002, -0.000]\n",
      "Epoch 4 [99/172] - Loss: 3.983 [-3.982, 0.001, -0.000]\n",
      "Epoch 4 [100/172] - Loss: 4.069 [-4.068, 0.001, -0.000]\n",
      "Epoch 4 [101/172] - Loss: 4.049 [-4.048, 0.001, -0.000]\n",
      "Epoch 4 [102/172] - Loss: 4.093 [-4.092, 0.002, -0.000]\n",
      "Epoch 4 [103/172] - Loss: 3.974 [-3.972, 0.001, -0.000]\n",
      "Epoch 4 [104/172] - Loss: 4.017 [-4.016, 0.002, -0.000]\n",
      "Epoch 4 [105/172] - Loss: 4.002 [-4.001, 0.001, -0.000]\n",
      "Epoch 4 [106/172] - Loss: 3.986 [-3.985, 0.002, -0.000]\n",
      "Epoch 4 [107/172] - Loss: 4.058 [-4.057, 0.001, -0.000]\n",
      "Epoch 4 [108/172] - Loss: 4.022 [-4.020, 0.002, -0.000]\n",
      "Epoch 4 [109/172] - Loss: 4.030 [-4.028, 0.001, -0.000]\n",
      "Epoch 4 [110/172] - Loss: 4.030 [-4.028, 0.001, -0.000]\n",
      "Epoch 4 [111/172] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [112/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 4 [113/172] - Loss: 4.044 [-4.043, 0.001, -0.000]\n",
      "Epoch 4 [114/172] - Loss: 4.014 [-4.013, 0.001, -0.000]\n",
      "Epoch 4 [115/172] - Loss: 4.031 [-4.030, 0.002, -0.000]\n",
      "Epoch 4 [116/172] - Loss: 3.977 [-3.975, 0.001, -0.000]\n",
      "Epoch 4 [117/172] - Loss: 4.067 [-4.066, 0.001, -0.000]\n",
      "Epoch 4 [118/172] - Loss: 4.031 [-4.029, 0.001, -0.000]\n",
      "Epoch 4 [119/172] - Loss: 3.987 [-3.986, 0.002, -0.000]\n",
      "Epoch 4 [120/172] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [121/172] - Loss: 4.041 [-4.039, 0.001, -0.000]\n",
      "Epoch 4 [122/172] - Loss: 4.032 [-4.031, 0.002, -0.000]\n",
      "Epoch 4 [123/172] - Loss: 4.030 [-4.028, 0.002, -0.000]\n",
      "Epoch 4 [124/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 4 [125/172] - Loss: 4.018 [-4.016, 0.001, -0.000]\n",
      "Epoch 4 [126/172] - Loss: 4.009 [-4.008, 0.002, -0.000]\n",
      "Epoch 4 [127/172] - Loss: 4.025 [-4.024, 0.001, -0.000]\n",
      "Epoch 4 [128/172] - Loss: 4.046 [-4.044, 0.002, -0.000]\n",
      "Epoch 4 [129/172] - Loss: 4.025 [-4.024, 0.002, -0.000]\n",
      "Epoch 4 [130/172] - Loss: 4.064 [-4.062, 0.001, -0.000]\n",
      "Epoch 4 [131/172] - Loss: 4.004 [-4.003, 0.002, -0.000]\n",
      "Epoch 4 [132/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 4 [133/172] - Loss: 4.059 [-4.058, 0.002, -0.000]\n",
      "Epoch 4 [134/172] - Loss: 4.064 [-4.063, 0.002, -0.000]\n",
      "Epoch 4 [135/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 4 [136/172] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 4 [137/172] - Loss: 4.028 [-4.027, 0.002, -0.000]\n",
      "Epoch 4 [138/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 4 [139/172] - Loss: 4.046 [-4.044, 0.002, -0.000]\n",
      "Epoch 4 [140/172] - Loss: 4.050 [-4.048, 0.001, -0.000]\n",
      "Epoch 4 [141/172] - Loss: 4.027 [-4.026, 0.002, -0.000]\n",
      "Epoch 4 [142/172] - Loss: 4.062 [-4.061, 0.002, -0.000]\n",
      "Epoch 4 [143/172] - Loss: 4.036 [-4.035, 0.001, -0.000]\n",
      "Epoch 4 [144/172] - Loss: 4.018 [-4.017, 0.002, -0.000]\n",
      "Epoch 4 [145/172] - Loss: 4.032 [-4.031, 0.001, -0.000]\n",
      "Epoch 4 [146/172] - Loss: 4.049 [-4.048, 0.002, -0.000]\n",
      "Epoch 4 [147/172] - Loss: 4.006 [-4.005, 0.001, -0.000]\n",
      "Epoch 4 [148/172] - Loss: 4.032 [-4.030, 0.001, -0.000]\n",
      "Epoch 4 [149/172] - Loss: 4.045 [-4.043, 0.002, -0.000]\n",
      "Epoch 4 [150/172] - Loss: 4.055 [-4.053, 0.002, -0.000]\n",
      "Epoch 4 [151/172] - Loss: 4.058 [-4.057, 0.002, -0.000]\n",
      "Epoch 4 [152/172] - Loss: 4.021 [-4.020, 0.002, -0.000]\n",
      "Epoch 4 [153/172] - Loss: 3.995 [-3.993, 0.001, -0.000]\n",
      "Epoch 4 [154/172] - Loss: 3.977 [-3.976, 0.001, -0.000]\n",
      "Epoch 4 [155/172] - Loss: 4.026 [-4.025, 0.001, -0.000]\n",
      "Epoch 4 [156/172] - Loss: 4.037 [-4.035, 0.001, -0.000]\n",
      "Epoch 4 [157/172] - Loss: 4.004 [-4.003, 0.001, -0.000]\n",
      "Epoch 4 [158/172] - Loss: 4.008 [-4.007, 0.002, -0.000]\n",
      "Epoch 4 [159/172] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [160/172] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 4 [161/172] - Loss: 4.031 [-4.029, 0.001, -0.000]\n",
      "Epoch 4 [162/172] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [163/172] - Loss: 4.016 [-4.014, 0.001, -0.000]\n",
      "Epoch 4 [164/172] - Loss: 4.073 [-4.072, 0.001, -0.000]\n",
      "Epoch 4 [165/172] - Loss: 4.017 [-4.016, 0.002, -0.000]\n",
      "Epoch 4 [166/172] - Loss: 4.054 [-4.052, 0.001, -0.000]\n",
      "Epoch 4 [167/172] - Loss: 4.024 [-4.023, 0.001, -0.000]\n",
      "Epoch 4 [168/172] - Loss: 4.029 [-4.028, 0.002, -0.000]\n",
      "Epoch 4 [169/172] - Loss: 3.969 [-3.968, 0.001, -0.000]\n",
      "Epoch 4 [170/172] - Loss: 3.998 [-3.996, 0.001, -0.000]\n",
      "Epoch 4 [171/172] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 5 [0/172] - Loss: 4.007 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [1/172] - Loss: 4.014 [-4.013, 0.001, -0.000]\n",
      "Epoch 5 [2/172] - Loss: 4.019 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [3/172] - Loss: 3.970 [-3.969, 0.001, -0.000]\n",
      "Epoch 5 [4/172] - Loss: 3.976 [-3.975, 0.001, -0.000]\n",
      "Epoch 5 [5/172] - Loss: 3.993 [-3.991, 0.001, -0.000]\n",
      "Epoch 5 [6/172] - Loss: 4.014 [-4.013, 0.001, -0.000]\n",
      "Epoch 5 [7/172] - Loss: 4.021 [-4.019, 0.001, -0.000]\n",
      "Epoch 5 [8/172] - Loss: 4.037 [-4.035, 0.001, -0.000]\n",
      "Epoch 5 [9/172] - Loss: 4.050 [-4.049, 0.002, -0.000]\n",
      "Epoch 5 [10/172] - Loss: 3.980 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [11/172] - Loss: 4.053 [-4.052, 0.001, -0.000]\n",
      "Epoch 5 [12/172] - Loss: 3.930 [-3.928, 0.001, -0.000]\n",
      "Epoch 5 [13/172] - Loss: 4.018 [-4.016, 0.001, -0.000]\n",
      "Epoch 5 [14/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 5 [15/172] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 5 [16/172] - Loss: 3.960 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [17/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 5 [18/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 5 [19/172] - Loss: 4.003 [-4.002, 0.002, -0.000]\n",
      "Epoch 5 [20/172] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 5 [21/172] - Loss: 4.002 [-4.000, 0.001, -0.000]\n",
      "Epoch 5 [22/172] - Loss: 4.018 [-4.016, 0.001, -0.000]\n",
      "Epoch 5 [23/172] - Loss: 3.967 [-3.966, 0.001, -0.000]\n",
      "Epoch 5 [24/172] - Loss: 4.021 [-4.019, 0.001, -0.000]\n",
      "Epoch 5 [25/172] - Loss: 3.982 [-3.980, 0.001, -0.000]\n",
      "Epoch 5 [26/172] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [27/172] - Loss: 4.035 [-4.034, 0.001, -0.000]\n",
      "Epoch 5 [28/172] - Loss: 4.017 [-4.015, 0.001, -0.000]\n",
      "Epoch 5 [29/172] - Loss: 3.987 [-3.986, 0.001, -0.000]\n",
      "Epoch 5 [30/172] - Loss: 4.042 [-4.040, 0.001, -0.000]\n",
      "Epoch 5 [31/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 5 [32/172] - Loss: 4.076 [-4.075, 0.001, -0.000]\n",
      "Epoch 5 [33/172] - Loss: 3.968 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [34/172] - Loss: 3.996 [-3.995, 0.001, -0.000]\n",
      "Epoch 5 [35/172] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 5 [36/172] - Loss: 3.983 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [37/172] - Loss: 4.015 [-4.014, 0.002, -0.000]\n",
      "Epoch 5 [38/172] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [39/172] - Loss: 4.035 [-4.034, 0.002, -0.000]\n",
      "Epoch 5 [40/172] - Loss: 4.033 [-4.032, 0.001, -0.000]\n",
      "Epoch 5 [41/172] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [42/172] - Loss: 3.979 [-3.977, 0.001, -0.000]\n",
      "Epoch 5 [43/172] - Loss: 4.027 [-4.025, 0.002, -0.000]\n",
      "Epoch 5 [44/172] - Loss: 4.025 [-4.023, 0.001, -0.000]\n",
      "Epoch 5 [45/172] - Loss: 4.028 [-4.027, 0.001, -0.000]\n",
      "Epoch 5 [46/172] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 5 [47/172] - Loss: 4.011 [-4.009, 0.001, -0.000]\n",
      "Epoch 5 [48/172] - Loss: 3.995 [-3.993, 0.001, -0.000]\n",
      "Epoch 5 [49/172] - Loss: 3.978 [-3.976, 0.001, -0.000]\n",
      "Epoch 5 [50/172] - Loss: 4.058 [-4.057, 0.002, -0.000]\n",
      "Epoch 5 [51/172] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 5 [52/172] - Loss: 4.062 [-4.060, 0.002, -0.000]\n",
      "Epoch 5 [53/172] - Loss: 4.003 [-4.002, 0.002, -0.000]\n",
      "Epoch 5 [54/172] - Loss: 3.960 [-3.959, 0.001, -0.000]\n",
      "Epoch 5 [55/172] - Loss: 3.966 [-3.965, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [56/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 5 [57/172] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [58/172] - Loss: 4.055 [-4.054, 0.002, -0.000]\n",
      "Epoch 5 [59/172] - Loss: 3.995 [-3.994, 0.002, -0.000]\n",
      "Epoch 5 [60/172] - Loss: 3.968 [-3.966, 0.001, -0.000]\n",
      "Epoch 5 [61/172] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 5 [62/172] - Loss: 4.088 [-4.086, 0.002, -0.000]\n",
      "Epoch 5 [63/172] - Loss: 4.001 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [64/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [65/172] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [66/172] - Loss: 4.010 [-4.009, 0.001, -0.000]\n",
      "Epoch 5 [67/172] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [68/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 5 [69/172] - Loss: 3.976 [-3.975, 0.001, -0.000]\n",
      "Epoch 5 [70/172] - Loss: 4.027 [-4.025, 0.002, -0.000]\n",
      "Epoch 5 [71/172] - Loss: 3.975 [-3.974, 0.002, -0.000]\n",
      "Epoch 5 [72/172] - Loss: 3.989 [-3.988, 0.001, -0.000]\n",
      "Epoch 5 [73/172] - Loss: 4.065 [-4.064, 0.002, -0.000]\n",
      "Epoch 5 [74/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [75/172] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 5 [76/172] - Loss: 3.983 [-3.982, 0.001, -0.000]\n",
      "Epoch 5 [77/172] - Loss: 3.969 [-3.967, 0.001, -0.000]\n",
      "Epoch 5 [78/172] - Loss: 3.986 [-3.985, 0.001, -0.000]\n",
      "Epoch 5 [79/172] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [80/172] - Loss: 3.948 [-3.946, 0.001, -0.000]\n",
      "Epoch 5 [81/172] - Loss: 4.023 [-4.022, 0.002, -0.000]\n",
      "Epoch 5 [82/172] - Loss: 4.008 [-4.006, 0.001, -0.000]\n",
      "Epoch 5 [83/172] - Loss: 4.053 [-4.052, 0.001, -0.000]\n",
      "Epoch 5 [84/172] - Loss: 3.994 [-3.993, 0.001, -0.000]\n",
      "Epoch 5 [85/172] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [86/172] - Loss: 4.026 [-4.025, 0.001, -0.000]\n",
      "Epoch 5 [87/172] - Loss: 4.046 [-4.044, 0.002, -0.000]\n",
      "Epoch 5 [88/172] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 5 [89/172] - Loss: 4.013 [-4.012, 0.001, -0.000]\n",
      "Epoch 5 [90/172] - Loss: 3.939 [-3.937, 0.001, -0.000]\n",
      "Epoch 5 [91/172] - Loss: 3.989 [-3.987, 0.001, -0.000]\n",
      "Epoch 5 [92/172] - Loss: 3.992 [-3.990, 0.001, -0.000]\n",
      "Epoch 5 [93/172] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [94/172] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [95/172] - Loss: 4.012 [-4.011, 0.001, -0.000]\n",
      "Epoch 5 [96/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 5 [97/172] - Loss: 4.018 [-4.017, 0.001, -0.000]\n",
      "Epoch 5 [98/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 5 [99/172] - Loss: 4.030 [-4.028, 0.002, -0.000]\n",
      "Epoch 5 [100/172] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [101/172] - Loss: 3.944 [-3.943, 0.001, -0.000]\n",
      "Epoch 5 [102/172] - Loss: 3.983 [-3.982, 0.001, -0.000]\n",
      "Epoch 5 [103/172] - Loss: 4.099 [-4.098, 0.002, -0.000]\n",
      "Epoch 5 [104/172] - Loss: 3.975 [-3.973, 0.001, -0.000]\n",
      "Epoch 5 [105/172] - Loss: 3.966 [-3.964, 0.001, -0.000]\n",
      "Epoch 5 [106/172] - Loss: 4.024 [-4.023, 0.002, -0.000]\n",
      "Epoch 5 [107/172] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [108/172] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [109/172] - Loss: 4.015 [-4.013, 0.001, -0.000]\n",
      "Epoch 5 [110/172] - Loss: 4.019 [-4.018, 0.001, -0.000]\n",
      "Epoch 5 [111/172] - Loss: 3.987 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [112/172] - Loss: 3.978 [-3.976, 0.001, -0.000]\n",
      "Epoch 5 [113/172] - Loss: 4.001 [-3.999, 0.001, -0.000]\n",
      "Epoch 5 [114/172] - Loss: 3.993 [-3.992, 0.001, -0.000]\n",
      "Epoch 5 [115/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [116/172] - Loss: 4.046 [-4.044, 0.002, -0.000]\n",
      "Epoch 5 [117/172] - Loss: 4.054 [-4.053, 0.002, -0.000]\n",
      "Epoch 5 [118/172] - Loss: 4.015 [-4.014, 0.002, -0.000]\n",
      "Epoch 5 [119/172] - Loss: 3.985 [-3.984, 0.001, -0.000]\n",
      "Epoch 5 [120/172] - Loss: 4.017 [-4.016, 0.001, -0.000]\n",
      "Epoch 5 [121/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 5 [122/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 5 [123/172] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [124/172] - Loss: 3.970 [-3.968, 0.001, -0.000]\n",
      "Epoch 5 [125/172] - Loss: 4.015 [-4.014, 0.002, -0.000]\n",
      "Epoch 5 [126/172] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 5 [127/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 5 [128/172] - Loss: 3.984 [-3.983, 0.001, -0.000]\n",
      "Epoch 5 [129/172] - Loss: 4.012 [-4.010, 0.001, -0.000]\n",
      "Epoch 5 [130/172] - Loss: 4.001 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [131/172] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [132/172] - Loss: 3.987 [-3.985, 0.001, -0.000]\n",
      "Epoch 5 [133/172] - Loss: 4.056 [-4.055, 0.001, -0.000]\n",
      "Epoch 5 [134/172] - Loss: 4.003 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [135/172] - Loss: 3.993 [-3.992, 0.001, -0.000]\n",
      "Epoch 5 [136/172] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [137/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 5 [138/172] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [139/172] - Loss: 3.997 [-3.996, 0.001, -0.000]\n",
      "Epoch 5 [140/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 5 [141/172] - Loss: 3.967 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [142/172] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 5 [143/172] - Loss: 3.972 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [144/172] - Loss: 3.945 [-3.944, 0.001, -0.000]\n",
      "Epoch 5 [145/172] - Loss: 4.006 [-4.004, 0.001, -0.000]\n",
      "Epoch 5 [146/172] - Loss: 3.972 [-3.970, 0.001, -0.000]\n",
      "Epoch 5 [147/172] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [148/172] - Loss: 3.954 [-3.953, 0.001, -0.000]\n",
      "Epoch 5 [149/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 5 [150/172] - Loss: 4.044 [-4.043, 0.001, -0.000]\n",
      "Epoch 5 [151/172] - Loss: 3.966 [-3.965, 0.001, -0.000]\n",
      "Epoch 5 [152/172] - Loss: 4.027 [-4.026, 0.001, -0.000]\n",
      "Epoch 5 [153/172] - Loss: 4.003 [-4.002, 0.002, -0.000]\n",
      "Epoch 5 [154/172] - Loss: 4.028 [-4.027, 0.001, -0.000]\n",
      "Epoch 5 [155/172] - Loss: 4.003 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [156/172] - Loss: 4.024 [-4.023, 0.001, -0.000]\n",
      "Epoch 5 [157/172] - Loss: 3.980 [-3.978, 0.001, -0.000]\n",
      "Epoch 5 [158/172] - Loss: 3.987 [-3.986, 0.001, -0.000]\n",
      "Epoch 5 [159/172] - Loss: 3.986 [-3.984, 0.001, -0.000]\n",
      "Epoch 5 [160/172] - Loss: 4.009 [-4.008, 0.002, -0.000]\n",
      "Epoch 5 [161/172] - Loss: 4.003 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [162/172] - Loss: 3.996 [-3.995, 0.002, -0.000]\n",
      "Epoch 5 [163/172] - Loss: 3.999 [-3.998, 0.001, -0.000]\n",
      "Epoch 5 [164/172] - Loss: 4.043 [-4.041, 0.001, -0.000]\n",
      "Epoch 5 [165/172] - Loss: 3.981 [-3.980, 0.001, -0.000]\n",
      "Epoch 5 [166/172] - Loss: 3.993 [-3.991, 0.001, -0.000]\n",
      "Epoch 5 [167/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 5 [168/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 5 [169/172] - Loss: 4.043 [-4.041, 0.001, -0.000]\n",
      "Epoch 5 [170/172] - Loss: 4.006 [-4.005, 0.001, -0.000]\n",
      "Epoch 5 [171/172] - Loss: 3.970 [-3.968, 0.001, -0.000]\n",
      "Epoch 6 [0/172] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 6 [1/172] - Loss: 3.940 [-3.938, 0.001, -0.000]\n",
      "Epoch 6 [2/172] - Loss: 4.043 [-4.042, 0.001, -0.000]\n",
      "Epoch 6 [3/172] - Loss: 3.986 [-3.984, 0.001, -0.000]\n",
      "Epoch 6 [4/172] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [5/172] - Loss: 3.973 [-3.972, 0.001, -0.000]\n",
      "Epoch 6 [6/172] - Loss: 3.946 [-3.945, 0.001, -0.000]\n",
      "Epoch 6 [7/172] - Loss: 3.967 [-3.966, 0.001, -0.000]\n",
      "Epoch 6 [8/172] - Loss: 4.052 [-4.050, 0.001, -0.000]\n",
      "Epoch 6 [9/172] - Loss: 3.985 [-3.983, 0.001, -0.000]\n",
      "Epoch 6 [10/172] - Loss: 4.001 [-4.000, 0.001, -0.000]\n",
      "Epoch 6 [11/172] - Loss: 3.948 [-3.946, 0.001, -0.000]\n",
      "Epoch 6 [12/172] - Loss: 4.014 [-4.012, 0.001, -0.000]\n",
      "Epoch 6 [13/172] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 6 [14/172] - Loss: 3.970 [-3.968, 0.001, -0.000]\n",
      "Epoch 6 [15/172] - Loss: 4.024 [-4.023, 0.001, -0.000]\n",
      "Epoch 6 [16/172] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 6 [17/172] - Loss: 3.948 [-3.947, 0.002, -0.000]\n",
      "Epoch 6 [18/172] - Loss: 4.009 [-4.008, 0.001, -0.000]\n",
      "Epoch 6 [19/172] - Loss: 4.021 [-4.020, 0.002, -0.000]\n",
      "Epoch 6 [20/172] - Loss: 3.985 [-3.984, 0.001, -0.000]\n",
      "Epoch 6 [21/172] - Loss: 3.967 [-3.966, 0.001, -0.000]\n",
      "Epoch 6 [22/172] - Loss: 4.000 [-3.999, 0.002, -0.000]\n",
      "Epoch 6 [23/172] - Loss: 3.997 [-3.996, 0.001, -0.000]\n",
      "Epoch 6 [24/172] - Loss: 3.998 [-3.996, 0.001, -0.000]\n",
      "Epoch 6 [25/172] - Loss: 3.964 [-3.963, 0.001, -0.000]\n",
      "Epoch 6 [26/172] - Loss: 3.954 [-3.953, 0.001, -0.000]\n",
      "Epoch 6 [27/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 6 [28/172] - Loss: 3.952 [-3.951, 0.001, -0.000]\n",
      "Epoch 6 [29/172] - Loss: 3.997 [-3.996, 0.001, -0.000]\n",
      "Epoch 6 [30/172] - Loss: 3.983 [-3.982, 0.001, -0.000]\n",
      "Epoch 6 [31/172] - Loss: 3.955 [-3.954, 0.001, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [32/172] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 6 [33/172] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 6 [34/172] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 6 [35/172] - Loss: 3.996 [-3.995, 0.002, -0.000]\n",
      "Epoch 6 [36/172] - Loss: 4.005 [-4.004, 0.001, -0.000]\n",
      "Epoch 6 [37/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 6 [38/172] - Loss: 3.923 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [39/172] - Loss: 3.946 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [40/172] - Loss: 3.976 [-3.974, 0.001, -0.000]\n",
      "Epoch 6 [41/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 6 [42/172] - Loss: 4.009 [-4.007, 0.001, -0.000]\n",
      "Epoch 6 [43/172] - Loss: 3.993 [-3.992, 0.002, -0.000]\n",
      "Epoch 6 [44/172] - Loss: 3.978 [-3.976, 0.001, -0.000]\n",
      "Epoch 6 [45/172] - Loss: 3.956 [-3.955, 0.001, -0.000]\n",
      "Epoch 6 [46/172] - Loss: 3.937 [-3.935, 0.001, -0.000]\n",
      "Epoch 6 [47/172] - Loss: 3.990 [-3.989, 0.001, -0.000]\n",
      "Epoch 6 [48/172] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 6 [49/172] - Loss: 3.997 [-3.996, 0.002, -0.000]\n",
      "Epoch 6 [50/172] - Loss: 3.932 [-3.930, 0.001, -0.000]\n",
      "Epoch 6 [51/172] - Loss: 3.980 [-3.979, 0.001, -0.000]\n",
      "Epoch 6 [52/172] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 6 [53/172] - Loss: 3.957 [-3.955, 0.001, -0.000]\n",
      "Epoch 6 [54/172] - Loss: 4.011 [-4.010, 0.001, -0.000]\n",
      "Epoch 6 [55/172] - Loss: 4.000 [-3.999, 0.001, -0.000]\n",
      "Epoch 6 [56/172] - Loss: 4.011 [-4.010, 0.002, -0.000]\n",
      "Epoch 6 [57/172] - Loss: 3.960 [-3.959, 0.002, -0.000]\n",
      "Epoch 6 [58/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 6 [59/172] - Loss: 3.964 [-3.963, 0.001, -0.000]\n",
      "Epoch 6 [60/172] - Loss: 4.001 [-4.000, 0.001, -0.000]\n",
      "Epoch 6 [61/172] - Loss: 3.962 [-3.960, 0.001, -0.000]\n",
      "Epoch 6 [62/172] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 6 [63/172] - Loss: 3.976 [-3.974, 0.001, -0.000]\n",
      "Epoch 6 [64/172] - Loss: 3.933 [-3.931, 0.001, -0.000]\n",
      "Epoch 6 [65/172] - Loss: 3.965 [-3.964, 0.001, -0.000]\n",
      "Epoch 6 [66/172] - Loss: 4.007 [-4.005, 0.001, -0.000]\n",
      "Epoch 6 [67/172] - Loss: 4.015 [-4.014, 0.001, -0.000]\n",
      "Epoch 6 [68/172] - Loss: 3.952 [-3.951, 0.001, -0.000]\n",
      "Epoch 6 [69/172] - Loss: 3.989 [-3.988, 0.001, -0.000]\n",
      "Epoch 6 [70/172] - Loss: 3.993 [-3.991, 0.001, -0.000]\n",
      "Epoch 6 [71/172] - Loss: 4.051 [-4.050, 0.001, -0.000]\n",
      "Epoch 6 [72/172] - Loss: 3.977 [-3.976, 0.002, -0.000]\n",
      "Epoch 6 [73/172] - Loss: 3.964 [-3.963, 0.001, -0.000]\n",
      "Epoch 6 [74/172] - Loss: 4.044 [-4.043, 0.002, -0.000]\n",
      "Epoch 6 [75/172] - Loss: 4.004 [-4.002, 0.001, -0.000]\n",
      "Epoch 6 [76/172] - Loss: 3.967 [-3.966, 0.001, -0.000]\n",
      "Epoch 6 [77/172] - Loss: 3.937 [-3.936, 0.001, -0.000]\n",
      "Epoch 6 [78/172] - Loss: 3.959 [-3.957, 0.001, -0.000]\n",
      "Epoch 6 [79/172] - Loss: 3.943 [-3.941, 0.001, -0.000]\n",
      "Epoch 6 [80/172] - Loss: 3.976 [-3.974, 0.001, -0.000]\n",
      "Epoch 6 [81/172] - Loss: 3.983 [-3.981, 0.001, -0.000]\n",
      "Epoch 6 [82/172] - Loss: 3.998 [-3.997, 0.001, -0.000]\n",
      "Epoch 6 [83/172] - Loss: 3.964 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [84/172] - Loss: 4.004 [-4.002, 0.002, -0.000]\n",
      "Epoch 6 [85/172] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 6 [86/172] - Loss: 3.991 [-3.989, 0.001, -0.000]\n",
      "Epoch 6 [87/172] - Loss: 3.990 [-3.988, 0.001, -0.000]\n",
      "Epoch 6 [88/172] - Loss: 3.902 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [89/172] - Loss: 3.934 [-3.932, 0.001, -0.000]\n",
      "Epoch 6 [90/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 6 [91/172] - Loss: 3.994 [-3.993, 0.002, -0.000]\n",
      "Epoch 6 [92/172] - Loss: 4.094 [-4.093, 0.001, -0.000]\n",
      "Epoch 6 [93/172] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 6 [94/172] - Loss: 3.995 [-3.993, 0.001, -0.000]\n",
      "Epoch 6 [95/172] - Loss: 3.984 [-3.983, 0.001, -0.000]\n",
      "Epoch 6 [96/172] - Loss: 3.959 [-3.957, 0.001, -0.000]\n",
      "Epoch 6 [97/172] - Loss: 3.961 [-3.959, 0.001, -0.000]\n",
      "Epoch 6 [98/172] - Loss: 3.973 [-3.971, 0.001, -0.000]\n",
      "Epoch 6 [99/172] - Loss: 3.979 [-3.978, 0.001, -0.000]\n",
      "Epoch 6 [100/172] - Loss: 4.002 [-4.001, 0.001, -0.000]\n",
      "Epoch 6 [101/172] - Loss: 3.966 [-3.965, 0.002, -0.000]\n",
      "Epoch 6 [102/172] - Loss: 3.959 [-3.957, 0.001, -0.000]\n",
      "Epoch 6 [103/172] - Loss: 3.954 [-3.952, 0.001, -0.000]\n",
      "Epoch 6 [104/172] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 6 [105/172] - Loss: 3.987 [-3.986, 0.001, -0.000]\n",
      "Epoch 6 [106/172] - Loss: 4.010 [-4.009, 0.002, -0.000]\n",
      "Epoch 6 [107/172] - Loss: 3.915 [-3.914, 0.001, -0.000]\n",
      "Epoch 6 [108/172] - Loss: 3.957 [-3.955, 0.001, -0.000]\n",
      "Epoch 6 [109/172] - Loss: 4.007 [-4.006, 0.001, -0.000]\n",
      "Epoch 6 [110/172] - Loss: 3.939 [-3.938, 0.002, -0.000]\n",
      "Epoch 6 [111/172] - Loss: 3.972 [-3.971, 0.001, -0.000]\n",
      "Epoch 6 [112/172] - Loss: 4.017 [-4.016, 0.001, -0.000]\n",
      "Epoch 6 [113/172] - Loss: 3.972 [-3.971, 0.002, -0.000]\n",
      "Epoch 6 [114/172] - Loss: 3.959 [-3.958, 0.001, -0.000]\n",
      "Epoch 6 [115/172] - Loss: 4.004 [-4.002, 0.002, -0.000]\n",
      "Epoch 6 [116/172] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [117/172] - Loss: 3.946 [-3.944, 0.001, -0.000]\n",
      "Epoch 6 [118/172] - Loss: 3.966 [-3.964, 0.001, -0.000]\n",
      "Epoch 6 [119/172] - Loss: 3.993 [-3.992, 0.001, -0.000]\n",
      "Epoch 6 [120/172] - Loss: 3.982 [-3.981, 0.001, -0.000]\n",
      "Epoch 6 [121/172] - Loss: 3.934 [-3.932, 0.001, -0.000]\n",
      "Epoch 6 [122/172] - Loss: 3.958 [-3.957, 0.001, -0.000]\n",
      "Epoch 6 [123/172] - Loss: 3.969 [-3.968, 0.001, -0.000]\n",
      "Epoch 6 [124/172] - Loss: 3.943 [-3.942, 0.001, -0.000]\n",
      "Epoch 6 [125/172] - Loss: 3.973 [-3.971, 0.001, -0.000]\n",
      "Epoch 6 [126/172] - Loss: 3.981 [-3.980, 0.001, -0.000]\n",
      "Epoch 6 [127/172] - Loss: 3.983 [-3.981, 0.001, -0.000]\n",
      "Epoch 6 [128/172] - Loss: 3.994 [-3.993, 0.001, -0.000]\n",
      "Epoch 6 [129/172] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 6 [130/172] - Loss: 3.960 [-3.958, 0.001, -0.000]\n",
      "Epoch 6 [131/172] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 6 [132/172] - Loss: 3.998 [-3.996, 0.001, -0.000]\n",
      "Epoch 6 [133/172] - Loss: 3.972 [-3.970, 0.001, -0.000]\n",
      "Epoch 6 [134/172] - Loss: 4.090 [-4.089, 0.001, -0.000]\n",
      "Epoch 6 [135/172] - Loss: 3.951 [-3.950, 0.001, -0.000]\n",
      "Epoch 6 [136/172] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 6 [137/172] - Loss: 3.998 [-3.996, 0.001, -0.000]\n",
      "Epoch 6 [138/172] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 6 [139/172] - Loss: 3.983 [-3.982, 0.002, -0.000]\n",
      "Epoch 6 [140/172] - Loss: 4.022 [-4.020, 0.001, -0.000]\n",
      "Epoch 6 [141/172] - Loss: 4.012 [-4.011, 0.001, -0.000]\n",
      "Epoch 6 [142/172] - Loss: 3.998 [-3.996, 0.001, -0.000]\n",
      "Epoch 6 [143/172] - Loss: 4.022 [-4.020, 0.002, -0.000]\n",
      "Epoch 6 [144/172] - Loss: 4.031 [-4.029, 0.001, -0.000]\n",
      "Epoch 6 [145/172] - Loss: 4.006 [-4.004, 0.001, -0.000]\n",
      "Epoch 6 [146/172] - Loss: 3.995 [-3.994, 0.001, -0.000]\n",
      "Epoch 6 [147/172] - Loss: 3.953 [-3.952, 0.001, -0.000]\n",
      "Epoch 6 [148/172] - Loss: 3.995 [-3.994, 0.002, -0.000]\n",
      "Epoch 6 [149/172] - Loss: 3.977 [-3.976, 0.002, -0.000]\n",
      "Epoch 6 [150/172] - Loss: 3.961 [-3.960, 0.001, -0.000]\n",
      "Epoch 6 [151/172] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 6 [152/172] - Loss: 3.974 [-3.973, 0.001, -0.000]\n",
      "Epoch 6 [153/172] - Loss: 3.992 [-3.991, 0.001, -0.000]\n",
      "Epoch 6 [154/172] - Loss: 4.042 [-4.040, 0.002, -0.000]\n",
      "Epoch 6 [155/172] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 6 [156/172] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 6 [157/172] - Loss: 4.022 [-4.021, 0.001, -0.000]\n",
      "Epoch 6 [158/172] - Loss: 3.993 [-3.992, 0.002, -0.000]\n",
      "Epoch 6 [159/172] - Loss: 3.959 [-3.957, 0.001, -0.000]\n",
      "Epoch 6 [160/172] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [161/172] - Loss: 3.977 [-3.976, 0.001, -0.000]\n",
      "Epoch 6 [162/172] - Loss: 4.058 [-4.056, 0.002, -0.000]\n",
      "Epoch 6 [163/172] - Loss: 3.941 [-3.939, 0.001, -0.000]\n",
      "Epoch 6 [164/172] - Loss: 3.955 [-3.954, 0.001, -0.000]\n",
      "Epoch 6 [165/172] - Loss: 3.953 [-3.952, 0.001, -0.000]\n",
      "Epoch 6 [166/172] - Loss: 3.935 [-3.934, 0.001, -0.000]\n",
      "Epoch 6 [167/172] - Loss: 3.972 [-3.970, 0.001, -0.000]\n",
      "Epoch 6 [168/172] - Loss: 4.029 [-4.028, 0.001, -0.000]\n",
      "Epoch 6 [169/172] - Loss: 3.984 [-3.982, 0.001, -0.000]\n",
      "Epoch 6 [170/172] - Loss: 4.041 [-4.039, 0.002, -0.000]\n",
      "Epoch 6 [171/172] - Loss: 3.955 [-3.954, 0.001, -0.000]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 1 epochs of training in this tutorial\n",
    "num_epochs = 6\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Because the grid is relatively small, we turn off the Toeplitz matrix multiplication and just perform them directly\n",
    "        # We find this to be more efficient when the grid is very small.\n",
    "        with gpytorch.settings.use_toeplitz(False):\n",
    "            output = model(x_batch)\n",
    "            log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "            loss = -(log_lik - kl_div + log_prior)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader), loss.item(), log_lik.item(), kl_div.item(), log_prior.item()))\n",
    "\n",
    "        # The actual optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`) using the standard SKI testing code, with no acceleration or precomputation. Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although our other tutorials demonstrate how to do this (for example, see the CIFAR tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 9.38260269165039\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
