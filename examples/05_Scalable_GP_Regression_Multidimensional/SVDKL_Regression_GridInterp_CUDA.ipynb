{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA) (w/ KISS-GP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use Deep Kernel Learning with SKI stochastic variational regression to rapidly train using minibatches on the `song` UCI dataset, which has over 500,000 training examples in 90 dimensions. \n",
    "\n",
    "Stochastic variational inference has several major advantages over the standard regression setting. Most notably, the ELBO used for optimization decomposes in such a way that stochastic gradient descent techniques can be used. See https://arxiv.org/pdf/1411.2005.pdf and https://arxiv.org/pdf/1611.00336.pdf for more technical details of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `song` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~340 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('song.mat'):\n",
    "    print('Downloading \\'song\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/mg91x4c0muatanp/song.mat?dl=1', 'song.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('song.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Feature Extractor\n",
    "\n",
    "Next, we define the neural network feature extractor used to define the deep kernel. In this case, we use a fully connected network with the architecture `d -> 1000 -> 500 -> 50 -> 2`, as described in the original DKL paper. All of the code below uses standard PyTorch implementations of neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())                  \n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))     \n",
    "        self.add_module('relu2', torch.nn.ReLU())                  \n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Regression Layer\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. To use grid interpolation for variational inference, we'll be using a `GridInterpolationVariationalStrategy`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions.\n",
    "\n",
    "See the CIFAR example for using an `AbstractVariationalGP` with an `AdditiveGridInterpolationVariationalStrategy`, which additionally assumes the kernel decomposes additively, which is a strong modelling assumption but allows us to use many more output features from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, GridInterpolationVariationalStrategy\n",
    "class GPRegressionLayer(AbstractVariationalGP):\n",
    "    def __init__(self, grid_size=20, grid_bounds=[(-1, 1), (-1, 1)]):\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=20*20)\n",
    "        variational_strategy = GridInterpolationVariationalStrategy(self,\n",
    "                                                                    grid_size=grid_size,\n",
    "                                                                    grid_bounds=grid_bounds,\n",
    "                                                                    variational_distribution=variational_distribution)\n",
    "        super(GPRegressionLayer, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "            log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1., sigma=0.1, log_transform=True)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Model\n",
    "\n",
    "With the feature extractor and GP regression layer defined, we can now define our full model. To do this, we simply create a module whose `forward()` method passes the data first through the feature extractor, and then through the GP regression layer.\n",
    "\n",
    "The only other interesting feature of the model below is that we use a helper function, `scale_to_bounds`, to ensure that the features extracted by the neural network fit within the grid bounds used for SKI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_features, grid_bounds=(-1., 1.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GPRegressionLayer()\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "\n",
    "model = DKLModel(feature_extractor, num_features=num_features).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the DKL model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalELBO`), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/403] - Loss: 0.878\n",
      "Epoch 1 [1/403] - Loss: 2.728\n",
      "Epoch 1 [2/403] - Loss: 1.097\n",
      "Epoch 1 [3/403] - Loss: 1.037\n",
      "Epoch 1 [4/403] - Loss: 1.128\n",
      "Epoch 1 [5/403] - Loss: 1.106\n",
      "Epoch 1 [6/403] - Loss: 0.977\n",
      "Epoch 1 [7/403] - Loss: 0.973\n",
      "Epoch 1 [8/403] - Loss: 0.985\n",
      "Epoch 1 [9/403] - Loss: 0.958\n",
      "Epoch 1 [10/403] - Loss: 0.973\n",
      "Epoch 1 [11/403] - Loss: 0.996\n",
      "Epoch 1 [12/403] - Loss: 0.999\n",
      "Epoch 1 [13/403] - Loss: 0.987\n",
      "Epoch 1 [14/403] - Loss: 1.052\n",
      "Epoch 1 [15/403] - Loss: 1.002\n",
      "Epoch 1 [16/403] - Loss: 1.004\n",
      "Epoch 1 [17/403] - Loss: 0.947\n",
      "Epoch 1 [18/403] - Loss: 0.944\n",
      "Epoch 1 [19/403] - Loss: 0.925\n",
      "Epoch 1 [20/403] - Loss: 1.014\n",
      "Epoch 1 [21/403] - Loss: 0.939\n",
      "Epoch 1 [22/403] - Loss: 0.932\n",
      "Epoch 1 [23/403] - Loss: 0.924\n",
      "Epoch 1 [24/403] - Loss: 0.923\n",
      "Epoch 1 [25/403] - Loss: 0.918\n",
      "Epoch 1 [26/403] - Loss: 0.914\n",
      "Epoch 1 [27/403] - Loss: 0.893\n",
      "Epoch 1 [28/403] - Loss: 0.898\n",
      "Epoch 1 [29/403] - Loss: 0.919\n",
      "Epoch 1 [30/403] - Loss: 1.039\n",
      "Epoch 1 [31/403] - Loss: 1.209\n",
      "Epoch 1 [32/403] - Loss: 1.155\n",
      "Epoch 1 [33/403] - Loss: 1.076\n",
      "Epoch 1 [34/403] - Loss: 0.942\n",
      "Epoch 1 [35/403] - Loss: 0.965\n",
      "Epoch 1 [36/403] - Loss: 0.943\n",
      "Epoch 1 [37/403] - Loss: 0.957\n",
      "Epoch 1 [38/403] - Loss: 0.943\n",
      "Epoch 1 [39/403] - Loss: 0.904\n",
      "Epoch 1 [40/403] - Loss: 0.946\n",
      "Epoch 1 [41/403] - Loss: 0.907\n",
      "Epoch 1 [42/403] - Loss: 0.887\n",
      "Epoch 1 [43/403] - Loss: 1.084\n",
      "Epoch 1 [44/403] - Loss: 0.908\n",
      "Epoch 1 [45/403] - Loss: 0.899\n",
      "Epoch 1 [46/403] - Loss: 0.895\n",
      "Epoch 1 [47/403] - Loss: 0.923\n",
      "Epoch 1 [48/403] - Loss: 0.903\n",
      "Epoch 1 [49/403] - Loss: 0.889\n",
      "Epoch 1 [50/403] - Loss: 0.872\n",
      "Epoch 1 [51/403] - Loss: 0.863\n",
      "Epoch 1 [52/403] - Loss: 0.886\n",
      "Epoch 1 [53/403] - Loss: 0.925\n",
      "Epoch 1 [54/403] - Loss: 0.868\n",
      "Epoch 1 [55/403] - Loss: 0.873\n",
      "Epoch 1 [56/403] - Loss: 0.869\n",
      "Epoch 1 [57/403] - Loss: 0.876\n",
      "Epoch 1 [58/403] - Loss: 0.853\n",
      "Epoch 1 [59/403] - Loss: 0.892\n",
      "Epoch 1 [60/403] - Loss: 0.877\n",
      "Epoch 1 [61/403] - Loss: 0.872\n",
      "Epoch 1 [62/403] - Loss: 0.933\n",
      "Epoch 1 [63/403] - Loss: 0.857\n",
      "Epoch 1 [64/403] - Loss: 3.671\n",
      "Epoch 1 [65/403] - Loss: 1.024\n",
      "Epoch 1 [66/403] - Loss: 1.391\n",
      "Epoch 1 [67/403] - Loss: 1.306\n",
      "Epoch 1 [68/403] - Loss: 1.235\n",
      "Epoch 1 [69/403] - Loss: 1.027\n",
      "Epoch 1 [70/403] - Loss: 1.059\n",
      "Epoch 1 [71/403] - Loss: 0.984\n",
      "Epoch 1 [72/403] - Loss: 1.007\n",
      "Epoch 1 [73/403] - Loss: 1.017\n",
      "Epoch 1 [74/403] - Loss: 1.030\n",
      "Epoch 1 [75/403] - Loss: 1.010\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 1 epochs of training in this tutorial\n",
    "num_epochs = 1\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_y.size(0))\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        # We don't really need a full epoch of training to get the parameters to be reasonable\n",
    "        if minibatch_i > 75:\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Because the grid is relatively small, we turn off the Toeplitz matrix multiplication and just perform them directly\n",
    "        # We find this to be more efficient when the grid is very small.\n",
    "        with gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f' % (i + 1, minibatch_i, len(train_loader), loss.item()))\n",
    "\n",
    "        # The actual optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`) using the standard SKI testing code, with no acceleration or precomputation. Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although our other tutorials demonstrate how to do this (for example, see the CIFAR tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.4737057387828827\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
