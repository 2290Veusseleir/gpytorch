{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA) (w/ SVGP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use Deep Kernel Learning with SVGP stochastic variational regression to rapidly train using minibatches on the `3droad` UCI dataset with hundreds of thousands of training examples. \n",
    "\n",
    "In contrast to the SVDKL_Regression_GridInterp_CUDA notebook, we'll be using SVGP (https://arxiv.org/pdf/1411.2005.pdf) here to learn the inducing point locations. Our implementation of SVGP is modified to be efficient with the inference techniques used in GPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `song` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~136 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Feature Extractor\n",
    "\n",
    "Next, we define the neural network feature extractor used to define the deep kernel. In this case, we use a fully connected network with the architecture `d -> 1000 -> 500 -> 50 -> 2`, as described in the original DKL paper. All of the code below uses standard PyTorch implementations of neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('bn1', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 1000))\n",
    "        self.add_module('bn2', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu2', torch.nn.ReLU())                       \n",
    "        self.add_module('linear3', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('bn3', torch.nn.BatchNorm1d(500))\n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('bn4', torch.nn.BatchNorm1d(50))\n",
    "        self.add_module('relu4', torch.nn.ReLU())                  \n",
    "        self.add_module('linear5', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Regression Layer\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. In this example, because we will be learning the inducing point locations, we'll be using a base `VariationalStrategy` with `learn_inducing_locations=True`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "class GPRegressionLayer(AbstractVariationalGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPRegressionLayer, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "            log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1., sigma=0.1, log_transform=True)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Model\n",
    "\n",
    "With the feature extractor and GP regression layer defined, we can now define our full model. To do this, we simply create a module whose `forward()` method passes the data first through the feature extractor, and then through the GP regression layer.\n",
    "\n",
    "The only other interesting feature of the model below is that we use a helper function, `scale_to_bounds`, to ensure that the features extracted by the neural network fit within the grid bounds used for SKI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, inducing_points, feature_extractor, num_features, grid_bounds=(-1., 1.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GPRegressionLayer(inducing_points)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "inducing_points = gpytorch.utils.grid.scale_to_bounds(feature_extractor(train_x[:500, :]), -1, 1)\n",
    "model = DKLModel(inducing_points=inducing_points, feature_extractor=feature_extractor, num_features=num_features).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the DKL model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalMarginalLogLikelihood` or ELBO), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/340] - Loss: 188.699 [-188.699, 0.000, -0.000]\n",
      "Epoch 1 [1/340] - Loss: 172.235 [-171.928, 0.308, -0.000]\n",
      "Epoch 1 [2/340] - Loss: 153.265 [-152.801, 0.464, -0.000]\n",
      "Epoch 1 [3/340] - Loss: 124.489 [-123.715, 0.774, -0.000]\n",
      "Epoch 1 [4/340] - Loss: 105.279 [-104.167, 1.112, -0.000]\n",
      "Epoch 1 [5/340] - Loss: 109.016 [-107.591, 1.426, -0.000]\n",
      "Epoch 1 [6/340] - Loss: 96.918 [-95.224, 1.695, -0.000]\n",
      "Epoch 1 [7/340] - Loss: 89.294 [-87.394, 1.899, -0.000]\n",
      "Epoch 1 [8/340] - Loss: 83.220 [-81.187, 2.033, -0.000]\n",
      "Epoch 1 [9/340] - Loss: 78.931 [-76.814, 2.117, -0.000]\n",
      "Epoch 1 [10/340] - Loss: 69.420 [-67.265, 2.155, -0.000]\n",
      "Epoch 1 [11/340] - Loss: 63.775 [-61.620, 2.155, -0.000]\n",
      "Epoch 1 [12/340] - Loss: 69.921 [-67.785, 2.136, -0.000]\n",
      "Epoch 1 [13/340] - Loss: 59.867 [-57.759, 2.108, -0.000]\n",
      "Epoch 1 [14/340] - Loss: 55.385 [-53.325, 2.060, -0.000]\n",
      "Epoch 1 [15/340] - Loss: 48.809 [-46.826, 1.983, -0.000]\n",
      "Epoch 1 [16/340] - Loss: 43.310 [-41.422, 1.888, -0.000]\n",
      "Epoch 1 [17/340] - Loss: 41.868 [-40.073, 1.796, -0.000]\n",
      "Epoch 1 [18/340] - Loss: 40.327 [-38.644, 1.683, -0.000]\n",
      "Epoch 1 [19/340] - Loss: 38.739 [-37.175, 1.564, -0.000]\n",
      "Epoch 1 [20/340] - Loss: 34.881 [-33.432, 1.449, -0.000]\n",
      "Epoch 1 [21/340] - Loss: 32.173 [-30.832, 1.342, -0.000]\n",
      "Epoch 1 [22/340] - Loss: 30.500 [-29.268, 1.232, -0.000]\n",
      "Epoch 1 [23/340] - Loss: 28.494 [-27.323, 1.172, -0.000]\n",
      "Epoch 1 [24/340] - Loss: 26.534 [-25.381, 1.154, -0.000]\n",
      "Epoch 1 [25/340] - Loss: 24.554 [-23.420, 1.134, -0.000]\n",
      "Epoch 1 [26/340] - Loss: 24.724 [-23.597, 1.127, -0.000]\n",
      "Epoch 1 [27/340] - Loss: 23.318 [-22.267, 1.051, -0.000]\n",
      "Epoch 1 [28/340] - Loss: 21.969 [-20.991, 0.977, -0.000]\n",
      "Epoch 1 [29/340] - Loss: 21.261 [-20.363, 0.898, -0.000]\n",
      "Epoch 1 [30/340] - Loss: 19.033 [-18.170, 0.863, -0.000]\n",
      "Epoch 1 [31/340] - Loss: 19.687 [-18.848, 0.839, -0.000]\n",
      "Epoch 1 [32/340] - Loss: 19.308 [-18.526, 0.781, -0.000]\n",
      "Epoch 1 [33/340] - Loss: 17.810 [-17.062, 0.748, -0.000]\n",
      "Epoch 1 [34/340] - Loss: 16.584 [-15.861, 0.722, -0.000]\n",
      "Epoch 1 [35/340] - Loss: 16.347 [-15.638, 0.708, -0.000]\n",
      "Epoch 1 [36/340] - Loss: 16.533 [-15.820, 0.713, -0.000]\n",
      "Epoch 1 [37/340] - Loss: 16.705 [-15.977, 0.728, -0.000]\n",
      "Epoch 1 [38/340] - Loss: 15.534 [-14.770, 0.764, -0.000]\n",
      "Epoch 1 [39/340] - Loss: 14.879 [-14.072, 0.807, -0.000]\n",
      "Epoch 1 [40/340] - Loss: 15.160 [-14.309, 0.851, -0.000]\n",
      "Epoch 1 [41/340] - Loss: 15.218 [-14.337, 0.881, -0.000]\n",
      "Epoch 1 [42/340] - Loss: 13.408 [-12.536, 0.872, -0.000]\n",
      "Epoch 1 [43/340] - Loss: 11.988 [-11.114, 0.874, -0.000]\n",
      "Epoch 1 [44/340] - Loss: 13.071 [-12.199, 0.872, -0.000]\n",
      "Epoch 1 [45/340] - Loss: 12.247 [-11.354, 0.893, -0.000]\n",
      "Epoch 1 [46/340] - Loss: 13.728 [-12.819, 0.909, -0.000]\n",
      "Epoch 1 [47/340] - Loss: 13.148 [-12.225, 0.923, -0.000]\n",
      "Epoch 1 [48/340] - Loss: 12.909 [-11.970, 0.938, -0.000]\n",
      "Epoch 1 [49/340] - Loss: 13.791 [-12.873, 0.918, -0.000]\n",
      "Epoch 1 [50/340] - Loss: 12.475 [-11.605, 0.870, -0.000]\n",
      "Epoch 1 [51/340] - Loss: 12.570 [-11.668, 0.902, -0.000]\n",
      "Epoch 1 [52/340] - Loss: 12.379 [-11.533, 0.845, -0.000]\n",
      "Epoch 1 [53/340] - Loss: 12.311 [-11.505, 0.805, -0.000]\n",
      "Epoch 1 [54/340] - Loss: 11.693 [-10.957, 0.737, -0.000]\n",
      "Epoch 1 [55/340] - Loss: 11.672 [-10.964, 0.709, -0.000]\n",
      "Epoch 1 [56/340] - Loss: 11.122 [-10.524, 0.598, -0.000]\n",
      "Epoch 1 [57/340] - Loss: 11.101 [-10.549, 0.552, -0.000]\n",
      "Epoch 1 [58/340] - Loss: 10.563 [-10.073, 0.490, -0.000]\n",
      "Epoch 1 [59/340] - Loss: 12.451 [-12.005, 0.446, -0.000]\n",
      "Epoch 1 [60/340] - Loss: 11.195 [-10.793, 0.402, -0.000]\n",
      "Epoch 1 [61/340] - Loss: 11.203 [-10.822, 0.381, -0.000]\n",
      "Epoch 1 [62/340] - Loss: 11.411 [-10.990, 0.420, -0.000]\n",
      "Epoch 1 [63/340] - Loss: 10.736 [-10.340, 0.396, -0.000]\n",
      "Epoch 1 [64/340] - Loss: 9.785 [-9.492, 0.293, -0.000]\n",
      "Epoch 1 [65/340] - Loss: 9.642 [-9.413, 0.228, -0.000]\n",
      "Epoch 1 [66/340] - Loss: 10.137 [-9.964, 0.173, -0.000]\n",
      "Epoch 1 [67/340] - Loss: 10.324 [-10.176, 0.148, -0.000]\n",
      "Epoch 1 [68/340] - Loss: 9.906 [-9.771, 0.135, -0.000]\n",
      "Epoch 1 [69/340] - Loss: 9.384 [-9.265, 0.119, -0.000]\n",
      "Epoch 1 [70/340] - Loss: 9.450 [-9.348, 0.102, -0.000]\n",
      "Epoch 1 [71/340] - Loss: 9.444 [-9.356, 0.088, -0.000]\n",
      "Epoch 1 [72/340] - Loss: 9.598 [-9.516, 0.083, -0.000]\n",
      "Epoch 1 [73/340] - Loss: 9.102 [-9.021, 0.081, -0.000]\n",
      "Epoch 1 [74/340] - Loss: 9.138 [-9.063, 0.075, -0.000]\n",
      "Epoch 1 [75/340] - Loss: 8.504 [-8.434, 0.070, -0.000]\n",
      "Epoch 1 [76/340] - Loss: 8.845 [-8.782, 0.063, -0.000]\n",
      "Epoch 1 [77/340] - Loss: 8.594 [-8.537, 0.057, -0.000]\n",
      "Epoch 1 [78/340] - Loss: 8.459 [-8.408, 0.050, -0.000]\n",
      "Epoch 1 [79/340] - Loss: 8.083 [-8.039, 0.044, -0.000]\n",
      "Epoch 1 [80/340] - Loss: 8.525 [-8.485, 0.040, -0.000]\n",
      "Epoch 1 [81/340] - Loss: 8.206 [-8.170, 0.036, -0.000]\n",
      "Epoch 1 [82/340] - Loss: 8.501 [-8.467, 0.034, -0.000]\n",
      "Epoch 1 [83/340] - Loss: 8.270 [-8.238, 0.032, -0.000]\n",
      "Epoch 1 [84/340] - Loss: 7.862 [-7.832, 0.030, -0.000]\n",
      "Epoch 1 [85/340] - Loss: 8.107 [-8.077, 0.030, -0.000]\n",
      "Epoch 1 [86/340] - Loss: 7.539 [-7.510, 0.029, -0.000]\n",
      "Epoch 1 [87/340] - Loss: 7.386 [-7.358, 0.028, -0.000]\n",
      "Epoch 1 [88/340] - Loss: 7.789 [-7.763, 0.026, -0.000]\n",
      "Epoch 1 [89/340] - Loss: 7.183 [-7.158, 0.025, -0.000]\n",
      "Epoch 1 [90/340] - Loss: 7.457 [-7.432, 0.025, -0.000]\n",
      "Epoch 1 [91/340] - Loss: 7.164 [-7.139, 0.025, -0.000]\n",
      "Epoch 1 [92/340] - Loss: 7.892 [-7.867, 0.025, -0.000]\n",
      "Epoch 1 [93/340] - Loss: 6.796 [-6.772, 0.024, -0.000]\n",
      "Epoch 1 [94/340] - Loss: 7.048 [-7.023, 0.025, -0.000]\n",
      "Epoch 1 [95/340] - Loss: 7.830 [-7.807, 0.023, -0.000]\n",
      "Epoch 1 [96/340] - Loss: 7.415 [-7.393, 0.022, -0.000]\n",
      "Epoch 1 [97/340] - Loss: 6.852 [-6.832, 0.021, -0.000]\n",
      "Epoch 1 [98/340] - Loss: 6.754 [-6.736, 0.018, -0.000]\n",
      "Epoch 1 [99/340] - Loss: 7.072 [-7.055, 0.017, -0.000]\n",
      "Epoch 1 [100/340] - Loss: 6.761 [-6.745, 0.016, -0.000]\n",
      "Epoch 1 [101/340] - Loss: 6.728 [-6.711, 0.017, -0.000]\n",
      "Epoch 1 [102/340] - Loss: 6.708 [-6.693, 0.015, -0.000]\n",
      "Epoch 1 [103/340] - Loss: 6.839 [-6.825, 0.014, -0.000]\n",
      "Epoch 1 [104/340] - Loss: 6.377 [-6.363, 0.014, -0.000]\n",
      "Epoch 1 [105/340] - Loss: 6.895 [-6.881, 0.013, -0.000]\n",
      "Epoch 1 [106/340] - Loss: 6.593 [-6.580, 0.013, -0.000]\n",
      "Epoch 1 [107/340] - Loss: 6.339 [-6.326, 0.013, -0.000]\n",
      "Epoch 1 [108/340] - Loss: 6.512 [-6.499, 0.013, -0.000]\n",
      "Epoch 1 [109/340] - Loss: 6.507 [-6.494, 0.013, -0.000]\n",
      "Epoch 1 [110/340] - Loss: 6.363 [-6.350, 0.013, -0.000]\n",
      "Epoch 1 [111/340] - Loss: 6.951 [-6.937, 0.014, -0.000]\n",
      "Epoch 1 [112/340] - Loss: 6.487 [-6.473, 0.014, -0.000]\n",
      "Epoch 1 [113/340] - Loss: 6.065 [-6.051, 0.014, -0.000]\n",
      "Epoch 1 [114/340] - Loss: 5.900 [-5.886, 0.014, -0.000]\n",
      "Epoch 1 [115/340] - Loss: 5.905 [-5.891, 0.014, -0.000]\n",
      "Epoch 1 [116/340] - Loss: 5.905 [-5.891, 0.014, -0.000]\n",
      "Epoch 1 [117/340] - Loss: 6.002 [-5.988, 0.014, -0.000]\n",
      "Epoch 1 [118/340] - Loss: 5.749 [-5.734, 0.015, -0.000]\n",
      "Epoch 1 [119/340] - Loss: 5.915 [-5.900, 0.015, -0.000]\n",
      "Epoch 1 [120/340] - Loss: 5.792 [-5.777, 0.015, -0.000]\n",
      "Epoch 1 [121/340] - Loss: 5.896 [-5.881, 0.015, -0.000]\n",
      "Epoch 1 [122/340] - Loss: 5.450 [-5.434, 0.016, -0.000]\n",
      "Epoch 1 [123/340] - Loss: 5.948 [-5.931, 0.017, -0.000]\n",
      "Epoch 1 [124/340] - Loss: 5.762 [-5.746, 0.016, -0.000]\n",
      "Epoch 1 [125/340] - Loss: 6.009 [-5.992, 0.017, -0.000]\n",
      "Epoch 1 [126/340] - Loss: 5.965 [-5.947, 0.018, -0.000]\n",
      "Epoch 1 [127/340] - Loss: 5.978 [-5.960, 0.018, -0.000]\n",
      "Epoch 1 [128/340] - Loss: 5.769 [-5.751, 0.018, -0.000]\n",
      "Epoch 1 [129/340] - Loss: 5.483 [-5.462, 0.021, -0.000]\n",
      "Epoch 1 [130/340] - Loss: 5.366 [-5.348, 0.018, -0.000]\n",
      "Epoch 1 [131/340] - Loss: 5.698 [-5.680, 0.018, -0.000]\n",
      "Epoch 1 [132/340] - Loss: 5.204 [-5.186, 0.019, -0.000]\n",
      "Epoch 1 [133/340] - Loss: 5.492 [-5.472, 0.020, -0.000]\n",
      "Epoch 1 [134/340] - Loss: 5.519 [-5.498, 0.020, -0.000]\n",
      "Epoch 1 [135/340] - Loss: 5.440 [-5.418, 0.022, -0.000]\n",
      "Epoch 1 [136/340] - Loss: 5.511 [-5.488, 0.023, -0.000]\n",
      "Epoch 1 [137/340] - Loss: 6.031 [-6.008, 0.023, -0.000]\n",
      "Epoch 1 [138/340] - Loss: 5.775 [-5.753, 0.022, -0.000]\n",
      "Epoch 1 [139/340] - Loss: 5.786 [-5.765, 0.021, -0.000]\n",
      "Epoch 1 [140/340] - Loss: 5.793 [-5.774, 0.020, -0.000]\n",
      "Epoch 1 [141/340] - Loss: 5.814 [-5.795, 0.019, -0.000]\n",
      "Epoch 1 [142/340] - Loss: 5.612 [-5.594, 0.018, -0.000]\n",
      "Epoch 1 [143/340] - Loss: 5.523 [-5.505, 0.018, -0.000]\n",
      "Epoch 1 [144/340] - Loss: 5.281 [-5.263, 0.019, -0.000]\n",
      "Epoch 1 [145/340] - Loss: 5.399 [-5.381, 0.019, -0.000]\n",
      "Epoch 1 [146/340] - Loss: 5.575 [-5.556, 0.019, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [147/340] - Loss: 5.302 [-5.281, 0.021, -0.000]\n",
      "Epoch 1 [148/340] - Loss: 6.141 [-6.122, 0.018, -0.000]\n",
      "Epoch 1 [149/340] - Loss: 5.242 [-5.225, 0.017, -0.000]\n",
      "Epoch 1 [150/340] - Loss: 5.312 [-5.295, 0.017, -0.000]\n",
      "Epoch 1 [151/340] - Loss: 5.065 [-5.048, 0.017, -0.000]\n",
      "Epoch 1 [152/340] - Loss: 5.481 [-5.464, 0.017, -0.000]\n",
      "Epoch 1 [153/340] - Loss: 5.616 [-5.599, 0.017, -0.000]\n",
      "Epoch 1 [154/340] - Loss: 5.182 [-5.164, 0.017, -0.000]\n",
      "Epoch 1 [155/340] - Loss: 5.043 [-5.026, 0.017, -0.000]\n",
      "Epoch 1 [156/340] - Loss: 5.924 [-5.907, 0.017, -0.000]\n",
      "Epoch 1 [157/340] - Loss: 5.781 [-5.766, 0.015, -0.000]\n",
      "Epoch 1 [158/340] - Loss: 5.499 [-5.484, 0.015, -0.000]\n",
      "Epoch 1 [159/340] - Loss: 5.674 [-5.660, 0.014, -0.000]\n",
      "Epoch 1 [160/340] - Loss: 7.214 [-7.201, 0.013, -0.000]\n",
      "Epoch 1 [161/340] - Loss: 6.009 [-5.996, 0.012, -0.000]\n",
      "Epoch 1 [162/340] - Loss: 5.632 [-5.620, 0.011, -0.000]\n",
      "Epoch 1 [163/340] - Loss: 5.539 [-5.528, 0.011, -0.000]\n",
      "Epoch 1 [164/340] - Loss: 5.411 [-5.400, 0.011, -0.000]\n",
      "Epoch 1 [165/340] - Loss: 5.357 [-5.347, 0.010, -0.000]\n",
      "Epoch 1 [166/340] - Loss: 5.658 [-5.649, 0.009, -0.000]\n",
      "Epoch 1 [167/340] - Loss: 5.507 [-5.498, 0.009, -0.000]\n",
      "Epoch 1 [168/340] - Loss: 5.413 [-5.405, 0.009, -0.000]\n",
      "Epoch 1 [169/340] - Loss: 5.342 [-5.334, 0.008, -0.000]\n",
      "Epoch 1 [170/340] - Loss: 5.260 [-5.251, 0.008, -0.000]\n",
      "Epoch 1 [171/340] - Loss: 5.327 [-5.319, 0.008, -0.000]\n",
      "Epoch 1 [172/340] - Loss: 4.843 [-4.835, 0.008, -0.000]\n",
      "Epoch 1 [173/340] - Loss: 4.965 [-4.957, 0.008, -0.000]\n",
      "Epoch 1 [174/340] - Loss: 5.045 [-5.036, 0.009, -0.000]\n",
      "Epoch 1 [175/340] - Loss: 5.020 [-5.011, 0.009, -0.000]\n",
      "Epoch 1 [176/340] - Loss: 4.857 [-4.848, 0.009, -0.000]\n",
      "Epoch 1 [177/340] - Loss: 4.726 [-4.718, 0.008, -0.000]\n",
      "Epoch 1 [178/340] - Loss: 4.827 [-4.819, 0.008, -0.000]\n",
      "Epoch 1 [179/340] - Loss: 4.918 [-4.910, 0.008, -0.000]\n",
      "Epoch 1 [180/340] - Loss: 4.720 [-4.712, 0.008, -0.000]\n",
      "Epoch 1 [181/340] - Loss: 4.998 [-4.985, 0.014, -0.000]\n",
      "Epoch 1 [182/340] - Loss: 4.947 [-4.939, 0.008, -0.000]\n",
      "Epoch 1 [183/340] - Loss: 4.981 [-4.972, 0.009, -0.000]\n",
      "Epoch 1 [184/340] - Loss: 4.877 [-4.869, 0.008, -0.000]\n",
      "Epoch 1 [185/340] - Loss: 5.038 [-5.029, 0.009, -0.000]\n",
      "Epoch 1 [186/340] - Loss: 5.009 [-4.999, 0.010, -0.000]\n",
      "Epoch 1 [187/340] - Loss: 5.165 [-5.156, 0.009, -0.000]\n",
      "Epoch 1 [188/340] - Loss: 4.902 [-4.892, 0.010, -0.000]\n",
      "Epoch 1 [189/340] - Loss: 4.974 [-4.964, 0.010, -0.000]\n",
      "Epoch 1 [190/340] - Loss: 4.753 [-4.744, 0.010, -0.000]\n",
      "Epoch 1 [191/340] - Loss: 4.822 [-4.814, 0.009, -0.000]\n",
      "Epoch 1 [192/340] - Loss: 4.930 [-4.921, 0.009, -0.000]\n",
      "Epoch 1 [193/340] - Loss: 4.663 [-4.654, 0.009, -0.000]\n",
      "Epoch 1 [194/340] - Loss: 5.036 [-5.027, 0.009, -0.000]\n",
      "Epoch 1 [195/340] - Loss: 5.103 [-5.094, 0.009, -0.000]\n",
      "Epoch 1 [196/340] - Loss: 4.678 [-4.668, 0.010, -0.000]\n",
      "Epoch 1 [197/340] - Loss: 4.739 [-4.728, 0.011, -0.000]\n",
      "Epoch 1 [198/340] - Loss: 4.703 [-4.692, 0.012, -0.000]\n",
      "Epoch 1 [199/340] - Loss: 4.742 [-4.731, 0.011, -0.000]\n",
      "Epoch 1 [200/340] - Loss: 5.050 [-5.039, 0.011, -0.000]\n",
      "Epoch 1 [201/340] - Loss: 4.739 [-4.728, 0.011, -0.000]\n",
      "Epoch 1 [202/340] - Loss: 4.886 [-4.875, 0.011, -0.000]\n",
      "Epoch 1 [203/340] - Loss: 4.752 [-4.741, 0.011, -0.000]\n",
      "Epoch 1 [204/340] - Loss: 4.594 [-4.582, 0.012, -0.000]\n",
      "Epoch 1 [205/340] - Loss: 4.449 [-4.436, 0.013, -0.000]\n",
      "Epoch 1 [206/340] - Loss: 4.779 [-4.765, 0.013, -0.000]\n",
      "Epoch 1 [207/340] - Loss: 4.738 [-4.727, 0.011, -0.000]\n",
      "Epoch 1 [208/340] - Loss: 4.809 [-4.798, 0.011, -0.000]\n",
      "Epoch 1 [209/340] - Loss: 4.824 [-4.813, 0.011, -0.000]\n",
      "Epoch 1 [210/340] - Loss: 4.774 [-4.764, 0.011, -0.000]\n",
      "Epoch 1 [211/340] - Loss: 4.706 [-4.695, 0.010, -0.000]\n",
      "Epoch 1 [212/340] - Loss: 4.743 [-4.733, 0.010, -0.000]\n",
      "Epoch 1 [213/340] - Loss: 4.572 [-4.562, 0.010, -0.000]\n",
      "Epoch 1 [214/340] - Loss: 4.557 [-4.546, 0.010, -0.000]\n",
      "Epoch 1 [215/340] - Loss: 4.498 [-4.487, 0.012, -0.000]\n",
      "Epoch 1 [216/340] - Loss: 4.477 [-4.464, 0.013, -0.000]\n",
      "Epoch 1 [217/340] - Loss: 4.553 [-4.542, 0.012, -0.000]\n",
      "Epoch 1 [218/340] - Loss: 4.377 [-4.365, 0.012, -0.000]\n",
      "Epoch 1 [219/340] - Loss: 4.497 [-4.486, 0.012, -0.000]\n",
      "Epoch 1 [220/340] - Loss: 4.751 [-4.738, 0.012, -0.000]\n",
      "Epoch 1 [221/340] - Loss: 4.557 [-4.545, 0.012, -0.000]\n",
      "Epoch 1 [222/340] - Loss: 4.608 [-4.595, 0.013, -0.000]\n",
      "Epoch 1 [223/340] - Loss: 4.964 [-4.952, 0.013, -0.000]\n",
      "Epoch 1 [224/340] - Loss: 4.806 [-4.793, 0.013, -0.000]\n",
      "Epoch 1 [225/340] - Loss: 4.672 [-4.660, 0.013, -0.000]\n",
      "Epoch 1 [226/340] - Loss: 4.831 [-4.818, 0.012, -0.000]\n",
      "Epoch 1 [227/340] - Loss: 4.660 [-4.648, 0.012, -0.000]\n",
      "Epoch 1 [228/340] - Loss: 4.589 [-4.577, 0.012, -0.000]\n",
      "Epoch 1 [229/340] - Loss: 4.743 [-4.731, 0.012, -0.000]\n",
      "Epoch 1 [230/340] - Loss: 4.678 [-4.667, 0.012, -0.000]\n",
      "Epoch 1 [231/340] - Loss: 4.668 [-4.657, 0.012, -0.000]\n",
      "Epoch 1 [232/340] - Loss: 4.559 [-4.548, 0.011, -0.000]\n",
      "Epoch 1 [233/340] - Loss: 4.551 [-4.539, 0.011, -0.000]\n",
      "Epoch 1 [234/340] - Loss: 4.579 [-4.568, 0.011, -0.000]\n",
      "Epoch 1 [235/340] - Loss: 4.425 [-4.415, 0.011, -0.000]\n",
      "Epoch 1 [236/340] - Loss: 4.354 [-4.343, 0.011, -0.000]\n",
      "Epoch 1 [237/340] - Loss: 4.555 [-4.545, 0.010, -0.000]\n",
      "Epoch 1 [238/340] - Loss: 4.504 [-4.494, 0.010, -0.000]\n",
      "Epoch 1 [239/340] - Loss: 4.539 [-4.528, 0.011, -0.000]\n",
      "Epoch 1 [240/340] - Loss: 4.576 [-4.566, 0.011, -0.000]\n",
      "Epoch 1 [241/340] - Loss: 4.721 [-4.710, 0.011, -0.000]\n",
      "Epoch 1 [242/340] - Loss: 4.425 [-4.413, 0.012, -0.000]\n",
      "Epoch 1 [243/340] - Loss: 4.462 [-4.449, 0.013, -0.000]\n",
      "Epoch 1 [244/340] - Loss: 4.549 [-4.536, 0.013, -0.000]\n",
      "Epoch 1 [245/340] - Loss: 4.467 [-4.458, 0.009, -0.000]\n",
      "Epoch 1 [246/340] - Loss: 4.477 [-4.468, 0.009, -0.000]\n",
      "Epoch 1 [247/340] - Loss: 4.424 [-4.414, 0.010, -0.000]\n",
      "Epoch 1 [248/340] - Loss: 4.649 [-4.639, 0.010, -0.000]\n",
      "Epoch 1 [249/340] - Loss: 4.440 [-4.430, 0.010, -0.000]\n",
      "Epoch 1 [250/340] - Loss: 4.433 [-4.423, 0.010, -0.000]\n",
      "Epoch 1 [251/340] - Loss: 4.593 [-4.583, 0.010, -0.000]\n",
      "Epoch 1 [252/340] - Loss: 4.343 [-4.333, 0.010, -0.000]\n",
      "Epoch 1 [253/340] - Loss: 4.372 [-4.362, 0.010, -0.000]\n",
      "Epoch 1 [254/340] - Loss: 4.377 [-4.366, 0.011, -0.000]\n",
      "Epoch 1 [255/340] - Loss: 4.478 [-4.468, 0.010, -0.000]\n",
      "Epoch 1 [256/340] - Loss: 4.364 [-4.355, 0.010, -0.000]\n",
      "Epoch 1 [257/340] - Loss: 4.525 [-4.516, 0.010, -0.000]\n",
      "Epoch 1 [258/340] - Loss: 4.295 [-4.286, 0.010, -0.000]\n",
      "Epoch 1 [259/340] - Loss: 4.269 [-4.259, 0.010, -0.000]\n",
      "Epoch 1 [260/340] - Loss: 4.281 [-4.271, 0.010, -0.000]\n",
      "Epoch 1 [261/340] - Loss: 4.335 [-4.325, 0.010, -0.000]\n",
      "Epoch 1 [262/340] - Loss: 4.271 [-4.261, 0.010, -0.000]\n",
      "Epoch 1 [263/340] - Loss: 4.335 [-4.325, 0.010, -0.000]\n",
      "Epoch 1 [264/340] - Loss: 4.266 [-4.256, 0.010, -0.000]\n",
      "Epoch 1 [265/340] - Loss: 4.542 [-4.532, 0.010, -0.000]\n",
      "Epoch 1 [266/340] - Loss: 4.303 [-4.293, 0.010, -0.000]\n",
      "Epoch 1 [267/340] - Loss: 4.443 [-4.433, 0.010, -0.000]\n",
      "Epoch 1 [268/340] - Loss: 4.471 [-4.461, 0.010, -0.000]\n",
      "Epoch 1 [269/340] - Loss: 4.372 [-4.362, 0.010, -0.000]\n",
      "Epoch 1 [270/340] - Loss: 4.352 [-4.342, 0.010, -0.000]\n",
      "Epoch 1 [271/340] - Loss: 4.291 [-4.281, 0.010, -0.000]\n",
      "Epoch 1 [272/340] - Loss: 4.521 [-4.511, 0.010, -0.000]\n",
      "Epoch 1 [273/340] - Loss: 4.172 [-4.162, 0.010, -0.000]\n",
      "Epoch 1 [274/340] - Loss: 4.309 [-4.299, 0.010, -0.000]\n",
      "Epoch 1 [275/340] - Loss: 4.256 [-4.246, 0.010, -0.000]\n",
      "Epoch 1 [276/340] - Loss: 4.312 [-4.302, 0.010, -0.000]\n",
      "Epoch 1 [277/340] - Loss: 4.236 [-4.227, 0.010, -0.000]\n",
      "Epoch 1 [278/340] - Loss: 4.237 [-4.227, 0.010, -0.000]\n",
      "Epoch 1 [279/340] - Loss: 4.323 [-4.313, 0.010, -0.000]\n",
      "Epoch 1 [280/340] - Loss: 4.270 [-4.259, 0.010, -0.000]\n",
      "Epoch 1 [281/340] - Loss: 4.254 [-4.243, 0.011, -0.000]\n",
      "Epoch 1 [282/340] - Loss: 4.363 [-4.351, 0.012, -0.000]\n",
      "Epoch 1 [283/340] - Loss: 4.141 [-4.129, 0.012, -0.000]\n",
      "Epoch 1 [284/340] - Loss: 4.300 [-4.288, 0.013, -0.000]\n",
      "Epoch 1 [285/340] - Loss: 4.296 [-4.283, 0.013, -0.000]\n",
      "Epoch 1 [286/340] - Loss: 4.222 [-4.209, 0.013, -0.000]\n",
      "Epoch 1 [287/340] - Loss: 4.315 [-4.301, 0.013, -0.000]\n",
      "Epoch 1 [288/340] - Loss: 4.257 [-4.244, 0.013, -0.000]\n",
      "Epoch 1 [289/340] - Loss: 4.163 [-4.150, 0.013, -0.000]\n",
      "Epoch 1 [290/340] - Loss: 4.299 [-4.285, 0.014, -0.000]\n",
      "Epoch 1 [291/340] - Loss: 4.251 [-4.238, 0.014, -0.000]\n",
      "Epoch 1 [292/340] - Loss: 4.232 [-4.220, 0.012, -0.000]\n",
      "Epoch 1 [293/340] - Loss: 4.293 [-4.281, 0.012, -0.000]\n",
      "Epoch 1 [294/340] - Loss: 4.328 [-4.317, 0.011, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [295/340] - Loss: 4.217 [-4.207, 0.011, -0.000]\n",
      "Epoch 1 [296/340] - Loss: 4.352 [-4.342, 0.011, -0.000]\n",
      "Epoch 1 [297/340] - Loss: 4.324 [-4.313, 0.010, -0.000]\n",
      "Epoch 1 [298/340] - Loss: 4.221 [-4.211, 0.010, -0.000]\n",
      "Epoch 1 [299/340] - Loss: 4.274 [-4.264, 0.010, -0.000]\n",
      "Epoch 1 [300/340] - Loss: 4.317 [-4.307, 0.009, -0.000]\n",
      "Epoch 1 [301/340] - Loss: 4.198 [-4.188, 0.009, -0.000]\n",
      "Epoch 1 [302/340] - Loss: 4.122 [-4.113, 0.009, -0.000]\n",
      "Epoch 1 [303/340] - Loss: 4.119 [-4.110, 0.009, -0.000]\n",
      "Epoch 1 [304/340] - Loss: 4.179 [-4.170, 0.009, -0.000]\n",
      "Epoch 1 [305/340] - Loss: 4.213 [-4.205, 0.008, -0.000]\n",
      "Epoch 1 [306/340] - Loss: 4.230 [-4.222, 0.008, -0.000]\n",
      "Epoch 1 [307/340] - Loss: 4.150 [-4.141, 0.008, -0.000]\n",
      "Epoch 1 [308/340] - Loss: 4.222 [-4.213, 0.008, -0.000]\n",
      "Epoch 1 [309/340] - Loss: 4.219 [-4.211, 0.009, -0.000]\n",
      "Epoch 1 [310/340] - Loss: 4.115 [-4.107, 0.009, -0.000]\n",
      "Epoch 1 [311/340] - Loss: 4.185 [-4.176, 0.009, -0.000]\n",
      "Epoch 1 [312/340] - Loss: 4.135 [-4.126, 0.009, -0.000]\n",
      "Epoch 1 [313/340] - Loss: 4.237 [-4.228, 0.009, -0.000]\n",
      "Epoch 1 [314/340] - Loss: 4.187 [-4.177, 0.009, -0.000]\n",
      "Epoch 1 [315/340] - Loss: 4.135 [-4.126, 0.009, -0.000]\n",
      "Epoch 1 [316/340] - Loss: 4.170 [-4.160, 0.009, -0.000]\n",
      "Epoch 1 [317/340] - Loss: 4.121 [-4.112, 0.009, -0.000]\n",
      "Epoch 1 [318/340] - Loss: 4.249 [-4.240, 0.009, -0.000]\n",
      "Epoch 1 [319/340] - Loss: 4.178 [-4.169, 0.009, -0.000]\n",
      "Epoch 1 [320/340] - Loss: 4.184 [-4.176, 0.009, -0.000]\n",
      "Epoch 1 [321/340] - Loss: 4.209 [-4.200, 0.009, -0.000]\n",
      "Epoch 1 [322/340] - Loss: 4.213 [-4.204, 0.009, -0.000]\n",
      "Epoch 1 [323/340] - Loss: 4.165 [-4.157, 0.009, -0.000]\n",
      "Epoch 1 [324/340] - Loss: 4.228 [-4.220, 0.008, -0.000]\n",
      "Epoch 1 [325/340] - Loss: 4.168 [-4.159, 0.009, -0.000]\n",
      "Epoch 1 [326/340] - Loss: 4.309 [-4.300, 0.009, -0.000]\n",
      "Epoch 1 [327/340] - Loss: 4.137 [-4.129, 0.008, -0.000]\n",
      "Epoch 1 [328/340] - Loss: 4.145 [-4.137, 0.008, -0.000]\n",
      "Epoch 1 [329/340] - Loss: 4.272 [-4.264, 0.008, -0.000]\n",
      "Epoch 1 [330/340] - Loss: 4.243 [-4.234, 0.008, -0.000]\n",
      "Epoch 1 [331/340] - Loss: 4.196 [-4.187, 0.008, -0.000]\n",
      "Epoch 1 [332/340] - Loss: 4.043 [-4.035, 0.008, -0.000]\n",
      "Epoch 1 [333/340] - Loss: 4.147 [-4.139, 0.009, -0.000]\n",
      "Epoch 1 [334/340] - Loss: 4.248 [-4.239, 0.009, -0.000]\n",
      "Epoch 1 [335/340] - Loss: 4.236 [-4.227, 0.009, -0.000]\n",
      "Epoch 1 [336/340] - Loss: 4.132 [-4.123, 0.009, -0.000]\n",
      "Epoch 1 [337/340] - Loss: 4.190 [-4.181, 0.010, -0.000]\n",
      "Epoch 1 [338/340] - Loss: 4.171 [-4.161, 0.010, -0.000]\n",
      "Epoch 1 [339/340] - Loss: 4.116 [-4.106, 0.010, -0.000]\n",
      "Epoch 2 [0/340] - Loss: 4.186 [-4.176, 0.010, -0.000]\n",
      "Epoch 2 [1/340] - Loss: 4.148 [-4.138, 0.010, -0.000]\n",
      "Epoch 2 [2/340] - Loss: 4.105 [-4.095, 0.010, -0.000]\n",
      "Epoch 2 [3/340] - Loss: 4.138 [-4.128, 0.010, -0.000]\n",
      "Epoch 2 [4/340] - Loss: 4.117 [-4.107, 0.010, -0.000]\n",
      "Epoch 2 [5/340] - Loss: 4.107 [-4.097, 0.010, -0.000]\n",
      "Epoch 2 [6/340] - Loss: 4.175 [-4.165, 0.010, -0.000]\n",
      "Epoch 2 [7/340] - Loss: 4.217 [-4.207, 0.010, -0.000]\n",
      "Epoch 2 [8/340] - Loss: 4.153 [-4.143, 0.010, -0.000]\n",
      "Epoch 2 [9/340] - Loss: 4.072 [-4.062, 0.010, -0.000]\n",
      "Epoch 2 [10/340] - Loss: 4.119 [-4.110, 0.010, -0.000]\n",
      "Epoch 2 [11/340] - Loss: 4.078 [-4.068, 0.010, -0.000]\n",
      "Epoch 2 [12/340] - Loss: 4.187 [-4.177, 0.010, -0.000]\n",
      "Epoch 2 [13/340] - Loss: 4.114 [-4.103, 0.010, -0.000]\n",
      "Epoch 2 [14/340] - Loss: 4.151 [-4.140, 0.011, -0.000]\n",
      "Epoch 2 [15/340] - Loss: 4.104 [-4.094, 0.011, -0.000]\n",
      "Epoch 2 [16/340] - Loss: 4.097 [-4.085, 0.011, -0.000]\n",
      "Epoch 2 [17/340] - Loss: 4.220 [-4.209, 0.012, -0.000]\n",
      "Epoch 2 [18/340] - Loss: 4.206 [-4.194, 0.011, -0.000]\n",
      "Epoch 2 [19/340] - Loss: 4.220 [-4.208, 0.011, -0.000]\n",
      "Epoch 2 [20/340] - Loss: 4.135 [-4.124, 0.011, -0.000]\n",
      "Epoch 2 [21/340] - Loss: 4.206 [-4.195, 0.011, -0.000]\n",
      "Epoch 2 [22/340] - Loss: 4.215 [-4.204, 0.011, -0.000]\n",
      "Epoch 2 [23/340] - Loss: 4.091 [-4.080, 0.011, -0.000]\n",
      "Epoch 2 [24/340] - Loss: 4.164 [-4.153, 0.011, -0.000]\n",
      "Epoch 2 [25/340] - Loss: 4.288 [-4.278, 0.010, -0.000]\n",
      "Epoch 2 [26/340] - Loss: 4.163 [-4.153, 0.010, -0.000]\n",
      "Epoch 2 [27/340] - Loss: 4.235 [-4.226, 0.009, -0.000]\n",
      "Epoch 2 [28/340] - Loss: 4.142 [-4.133, 0.009, -0.000]\n",
      "Epoch 2 [29/340] - Loss: 4.134 [-4.125, 0.009, -0.000]\n",
      "Epoch 2 [30/340] - Loss: 4.273 [-4.264, 0.009, -0.000]\n",
      "Epoch 2 [31/340] - Loss: 4.128 [-4.119, 0.009, -0.000]\n",
      "Epoch 2 [32/340] - Loss: 4.086 [-4.077, 0.009, -0.000]\n",
      "Epoch 2 [33/340] - Loss: 4.111 [-4.101, 0.010, -0.000]\n",
      "Epoch 2 [34/340] - Loss: 4.077 [-4.067, 0.010, -0.000]\n",
      "Epoch 2 [35/340] - Loss: 4.218 [-4.207, 0.011, -0.000]\n",
      "Epoch 2 [36/340] - Loss: 4.157 [-4.145, 0.011, -0.000]\n",
      "Epoch 2 [37/340] - Loss: 4.007 [-3.996, 0.011, -0.000]\n",
      "Epoch 2 [38/340] - Loss: 4.133 [-4.122, 0.011, -0.000]\n",
      "Epoch 2 [39/340] - Loss: 4.030 [-4.019, 0.010, -0.000]\n",
      "Epoch 2 [40/340] - Loss: 4.088 [-4.078, 0.010, -0.000]\n",
      "Epoch 2 [41/340] - Loss: 4.061 [-4.051, 0.010, -0.000]\n",
      "Epoch 2 [42/340] - Loss: 4.060 [-4.051, 0.009, -0.000]\n",
      "Epoch 2 [43/340] - Loss: 4.106 [-4.096, 0.010, -0.000]\n",
      "Epoch 2 [44/340] - Loss: 4.164 [-4.154, 0.010, -0.000]\n",
      "Epoch 2 [45/340] - Loss: 4.096 [-4.086, 0.010, -0.000]\n",
      "Epoch 2 [46/340] - Loss: 4.133 [-4.123, 0.010, -0.000]\n",
      "Epoch 2 [47/340] - Loss: 4.051 [-4.040, 0.010, -0.000]\n",
      "Epoch 2 [48/340] - Loss: 4.084 [-4.074, 0.010, -0.000]\n",
      "Epoch 2 [49/340] - Loss: 4.048 [-4.038, 0.011, -0.000]\n",
      "Epoch 2 [50/340] - Loss: 4.171 [-4.160, 0.011, -0.000]\n",
      "Epoch 2 [51/340] - Loss: 4.178 [-4.167, 0.011, -0.000]\n",
      "Epoch 2 [52/340] - Loss: 4.098 [-4.088, 0.011, -0.000]\n",
      "Epoch 2 [53/340] - Loss: 4.169 [-4.158, 0.011, -0.000]\n",
      "Epoch 2 [54/340] - Loss: 4.084 [-4.073, 0.011, -0.000]\n",
      "Epoch 2 [55/340] - Loss: 4.134 [-4.123, 0.011, -0.000]\n",
      "Epoch 2 [56/340] - Loss: 4.068 [-4.058, 0.010, -0.000]\n",
      "Epoch 2 [57/340] - Loss: 4.214 [-4.204, 0.010, -0.000]\n",
      "Epoch 2 [58/340] - Loss: 4.220 [-4.211, 0.010, -0.000]\n",
      "Epoch 2 [59/340] - Loss: 4.199 [-4.189, 0.010, -0.000]\n",
      "Epoch 2 [60/340] - Loss: 4.149 [-4.139, 0.010, -0.000]\n",
      "Epoch 2 [61/340] - Loss: 4.237 [-4.226, 0.011, -0.000]\n",
      "Epoch 2 [62/340] - Loss: 4.197 [-4.186, 0.011, -0.000]\n",
      "Epoch 2 [63/340] - Loss: 4.069 [-4.058, 0.011, -0.000]\n",
      "Epoch 2 [64/340] - Loss: 4.109 [-4.099, 0.011, -0.000]\n",
      "Epoch 2 [65/340] - Loss: 4.090 [-4.079, 0.011, -0.000]\n",
      "Epoch 2 [66/340] - Loss: 4.188 [-4.177, 0.011, -0.000]\n",
      "Epoch 2 [67/340] - Loss: 4.158 [-4.147, 0.011, -0.000]\n",
      "Epoch 2 [68/340] - Loss: 4.101 [-4.090, 0.011, -0.000]\n",
      "Epoch 2 [69/340] - Loss: 4.124 [-4.112, 0.011, -0.000]\n",
      "Epoch 2 [70/340] - Loss: 4.139 [-4.128, 0.011, -0.000]\n",
      "Epoch 2 [71/340] - Loss: 4.154 [-4.142, 0.011, -0.000]\n",
      "Epoch 2 [72/340] - Loss: 4.148 [-4.137, 0.011, -0.000]\n",
      "Epoch 2 [73/340] - Loss: 4.245 [-4.234, 0.011, -0.000]\n",
      "Epoch 2 [74/340] - Loss: 4.083 [-4.073, 0.010, -0.000]\n",
      "Epoch 2 [75/340] - Loss: 4.109 [-4.099, 0.010, -0.000]\n",
      "Epoch 2 [76/340] - Loss: 4.262 [-4.253, 0.009, -0.000]\n",
      "Epoch 2 [77/340] - Loss: 4.064 [-4.056, 0.008, -0.000]\n",
      "Epoch 2 [78/340] - Loss: 4.127 [-4.119, 0.008, -0.000]\n",
      "Epoch 2 [79/340] - Loss: 4.124 [-4.116, 0.008, -0.000]\n",
      "Epoch 2 [80/340] - Loss: 4.215 [-4.207, 0.008, -0.000]\n",
      "Epoch 2 [81/340] - Loss: 4.146 [-4.138, 0.008, -0.000]\n",
      "Epoch 2 [82/340] - Loss: 4.125 [-4.117, 0.008, -0.000]\n",
      "Epoch 2 [83/340] - Loss: 4.070 [-4.062, 0.008, -0.000]\n",
      "Epoch 2 [84/340] - Loss: 4.127 [-4.119, 0.008, -0.000]\n",
      "Epoch 2 [85/340] - Loss: 4.084 [-4.076, 0.008, -0.000]\n",
      "Epoch 2 [86/340] - Loss: 4.020 [-4.011, 0.008, -0.000]\n",
      "Epoch 2 [87/340] - Loss: 4.053 [-4.045, 0.009, -0.000]\n",
      "Epoch 2 [88/340] - Loss: 4.077 [-4.068, 0.009, -0.000]\n",
      "Epoch 2 [89/340] - Loss: 4.048 [-4.039, 0.009, -0.000]\n",
      "Epoch 2 [90/340] - Loss: 3.990 [-3.980, 0.010, -0.000]\n",
      "Epoch 2 [91/340] - Loss: 3.973 [-3.963, 0.010, -0.000]\n",
      "Epoch 2 [92/340] - Loss: 4.023 [-4.013, 0.010, -0.000]\n",
      "Epoch 2 [93/340] - Loss: 4.048 [-4.037, 0.010, -0.000]\n",
      "Epoch 2 [94/340] - Loss: 4.081 [-4.070, 0.011, -0.000]\n",
      "Epoch 2 [95/340] - Loss: 4.143 [-4.132, 0.010, -0.000]\n",
      "Epoch 2 [96/340] - Loss: 4.027 [-4.017, 0.011, -0.000]\n",
      "Epoch 2 [97/340] - Loss: 4.044 [-4.033, 0.011, -0.000]\n",
      "Epoch 2 [98/340] - Loss: 4.005 [-3.994, 0.012, -0.000]\n",
      "Epoch 2 [99/340] - Loss: 4.113 [-4.101, 0.012, -0.000]\n",
      "Epoch 2 [100/340] - Loss: 4.109 [-4.097, 0.012, -0.000]\n",
      "Epoch 2 [101/340] - Loss: 4.088 [-4.077, 0.011, -0.000]\n",
      "Epoch 2 [102/340] - Loss: 4.004 [-3.994, 0.010, -0.000]\n",
      "Epoch 2 [103/340] - Loss: 4.057 [-4.047, 0.010, -0.000]\n",
      "Epoch 2 [104/340] - Loss: 3.984 [-3.975, 0.009, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [105/340] - Loss: 4.050 [-4.041, 0.009, -0.000]\n",
      "Epoch 2 [106/340] - Loss: 4.066 [-4.058, 0.008, -0.000]\n",
      "Epoch 2 [107/340] - Loss: 4.174 [-4.165, 0.008, -0.000]\n",
      "Epoch 2 [108/340] - Loss: 4.149 [-4.141, 0.008, -0.000]\n",
      "Epoch 2 [109/340] - Loss: 4.095 [-4.087, 0.009, -0.000]\n",
      "Epoch 2 [110/340] - Loss: 3.992 [-3.983, 0.009, -0.000]\n",
      "Epoch 2 [111/340] - Loss: 4.182 [-4.173, 0.009, -0.000]\n",
      "Epoch 2 [112/340] - Loss: 4.079 [-4.069, 0.009, -0.000]\n",
      "Epoch 2 [113/340] - Loss: 4.056 [-4.047, 0.009, -0.000]\n",
      "Epoch 2 [114/340] - Loss: 4.200 [-4.192, 0.009, -0.000]\n",
      "Epoch 2 [115/340] - Loss: 3.974 [-3.965, 0.009, -0.000]\n",
      "Epoch 2 [116/340] - Loss: 4.180 [-4.171, 0.009, -0.000]\n",
      "Epoch 2 [117/340] - Loss: 4.110 [-4.101, 0.009, -0.000]\n",
      "Epoch 2 [118/340] - Loss: 3.956 [-3.947, 0.009, -0.000]\n",
      "Epoch 2 [119/340] - Loss: 4.125 [-4.116, 0.009, -0.000]\n",
      "Epoch 2 [120/340] - Loss: 4.037 [-4.028, 0.009, -0.000]\n",
      "Epoch 2 [121/340] - Loss: 4.103 [-4.094, 0.009, -0.000]\n",
      "Epoch 2 [122/340] - Loss: 4.161 [-4.153, 0.008, -0.000]\n",
      "Epoch 2 [123/340] - Loss: 4.130 [-4.122, 0.007, -0.000]\n",
      "Epoch 2 [124/340] - Loss: 4.067 [-4.060, 0.007, -0.000]\n",
      "Epoch 2 [125/340] - Loss: 4.053 [-4.046, 0.006, -0.000]\n",
      "Epoch 2 [126/340] - Loss: 4.069 [-4.063, 0.006, -0.000]\n",
      "Epoch 2 [127/340] - Loss: 3.996 [-3.990, 0.006, -0.000]\n",
      "Epoch 2 [128/340] - Loss: 4.059 [-4.053, 0.006, -0.000]\n",
      "Epoch 2 [129/340] - Loss: 3.980 [-3.974, 0.006, -0.000]\n",
      "Epoch 2 [130/340] - Loss: 4.003 [-3.997, 0.006, -0.000]\n",
      "Epoch 2 [131/340] - Loss: 4.132 [-4.125, 0.006, -0.000]\n",
      "Epoch 2 [132/340] - Loss: 4.094 [-4.087, 0.007, -0.000]\n",
      "Epoch 2 [133/340] - Loss: 4.147 [-4.140, 0.008, -0.000]\n",
      "Epoch 2 [134/340] - Loss: 4.104 [-4.096, 0.008, -0.000]\n",
      "Epoch 2 [135/340] - Loss: 4.049 [-4.041, 0.008, -0.000]\n",
      "Epoch 2 [136/340] - Loss: 4.149 [-4.141, 0.008, -0.000]\n",
      "Epoch 2 [137/340] - Loss: 4.099 [-4.092, 0.007, -0.000]\n",
      "Epoch 2 [138/340] - Loss: 4.131 [-4.124, 0.007, -0.000]\n",
      "Epoch 2 [139/340] - Loss: 4.095 [-4.088, 0.007, -0.000]\n",
      "Epoch 2 [140/340] - Loss: 4.029 [-4.022, 0.007, -0.000]\n",
      "Epoch 2 [141/340] - Loss: 4.105 [-4.097, 0.007, -0.000]\n",
      "Epoch 2 [142/340] - Loss: 4.138 [-4.130, 0.007, -0.000]\n",
      "Epoch 2 [143/340] - Loss: 4.172 [-4.165, 0.007, -0.000]\n",
      "Epoch 2 [144/340] - Loss: 4.094 [-4.087, 0.007, -0.000]\n",
      "Epoch 2 [145/340] - Loss: 4.016 [-4.009, 0.007, -0.000]\n",
      "Epoch 2 [146/340] - Loss: 4.053 [-4.046, 0.007, -0.000]\n",
      "Epoch 2 [147/340] - Loss: 4.007 [-4.000, 0.007, -0.000]\n",
      "Epoch 2 [148/340] - Loss: 3.940 [-3.933, 0.007, -0.000]\n",
      "Epoch 2 [149/340] - Loss: 4.153 [-4.146, 0.007, -0.000]\n",
      "Epoch 2 [150/340] - Loss: 4.476 [-4.468, 0.008, -0.000]\n",
      "Epoch 2 [151/340] - Loss: 4.283 [-4.276, 0.007, -0.000]\n",
      "Epoch 2 [152/340] - Loss: 4.305 [-4.299, 0.006, -0.000]\n",
      "Epoch 2 [153/340] - Loss: 4.107 [-4.101, 0.006, -0.000]\n",
      "Epoch 2 [154/340] - Loss: 4.227 [-4.222, 0.006, -0.000]\n",
      "Epoch 2 [155/340] - Loss: 4.163 [-4.157, 0.006, -0.000]\n",
      "Epoch 2 [156/340] - Loss: 4.097 [-4.091, 0.005, -0.000]\n",
      "Epoch 2 [157/340] - Loss: 4.086 [-4.081, 0.005, -0.000]\n",
      "Epoch 2 [158/340] - Loss: 4.140 [-4.134, 0.005, -0.000]\n",
      "Epoch 2 [159/340] - Loss: 4.085 [-4.080, 0.005, -0.000]\n",
      "Epoch 2 [160/340] - Loss: 4.061 [-4.056, 0.005, -0.000]\n",
      "Epoch 2 [161/340] - Loss: 4.074 [-4.069, 0.006, -0.000]\n",
      "Epoch 2 [162/340] - Loss: 4.030 [-4.024, 0.005, -0.000]\n",
      "Epoch 2 [163/340] - Loss: 4.025 [-4.020, 0.006, -0.000]\n",
      "Epoch 2 [164/340] - Loss: 4.109 [-4.103, 0.006, -0.000]\n",
      "Epoch 2 [165/340] - Loss: 4.056 [-4.050, 0.006, -0.000]\n",
      "Epoch 2 [166/340] - Loss: 3.990 [-3.984, 0.006, -0.000]\n",
      "Epoch 2 [167/340] - Loss: 3.995 [-3.989, 0.006, -0.000]\n",
      "Epoch 2 [168/340] - Loss: 4.014 [-4.007, 0.007, -0.000]\n",
      "Epoch 2 [169/340] - Loss: 4.002 [-3.995, 0.007, -0.000]\n",
      "Epoch 2 [170/340] - Loss: 4.074 [-4.067, 0.007, -0.000]\n",
      "Epoch 2 [171/340] - Loss: 4.059 [-4.051, 0.007, -0.000]\n",
      "Epoch 2 [172/340] - Loss: 4.043 [-4.036, 0.008, -0.000]\n",
      "Epoch 2 [173/340] - Loss: 4.040 [-4.032, 0.008, -0.000]\n",
      "Epoch 2 [174/340] - Loss: 3.941 [-3.933, 0.008, -0.000]\n",
      "Epoch 2 [175/340] - Loss: 4.023 [-4.015, 0.008, -0.000]\n",
      "Epoch 2 [176/340] - Loss: 4.062 [-4.053, 0.008, -0.000]\n",
      "Epoch 2 [177/340] - Loss: 4.079 [-4.071, 0.009, -0.000]\n",
      "Epoch 2 [178/340] - Loss: 4.081 [-4.073, 0.009, -0.000]\n",
      "Epoch 2 [179/340] - Loss: 4.128 [-4.119, 0.009, -0.000]\n",
      "Epoch 2 [180/340] - Loss: 4.094 [-4.086, 0.008, -0.000]\n",
      "Epoch 2 [181/340] - Loss: 4.092 [-4.084, 0.008, -0.000]\n",
      "Epoch 2 [182/340] - Loss: 4.134 [-4.126, 0.008, -0.000]\n",
      "Epoch 2 [183/340] - Loss: 4.077 [-4.069, 0.008, -0.000]\n",
      "Epoch 2 [184/340] - Loss: 4.093 [-4.084, 0.008, -0.000]\n",
      "Epoch 2 [185/340] - Loss: 4.037 [-4.029, 0.009, -0.000]\n",
      "Epoch 2 [186/340] - Loss: 3.988 [-3.980, 0.009, -0.000]\n",
      "Epoch 2 [187/340] - Loss: 4.100 [-4.091, 0.009, -0.000]\n",
      "Epoch 2 [188/340] - Loss: 4.067 [-4.060, 0.007, -0.000]\n",
      "Epoch 2 [189/340] - Loss: 4.086 [-4.080, 0.006, -0.000]\n",
      "Epoch 2 [190/340] - Loss: 4.014 [-4.008, 0.006, -0.000]\n",
      "Epoch 2 [191/340] - Loss: 4.046 [-4.040, 0.006, -0.000]\n",
      "Epoch 2 [192/340] - Loss: 4.085 [-4.079, 0.006, -0.000]\n",
      "Epoch 2 [193/340] - Loss: 4.011 [-4.005, 0.006, -0.000]\n",
      "Epoch 2 [194/340] - Loss: 4.116 [-4.110, 0.006, -0.000]\n",
      "Epoch 2 [195/340] - Loss: 3.960 [-3.954, 0.006, -0.000]\n",
      "Epoch 2 [196/340] - Loss: 4.153 [-4.148, 0.006, -0.000]\n",
      "Epoch 2 [197/340] - Loss: 4.059 [-4.053, 0.006, -0.000]\n",
      "Epoch 2 [198/340] - Loss: 4.057 [-4.051, 0.006, -0.000]\n",
      "Epoch 2 [199/340] - Loss: 4.049 [-4.043, 0.006, -0.000]\n",
      "Epoch 2 [200/340] - Loss: 3.994 [-3.988, 0.006, -0.000]\n",
      "Epoch 2 [201/340] - Loss: 4.050 [-4.043, 0.006, -0.000]\n",
      "Epoch 2 [202/340] - Loss: 3.976 [-3.969, 0.007, -0.000]\n",
      "Epoch 2 [203/340] - Loss: 3.993 [-3.986, 0.007, -0.000]\n",
      "Epoch 2 [204/340] - Loss: 4.031 [-4.024, 0.007, -0.000]\n",
      "Epoch 2 [205/340] - Loss: 4.005 [-3.998, 0.007, -0.000]\n",
      "Epoch 2 [206/340] - Loss: 4.021 [-4.013, 0.007, -0.000]\n",
      "Epoch 2 [207/340] - Loss: 3.955 [-3.948, 0.008, -0.000]\n",
      "Epoch 2 [208/340] - Loss: 4.009 [-4.002, 0.008, -0.000]\n",
      "Epoch 2 [209/340] - Loss: 4.022 [-4.015, 0.007, -0.000]\n",
      "Epoch 2 [210/340] - Loss: 4.038 [-4.031, 0.006, -0.000]\n",
      "Epoch 2 [211/340] - Loss: 4.003 [-3.997, 0.006, -0.000]\n",
      "Epoch 2 [212/340] - Loss: 4.023 [-4.018, 0.006, -0.000]\n",
      "Epoch 2 [213/340] - Loss: 4.050 [-4.044, 0.006, -0.000]\n",
      "Epoch 2 [214/340] - Loss: 4.061 [-4.055, 0.006, -0.000]\n",
      "Epoch 2 [215/340] - Loss: 3.996 [-3.990, 0.006, -0.000]\n",
      "Epoch 2 [216/340] - Loss: 4.030 [-4.024, 0.006, -0.000]\n",
      "Epoch 2 [217/340] - Loss: 4.042 [-4.036, 0.006, -0.000]\n",
      "Epoch 2 [218/340] - Loss: 4.112 [-4.106, 0.006, -0.000]\n",
      "Epoch 2 [219/340] - Loss: 3.983 [-3.978, 0.006, -0.000]\n",
      "Epoch 2 [220/340] - Loss: 4.036 [-4.031, 0.006, -0.000]\n",
      "Epoch 2 [221/340] - Loss: 4.004 [-3.998, 0.006, -0.000]\n",
      "Epoch 2 [222/340] - Loss: 3.955 [-3.950, 0.005, -0.000]\n",
      "Epoch 2 [223/340] - Loss: 4.005 [-3.999, 0.006, -0.000]\n",
      "Epoch 2 [224/340] - Loss: 4.031 [-4.026, 0.006, -0.000]\n",
      "Epoch 2 [225/340] - Loss: 4.060 [-4.054, 0.006, -0.000]\n",
      "Epoch 2 [226/340] - Loss: 4.099 [-4.093, 0.006, -0.000]\n",
      "Epoch 2 [227/340] - Loss: 4.080 [-4.074, 0.006, -0.000]\n",
      "Epoch 2 [228/340] - Loss: 4.050 [-4.044, 0.006, -0.000]\n",
      "Epoch 2 [229/340] - Loss: 4.023 [-4.017, 0.006, -0.000]\n",
      "Epoch 2 [230/340] - Loss: 4.035 [-4.029, 0.006, -0.000]\n",
      "Epoch 2 [231/340] - Loss: 4.034 [-4.027, 0.007, -0.000]\n",
      "Epoch 2 [232/340] - Loss: 4.179 [-4.172, 0.007, -0.000]\n",
      "Epoch 2 [233/340] - Loss: 4.098 [-4.091, 0.007, -0.000]\n",
      "Epoch 2 [234/340] - Loss: 3.993 [-3.986, 0.007, -0.000]\n",
      "Epoch 2 [235/340] - Loss: 4.001 [-3.994, 0.007, -0.000]\n",
      "Epoch 2 [236/340] - Loss: 4.037 [-4.031, 0.007, -0.000]\n",
      "Epoch 2 [237/340] - Loss: 4.081 [-4.074, 0.007, -0.000]\n",
      "Epoch 2 [238/340] - Loss: 3.994 [-3.987, 0.007, -0.000]\n",
      "Epoch 2 [239/340] - Loss: 4.084 [-4.077, 0.007, -0.000]\n",
      "Epoch 2 [240/340] - Loss: 3.998 [-3.991, 0.007, -0.000]\n",
      "Epoch 2 [241/340] - Loss: 4.101 [-4.094, 0.007, -0.000]\n",
      "Epoch 2 [242/340] - Loss: 4.011 [-4.004, 0.007, -0.000]\n",
      "Epoch 2 [243/340] - Loss: 4.104 [-4.097, 0.007, -0.000]\n",
      "Epoch 2 [244/340] - Loss: 4.062 [-4.055, 0.007, -0.000]\n",
      "Epoch 2 [245/340] - Loss: 4.045 [-4.037, 0.007, -0.000]\n",
      "Epoch 2 [246/340] - Loss: 4.169 [-4.162, 0.007, -0.000]\n",
      "Epoch 2 [247/340] - Loss: 4.170 [-4.163, 0.007, -0.000]\n",
      "Epoch 2 [248/340] - Loss: 4.119 [-4.112, 0.007, -0.000]\n",
      "Epoch 2 [249/340] - Loss: 4.140 [-4.134, 0.007, -0.000]\n",
      "Epoch 2 [250/340] - Loss: 3.982 [-3.975, 0.007, -0.000]\n",
      "Epoch 2 [251/340] - Loss: 4.100 [-4.093, 0.007, -0.000]\n",
      "Epoch 2 [252/340] - Loss: 4.028 [-4.022, 0.007, -0.000]\n",
      "Epoch 2 [253/340] - Loss: 4.089 [-4.083, 0.007, -0.000]\n",
      "Epoch 2 [254/340] - Loss: 4.007 [-4.000, 0.007, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [255/340] - Loss: 4.051 [-4.045, 0.007, -0.000]\n",
      "Epoch 2 [256/340] - Loss: 4.124 [-4.118, 0.006, -0.000]\n",
      "Epoch 2 [257/340] - Loss: 4.033 [-4.027, 0.006, -0.000]\n",
      "Epoch 2 [258/340] - Loss: 3.994 [-3.989, 0.005, -0.000]\n",
      "Epoch 2 [259/340] - Loss: 4.005 [-3.999, 0.005, -0.000]\n",
      "Epoch 2 [260/340] - Loss: 4.025 [-4.020, 0.005, -0.000]\n",
      "Epoch 2 [261/340] - Loss: 4.073 [-4.067, 0.005, -0.000]\n",
      "Epoch 2 [262/340] - Loss: 4.095 [-4.090, 0.005, -0.000]\n",
      "Epoch 2 [263/340] - Loss: 4.086 [-4.081, 0.005, -0.000]\n",
      "Epoch 2 [264/340] - Loss: 4.063 [-4.059, 0.005, -0.000]\n",
      "Epoch 2 [265/340] - Loss: 4.028 [-4.023, 0.005, -0.000]\n",
      "Epoch 2 [266/340] - Loss: 4.075 [-4.070, 0.005, -0.000]\n",
      "Epoch 2 [267/340] - Loss: 4.082 [-4.077, 0.005, -0.000]\n",
      "Epoch 2 [268/340] - Loss: 4.003 [-3.999, 0.005, -0.000]\n",
      "Epoch 2 [269/340] - Loss: 4.120 [-4.115, 0.005, -0.000]\n",
      "Epoch 2 [270/340] - Loss: 4.037 [-4.032, 0.005, -0.000]\n",
      "Epoch 2 [271/340] - Loss: 3.984 [-3.979, 0.005, -0.000]\n",
      "Epoch 2 [272/340] - Loss: 4.075 [-4.070, 0.005, -0.000]\n",
      "Epoch 2 [273/340] - Loss: 4.044 [-4.039, 0.005, -0.000]\n",
      "Epoch 2 [274/340] - Loss: 4.115 [-4.110, 0.005, -0.000]\n",
      "Epoch 2 [275/340] - Loss: 4.038 [-4.033, 0.005, -0.000]\n",
      "Epoch 2 [276/340] - Loss: 4.017 [-4.011, 0.005, -0.000]\n",
      "Epoch 2 [277/340] - Loss: 4.082 [-4.076, 0.005, -0.000]\n",
      "Epoch 2 [278/340] - Loss: 3.998 [-3.993, 0.005, -0.000]\n",
      "Epoch 2 [279/340] - Loss: 3.988 [-3.983, 0.005, -0.000]\n",
      "Epoch 2 [280/340] - Loss: 4.037 [-4.032, 0.005, -0.000]\n",
      "Epoch 2 [281/340] - Loss: 4.089 [-4.083, 0.005, -0.000]\n",
      "Epoch 2 [282/340] - Loss: 4.033 [-4.028, 0.006, -0.000]\n",
      "Epoch 2 [283/340] - Loss: 3.995 [-3.989, 0.006, -0.000]\n",
      "Epoch 2 [284/340] - Loss: 4.040 [-4.034, 0.006, -0.000]\n",
      "Epoch 2 [285/340] - Loss: 3.972 [-3.966, 0.006, -0.000]\n",
      "Epoch 2 [286/340] - Loss: 4.078 [-4.072, 0.006, -0.000]\n",
      "Epoch 2 [287/340] - Loss: 4.004 [-3.998, 0.006, -0.000]\n",
      "Epoch 2 [288/340] - Loss: 4.026 [-4.020, 0.007, -0.000]\n",
      "Epoch 2 [289/340] - Loss: 4.085 [-4.078, 0.007, -0.000]\n",
      "Epoch 2 [290/340] - Loss: 3.994 [-3.986, 0.007, -0.000]\n",
      "Epoch 2 [291/340] - Loss: 4.052 [-4.045, 0.007, -0.000]\n",
      "Epoch 2 [292/340] - Loss: 4.031 [-4.023, 0.007, -0.000]\n",
      "Epoch 2 [293/340] - Loss: 3.992 [-3.985, 0.007, -0.000]\n",
      "Epoch 2 [294/340] - Loss: 4.030 [-4.022, 0.007, -0.000]\n",
      "Epoch 2 [295/340] - Loss: 4.013 [-4.006, 0.007, -0.000]\n",
      "Epoch 2 [296/340] - Loss: 4.026 [-4.019, 0.007, -0.000]\n",
      "Epoch 2 [297/340] - Loss: 4.077 [-4.071, 0.007, -0.000]\n",
      "Epoch 2 [298/340] - Loss: 4.046 [-4.040, 0.007, -0.000]\n",
      "Epoch 2 [299/340] - Loss: 4.024 [-4.018, 0.006, -0.000]\n",
      "Epoch 2 [300/340] - Loss: 4.052 [-4.046, 0.006, -0.000]\n",
      "Epoch 2 [301/340] - Loss: 4.099 [-4.093, 0.006, -0.000]\n",
      "Epoch 2 [302/340] - Loss: 3.975 [-3.969, 0.006, -0.000]\n",
      "Epoch 2 [303/340] - Loss: 4.103 [-4.097, 0.005, -0.000]\n",
      "Epoch 2 [304/340] - Loss: 4.028 [-4.022, 0.005, -0.000]\n",
      "Epoch 2 [305/340] - Loss: 4.041 [-4.036, 0.005, -0.000]\n",
      "Epoch 2 [306/340] - Loss: 4.097 [-4.092, 0.005, -0.000]\n",
      "Epoch 2 [307/340] - Loss: 4.081 [-4.076, 0.005, -0.000]\n",
      "Epoch 2 [308/340] - Loss: 4.095 [-4.090, 0.005, -0.000]\n",
      "Epoch 2 [309/340] - Loss: 4.046 [-4.041, 0.005, -0.000]\n",
      "Epoch 2 [310/340] - Loss: 4.046 [-4.042, 0.005, -0.000]\n",
      "Epoch 2 [311/340] - Loss: 4.056 [-4.051, 0.005, -0.000]\n",
      "Epoch 2 [312/340] - Loss: 3.955 [-3.950, 0.005, -0.000]\n",
      "Epoch 2 [313/340] - Loss: 4.030 [-4.025, 0.005, -0.000]\n",
      "Epoch 2 [314/340] - Loss: 4.037 [-4.032, 0.006, -0.000]\n",
      "Epoch 2 [315/340] - Loss: 4.053 [-4.048, 0.006, -0.000]\n",
      "Epoch 2 [316/340] - Loss: 3.987 [-3.981, 0.006, -0.000]\n",
      "Epoch 2 [317/340] - Loss: 4.039 [-4.034, 0.005, -0.000]\n",
      "Epoch 2 [318/340] - Loss: 4.026 [-4.022, 0.005, -0.000]\n",
      "Epoch 2 [319/340] - Loss: 3.980 [-3.975, 0.005, -0.000]\n",
      "Epoch 2 [320/340] - Loss: 4.047 [-4.042, 0.004, -0.000]\n",
      "Epoch 2 [321/340] - Loss: 4.055 [-4.051, 0.004, -0.000]\n",
      "Epoch 2 [322/340] - Loss: 4.008 [-4.004, 0.004, -0.000]\n",
      "Epoch 2 [323/340] - Loss: 3.979 [-3.974, 0.004, -0.000]\n",
      "Epoch 2 [324/340] - Loss: 4.037 [-4.033, 0.004, -0.000]\n",
      "Epoch 2 [325/340] - Loss: 4.043 [-4.039, 0.004, -0.000]\n",
      "Epoch 2 [326/340] - Loss: 4.123 [-4.119, 0.004, -0.000]\n",
      "Epoch 2 [327/340] - Loss: 4.041 [-4.037, 0.004, -0.000]\n",
      "Epoch 2 [328/340] - Loss: 3.993 [-3.989, 0.004, -0.000]\n",
      "Epoch 2 [329/340] - Loss: 4.038 [-4.034, 0.004, -0.000]\n",
      "Epoch 2 [330/340] - Loss: 4.015 [-4.012, 0.004, -0.000]\n",
      "Epoch 2 [331/340] - Loss: 4.027 [-4.023, 0.004, -0.000]\n",
      "Epoch 2 [332/340] - Loss: 4.116 [-4.112, 0.004, -0.000]\n",
      "Epoch 2 [333/340] - Loss: 4.026 [-4.022, 0.004, -0.000]\n",
      "Epoch 2 [334/340] - Loss: 4.058 [-4.054, 0.004, -0.000]\n",
      "Epoch 2 [335/340] - Loss: 4.002 [-3.997, 0.004, -0.000]\n",
      "Epoch 2 [336/340] - Loss: 4.062 [-4.058, 0.004, -0.000]\n",
      "Epoch 2 [337/340] - Loss: 4.044 [-4.040, 0.004, -0.000]\n",
      "Epoch 2 [338/340] - Loss: 4.063 [-4.058, 0.005, -0.000]\n",
      "Epoch 2 [339/340] - Loss: 3.964 [-3.959, 0.005, -0.000]\n",
      "Epoch 3 [0/340] - Loss: 3.958 [-3.952, 0.006, -0.000]\n",
      "Epoch 3 [1/340] - Loss: 3.995 [-3.989, 0.006, -0.000]\n",
      "Epoch 3 [2/340] - Loss: 4.097 [-4.090, 0.006, -0.000]\n",
      "Epoch 3 [3/340] - Loss: 4.040 [-4.033, 0.007, -0.000]\n",
      "Epoch 3 [4/340] - Loss: 4.027 [-4.019, 0.007, -0.000]\n",
      "Epoch 3 [5/340] - Loss: 4.051 [-4.043, 0.008, -0.000]\n",
      "Epoch 3 [6/340] - Loss: 3.946 [-3.939, 0.007, -0.000]\n",
      "Epoch 3 [7/340] - Loss: 4.073 [-4.066, 0.007, -0.000]\n",
      "Epoch 3 [8/340] - Loss: 4.016 [-4.010, 0.006, -0.000]\n",
      "Epoch 3 [9/340] - Loss: 3.987 [-3.981, 0.006, -0.000]\n",
      "Epoch 3 [10/340] - Loss: 4.066 [-4.060, 0.005, -0.000]\n",
      "Epoch 3 [11/340] - Loss: 4.024 [-4.019, 0.005, -0.000]\n",
      "Epoch 3 [12/340] - Loss: 4.022 [-4.017, 0.005, -0.000]\n",
      "Epoch 3 [13/340] - Loss: 4.135 [-4.131, 0.005, -0.000]\n",
      "Epoch 3 [14/340] - Loss: 4.077 [-4.072, 0.004, -0.000]\n",
      "Epoch 3 [15/340] - Loss: 4.083 [-4.078, 0.004, -0.000]\n",
      "Epoch 3 [16/340] - Loss: 4.158 [-4.153, 0.004, -0.000]\n",
      "Epoch 3 [17/340] - Loss: 4.143 [-4.138, 0.005, -0.000]\n",
      "Epoch 3 [18/340] - Loss: 4.055 [-4.051, 0.005, -0.000]\n",
      "Epoch 3 [19/340] - Loss: 3.999 [-3.995, 0.005, -0.000]\n",
      "Epoch 3 [20/340] - Loss: 4.101 [-4.096, 0.005, -0.000]\n",
      "Epoch 3 [21/340] - Loss: 4.148 [-4.143, 0.005, -0.000]\n",
      "Epoch 3 [22/340] - Loss: 3.990 [-3.985, 0.005, -0.000]\n",
      "Epoch 3 [23/340] - Loss: 3.974 [-3.969, 0.005, -0.000]\n",
      "Epoch 3 [24/340] - Loss: 4.102 [-4.097, 0.005, -0.000]\n",
      "Epoch 3 [25/340] - Loss: 4.029 [-4.024, 0.005, -0.000]\n",
      "Epoch 3 [26/340] - Loss: 4.032 [-4.027, 0.005, -0.000]\n",
      "Epoch 3 [27/340] - Loss: 4.010 [-4.004, 0.005, -0.000]\n",
      "Epoch 3 [28/340] - Loss: 3.940 [-3.934, 0.006, -0.000]\n",
      "Epoch 3 [29/340] - Loss: 4.060 [-4.054, 0.006, -0.000]\n",
      "Epoch 3 [30/340] - Loss: 3.992 [-3.985, 0.006, -0.000]\n",
      "Epoch 3 [31/340] - Loss: 4.085 [-4.079, 0.006, -0.000]\n",
      "Epoch 3 [32/340] - Loss: 4.067 [-4.061, 0.007, -0.000]\n",
      "Epoch 3 [33/340] - Loss: 4.013 [-4.006, 0.007, -0.000]\n",
      "Epoch 3 [34/340] - Loss: 3.979 [-3.971, 0.008, -0.000]\n",
      "Epoch 3 [35/340] - Loss: 3.986 [-3.977, 0.009, -0.000]\n",
      "Epoch 3 [36/340] - Loss: 4.085 [-4.078, 0.007, -0.000]\n",
      "Epoch 3 [37/340] - Loss: 3.968 [-3.962, 0.006, -0.000]\n",
      "Epoch 3 [38/340] - Loss: 4.005 [-3.999, 0.005, -0.000]\n",
      "Epoch 3 [39/340] - Loss: 4.003 [-3.998, 0.005, -0.000]\n",
      "Epoch 3 [40/340] - Loss: 4.049 [-4.044, 0.005, -0.000]\n",
      "Epoch 3 [41/340] - Loss: 4.016 [-4.011, 0.005, -0.000]\n",
      "Epoch 3 [42/340] - Loss: 3.978 [-3.973, 0.005, -0.000]\n",
      "Epoch 3 [43/340] - Loss: 4.088 [-4.083, 0.005, -0.000]\n",
      "Epoch 3 [44/340] - Loss: 4.053 [-4.048, 0.005, -0.000]\n",
      "Epoch 3 [45/340] - Loss: 4.055 [-4.050, 0.005, -0.000]\n",
      "Epoch 3 [46/340] - Loss: 3.989 [-3.985, 0.005, -0.000]\n",
      "Epoch 3 [47/340] - Loss: 3.961 [-3.956, 0.005, -0.000]\n",
      "Epoch 3 [48/340] - Loss: 4.108 [-4.103, 0.005, -0.000]\n",
      "Epoch 3 [49/340] - Loss: 3.983 [-3.978, 0.005, -0.000]\n",
      "Epoch 3 [50/340] - Loss: 4.093 [-4.088, 0.005, -0.000]\n",
      "Epoch 3 [51/340] - Loss: 4.095 [-4.090, 0.005, -0.000]\n",
      "Epoch 3 [52/340] - Loss: 4.130 [-4.126, 0.005, -0.000]\n",
      "Epoch 3 [53/340] - Loss: 4.088 [-4.084, 0.005, -0.000]\n",
      "Epoch 3 [54/340] - Loss: 4.096 [-4.092, 0.004, -0.000]\n",
      "Epoch 3 [55/340] - Loss: 4.132 [-4.127, 0.004, -0.000]\n",
      "Epoch 3 [56/340] - Loss: 4.039 [-4.035, 0.004, -0.000]\n",
      "Epoch 3 [57/340] - Loss: 4.044 [-4.040, 0.004, -0.000]\n",
      "Epoch 3 [58/340] - Loss: 4.054 [-4.050, 0.004, -0.000]\n",
      "Epoch 3 [59/340] - Loss: 4.075 [-4.071, 0.004, -0.000]\n",
      "Epoch 3 [60/340] - Loss: 3.983 [-3.979, 0.004, -0.000]\n",
      "Epoch 3 [61/340] - Loss: 4.054 [-4.050, 0.004, -0.000]\n",
      "Epoch 3 [62/340] - Loss: 3.989 [-3.985, 0.004, -0.000]\n",
      "Epoch 3 [63/340] - Loss: 4.075 [-4.071, 0.004, -0.000]\n",
      "Epoch 3 [64/340] - Loss: 3.974 [-3.969, 0.004, -0.000]\n",
      "Epoch 3 [65/340] - Loss: 4.063 [-4.059, 0.004, -0.000]\n",
      "Epoch 3 [66/340] - Loss: 4.027 [-4.022, 0.005, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [67/340] - Loss: 4.044 [-4.039, 0.005, -0.000]\n",
      "Epoch 3 [68/340] - Loss: 3.944 [-3.939, 0.005, -0.000]\n",
      "Epoch 3 [69/340] - Loss: 4.054 [-4.048, 0.006, -0.000]\n",
      "Epoch 3 [70/340] - Loss: 4.016 [-4.009, 0.006, -0.000]\n",
      "Epoch 3 [71/340] - Loss: 4.064 [-4.058, 0.006, -0.000]\n",
      "Epoch 3 [72/340] - Loss: 4.049 [-4.043, 0.006, -0.000]\n",
      "Epoch 3 [73/340] - Loss: 4.046 [-4.041, 0.005, -0.000]\n",
      "Epoch 3 [74/340] - Loss: 3.971 [-3.967, 0.005, -0.000]\n",
      "Epoch 3 [75/340] - Loss: 4.021 [-4.016, 0.005, -0.000]\n",
      "Epoch 3 [76/340] - Loss: 4.044 [-4.040, 0.005, -0.000]\n",
      "Epoch 3 [77/340] - Loss: 4.069 [-4.064, 0.004, -0.000]\n",
      "Epoch 3 [78/340] - Loss: 4.055 [-4.051, 0.004, -0.000]\n",
      "Epoch 3 [79/340] - Loss: 4.116 [-4.112, 0.004, -0.000]\n",
      "Epoch 3 [80/340] - Loss: 4.117 [-4.112, 0.004, -0.000]\n",
      "Epoch 3 [81/340] - Loss: 4.116 [-4.111, 0.004, -0.000]\n",
      "Epoch 3 [82/340] - Loss: 4.052 [-4.048, 0.004, -0.000]\n",
      "Epoch 3 [83/340] - Loss: 4.070 [-4.065, 0.004, -0.000]\n",
      "Epoch 3 [84/340] - Loss: 4.011 [-4.007, 0.004, -0.000]\n",
      "Epoch 3 [85/340] - Loss: 4.056 [-4.051, 0.005, -0.000]\n",
      "Epoch 3 [86/340] - Loss: 4.049 [-4.044, 0.005, -0.000]\n",
      "Epoch 3 [87/340] - Loss: 4.044 [-4.040, 0.005, -0.000]\n",
      "Epoch 3 [88/340] - Loss: 4.017 [-4.012, 0.005, -0.000]\n",
      "Epoch 3 [89/340] - Loss: 4.069 [-4.064, 0.005, -0.000]\n",
      "Epoch 3 [90/340] - Loss: 4.036 [-4.030, 0.006, -0.000]\n",
      "Epoch 3 [91/340] - Loss: 4.021 [-4.015, 0.006, -0.000]\n",
      "Epoch 3 [92/340] - Loss: 4.098 [-4.091, 0.006, -0.000]\n",
      "Epoch 3 [93/340] - Loss: 4.070 [-4.064, 0.006, -0.000]\n",
      "Epoch 3 [94/340] - Loss: 4.002 [-3.996, 0.006, -0.000]\n",
      "Epoch 3 [95/340] - Loss: 4.054 [-4.049, 0.005, -0.000]\n",
      "Epoch 3 [96/340] - Loss: 4.066 [-4.061, 0.005, -0.000]\n",
      "Epoch 3 [97/340] - Loss: 4.101 [-4.097, 0.004, -0.000]\n",
      "Epoch 3 [98/340] - Loss: 4.021 [-4.017, 0.004, -0.000]\n",
      "Epoch 3 [99/340] - Loss: 4.080 [-4.076, 0.004, -0.000]\n",
      "Epoch 3 [100/340] - Loss: 4.133 [-4.130, 0.003, -0.000]\n",
      "Epoch 3 [101/340] - Loss: 4.009 [-4.006, 0.003, -0.000]\n",
      "Epoch 3 [102/340] - Loss: 4.106 [-4.103, 0.003, -0.000]\n",
      "Epoch 3 [103/340] - Loss: 3.978 [-3.976, 0.003, -0.000]\n",
      "Epoch 3 [104/340] - Loss: 4.075 [-4.072, 0.003, -0.000]\n",
      "Epoch 3 [105/340] - Loss: 4.049 [-4.046, 0.003, -0.000]\n",
      "Epoch 3 [106/340] - Loss: 4.055 [-4.052, 0.003, -0.000]\n",
      "Epoch 3 [107/340] - Loss: 3.991 [-3.988, 0.003, -0.000]\n",
      "Epoch 3 [108/340] - Loss: 4.065 [-4.062, 0.003, -0.000]\n",
      "Epoch 3 [109/340] - Loss: 4.024 [-4.021, 0.003, -0.000]\n",
      "Epoch 3 [110/340] - Loss: 4.037 [-4.034, 0.003, -0.000]\n",
      "Epoch 3 [111/340] - Loss: 4.059 [-4.057, 0.003, -0.000]\n",
      "Epoch 3 [112/340] - Loss: 4.041 [-4.038, 0.003, -0.000]\n",
      "Epoch 3 [113/340] - Loss: 4.055 [-4.052, 0.003, -0.000]\n",
      "Epoch 3 [114/340] - Loss: 4.125 [-4.122, 0.003, -0.000]\n",
      "Epoch 3 [115/340] - Loss: 4.043 [-4.041, 0.003, -0.000]\n",
      "Epoch 3 [116/340] - Loss: 4.008 [-4.005, 0.003, -0.000]\n",
      "Epoch 3 [117/340] - Loss: 4.101 [-4.098, 0.003, -0.000]\n",
      "Epoch 3 [118/340] - Loss: 3.986 [-3.983, 0.003, -0.000]\n",
      "Epoch 3 [119/340] - Loss: 3.999 [-3.996, 0.003, -0.000]\n",
      "Epoch 3 [120/340] - Loss: 3.972 [-3.969, 0.003, -0.000]\n",
      "Epoch 3 [121/340] - Loss: 4.007 [-4.004, 0.003, -0.000]\n",
      "Epoch 3 [122/340] - Loss: 4.032 [-4.028, 0.003, -0.000]\n",
      "Epoch 3 [123/340] - Loss: 3.965 [-3.962, 0.003, -0.000]\n",
      "Epoch 3 [124/340] - Loss: 3.980 [-3.977, 0.003, -0.000]\n",
      "Epoch 3 [125/340] - Loss: 3.997 [-3.993, 0.003, -0.000]\n",
      "Epoch 3 [126/340] - Loss: 3.980 [-3.977, 0.003, -0.000]\n",
      "Epoch 3 [127/340] - Loss: 4.002 [-3.998, 0.003, -0.000]\n",
      "Epoch 3 [128/340] - Loss: 4.061 [-4.058, 0.003, -0.000]\n",
      "Epoch 3 [129/340] - Loss: 4.034 [-4.031, 0.003, -0.000]\n",
      "Epoch 3 [130/340] - Loss: 4.026 [-4.023, 0.003, -0.000]\n",
      "Epoch 3 [131/340] - Loss: 3.996 [-3.993, 0.003, -0.000]\n",
      "Epoch 3 [132/340] - Loss: 4.031 [-4.028, 0.003, -0.000]\n",
      "Epoch 3 [133/340] - Loss: 4.015 [-4.012, 0.003, -0.000]\n",
      "Epoch 3 [134/340] - Loss: 3.962 [-3.959, 0.003, -0.000]\n",
      "Epoch 3 [135/340] - Loss: 3.968 [-3.965, 0.003, -0.000]\n",
      "Epoch 3 [136/340] - Loss: 3.952 [-3.949, 0.003, -0.000]\n",
      "Epoch 3 [137/340] - Loss: 3.969 [-3.966, 0.003, -0.000]\n",
      "Epoch 3 [138/340] - Loss: 4.070 [-4.066, 0.003, -0.000]\n",
      "Epoch 3 [139/340] - Loss: 3.967 [-3.963, 0.004, -0.000]\n",
      "Epoch 3 [140/340] - Loss: 4.016 [-4.012, 0.004, -0.000]\n",
      "Epoch 3 [141/340] - Loss: 4.044 [-4.040, 0.004, -0.000]\n",
      "Epoch 3 [142/340] - Loss: 3.986 [-3.982, 0.004, -0.000]\n",
      "Epoch 3 [143/340] - Loss: 4.019 [-4.015, 0.004, -0.000]\n",
      "Epoch 3 [144/340] - Loss: 4.056 [-4.052, 0.004, -0.000]\n",
      "Epoch 3 [145/340] - Loss: 4.099 [-4.095, 0.004, -0.000]\n",
      "Epoch 3 [146/340] - Loss: 4.045 [-4.042, 0.004, -0.000]\n",
      "Epoch 3 [147/340] - Loss: 4.060 [-4.056, 0.004, -0.000]\n",
      "Epoch 3 [148/340] - Loss: 4.070 [-4.067, 0.004, -0.000]\n",
      "Epoch 3 [149/340] - Loss: 4.097 [-4.093, 0.003, -0.000]\n",
      "Epoch 3 [150/340] - Loss: 3.991 [-3.988, 0.003, -0.000]\n",
      "Epoch 3 [151/340] - Loss: 4.025 [-4.022, 0.003, -0.000]\n",
      "Epoch 3 [152/340] - Loss: 4.028 [-4.025, 0.003, -0.000]\n",
      "Epoch 3 [153/340] - Loss: 4.028 [-4.025, 0.003, -0.000]\n",
      "Epoch 3 [154/340] - Loss: 4.021 [-4.018, 0.003, -0.000]\n",
      "Epoch 3 [155/340] - Loss: 4.097 [-4.094, 0.003, -0.000]\n",
      "Epoch 3 [156/340] - Loss: 4.106 [-4.103, 0.003, -0.000]\n",
      "Epoch 3 [157/340] - Loss: 4.124 [-4.121, 0.003, -0.000]\n",
      "Epoch 3 [158/340] - Loss: 4.021 [-4.018, 0.003, -0.000]\n",
      "Epoch 3 [159/340] - Loss: 4.038 [-4.035, 0.003, -0.000]\n",
      "Epoch 3 [160/340] - Loss: 4.012 [-4.009, 0.003, -0.000]\n",
      "Epoch 3 [161/340] - Loss: 4.026 [-4.023, 0.003, -0.000]\n",
      "Epoch 3 [162/340] - Loss: 4.036 [-4.032, 0.004, -0.000]\n",
      "Epoch 3 [163/340] - Loss: 4.081 [-4.078, 0.004, -0.000]\n",
      "Epoch 3 [164/340] - Loss: 4.040 [-4.037, 0.004, -0.000]\n",
      "Epoch 3 [165/340] - Loss: 4.092 [-4.088, 0.003, -0.000]\n",
      "Epoch 3 [166/340] - Loss: 4.047 [-4.044, 0.003, -0.000]\n",
      "Epoch 3 [167/340] - Loss: 4.005 [-4.001, 0.003, -0.000]\n",
      "Epoch 3 [168/340] - Loss: 4.040 [-4.036, 0.003, -0.000]\n",
      "Epoch 3 [169/340] - Loss: 4.079 [-4.076, 0.003, -0.000]\n",
      "Epoch 3 [170/340] - Loss: 3.949 [-3.946, 0.003, -0.000]\n",
      "Epoch 3 [171/340] - Loss: 4.158 [-4.155, 0.003, -0.000]\n",
      "Epoch 3 [172/340] - Loss: 4.103 [-4.100, 0.003, -0.000]\n",
      "Epoch 3 [173/340] - Loss: 3.995 [-3.992, 0.003, -0.000]\n",
      "Epoch 3 [174/340] - Loss: 4.023 [-4.021, 0.003, -0.000]\n",
      "Epoch 3 [175/340] - Loss: 4.074 [-4.071, 0.003, -0.000]\n",
      "Epoch 3 [176/340] - Loss: 4.064 [-4.062, 0.003, -0.000]\n",
      "Epoch 3 [177/340] - Loss: 4.067 [-4.064, 0.003, -0.000]\n",
      "Epoch 3 [178/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 3 [179/340] - Loss: 4.050 [-4.048, 0.002, -0.000]\n",
      "Epoch 3 [180/340] - Loss: 4.076 [-4.073, 0.002, -0.000]\n",
      "Epoch 3 [181/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 3 [182/340] - Loss: 4.023 [-4.020, 0.002, -0.000]\n",
      "Epoch 3 [183/340] - Loss: 4.127 [-4.124, 0.002, -0.000]\n",
      "Epoch 3 [184/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 3 [185/340] - Loss: 4.010 [-4.007, 0.002, -0.000]\n",
      "Epoch 3 [186/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 3 [187/340] - Loss: 4.090 [-4.088, 0.002, -0.000]\n",
      "Epoch 3 [188/340] - Loss: 4.053 [-4.051, 0.002, -0.000]\n",
      "Epoch 3 [189/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 3 [190/340] - Loss: 4.067 [-4.065, 0.002, -0.000]\n",
      "Epoch 3 [191/340] - Loss: 4.170 [-4.167, 0.002, -0.000]\n",
      "Epoch 3 [192/340] - Loss: 4.079 [-4.076, 0.002, -0.000]\n",
      "Epoch 3 [193/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 3 [194/340] - Loss: 4.101 [-4.099, 0.002, -0.000]\n",
      "Epoch 3 [195/340] - Loss: 4.081 [-4.079, 0.002, -0.000]\n",
      "Epoch 3 [196/340] - Loss: 4.046 [-4.044, 0.002, -0.000]\n",
      "Epoch 3 [197/340] - Loss: 4.070 [-4.068, 0.002, -0.000]\n",
      "Epoch 3 [198/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 3 [199/340] - Loss: 4.038 [-4.035, 0.002, -0.000]\n",
      "Epoch 3 [200/340] - Loss: 3.970 [-3.967, 0.002, -0.000]\n",
      "Epoch 3 [201/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 3 [202/340] - Loss: 4.091 [-4.088, 0.002, -0.000]\n",
      "Epoch 3 [203/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 3 [204/340] - Loss: 4.077 [-4.074, 0.002, -0.000]\n",
      "Epoch 3 [205/340] - Loss: 4.044 [-4.041, 0.002, -0.000]\n",
      "Epoch 3 [206/340] - Loss: 4.070 [-4.068, 0.002, -0.000]\n",
      "Epoch 3 [207/340] - Loss: 4.034 [-4.032, 0.002, -0.000]\n",
      "Epoch 3 [208/340] - Loss: 4.028 [-4.025, 0.002, -0.000]\n",
      "Epoch 3 [209/340] - Loss: 4.038 [-4.036, 0.002, -0.000]\n",
      "Epoch 3 [210/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 3 [211/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 3 [212/340] - Loss: 4.093 [-4.091, 0.002, -0.000]\n",
      "Epoch 3 [213/340] - Loss: 4.002 [-3.999, 0.003, -0.000]\n",
      "Epoch 3 [214/340] - Loss: 4.024 [-4.021, 0.003, -0.000]\n",
      "Epoch 3 [215/340] - Loss: 4.008 [-4.005, 0.003, -0.000]\n",
      "Epoch 3 [216/340] - Loss: 4.080 [-4.078, 0.003, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [217/340] - Loss: 3.997 [-3.994, 0.003, -0.000]\n",
      "Epoch 3 [218/340] - Loss: 3.985 [-3.982, 0.003, -0.000]\n",
      "Epoch 3 [219/340] - Loss: 4.034 [-4.031, 0.003, -0.000]\n",
      "Epoch 3 [220/340] - Loss: 4.046 [-4.043, 0.003, -0.000]\n",
      "Epoch 3 [221/340] - Loss: 4.090 [-4.087, 0.003, -0.000]\n",
      "Epoch 3 [222/340] - Loss: 4.049 [-4.046, 0.003, -0.000]\n",
      "Epoch 3 [223/340] - Loss: 4.086 [-4.082, 0.003, -0.000]\n",
      "Epoch 3 [224/340] - Loss: 4.041 [-4.038, 0.003, -0.000]\n",
      "Epoch 3 [225/340] - Loss: 3.991 [-3.989, 0.003, -0.000]\n",
      "Epoch 3 [226/340] - Loss: 4.008 [-4.005, 0.003, -0.000]\n",
      "Epoch 3 [227/340] - Loss: 4.100 [-4.098, 0.003, -0.000]\n",
      "Epoch 3 [228/340] - Loss: 4.032 [-4.029, 0.003, -0.000]\n",
      "Epoch 3 [229/340] - Loss: 3.996 [-3.993, 0.003, -0.000]\n",
      "Epoch 3 [230/340] - Loss: 4.008 [-4.005, 0.003, -0.000]\n",
      "Epoch 3 [231/340] - Loss: 4.032 [-4.029, 0.003, -0.000]\n",
      "Epoch 3 [232/340] - Loss: 4.102 [-4.099, 0.003, -0.000]\n",
      "Epoch 3 [233/340] - Loss: 3.969 [-3.965, 0.003, -0.000]\n",
      "Epoch 3 [234/340] - Loss: 4.036 [-4.033, 0.004, -0.000]\n",
      "Epoch 3 [235/340] - Loss: 4.017 [-4.013, 0.004, -0.000]\n",
      "Epoch 3 [236/340] - Loss: 4.058 [-4.055, 0.004, -0.000]\n",
      "Epoch 3 [237/340] - Loss: 4.053 [-4.049, 0.004, -0.000]\n",
      "Epoch 3 [238/340] - Loss: 4.062 [-4.058, 0.004, -0.000]\n",
      "Epoch 3 [239/340] - Loss: 4.017 [-4.014, 0.003, -0.000]\n",
      "Epoch 3 [240/340] - Loss: 4.004 [-4.001, 0.003, -0.000]\n",
      "Epoch 3 [241/340] - Loss: 3.984 [-3.981, 0.002, -0.000]\n",
      "Epoch 3 [242/340] - Loss: 4.079 [-4.077, 0.002, -0.000]\n",
      "Epoch 3 [243/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 3 [244/340] - Loss: 4.073 [-4.071, 0.002, -0.000]\n",
      "Epoch 3 [245/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 3 [246/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 3 [247/340] - Loss: 4.017 [-4.014, 0.002, -0.000]\n",
      "Epoch 3 [248/340] - Loss: 4.098 [-4.096, 0.002, -0.000]\n",
      "Epoch 3 [249/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 3 [250/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 3 [251/340] - Loss: 3.977 [-3.975, 0.003, -0.000]\n",
      "Epoch 3 [252/340] - Loss: 4.050 [-4.047, 0.003, -0.000]\n",
      "Epoch 3 [253/340] - Loss: 4.049 [-4.046, 0.003, -0.000]\n",
      "Epoch 3 [254/340] - Loss: 3.998 [-3.995, 0.003, -0.000]\n",
      "Epoch 3 [255/340] - Loss: 4.023 [-4.021, 0.003, -0.000]\n",
      "Epoch 3 [256/340] - Loss: 3.982 [-3.979, 0.003, -0.000]\n",
      "Epoch 3 [257/340] - Loss: 4.036 [-4.033, 0.003, -0.000]\n",
      "Epoch 3 [258/340] - Loss: 4.094 [-4.091, 0.003, -0.000]\n",
      "Epoch 3 [259/340] - Loss: 3.976 [-3.973, 0.003, -0.000]\n",
      "Epoch 3 [260/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 3 [261/340] - Loss: 3.993 [-3.990, 0.003, -0.000]\n",
      "Epoch 3 [262/340] - Loss: 4.092 [-4.089, 0.003, -0.000]\n",
      "Epoch 3 [263/340] - Loss: 4.087 [-4.083, 0.003, -0.000]\n",
      "Epoch 3 [264/340] - Loss: 4.046 [-4.043, 0.003, -0.000]\n",
      "Epoch 3 [265/340] - Loss: 3.996 [-3.992, 0.003, -0.000]\n",
      "Epoch 3 [266/340] - Loss: 3.988 [-3.985, 0.003, -0.000]\n",
      "Epoch 3 [267/340] - Loss: 4.069 [-4.066, 0.003, -0.000]\n",
      "Epoch 3 [268/340] - Loss: 4.055 [-4.052, 0.003, -0.000]\n",
      "Epoch 3 [269/340] - Loss: 3.994 [-3.991, 0.003, -0.000]\n",
      "Epoch 3 [270/340] - Loss: 3.987 [-3.984, 0.003, -0.000]\n",
      "Epoch 3 [271/340] - Loss: 4.027 [-4.024, 0.003, -0.000]\n",
      "Epoch 3 [272/340] - Loss: 4.036 [-4.033, 0.003, -0.000]\n",
      "Epoch 3 [273/340] - Loss: 4.047 [-4.045, 0.003, -0.000]\n",
      "Epoch 3 [274/340] - Loss: 4.052 [-4.049, 0.003, -0.000]\n",
      "Epoch 3 [275/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 3 [276/340] - Loss: 4.060 [-4.057, 0.003, -0.000]\n",
      "Epoch 3 [277/340] - Loss: 4.020 [-4.017, 0.003, -0.000]\n",
      "Epoch 3 [278/340] - Loss: 4.043 [-4.040, 0.003, -0.000]\n",
      "Epoch 3 [279/340] - Loss: 4.104 [-4.100, 0.003, -0.000]\n",
      "Epoch 3 [280/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 3 [281/340] - Loss: 4.053 [-4.050, 0.003, -0.000]\n",
      "Epoch 3 [282/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 3 [283/340] - Loss: 3.979 [-3.977, 0.003, -0.000]\n",
      "Epoch 3 [284/340] - Loss: 4.044 [-4.041, 0.003, -0.000]\n",
      "Epoch 3 [285/340] - Loss: 4.078 [-4.075, 0.003, -0.000]\n",
      "Epoch 3 [286/340] - Loss: 4.029 [-4.026, 0.003, -0.000]\n",
      "Epoch 3 [287/340] - Loss: 4.124 [-4.121, 0.003, -0.000]\n",
      "Epoch 3 [288/340] - Loss: 4.054 [-4.051, 0.003, -0.000]\n",
      "Epoch 3 [289/340] - Loss: 4.139 [-4.136, 0.003, -0.000]\n",
      "Epoch 3 [290/340] - Loss: 4.085 [-4.082, 0.003, -0.000]\n",
      "Epoch 3 [291/340] - Loss: 4.017 [-4.014, 0.003, -0.000]\n",
      "Epoch 3 [292/340] - Loss: 4.008 [-4.005, 0.003, -0.000]\n",
      "Epoch 3 [293/340] - Loss: 4.041 [-4.038, 0.003, -0.000]\n",
      "Epoch 3 [294/340] - Loss: 4.079 [-4.077, 0.002, -0.000]\n",
      "Epoch 3 [295/340] - Loss: 4.036 [-4.033, 0.002, -0.000]\n",
      "Epoch 3 [296/340] - Loss: 4.047 [-4.045, 0.002, -0.000]\n",
      "Epoch 3 [297/340] - Loss: 4.172 [-4.170, 0.002, -0.000]\n",
      "Epoch 3 [298/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 3 [299/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n",
      "Epoch 3 [300/340] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 3 [301/340] - Loss: 4.110 [-4.108, 0.002, -0.000]\n",
      "Epoch 3 [302/340] - Loss: 4.073 [-4.070, 0.002, -0.000]\n",
      "Epoch 3 [303/340] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 3 [304/340] - Loss: 4.111 [-4.108, 0.002, -0.000]\n",
      "Epoch 3 [305/340] - Loss: 4.102 [-4.099, 0.002, -0.000]\n",
      "Epoch 3 [306/340] - Loss: 4.128 [-4.126, 0.002, -0.000]\n",
      "Epoch 3 [307/340] - Loss: 4.097 [-4.095, 0.002, -0.000]\n",
      "Epoch 3 [308/340] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 3 [309/340] - Loss: 4.022 [-4.020, 0.002, -0.000]\n",
      "Epoch 3 [310/340] - Loss: 4.093 [-4.091, 0.002, -0.000]\n",
      "Epoch 3 [311/340] - Loss: 4.025 [-4.022, 0.002, -0.000]\n",
      "Epoch 3 [312/340] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 3 [313/340] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 3 [314/340] - Loss: 3.990 [-3.988, 0.003, -0.000]\n",
      "Epoch 3 [315/340] - Loss: 3.978 [-3.976, 0.003, -0.000]\n",
      "Epoch 3 [316/340] - Loss: 4.012 [-4.009, 0.003, -0.000]\n",
      "Epoch 3 [317/340] - Loss: 4.031 [-4.028, 0.003, -0.000]\n",
      "Epoch 3 [318/340] - Loss: 4.047 [-4.044, 0.003, -0.000]\n",
      "Epoch 3 [319/340] - Loss: 4.038 [-4.035, 0.003, -0.000]\n",
      "Epoch 3 [320/340] - Loss: 3.999 [-3.997, 0.003, -0.000]\n",
      "Epoch 3 [321/340] - Loss: 4.004 [-4.001, 0.003, -0.000]\n",
      "Epoch 3 [322/340] - Loss: 4.014 [-4.011, 0.003, -0.000]\n",
      "Epoch 3 [323/340] - Loss: 4.041 [-4.038, 0.003, -0.000]\n",
      "Epoch 3 [324/340] - Loss: 4.076 [-4.074, 0.002, -0.000]\n",
      "Epoch 3 [325/340] - Loss: 4.002 [-3.999, 0.002, -0.000]\n",
      "Epoch 3 [326/340] - Loss: 4.059 [-4.057, 0.002, -0.000]\n",
      "Epoch 3 [327/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 3 [328/340] - Loss: 4.052 [-4.049, 0.003, -0.000]\n",
      "Epoch 3 [329/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 3 [330/340] - Loss: 4.043 [-4.041, 0.003, -0.000]\n",
      "Epoch 3 [331/340] - Loss: 4.063 [-4.061, 0.003, -0.000]\n",
      "Epoch 3 [332/340] - Loss: 4.049 [-4.046, 0.003, -0.000]\n",
      "Epoch 3 [333/340] - Loss: 4.064 [-4.062, 0.003, -0.000]\n",
      "Epoch 3 [334/340] - Loss: 4.015 [-4.012, 0.003, -0.000]\n",
      "Epoch 3 [335/340] - Loss: 4.062 [-4.059, 0.003, -0.000]\n",
      "Epoch 3 [336/340] - Loss: 4.067 [-4.064, 0.003, -0.000]\n",
      "Epoch 3 [337/340] - Loss: 4.080 [-4.077, 0.003, -0.000]\n",
      "Epoch 3 [338/340] - Loss: 4.057 [-4.054, 0.003, -0.000]\n",
      "Epoch 3 [339/340] - Loss: 4.049 [-4.046, 0.003, -0.000]\n",
      "Epoch 4 [0/340] - Loss: 4.110 [-4.108, 0.003, -0.000]\n",
      "Epoch 4 [1/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 4 [2/340] - Loss: 4.029 [-4.026, 0.003, -0.000]\n",
      "Epoch 4 [3/340] - Loss: 4.040 [-4.037, 0.003, -0.000]\n",
      "Epoch 4 [4/340] - Loss: 4.033 [-4.030, 0.003, -0.000]\n",
      "Epoch 4 [5/340] - Loss: 4.062 [-4.059, 0.003, -0.000]\n",
      "Epoch 4 [6/340] - Loss: 4.100 [-4.097, 0.002, -0.000]\n",
      "Epoch 4 [7/340] - Loss: 4.087 [-4.085, 0.002, -0.000]\n",
      "Epoch 4 [8/340] - Loss: 4.067 [-4.065, 0.002, -0.000]\n",
      "Epoch 4 [9/340] - Loss: 4.041 [-4.038, 0.002, -0.000]\n",
      "Epoch 4 [10/340] - Loss: 4.031 [-4.029, 0.002, -0.000]\n",
      "Epoch 4 [11/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 4 [12/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [13/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 4 [14/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [15/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 4 [16/340] - Loss: 3.998 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [17/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 4 [18/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [19/340] - Loss: 3.920 [-3.918, 0.002, -0.000]\n",
      "Epoch 4 [20/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 4 [21/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 4 [22/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 4 [23/340] - Loss: 4.084 [-4.082, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [24/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 4 [25/340] - Loss: 3.938 [-3.936, 0.002, -0.000]\n",
      "Epoch 4 [26/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 4 [27/340] - Loss: 4.008 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [28/340] - Loss: 4.062 [-4.060, 0.002, -0.000]\n",
      "Epoch 4 [29/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [30/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [31/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 4 [32/340] - Loss: 4.055 [-4.053, 0.002, -0.000]\n",
      "Epoch 4 [33/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 4 [34/340] - Loss: 4.016 [-4.013, 0.002, -0.000]\n",
      "Epoch 4 [35/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [36/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [37/340] - Loss: 4.024 [-4.021, 0.002, -0.000]\n",
      "Epoch 4 [38/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [39/340] - Loss: 4.011 [-4.008, 0.002, -0.000]\n",
      "Epoch 4 [40/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [41/340] - Loss: 4.057 [-4.055, 0.002, -0.000]\n",
      "Epoch 4 [42/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [43/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 4 [44/340] - Loss: 4.081 [-4.079, 0.002, -0.000]\n",
      "Epoch 4 [45/340] - Loss: 4.084 [-4.082, 0.002, -0.000]\n",
      "Epoch 4 [46/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 4 [47/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [48/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 4 [49/340] - Loss: 3.906 [-3.904, 0.002, -0.000]\n",
      "Epoch 4 [50/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [51/340] - Loss: 4.049 [-4.047, 0.002, -0.000]\n",
      "Epoch 4 [52/340] - Loss: 4.011 [-4.008, 0.002, -0.000]\n",
      "Epoch 4 [53/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 4 [54/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 4 [55/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [56/340] - Loss: 4.019 [-4.017, 0.002, -0.000]\n",
      "Epoch 4 [57/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [58/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 4 [59/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 4 [60/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [61/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 4 [62/340] - Loss: 3.955 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [63/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [64/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 4 [65/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [66/340] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 4 [67/340] - Loss: 3.956 [-3.953, 0.002, -0.000]\n",
      "Epoch 4 [68/340] - Loss: 3.982 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [69/340] - Loss: 4.003 [-4.000, 0.002, -0.000]\n",
      "Epoch 4 [70/340] - Loss: 3.966 [-3.963, 0.002, -0.000]\n",
      "Epoch 4 [71/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 4 [72/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [73/340] - Loss: 3.928 [-3.925, 0.002, -0.000]\n",
      "Epoch 4 [74/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 4 [75/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 4 [76/340] - Loss: 3.942 [-3.939, 0.002, -0.000]\n",
      "Epoch 4 [77/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 4 [78/340] - Loss: 3.992 [-3.989, 0.002, -0.000]\n",
      "Epoch 4 [79/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [80/340] - Loss: 4.048 [-4.046, 0.002, -0.000]\n",
      "Epoch 4 [81/340] - Loss: 3.947 [-3.944, 0.002, -0.000]\n",
      "Epoch 4 [82/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 4 [83/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [84/340] - Loss: 3.969 [-3.966, 0.002, -0.000]\n",
      "Epoch 4 [85/340] - Loss: 4.021 [-4.018, 0.002, -0.000]\n",
      "Epoch 4 [86/340] - Loss: 3.957 [-3.954, 0.003, -0.000]\n",
      "Epoch 4 [87/340] - Loss: 3.992 [-3.989, 0.002, -0.000]\n",
      "Epoch 4 [88/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [89/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [90/340] - Loss: 3.972 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [91/340] - Loss: 3.962 [-3.960, 0.003, -0.000]\n",
      "Epoch 4 [92/340] - Loss: 3.929 [-3.926, 0.003, -0.000]\n",
      "Epoch 4 [93/340] - Loss: 3.964 [-3.961, 0.002, -0.000]\n",
      "Epoch 4 [94/340] - Loss: 3.983 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [95/340] - Loss: 3.970 [-3.968, 0.003, -0.000]\n",
      "Epoch 4 [96/340] - Loss: 3.926 [-3.924, 0.003, -0.000]\n",
      "Epoch 4 [97/340] - Loss: 3.916 [-3.914, 0.003, -0.000]\n",
      "Epoch 4 [98/340] - Loss: 4.001 [-3.999, 0.002, -0.000]\n",
      "Epoch 4 [99/340] - Loss: 3.923 [-3.921, 0.003, -0.000]\n",
      "Epoch 4 [100/340] - Loss: 3.961 [-3.958, 0.003, -0.000]\n",
      "Epoch 4 [101/340] - Loss: 3.962 [-3.959, 0.003, -0.000]\n",
      "Epoch 4 [102/340] - Loss: 4.003 [-4.000, 0.003, -0.000]\n",
      "Epoch 4 [103/340] - Loss: 3.972 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [104/340] - Loss: 3.938 [-3.936, 0.002, -0.000]\n",
      "Epoch 4 [105/340] - Loss: 3.947 [-3.945, 0.003, -0.000]\n",
      "Epoch 4 [106/340] - Loss: 3.985 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [107/340] - Loss: 3.930 [-3.928, 0.003, -0.000]\n",
      "Epoch 4 [108/340] - Loss: 3.935 [-3.932, 0.003, -0.000]\n",
      "Epoch 4 [109/340] - Loss: 4.005 [-4.003, 0.003, -0.000]\n",
      "Epoch 4 [110/340] - Loss: 3.969 [-3.966, 0.002, -0.000]\n",
      "Epoch 4 [111/340] - Loss: 3.966 [-3.963, 0.003, -0.000]\n",
      "Epoch 4 [112/340] - Loss: 3.944 [-3.942, 0.003, -0.000]\n",
      "Epoch 4 [113/340] - Loss: 3.957 [-3.954, 0.003, -0.000]\n",
      "Epoch 4 [114/340] - Loss: 3.953 [-3.950, 0.003, -0.000]\n",
      "Epoch 4 [115/340] - Loss: 3.983 [-3.980, 0.003, -0.000]\n",
      "Epoch 4 [116/340] - Loss: 3.972 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [117/340] - Loss: 3.959 [-3.956, 0.002, -0.000]\n",
      "Epoch 4 [118/340] - Loss: 3.959 [-3.956, 0.002, -0.000]\n",
      "Epoch 4 [119/340] - Loss: 3.994 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [120/340] - Loss: 3.945 [-3.942, 0.002, -0.000]\n",
      "Epoch 4 [121/340] - Loss: 3.996 [-3.993, 0.003, -0.000]\n",
      "Epoch 4 [122/340] - Loss: 4.031 [-4.028, 0.003, -0.000]\n",
      "Epoch 4 [123/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 4 [124/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [125/340] - Loss: 4.024 [-4.021, 0.002, -0.000]\n",
      "Epoch 4 [126/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 4 [127/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 4 [128/340] - Loss: 4.027 [-4.024, 0.003, -0.000]\n",
      "Epoch 4 [129/340] - Loss: 3.986 [-3.983, 0.002, -0.000]\n",
      "Epoch 4 [130/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [131/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [132/340] - Loss: 3.982 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [133/340] - Loss: 3.890 [-3.888, 0.002, -0.000]\n",
      "Epoch 4 [134/340] - Loss: 4.001 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [135/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 4 [136/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 4 [137/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [138/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [139/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 4 [140/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 4 [141/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 4 [142/340] - Loss: 4.004 [-4.001, 0.002, -0.000]\n",
      "Epoch 4 [143/340] - Loss: 3.940 [-3.937, 0.002, -0.000]\n",
      "Epoch 4 [144/340] - Loss: 3.921 [-3.918, 0.002, -0.000]\n",
      "Epoch 4 [145/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [146/340] - Loss: 3.915 [-3.912, 0.002, -0.000]\n",
      "Epoch 4 [147/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 4 [148/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 4 [149/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 4 [150/340] - Loss: 3.931 [-3.928, 0.002, -0.000]\n",
      "Epoch 4 [151/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 4 [152/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 4 [153/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [154/340] - Loss: 3.955 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [155/340] - Loss: 3.948 [-3.946, 0.003, -0.000]\n",
      "Epoch 4 [156/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [157/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 4 [158/340] - Loss: 3.981 [-3.978, 0.002, -0.000]\n",
      "Epoch 4 [159/340] - Loss: 3.934 [-3.931, 0.002, -0.000]\n",
      "Epoch 4 [160/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 4 [161/340] - Loss: 3.999 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [162/340] - Loss: 3.909 [-3.907, 0.002, -0.000]\n",
      "Epoch 4 [163/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 4 [164/340] - Loss: 3.983 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [165/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 4 [166/340] - Loss: 3.909 [-3.906, 0.002, -0.000]\n",
      "Epoch 4 [167/340] - Loss: 3.993 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [168/340] - Loss: 3.959 [-3.956, 0.002, -0.000]\n",
      "Epoch 4 [169/340] - Loss: 3.985 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [170/340] - Loss: 4.087 [-4.085, 0.002, -0.000]\n",
      "Epoch 4 [171/340] - Loss: 3.975 [-3.972, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [172/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [173/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 4 [174/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 4 [175/340] - Loss: 3.928 [-3.925, 0.002, -0.000]\n",
      "Epoch 4 [176/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 4 [177/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 4 [178/340] - Loss: 4.017 [-4.015, 0.003, -0.000]\n",
      "Epoch 4 [179/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [180/340] - Loss: 3.990 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [181/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 4 [182/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [183/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [184/340] - Loss: 3.973 [-3.970, 0.002, -0.000]\n",
      "Epoch 4 [185/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [186/340] - Loss: 3.958 [-3.955, 0.002, -0.000]\n",
      "Epoch 4 [187/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 4 [188/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 4 [189/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 4 [190/340] - Loss: 3.975 [-3.972, 0.002, -0.000]\n",
      "Epoch 4 [191/340] - Loss: 3.955 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [192/340] - Loss: 4.020 [-4.017, 0.002, -0.000]\n",
      "Epoch 4 [193/340] - Loss: 3.963 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [194/340] - Loss: 3.957 [-3.954, 0.003, -0.000]\n",
      "Epoch 4 [195/340] - Loss: 3.978 [-3.976, 0.003, -0.000]\n",
      "Epoch 4 [196/340] - Loss: 3.925 [-3.923, 0.003, -0.000]\n",
      "Epoch 4 [197/340] - Loss: 3.908 [-3.906, 0.003, -0.000]\n",
      "Epoch 4 [198/340] - Loss: 4.017 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [199/340] - Loss: 3.928 [-3.925, 0.003, -0.000]\n",
      "Epoch 4 [200/340] - Loss: 4.021 [-4.019, 0.003, -0.000]\n",
      "Epoch 4 [201/340] - Loss: 3.986 [-3.983, 0.003, -0.000]\n",
      "Epoch 4 [202/340] - Loss: 4.029 [-4.027, 0.003, -0.000]\n",
      "Epoch 4 [203/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 4 [204/340] - Loss: 3.992 [-3.989, 0.003, -0.000]\n",
      "Epoch 4 [205/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [206/340] - Loss: 3.953 [-3.951, 0.003, -0.000]\n",
      "Epoch 4 [207/340] - Loss: 3.975 [-3.973, 0.003, -0.000]\n",
      "Epoch 4 [208/340] - Loss: 4.123 [-4.121, 0.003, -0.000]\n",
      "Epoch 4 [209/340] - Loss: 3.938 [-3.936, 0.002, -0.000]\n",
      "Epoch 4 [210/340] - Loss: 3.958 [-3.955, 0.003, -0.000]\n",
      "Epoch 4 [211/340] - Loss: 3.942 [-3.939, 0.003, -0.000]\n",
      "Epoch 4 [212/340] - Loss: 4.045 [-4.042, 0.002, -0.000]\n",
      "Epoch 4 [213/340] - Loss: 3.966 [-3.963, 0.002, -0.000]\n",
      "Epoch 4 [214/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [215/340] - Loss: 3.989 [-3.986, 0.003, -0.000]\n",
      "Epoch 4 [216/340] - Loss: 3.965 [-3.962, 0.003, -0.000]\n",
      "Epoch 4 [217/340] - Loss: 3.917 [-3.914, 0.003, -0.000]\n",
      "Epoch 4 [218/340] - Loss: 3.993 [-3.990, 0.003, -0.000]\n",
      "Epoch 4 [219/340] - Loss: 4.016 [-4.013, 0.003, -0.000]\n",
      "Epoch 4 [220/340] - Loss: 3.916 [-3.913, 0.003, -0.000]\n",
      "Epoch 4 [221/340] - Loss: 3.951 [-3.949, 0.003, -0.000]\n",
      "Epoch 4 [222/340] - Loss: 3.982 [-3.979, 0.003, -0.000]\n",
      "Epoch 4 [223/340] - Loss: 4.016 [-4.014, 0.003, -0.000]\n",
      "Epoch 4 [224/340] - Loss: 3.989 [-3.987, 0.003, -0.000]\n",
      "Epoch 4 [225/340] - Loss: 3.978 [-3.975, 0.003, -0.000]\n",
      "Epoch 4 [226/340] - Loss: 3.955 [-3.952, 0.003, -0.000]\n",
      "Epoch 4 [227/340] - Loss: 3.960 [-3.958, 0.003, -0.000]\n",
      "Epoch 4 [228/340] - Loss: 4.013 [-4.011, 0.003, -0.000]\n",
      "Epoch 4 [229/340] - Loss: 3.955 [-3.952, 0.003, -0.000]\n",
      "Epoch 4 [230/340] - Loss: 3.950 [-3.948, 0.003, -0.000]\n",
      "Epoch 4 [231/340] - Loss: 4.033 [-4.031, 0.003, -0.000]\n",
      "Epoch 4 [232/340] - Loss: 3.978 [-3.976, 0.003, -0.000]\n",
      "Epoch 4 [233/340] - Loss: 3.991 [-3.988, 0.003, -0.000]\n",
      "Epoch 4 [234/340] - Loss: 3.970 [-3.968, 0.003, -0.000]\n",
      "Epoch 4 [235/340] - Loss: 4.087 [-4.085, 0.003, -0.000]\n",
      "Epoch 4 [236/340] - Loss: 3.965 [-3.962, 0.003, -0.000]\n",
      "Epoch 4 [237/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 4 [238/340] - Loss: 3.994 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [239/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 4 [240/340] - Loss: 3.992 [-3.989, 0.003, -0.000]\n",
      "Epoch 4 [241/340] - Loss: 3.959 [-3.956, 0.002, -0.000]\n",
      "Epoch 4 [242/340] - Loss: 3.934 [-3.931, 0.002, -0.000]\n",
      "Epoch 4 [243/340] - Loss: 4.017 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [244/340] - Loss: 4.019 [-4.017, 0.002, -0.000]\n",
      "Epoch 4 [245/340] - Loss: 3.911 [-3.908, 0.002, -0.000]\n",
      "Epoch 4 [246/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [247/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 4 [248/340] - Loss: 4.045 [-4.043, 0.002, -0.000]\n",
      "Epoch 4 [249/340] - Loss: 3.880 [-3.878, 0.002, -0.000]\n",
      "Epoch 4 [250/340] - Loss: 3.981 [-3.978, 0.002, -0.000]\n",
      "Epoch 4 [251/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 4 [252/340] - Loss: 4.009 [-4.006, 0.002, -0.000]\n",
      "Epoch 4 [253/340] - Loss: 3.916 [-3.913, 0.002, -0.000]\n",
      "Epoch 4 [254/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [255/340] - Loss: 3.951 [-3.948, 0.002, -0.000]\n",
      "Epoch 4 [256/340] - Loss: 3.948 [-3.945, 0.002, -0.000]\n",
      "Epoch 4 [257/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 4 [258/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 4 [259/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [260/340] - Loss: 3.950 [-3.947, 0.002, -0.000]\n",
      "Epoch 4 [261/340] - Loss: 3.930 [-3.927, 0.002, -0.000]\n",
      "Epoch 4 [262/340] - Loss: 4.018 [-4.015, 0.002, -0.000]\n",
      "Epoch 4 [263/340] - Loss: 3.944 [-3.941, 0.002, -0.000]\n",
      "Epoch 4 [264/340] - Loss: 3.966 [-3.963, 0.002, -0.000]\n",
      "Epoch 4 [265/340] - Loss: 4.001 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [266/340] - Loss: 3.963 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [267/340] - Loss: 3.987 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [268/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 4 [269/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 4 [270/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 4 [271/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 4 [272/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 4 [273/340] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 4 [274/340] - Loss: 3.988 [-3.985, 0.002, -0.000]\n",
      "Epoch 4 [275/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [276/340] - Loss: 3.939 [-3.936, 0.002, -0.000]\n",
      "Epoch 4 [277/340] - Loss: 3.984 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [278/340] - Loss: 4.008 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [279/340] - Loss: 3.963 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [280/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 4 [281/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [282/340] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 4 [283/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 4 [284/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 4 [285/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 4 [286/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 4 [287/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [288/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 4 [289/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 4 [290/340] - Loss: 3.997 [-3.994, 0.002, -0.000]\n",
      "Epoch 4 [291/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [292/340] - Loss: 3.920 [-3.918, 0.002, -0.000]\n",
      "Epoch 4 [293/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 4 [294/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [295/340] - Loss: 3.944 [-3.941, 0.002, -0.000]\n",
      "Epoch 4 [296/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [297/340] - Loss: 3.979 [-3.976, 0.002, -0.000]\n",
      "Epoch 4 [298/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 4 [299/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 4 [300/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 4 [301/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [302/340] - Loss: 3.963 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [303/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [304/340] - Loss: 3.947 [-3.944, 0.002, -0.000]\n",
      "Epoch 4 [305/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 4 [306/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 4 [307/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 4 [308/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 4 [309/340] - Loss: 4.049 [-4.047, 0.002, -0.000]\n",
      "Epoch 4 [310/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 4 [311/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 4 [312/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 4 [313/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [314/340] - Loss: 4.030 [-4.028, 0.002, -0.000]\n",
      "Epoch 4 [315/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [316/340] - Loss: 3.905 [-3.903, 0.002, -0.000]\n",
      "Epoch 4 [317/340] - Loss: 4.055 [-4.053, 0.002, -0.000]\n",
      "Epoch 4 [318/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [319/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [320/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 4 [321/340] - Loss: 3.972 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [322/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [323/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 4 [324/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [325/340] - Loss: 3.983 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [326/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 4 [327/340] - Loss: 4.068 [-4.066, 0.002, -0.000]\n",
      "Epoch 4 [328/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 4 [329/340] - Loss: 4.023 [-4.020, 0.002, -0.000]\n",
      "Epoch 4 [330/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 4 [331/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 4 [332/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [333/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [334/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [335/340] - Loss: 3.944 [-3.941, 0.002, -0.000]\n",
      "Epoch 4 [336/340] - Loss: 3.968 [-3.965, 0.002, -0.000]\n",
      "Epoch 4 [337/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 4 [338/340] - Loss: 3.960 [-3.957, 0.002, -0.000]\n",
      "Epoch 4 [339/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 5 [0/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [1/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 5 [2/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [3/340] - Loss: 3.964 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [4/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [5/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 5 [6/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [7/340] - Loss: 3.900 [-3.898, 0.002, -0.000]\n",
      "Epoch 5 [8/340] - Loss: 3.931 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [9/340] - Loss: 4.053 [-4.051, 0.002, -0.000]\n",
      "Epoch 5 [10/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [11/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [12/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [13/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 5 [14/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [15/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [16/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [17/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [18/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [19/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 5 [20/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [21/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 5 [22/340] - Loss: 3.929 [-3.926, 0.002, -0.000]\n",
      "Epoch 5 [23/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [24/340] - Loss: 3.967 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [25/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [26/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [27/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [28/340] - Loss: 3.894 [-3.892, 0.002, -0.000]\n",
      "Epoch 5 [29/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 5 [30/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [31/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [32/340] - Loss: 4.023 [-4.021, 0.002, -0.000]\n",
      "Epoch 5 [33/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 5 [34/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 5 [35/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [36/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [37/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [38/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 5 [39/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 5 [40/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 5 [41/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [42/340] - Loss: 3.902 [-3.900, 0.002, -0.000]\n",
      "Epoch 5 [43/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 5 [44/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [45/340] - Loss: 3.979 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [46/340] - Loss: 3.964 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [47/340] - Loss: 3.893 [-3.891, 0.002, -0.000]\n",
      "Epoch 5 [48/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 5 [49/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [50/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 5 [51/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [52/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [53/340] - Loss: 3.931 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [54/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [55/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [56/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 5 [57/340] - Loss: 3.888 [-3.885, 0.002, -0.000]\n",
      "Epoch 5 [58/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [59/340] - Loss: 3.957 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [60/340] - Loss: 3.959 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [61/340] - Loss: 3.976 [-3.973, 0.002, -0.000]\n",
      "Epoch 5 [62/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 5 [63/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [64/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 5 [65/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 5 [66/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [67/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [68/340] - Loss: 3.949 [-3.946, 0.002, -0.000]\n",
      "Epoch 5 [69/340] - Loss: 4.066 [-4.063, 0.002, -0.000]\n",
      "Epoch 5 [70/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [71/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [72/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 5 [73/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [74/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 5 [75/340] - Loss: 3.906 [-3.904, 0.002, -0.000]\n",
      "Epoch 5 [76/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 5 [77/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [78/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [79/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 5 [80/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 5 [81/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 5 [82/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 5 [83/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [84/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 5 [85/340] - Loss: 3.946 [-3.943, 0.002, -0.000]\n",
      "Epoch 5 [86/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 5 [87/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [88/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [89/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [90/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 5 [91/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [92/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 5 [93/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [94/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 5 [95/340] - Loss: 4.072 [-4.070, 0.002, -0.000]\n",
      "Epoch 5 [96/340] - Loss: 3.967 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [97/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [98/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [99/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 5 [100/340] - Loss: 4.019 [-4.016, 0.002, -0.000]\n",
      "Epoch 5 [101/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 5 [102/340] - Loss: 3.986 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [103/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 5 [104/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [105/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [106/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 5 [107/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 5 [108/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 5 [109/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [110/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 5 [111/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [112/340] - Loss: 3.939 [-3.936, 0.002, -0.000]\n",
      "Epoch 5 [113/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 5 [114/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 5 [115/340] - Loss: 3.975 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [116/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 5 [117/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 5 [118/340] - Loss: 4.038 [-4.035, 0.002, -0.000]\n",
      "Epoch 5 [119/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 5 [120/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [121/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 5 [122/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [123/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 5 [124/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [125/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [126/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 5 [127/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [128/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [129/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [130/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 5 [131/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 5 [132/340] - Loss: 3.951 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [133/340] - Loss: 3.900 [-3.898, 0.002, -0.000]\n",
      "Epoch 5 [134/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 5 [135/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [136/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 5 [137/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 5 [138/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 5 [139/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [140/340] - Loss: 3.933 [-3.930, 0.002, -0.000]\n",
      "Epoch 5 [141/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 5 [142/340] - Loss: 3.921 [-3.918, 0.002, -0.000]\n",
      "Epoch 5 [143/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [144/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [145/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 5 [146/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [147/340] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 5 [148/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [149/340] - Loss: 3.909 [-3.907, 0.002, -0.000]\n",
      "Epoch 5 [150/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [151/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 5 [152/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [153/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [154/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 5 [155/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [156/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 5 [157/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 5 [158/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [159/340] - Loss: 4.037 [-4.034, 0.002, -0.000]\n",
      "Epoch 5 [160/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [161/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [162/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 5 [163/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 5 [164/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 5 [165/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [166/340] - Loss: 3.957 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [167/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [168/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [169/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 5 [170/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [171/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [172/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 5 [173/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [174/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [175/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [176/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 5 [177/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 5 [178/340] - Loss: 3.969 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [179/340] - Loss: 3.921 [-3.919, 0.002, -0.000]\n",
      "Epoch 5 [180/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [181/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 5 [182/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 5 [183/340] - Loss: 3.907 [-3.905, 0.002, -0.000]\n",
      "Epoch 5 [184/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 5 [185/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [186/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 5 [187/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 5 [188/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 5 [189/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 5 [190/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [191/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 5 [192/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [193/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [194/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 5 [195/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [196/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [197/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 5 [198/340] - Loss: 3.987 [-3.984, 0.002, -0.000]\n",
      "Epoch 5 [199/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [200/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [201/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 5 [202/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [203/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [204/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [205/340] - Loss: 4.023 [-4.021, 0.002, -0.000]\n",
      "Epoch 5 [206/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [207/340] - Loss: 3.880 [-3.878, 0.002, -0.000]\n",
      "Epoch 5 [208/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [209/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [210/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [211/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [212/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [213/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 5 [214/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 5 [215/340] - Loss: 3.891 [-3.888, 0.002, -0.000]\n",
      "Epoch 5 [216/340] - Loss: 3.927 [-3.924, 0.002, -0.000]\n",
      "Epoch 5 [217/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [218/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 5 [219/340] - Loss: 3.920 [-3.918, 0.002, -0.000]\n",
      "Epoch 5 [220/340] - Loss: 3.927 [-3.924, 0.002, -0.000]\n",
      "Epoch 5 [221/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [222/340] - Loss: 3.861 [-3.859, 0.002, -0.000]\n",
      "Epoch 5 [223/340] - Loss: 4.023 [-4.021, 0.002, -0.000]\n",
      "Epoch 5 [224/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 5 [225/340] - Loss: 4.019 [-4.016, 0.002, -0.000]\n",
      "Epoch 5 [226/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [227/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 5 [228/340] - Loss: 3.930 [-3.927, 0.002, -0.000]\n",
      "Epoch 5 [229/340] - Loss: 3.993 [-3.990, 0.002, -0.000]\n",
      "Epoch 5 [230/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [231/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [232/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 5 [233/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 5 [234/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [235/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [236/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 5 [237/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [238/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 5 [239/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [240/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 5 [241/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [242/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [243/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [244/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 5 [245/340] - Loss: 4.096 [-4.093, 0.002, -0.000]\n",
      "Epoch 5 [246/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [247/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 5 [248/340] - Loss: 3.909 [-3.907, 0.002, -0.000]\n",
      "Epoch 5 [249/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [250/340] - Loss: 3.855 [-3.853, 0.002, -0.000]\n",
      "Epoch 5 [251/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 5 [252/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 5 [253/340] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 5 [254/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [255/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [256/340] - Loss: 3.938 [-3.936, 0.002, -0.000]\n",
      "Epoch 5 [257/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 5 [258/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [259/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [260/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [261/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [262/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [263/340] - Loss: 3.880 [-3.878, 0.002, -0.000]\n",
      "Epoch 5 [264/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 5 [265/340] - Loss: 3.906 [-3.904, 0.002, -0.000]\n",
      "Epoch 5 [266/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 5 [267/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [268/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [269/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [270/340] - Loss: 3.954 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [271/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 5 [272/340] - Loss: 3.898 [-3.896, 0.002, -0.000]\n",
      "Epoch 5 [273/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [274/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [275/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [276/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [277/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 5 [278/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 5 [279/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [280/340] - Loss: 3.958 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [281/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [282/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [283/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [284/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [285/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 5 [286/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [287/340] - Loss: 4.041 [-4.039, 0.002, -0.000]\n",
      "Epoch 5 [288/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [289/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [290/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [291/340] - Loss: 3.979 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [292/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [293/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 5 [294/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [295/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [296/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [297/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 5 [298/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [299/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 5 [300/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 5 [301/340] - Loss: 3.933 [-3.930, 0.002, -0.000]\n",
      "Epoch 5 [302/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 5 [303/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [304/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 5 [305/340] - Loss: 3.941 [-3.938, 0.002, -0.000]\n",
      "Epoch 5 [306/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [307/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [308/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [309/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 5 [310/340] - Loss: 3.913 [-3.911, 0.002, -0.000]\n",
      "Epoch 5 [311/340] - Loss: 3.926 [-3.923, 0.002, -0.000]\n",
      "Epoch 5 [312/340] - Loss: 3.910 [-3.908, 0.002, -0.000]\n",
      "Epoch 5 [313/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 5 [314/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 5 [315/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [316/340] - Loss: 3.951 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [317/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 5 [318/340] - Loss: 3.988 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [319/340] - Loss: 3.946 [-3.943, 0.002, -0.000]\n",
      "Epoch 5 [320/340] - Loss: 3.976 [-3.973, 0.002, -0.000]\n",
      "Epoch 5 [321/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 5 [322/340] - Loss: 3.905 [-3.903, 0.002, -0.000]\n",
      "Epoch 5 [323/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [324/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 5 [325/340] - Loss: 3.894 [-3.892, 0.002, -0.000]\n",
      "Epoch 5 [326/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [327/340] - Loss: 3.895 [-3.893, 0.002, -0.000]\n",
      "Epoch 5 [328/340] - Loss: 3.990 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [329/340] - Loss: 4.023 [-4.020, 0.002, -0.000]\n",
      "Epoch 5 [330/340] - Loss: 3.933 [-3.930, 0.002, -0.000]\n",
      "Epoch 5 [331/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 5 [332/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 5 [333/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 5 [334/340] - Loss: 3.955 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [335/340] - Loss: 3.902 [-3.899, 0.002, -0.000]\n",
      "Epoch 5 [336/340] - Loss: 4.007 [-4.004, 0.002, -0.000]\n",
      "Epoch 5 [337/340] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 5 [338/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [339/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 6 [0/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [1/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [2/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [3/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [4/340] - Loss: 3.967 [-3.964, 0.002, -0.000]\n",
      "Epoch 6 [5/340] - Loss: 4.035 [-4.033, 0.002, -0.000]\n",
      "Epoch 6 [6/340] - Loss: 3.936 [-3.933, 0.002, -0.000]\n",
      "Epoch 6 [7/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [8/340] - Loss: 3.895 [-3.893, 0.002, -0.000]\n",
      "Epoch 6 [9/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [10/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 6 [11/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [12/340] - Loss: 3.890 [-3.888, 0.002, -0.000]\n",
      "Epoch 6 [13/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [14/340] - Loss: 3.902 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [15/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [16/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [17/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [18/340] - Loss: 3.970 [-3.967, 0.002, -0.000]\n",
      "Epoch 6 [19/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [20/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [21/340] - Loss: 3.883 [-3.881, 0.002, -0.000]\n",
      "Epoch 6 [22/340] - Loss: 3.889 [-3.886, 0.002, -0.000]\n",
      "Epoch 6 [23/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [24/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 6 [25/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [26/340] - Loss: 4.052 [-4.050, 0.002, -0.000]\n",
      "Epoch 6 [27/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 6 [28/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [29/340] - Loss: 3.890 [-3.888, 0.002, -0.000]\n",
      "Epoch 6 [30/340] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [31/340] - Loss: 3.958 [-3.955, 0.002, -0.000]\n",
      "Epoch 6 [32/340] - Loss: 3.905 [-3.903, 0.002, -0.000]\n",
      "Epoch 6 [33/340] - Loss: 3.888 [-3.886, 0.002, -0.000]\n",
      "Epoch 6 [34/340] - Loss: 3.892 [-3.890, 0.002, -0.000]\n",
      "Epoch 6 [35/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [36/340] - Loss: 3.921 [-3.919, 0.002, -0.000]\n",
      "Epoch 6 [37/340] - Loss: 3.917 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [38/340] - Loss: 3.916 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [39/340] - Loss: 3.868 [-3.865, 0.002, -0.000]\n",
      "Epoch 6 [40/340] - Loss: 3.933 [-3.930, 0.002, -0.000]\n",
      "Epoch 6 [41/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [42/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [43/340] - Loss: 3.841 [-3.839, 0.002, -0.000]\n",
      "Epoch 6 [44/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [45/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 6 [46/340] - Loss: 3.900 [-3.897, 0.002, -0.000]\n",
      "Epoch 6 [47/340] - Loss: 3.913 [-3.911, 0.002, -0.000]\n",
      "Epoch 6 [48/340] - Loss: 3.881 [-3.878, 0.002, -0.000]\n",
      "Epoch 6 [49/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [50/340] - Loss: 3.890 [-3.888, 0.002, -0.000]\n",
      "Epoch 6 [51/340] - Loss: 3.905 [-3.903, 0.002, -0.000]\n",
      "Epoch 6 [52/340] - Loss: 3.927 [-3.924, 0.002, -0.000]\n",
      "Epoch 6 [53/340] - Loss: 3.883 [-3.881, 0.002, -0.000]\n",
      "Epoch 6 [54/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 6 [55/340] - Loss: 3.939 [-3.936, 0.002, -0.000]\n",
      "Epoch 6 [56/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [57/340] - Loss: 3.904 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [58/340] - Loss: 3.888 [-3.885, 0.002, -0.000]\n",
      "Epoch 6 [59/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [60/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [61/340] - Loss: 3.906 [-3.904, 0.002, -0.000]\n",
      "Epoch 6 [62/340] - Loss: 3.920 [-3.917, 0.002, -0.000]\n",
      "Epoch 6 [63/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [64/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 6 [65/340] - Loss: 3.949 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [66/340] - Loss: 3.899 [-3.897, 0.002, -0.000]\n",
      "Epoch 6 [67/340] - Loss: 3.886 [-3.884, 0.002, -0.000]\n",
      "Epoch 6 [68/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [69/340] - Loss: 3.916 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [70/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 6 [71/340] - Loss: 3.898 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [72/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [73/340] - Loss: 3.893 [-3.891, 0.002, -0.000]\n",
      "Epoch 6 [74/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 6 [75/340] - Loss: 3.903 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [76/340] - Loss: 3.888 [-3.886, 0.002, -0.000]\n",
      "Epoch 6 [77/340] - Loss: 3.895 [-3.893, 0.002, -0.000]\n",
      "Epoch 6 [78/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [79/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [80/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [81/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [82/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [83/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 6 [84/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [85/340] - Loss: 3.886 [-3.884, 0.002, -0.000]\n",
      "Epoch 6 [86/340] - Loss: 3.891 [-3.889, 0.002, -0.000]\n",
      "Epoch 6 [87/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [88/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [89/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 6 [90/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [91/340] - Loss: 3.896 [-3.894, 0.002, -0.000]\n",
      "Epoch 6 [92/340] - Loss: 3.963 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [93/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [94/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 6 [95/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 6 [96/340] - Loss: 3.891 [-3.889, 0.002, -0.000]\n",
      "Epoch 6 [97/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 6 [98/340] - Loss: 4.017 [-4.014, 0.002, -0.000]\n",
      "Epoch 6 [99/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 6 [100/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [101/340] - Loss: 3.892 [-3.890, 0.002, -0.000]\n",
      "Epoch 6 [102/340] - Loss: 3.889 [-3.886, 0.002, -0.000]\n",
      "Epoch 6 [103/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [104/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 6 [105/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 6 [106/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [107/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [108/340] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [109/340] - Loss: 3.949 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [110/340] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [111/340] - Loss: 3.966 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [112/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 6 [113/340] - Loss: 3.909 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [114/340] - Loss: 3.868 [-3.865, 0.002, -0.000]\n",
      "Epoch 6 [115/340] - Loss: 3.911 [-3.908, 0.002, -0.000]\n",
      "Epoch 6 [116/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [117/340] - Loss: 3.916 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [118/340] - Loss: 3.898 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [119/340] - Loss: 3.887 [-3.885, 0.002, -0.000]\n",
      "Epoch 6 [120/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [121/340] - Loss: 3.882 [-3.880, 0.002, -0.000]\n",
      "Epoch 6 [122/340] - Loss: 3.885 [-3.882, 0.002, -0.000]\n",
      "Epoch 6 [123/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [124/340] - Loss: 3.890 [-3.888, 0.002, -0.000]\n",
      "Epoch 6 [125/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [126/340] - Loss: 3.909 [-3.907, 0.002, -0.000]\n",
      "Epoch 6 [127/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [128/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 6 [129/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 6 [130/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 6 [131/340] - Loss: 3.978 [-3.975, 0.002, -0.000]\n",
      "Epoch 6 [132/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 6 [133/340] - Loss: 3.917 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [134/340] - Loss: 3.914 [-3.911, 0.002, -0.000]\n",
      "Epoch 6 [135/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [136/340] - Loss: 3.945 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [137/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 6 [138/340] - Loss: 3.900 [-3.897, 0.002, -0.000]\n",
      "Epoch 6 [139/340] - Loss: 3.890 [-3.887, 0.002, -0.000]\n",
      "Epoch 6 [140/340] - Loss: 3.902 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [141/340] - Loss: 3.868 [-3.866, 0.002, -0.000]\n",
      "Epoch 6 [142/340] - Loss: 3.894 [-3.892, 0.002, -0.000]\n",
      "Epoch 6 [143/340] - Loss: 3.916 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [144/340] - Loss: 3.927 [-3.924, 0.002, -0.000]\n",
      "Epoch 6 [145/340] - Loss: 3.966 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [146/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 6 [147/340] - Loss: 3.892 [-3.890, 0.002, -0.000]\n",
      "Epoch 6 [148/340] - Loss: 3.896 [-3.894, 0.002, -0.000]\n",
      "Epoch 6 [149/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 6 [150/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [151/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [152/340] - Loss: 3.914 [-3.911, 0.002, -0.000]\n",
      "Epoch 6 [153/340] - Loss: 3.933 [-3.930, 0.002, -0.000]\n",
      "Epoch 6 [154/340] - Loss: 3.893 [-3.891, 0.002, -0.000]\n",
      "Epoch 6 [155/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [156/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 6 [157/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 6 [158/340] - Loss: 3.868 [-3.866, 0.002, -0.000]\n",
      "Epoch 6 [159/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [160/340] - Loss: 3.895 [-3.893, 0.002, -0.000]\n",
      "Epoch 6 [161/340] - Loss: 3.865 [-3.863, 0.002, -0.000]\n",
      "Epoch 6 [162/340] - Loss: 3.930 [-3.927, 0.002, -0.000]\n",
      "Epoch 6 [163/340] - Loss: 3.935 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [164/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 6 [165/340] - Loss: 3.907 [-3.905, 0.002, -0.000]\n",
      "Epoch 6 [166/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [167/340] - Loss: 3.960 [-3.957, 0.002, -0.000]\n",
      "Epoch 6 [168/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 6 [169/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [170/340] - Loss: 3.905 [-3.902, 0.002, -0.000]\n",
      "Epoch 6 [171/340] - Loss: 3.887 [-3.885, 0.002, -0.000]\n",
      "Epoch 6 [172/340] - Loss: 3.898 [-3.896, 0.002, -0.000]\n",
      "Epoch 6 [173/340] - Loss: 3.916 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [174/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [175/340] - Loss: 3.906 [-3.903, 0.002, -0.000]\n",
      "Epoch 6 [176/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [177/340] - Loss: 3.919 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [178/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 6 [179/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 6 [180/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 6 [181/340] - Loss: 3.920 [-3.918, 0.002, -0.000]\n",
      "Epoch 6 [182/340] - Loss: 3.879 [-3.877, 0.002, -0.000]\n",
      "Epoch 6 [183/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 6 [184/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 6 [185/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 6 [186/340] - Loss: 3.894 [-3.891, 0.002, -0.000]\n",
      "Epoch 6 [187/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 6 [188/340] - Loss: 3.902 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [189/340] - Loss: 3.904 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [190/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [191/340] - Loss: 3.888 [-3.886, 0.002, -0.000]\n",
      "Epoch 6 [192/340] - Loss: 3.878 [-3.876, 0.002, -0.000]\n",
      "Epoch 6 [193/340] - Loss: 3.920 [-3.918, 0.002, -0.000]\n",
      "Epoch 6 [194/340] - Loss: 3.871 [-3.869, 0.002, -0.000]\n",
      "Epoch 6 [195/340] - Loss: 3.944 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [196/340] - Loss: 3.900 [-3.897, 0.002, -0.000]\n",
      "Epoch 6 [197/340] - Loss: 3.892 [-3.890, 0.002, -0.000]\n",
      "Epoch 6 [198/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [199/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 6 [200/340] - Loss: 3.891 [-3.889, 0.002, -0.000]\n",
      "Epoch 6 [201/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 6 [202/340] - Loss: 3.874 [-3.872, 0.002, -0.000]\n",
      "Epoch 6 [203/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 6 [204/340] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [205/340] - Loss: 3.933 [-3.930, 0.002, -0.000]\n",
      "Epoch 6 [206/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [207/340] - Loss: 3.898 [-3.896, 0.002, -0.000]\n",
      "Epoch 6 [208/340] - Loss: 3.922 [-3.919, 0.002, -0.000]\n",
      "Epoch 6 [209/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [210/340] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [211/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 6 [212/340] - Loss: 3.907 [-3.905, 0.002, -0.000]\n",
      "Epoch 6 [213/340] - Loss: 3.905 [-3.903, 0.002, -0.000]\n",
      "Epoch 6 [214/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 6 [215/340] - Loss: 3.900 [-3.898, 0.002, -0.000]\n",
      "Epoch 6 [216/340] - Loss: 3.917 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [217/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [218/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 6 [219/340] - Loss: 3.902 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [220/340] - Loss: 3.854 [-3.852, 0.002, -0.000]\n",
      "Epoch 6 [221/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 6 [222/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [223/340] - Loss: 3.891 [-3.888, 0.002, -0.000]\n",
      "Epoch 6 [224/340] - Loss: 3.957 [-3.954, 0.002, -0.000]\n",
      "Epoch 6 [225/340] - Loss: 3.982 [-3.979, 0.002, -0.000]\n",
      "Epoch 6 [226/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [227/340] - Loss: 3.921 [-3.919, 0.002, -0.000]\n",
      "Epoch 6 [228/340] - Loss: 3.909 [-3.907, 0.002, -0.000]\n",
      "Epoch 6 [229/340] - Loss: 3.896 [-3.894, 0.002, -0.000]\n",
      "Epoch 6 [230/340] - Loss: 3.909 [-3.907, 0.002, -0.000]\n",
      "Epoch 6 [231/340] - Loss: 3.875 [-3.873, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [232/340] - Loss: 3.895 [-3.893, 0.002, -0.000]\n",
      "Epoch 6 [233/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 6 [234/340] - Loss: 3.874 [-3.871, 0.002, -0.000]\n",
      "Epoch 6 [235/340] - Loss: 3.900 [-3.897, 0.002, -0.000]\n",
      "Epoch 6 [236/340] - Loss: 3.964 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [237/340] - Loss: 3.851 [-3.849, 0.002, -0.000]\n",
      "Epoch 6 [238/340] - Loss: 3.900 [-3.898, 0.002, -0.000]\n",
      "Epoch 6 [239/340] - Loss: 3.940 [-3.937, 0.002, -0.000]\n",
      "Epoch 6 [240/340] - Loss: 3.956 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [241/340] - Loss: 3.894 [-3.891, 0.002, -0.000]\n",
      "Epoch 6 [242/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [243/340] - Loss: 3.902 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [244/340] - Loss: 3.934 [-3.931, 0.002, -0.000]\n",
      "Epoch 6 [245/340] - Loss: 3.913 [-3.911, 0.002, -0.000]\n",
      "Epoch 6 [246/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 6 [247/340] - Loss: 3.905 [-3.903, 0.002, -0.000]\n",
      "Epoch 6 [248/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [249/340] - Loss: 3.875 [-3.872, 0.002, -0.000]\n",
      "Epoch 6 [250/340] - Loss: 4.031 [-4.028, 0.002, -0.000]\n",
      "Epoch 6 [251/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 6 [252/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [253/340] - Loss: 3.949 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [254/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [255/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 6 [256/340] - Loss: 3.848 [-3.846, 0.002, -0.000]\n",
      "Epoch 6 [257/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [258/340] - Loss: 3.907 [-3.905, 0.002, -0.000]\n",
      "Epoch 6 [259/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 6 [260/340] - Loss: 3.898 [-3.896, 0.002, -0.000]\n",
      "Epoch 6 [261/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 6 [262/340] - Loss: 3.997 [-3.994, 0.002, -0.000]\n",
      "Epoch 6 [263/340] - Loss: 3.885 [-3.883, 0.002, -0.000]\n",
      "Epoch 6 [264/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 6 [265/340] - Loss: 3.902 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [266/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [267/340] - Loss: 3.938 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [268/340] - Loss: 3.865 [-3.863, 0.002, -0.000]\n",
      "Epoch 6 [269/340] - Loss: 3.919 [-3.917, 0.002, -0.000]\n",
      "Epoch 6 [270/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 6 [271/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [272/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 6 [273/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 6 [274/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 6 [275/340] - Loss: 3.888 [-3.886, 0.002, -0.000]\n",
      "Epoch 6 [276/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [277/340] - Loss: 3.881 [-3.879, 0.002, -0.000]\n",
      "Epoch 6 [278/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [279/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 6 [280/340] - Loss: 3.894 [-3.892, 0.002, -0.000]\n",
      "Epoch 6 [281/340] - Loss: 3.858 [-3.856, 0.002, -0.000]\n",
      "Epoch 6 [282/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [283/340] - Loss: 3.929 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [284/340] - Loss: 3.904 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [285/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [286/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [287/340] - Loss: 3.910 [-3.908, 0.002, -0.000]\n",
      "Epoch 6 [288/340] - Loss: 3.900 [-3.898, 0.002, -0.000]\n",
      "Epoch 6 [289/340] - Loss: 3.932 [-3.929, 0.002, -0.000]\n",
      "Epoch 6 [290/340] - Loss: 3.903 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [291/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 6 [292/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [293/340] - Loss: 3.855 [-3.853, 0.002, -0.000]\n",
      "Epoch 6 [294/340] - Loss: 3.887 [-3.885, 0.002, -0.000]\n",
      "Epoch 6 [295/340] - Loss: 3.864 [-3.862, 0.002, -0.000]\n",
      "Epoch 6 [296/340] - Loss: 3.890 [-3.887, 0.002, -0.000]\n",
      "Epoch 6 [297/340] - Loss: 3.893 [-3.891, 0.002, -0.000]\n",
      "Epoch 6 [298/340] - Loss: 3.889 [-3.886, 0.002, -0.000]\n",
      "Epoch 6 [299/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [300/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 6 [301/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 6 [302/340] - Loss: 3.878 [-3.876, 0.002, -0.000]\n",
      "Epoch 6 [303/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [304/340] - Loss: 3.911 [-3.908, 0.002, -0.000]\n",
      "Epoch 6 [305/340] - Loss: 3.919 [-3.917, 0.002, -0.000]\n",
      "Epoch 6 [306/340] - Loss: 3.876 [-3.873, 0.002, -0.000]\n",
      "Epoch 6 [307/340] - Loss: 3.855 [-3.853, 0.002, -0.000]\n",
      "Epoch 6 [308/340] - Loss: 3.876 [-3.874, 0.002, -0.000]\n",
      "Epoch 6 [309/340] - Loss: 3.956 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [310/340] - Loss: 3.894 [-3.892, 0.002, -0.000]\n",
      "Epoch 6 [311/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 6 [312/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 6 [313/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 6 [314/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 6 [315/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 6 [316/340] - Loss: 3.877 [-3.875, 0.002, -0.000]\n",
      "Epoch 6 [317/340] - Loss: 3.876 [-3.874, 0.002, -0.000]\n",
      "Epoch 6 [318/340] - Loss: 3.898 [-3.896, 0.002, -0.000]\n",
      "Epoch 6 [319/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [320/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 6 [321/340] - Loss: 3.994 [-3.991, 0.002, -0.000]\n",
      "Epoch 6 [322/340] - Loss: 3.944 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [323/340] - Loss: 3.955 [-3.952, 0.002, -0.000]\n",
      "Epoch 6 [324/340] - Loss: 3.901 [-3.898, 0.002, -0.000]\n",
      "Epoch 6 [325/340] - Loss: 3.868 [-3.866, 0.002, -0.000]\n",
      "Epoch 6 [326/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [327/340] - Loss: 3.944 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [328/340] - Loss: 3.921 [-3.919, 0.002, -0.000]\n",
      "Epoch 6 [329/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [330/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 6 [331/340] - Loss: 3.923 [-3.920, 0.002, -0.000]\n",
      "Epoch 6 [332/340] - Loss: 3.872 [-3.870, 0.002, -0.000]\n",
      "Epoch 6 [333/340] - Loss: 3.880 [-3.877, 0.002, -0.000]\n",
      "Epoch 6 [334/340] - Loss: 3.957 [-3.954, 0.002, -0.000]\n",
      "Epoch 6 [335/340] - Loss: 3.877 [-3.874, 0.002, -0.000]\n",
      "Epoch 6 [336/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [337/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [338/340] - Loss: 3.884 [-3.882, 0.002, -0.000]\n",
      "Epoch 6 [339/340] - Loss: 3.833 [-3.831, 0.002, -0.000]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 1 epochs of training in this tutorial\n",
    "num_epochs = 6\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Because the grid is relatively small, we turn off the Toeplitz matrix multiplication and just perform them directly\n",
    "        # We find this to be more efficient when the grid is very small.\n",
    "        with gpytorch.settings.use_toeplitz(False):\n",
    "            output = model(x_batch)\n",
    "            log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "            loss = -(log_lik - kl_div + log_prior)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader), loss.item(), log_lik.item(), kl_div.item(), log_prior.item()))\n",
    "\n",
    "        # The actual optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`) using the standard SKI testing code, with no acceleration or precomputation. Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although our other tutorials demonstrate how to do this (for example, see the CIFAR tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 8.560413360595703\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
