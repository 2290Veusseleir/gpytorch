{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA) (w/ SVGP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use Deep Kernel Learning with SVGP stochastic variational regression to rapidly train using minibatches on the `3droad` UCI dataset with hundreds of thousands of training examples. \n",
    "\n",
    "In contrast to the SVDKL_Regression_GridInterp_CUDA notebook, we'll be using SVGP (https://arxiv.org/pdf/1411.2005.pdf) here to learn the inducing point locations. Our implementation of SVGP is modified to be efficient with the inference techniques used in GPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `song` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~136 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading '3droad' UCI dataset...\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Feature Extractor\n",
    "\n",
    "Next, we define the neural network feature extractor used to define the deep kernel. In this case, we use a fully connected network with the architecture `d -> 1000 -> 500 -> 50 -> 2`, as described in the original DKL paper. All of the code below uses standard PyTorch implementations of neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('bn1', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 1000))\n",
    "        self.add_module('bn2', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu2', torch.nn.ReLU())                       \n",
    "        self.add_module('linear3', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('bn3', torch.nn.BatchNorm1d(500))\n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('bn4', torch.nn.BatchNorm1d(50))\n",
    "        self.add_module('relu4', torch.nn.ReLU())                  \n",
    "        self.add_module('linear5', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Regression Layer\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. In this example, because we will be learning the inducing point locations, we'll be using a base `VariationalStrategy` with `learn_inducing_locations=True`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "class GPRegressionLayer(AbstractVariationalGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPRegressionLayer, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "            log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1., sigma=0.1, log_transform=True)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Model\n",
    "\n",
    "With the feature extractor and GP regression layer defined, we can now define our full model. To do this, we simply create a module whose `forward()` method passes the data first through the feature extractor, and then through the GP regression layer.\n",
    "\n",
    "The only other interesting feature of the model below is that we use a helper function, `scale_to_bounds`, to ensure that the features extracted by the neural network fit within the grid bounds used for SKI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, inducing_points, feature_extractor, num_features, grid_bounds=(-1., 1.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GPRegressionLayer(inducing_points)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "inducing_points = gpytorch.utils.grid.scale_to_bounds(feature_extractor(train_x[:500, :]), -1, 1)\n",
    "model = DKLModel(inducing_points=inducing_points, feature_extractor=feature_extractor, num_features=num_features).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the DKL model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalMarginalLogLikelihood` or ELBO), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrg365/gpytorch/gpytorch/utils/cholesky.py:14: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  potrf_list = [sub_mat.potrf() for sub_mat in mat.view(-1, *mat.shape[-2:])]\n",
      "/home/jrg365/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py:74: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  ld_one = lr_flipped.potrf().diag().log().sum() * 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/340] - Loss: 171.225 [-169.997, 1.229, -0.000]\n",
      "Epoch 1 [1/340] - Loss: 167.302 [-166.596, 0.707, -0.000]\n",
      "Epoch 1 [2/340] - Loss: 139.774 [-139.006, 0.768, -0.000]\n",
      "Epoch 1 [3/340] - Loss: 125.479 [-124.646, 0.833, -0.000]\n",
      "Epoch 1 [4/340] - Loss: 119.970 [-119.137, 0.833, -0.000]\n",
      "Epoch 1 [5/340] - Loss: 114.708 [-113.955, 0.753, -0.000]\n",
      "Epoch 1 [6/340] - Loss: 94.375 [-93.734, 0.641, -0.000]\n",
      "Epoch 1 [7/340] - Loss: 87.867 [-87.327, 0.541, -0.000]\n",
      "Epoch 1 [8/340] - Loss: 76.002 [-75.536, 0.466, -0.000]\n",
      "Epoch 1 [9/340] - Loss: 68.272 [-67.854, 0.417, -0.000]\n",
      "Epoch 1 [10/340] - Loss: 66.167 [-65.778, 0.389, -0.000]\n",
      "Epoch 1 [11/340] - Loss: 63.866 [-63.465, 0.400, -0.000]\n",
      "Epoch 1 [12/340] - Loss: 54.164 [-53.690, 0.474, -0.000]\n",
      "Epoch 1 [13/340] - Loss: 52.675 [-52.107, 0.569, -0.000]\n",
      "Epoch 1 [14/340] - Loss: 45.446 [-44.798, 0.649, -0.000]\n",
      "Epoch 1 [15/340] - Loss: 40.557 [-39.868, 0.690, -0.000]\n",
      "Epoch 1 [16/340] - Loss: 42.817 [-42.124, 0.693, -0.000]\n",
      "Epoch 1 [17/340] - Loss: 41.038 [-40.369, 0.669, -0.000]\n",
      "Epoch 1 [18/340] - Loss: 37.831 [-37.201, 0.630, -0.000]\n",
      "Epoch 1 [19/340] - Loss: 36.408 [-35.826, 0.582, -0.000]\n",
      "Epoch 1 [20/340] - Loss: 34.367 [-33.829, 0.537, -0.000]\n",
      "Epoch 1 [21/340] - Loss: 31.479 [-30.977, 0.502, -0.000]\n",
      "Epoch 1 [22/340] - Loss: 26.878 [-26.399, 0.478, -0.000]\n",
      "Epoch 1 [23/340] - Loss: 27.399 [-26.931, 0.468, -0.000]\n",
      "Epoch 1 [24/340] - Loss: 24.219 [-23.753, 0.466, -0.000]\n",
      "Epoch 1 [25/340] - Loss: 23.667 [-23.188, 0.479, -0.000]\n",
      "Epoch 1 [26/340] - Loss: 25.618 [-25.122, 0.496, -0.000]\n",
      "Epoch 1 [27/340] - Loss: 21.535 [-21.018, 0.517, -0.000]\n",
      "Epoch 1 [28/340] - Loss: 23.992 [-23.456, 0.536, -0.000]\n",
      "Epoch 1 [29/340] - Loss: 20.783 [-20.259, 0.524, -0.000]\n",
      "Epoch 1 [30/340] - Loss: 19.764 [-19.253, 0.511, -0.000]\n",
      "Epoch 1 [31/340] - Loss: 19.011 [-18.521, 0.490, -0.000]\n",
      "Epoch 1 [32/340] - Loss: 17.376 [-16.912, 0.464, -0.000]\n",
      "Epoch 1 [33/340] - Loss: 17.816 [-17.375, 0.441, -0.000]\n",
      "Epoch 1 [34/340] - Loss: 17.283 [-16.857, 0.425, -0.000]\n",
      "Epoch 1 [35/340] - Loss: 16.922 [-16.509, 0.413, -0.000]\n",
      "Epoch 1 [36/340] - Loss: 17.189 [-16.788, 0.401, -0.000]\n",
      "Epoch 1 [37/340] - Loss: 16.008 [-15.617, 0.391, -0.000]\n",
      "Epoch 1 [38/340] - Loss: 15.515 [-15.133, 0.382, -0.000]\n",
      "Epoch 1 [39/340] - Loss: 14.965 [-14.593, 0.372, -0.000]\n",
      "Epoch 1 [40/340] - Loss: 14.511 [-14.154, 0.357, -0.000]\n",
      "Epoch 1 [41/340] - Loss: 12.808 [-12.468, 0.340, -0.000]\n",
      "Epoch 1 [42/340] - Loss: 14.154 [-13.831, 0.322, -0.000]\n",
      "Epoch 1 [43/340] - Loss: 12.565 [-12.256, 0.308, -0.000]\n",
      "Epoch 1 [44/340] - Loss: 12.354 [-12.056, 0.298, -0.000]\n",
      "Epoch 1 [45/340] - Loss: 13.221 [-12.931, 0.290, -0.000]\n",
      "Epoch 1 [46/340] - Loss: 11.543 [-11.263, 0.280, -0.000]\n",
      "Epoch 1 [47/340] - Loss: 12.706 [-12.437, 0.270, -0.000]\n",
      "Epoch 1 [48/340] - Loss: 12.808 [-12.543, 0.265, -0.000]\n",
      "Epoch 1 [49/340] - Loss: 11.576 [-11.306, 0.270, -0.000]\n",
      "Epoch 1 [50/340] - Loss: 12.121 [-11.840, 0.281, -0.000]\n",
      "Epoch 1 [51/340] - Loss: 12.631 [-12.345, 0.286, -0.000]\n",
      "Epoch 1 [52/340] - Loss: 10.891 [-10.594, 0.297, -0.000]\n",
      "Epoch 1 [53/340] - Loss: 10.424 [-10.115, 0.309, -0.000]\n",
      "Epoch 1 [54/340] - Loss: 11.971 [-11.651, 0.319, -0.000]\n",
      "Epoch 1 [55/340] - Loss: 10.999 [-10.676, 0.322, -0.000]\n",
      "Epoch 1 [56/340] - Loss: 9.827 [-9.502, 0.325, -0.000]\n",
      "Epoch 1 [57/340] - Loss: 10.507 [-10.186, 0.320, -0.000]\n",
      "Epoch 1 [58/340] - Loss: 9.534 [-9.219, 0.315, -0.000]\n",
      "Epoch 1 [59/340] - Loss: 10.113 [-9.806, 0.307, -0.000]\n",
      "Epoch 1 [60/340] - Loss: 9.941 [-9.653, 0.288, -0.000]\n",
      "Epoch 1 [61/340] - Loss: 9.831 [-9.555, 0.275, -0.000]\n",
      "Epoch 1 [62/340] - Loss: 8.994 [-8.741, 0.253, -0.000]\n",
      "Epoch 1 [63/340] - Loss: 9.168 [-8.933, 0.235, -0.000]\n",
      "Epoch 1 [64/340] - Loss: 9.597 [-9.379, 0.219, -0.000]\n",
      "Epoch 1 [65/340] - Loss: 9.573 [-9.365, 0.208, -0.000]\n",
      "Epoch 1 [66/340] - Loss: 8.547 [-8.351, 0.196, -0.000]\n",
      "Epoch 1 [67/340] - Loss: 9.051 [-8.865, 0.186, -0.000]\n",
      "Epoch 1 [68/340] - Loss: 9.439 [-9.261, 0.179, -0.000]\n",
      "Epoch 1 [69/340] - Loss: 8.689 [-8.515, 0.174, -0.000]\n",
      "Epoch 1 [70/340] - Loss: 8.071 [-7.899, 0.173, -0.000]\n",
      "Epoch 1 [71/340] - Loss: 8.231 [-8.057, 0.174, -0.000]\n",
      "Epoch 1 [72/340] - Loss: 8.446 [-8.267, 0.179, -0.000]\n",
      "Epoch 1 [73/340] - Loss: 8.670 [-8.489, 0.181, -0.000]\n",
      "Epoch 1 [74/340] - Loss: 8.049 [-7.871, 0.179, -0.000]\n",
      "Epoch 1 [75/340] - Loss: 8.739 [-8.553, 0.186, -0.000]\n",
      "Epoch 1 [76/340] - Loss: 8.326 [-8.161, 0.165, -0.000]\n",
      "Epoch 1 [77/340] - Loss: 8.502 [-8.349, 0.153, -0.000]\n",
      "Epoch 1 [78/340] - Loss: 7.869 [-7.722, 0.146, -0.000]\n",
      "Epoch 1 [79/340] - Loss: 8.364 [-8.221, 0.144, -0.000]\n",
      "Epoch 1 [80/340] - Loss: 7.937 [-7.796, 0.141, -0.000]\n",
      "Epoch 1 [81/340] - Loss: 7.654 [-7.505, 0.148, -0.000]\n",
      "Epoch 1 [82/340] - Loss: 7.779 [-7.618, 0.161, -0.000]\n",
      "Epoch 1 [83/340] - Loss: 8.277 [-8.104, 0.173, -0.000]\n",
      "Epoch 1 [84/340] - Loss: 7.830 [-7.644, 0.186, -0.000]\n",
      "Epoch 1 [85/340] - Loss: 7.446 [-7.248, 0.198, -0.000]\n",
      "Epoch 1 [86/340] - Loss: 7.811 [-7.600, 0.210, -0.000]\n",
      "Epoch 1 [87/340] - Loss: 7.684 [-7.465, 0.219, -0.000]\n",
      "Epoch 1 [88/340] - Loss: 8.495 [-8.269, 0.226, -0.000]\n",
      "Epoch 1 [89/340] - Loss: 7.403 [-7.171, 0.231, -0.000]\n",
      "Epoch 1 [90/340] - Loss: 7.882 [-7.648, 0.234, -0.000]\n",
      "Epoch 1 [91/340] - Loss: 7.293 [-7.059, 0.234, -0.000]\n",
      "Epoch 1 [92/340] - Loss: 7.655 [-7.426, 0.229, -0.000]\n",
      "Epoch 1 [93/340] - Loss: 7.425 [-7.203, 0.222, -0.000]\n",
      "Epoch 1 [94/340] - Loss: 6.965 [-6.752, 0.213, -0.000]\n",
      "Epoch 1 [95/340] - Loss: 6.743 [-6.542, 0.201, -0.000]\n",
      "Epoch 1 [96/340] - Loss: 7.218 [-7.022, 0.196, -0.000]\n",
      "Epoch 1 [97/340] - Loss: 7.935 [-7.743, 0.192, -0.000]\n",
      "Epoch 1 [98/340] - Loss: 7.285 [-7.094, 0.190, -0.000]\n",
      "Epoch 1 [99/340] - Loss: 7.091 [-6.889, 0.202, -0.000]\n",
      "Epoch 1 [100/340] - Loss: 6.455 [-6.229, 0.226, -0.000]\n",
      "Epoch 1 [101/340] - Loss: 7.358 [-7.104, 0.254, -0.000]\n",
      "Epoch 1 [102/340] - Loss: 7.030 [-6.796, 0.234, -0.000]\n",
      "Epoch 1 [103/340] - Loss: 6.953 [-6.744, 0.209, -0.000]\n",
      "Epoch 1 [104/340] - Loss: 7.230 [-7.045, 0.185, -0.000]\n",
      "Epoch 1 [105/340] - Loss: 7.218 [-7.045, 0.172, -0.000]\n",
      "Epoch 1 [106/340] - Loss: 6.837 [-6.724, 0.114, -0.000]\n",
      "Epoch 1 [107/340] - Loss: 8.750 [-8.676, 0.074, -0.000]\n",
      "Epoch 1 [108/340] - Loss: 6.966 [-6.841, 0.124, -0.000]\n",
      "Epoch 1 [109/340] - Loss: 6.787 [-6.614, 0.172, -0.000]\n",
      "Epoch 1 [110/340] - Loss: 7.488 [-7.264, 0.224, -0.000]\n",
      "Epoch 1 [111/340] - Loss: 6.951 [-6.689, 0.262, -0.000]\n",
      "Epoch 1 [112/340] - Loss: 6.807 [-6.513, 0.294, -0.000]\n",
      "Epoch 1 [113/340] - Loss: 7.060 [-6.743, 0.317, -0.000]\n",
      "Epoch 1 [114/340] - Loss: 6.845 [-6.510, 0.335, -0.000]\n",
      "Epoch 1 [115/340] - Loss: 6.906 [-6.557, 0.349, -0.000]\n",
      "Epoch 1 [116/340] - Loss: 7.152 [-6.791, 0.360, -0.000]\n",
      "Epoch 1 [117/340] - Loss: 7.055 [-6.686, 0.369, -0.000]\n",
      "Epoch 1 [118/340] - Loss: 7.193 [-6.819, 0.374, -0.000]\n",
      "Epoch 1 [119/340] - Loss: 7.461 [-7.088, 0.373, -0.000]\n",
      "Epoch 1 [120/340] - Loss: 7.233 [-6.864, 0.369, -0.000]\n",
      "Epoch 1 [121/340] - Loss: 7.084 [-6.723, 0.360, -0.000]\n",
      "Epoch 1 [122/340] - Loss: 6.832 [-6.482, 0.350, -0.000]\n",
      "Epoch 1 [123/340] - Loss: 7.047 [-6.709, 0.337, -0.000]\n",
      "Epoch 1 [124/340] - Loss: 6.779 [-6.457, 0.322, -0.000]\n",
      "Epoch 1 [125/340] - Loss: 6.448 [-6.141, 0.306, -0.000]\n",
      "Epoch 1 [126/340] - Loss: 6.548 [-6.257, 0.290, -0.000]\n",
      "Epoch 1 [127/340] - Loss: 6.348 [-6.073, 0.274, -0.000]\n",
      "Epoch 1 [128/340] - Loss: 6.553 [-6.294, 0.259, -0.000]\n",
      "Epoch 1 [129/340] - Loss: 6.234 [-5.990, 0.244, -0.000]\n",
      "Epoch 1 [130/340] - Loss: 6.298 [-6.069, 0.228, -0.000]\n",
      "Epoch 1 [131/340] - Loss: 6.581 [-6.367, 0.214, -0.000]\n",
      "Epoch 1 [132/340] - Loss: 5.848 [-5.650, 0.198, -0.000]\n",
      "Epoch 1 [133/340] - Loss: 6.259 [-6.074, 0.185, -0.000]\n",
      "Epoch 1 [134/340] - Loss: 6.048 [-5.876, 0.172, -0.000]\n",
      "Epoch 1 [135/340] - Loss: 6.356 [-6.195, 0.160, -0.000]\n",
      "Epoch 1 [136/340] - Loss: 6.124 [-5.974, 0.150, -0.000]\n",
      "Epoch 1 [137/340] - Loss: 6.231 [-6.090, 0.141, -0.000]\n",
      "Epoch 1 [138/340] - Loss: 5.956 [-5.823, 0.132, -0.000]\n",
      "Epoch 1 [139/340] - Loss: 6.012 [-5.888, 0.124, -0.000]\n",
      "Epoch 1 [140/340] - Loss: 6.212 [-6.094, 0.118, -0.000]\n",
      "Epoch 1 [141/340] - Loss: 6.019 [-5.902, 0.117, -0.000]\n",
      "Epoch 1 [142/340] - Loss: 5.855 [-5.737, 0.118, -0.000]\n",
      "Epoch 1 [143/340] - Loss: 5.924 [-5.804, 0.120, -0.000]\n",
      "Epoch 1 [144/340] - Loss: 6.020 [-5.895, 0.124, -0.000]\n",
      "Epoch 1 [145/340] - Loss: 5.845 [-5.720, 0.125, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [146/340] - Loss: 6.269 [-6.153, 0.116, -0.000]\n",
      "Epoch 1 [147/340] - Loss: 5.786 [-5.675, 0.111, -0.000]\n",
      "Epoch 1 [148/340] - Loss: 6.006 [-5.895, 0.111, -0.000]\n",
      "Epoch 1 [149/340] - Loss: 6.080 [-5.961, 0.119, -0.000]\n",
      "Epoch 1 [150/340] - Loss: 6.965 [-6.844, 0.120, -0.000]\n",
      "Epoch 1 [151/340] - Loss: 6.138 [-6.028, 0.111, -0.000]\n",
      "Epoch 1 [152/340] - Loss: 6.168 [-6.078, 0.090, -0.000]\n",
      "Epoch 1 [153/340] - Loss: 5.905 [-5.828, 0.076, -0.000]\n",
      "Epoch 1 [154/340] - Loss: 5.825 [-5.755, 0.069, -0.000]\n",
      "Epoch 1 [155/340] - Loss: 5.967 [-5.892, 0.076, -0.000]\n",
      "Epoch 1 [156/340] - Loss: 9.614 [-9.537, 0.078, -0.000]\n",
      "Epoch 1 [157/340] - Loss: 6.037 [-5.973, 0.064, -0.000]\n",
      "Epoch 1 [158/340] - Loss: 6.340 [-6.290, 0.050, -0.000]\n",
      "Epoch 1 [159/340] - Loss: 5.718 [-5.657, 0.060, -0.000]\n",
      "Epoch 1 [160/340] - Loss: 5.786 [-5.728, 0.059, -0.000]\n",
      "Epoch 1 [161/340] - Loss: 5.847 [-5.803, 0.044, -0.000]\n",
      "Epoch 1 [162/340] - Loss: 5.724 [-5.683, 0.041, -0.000]\n",
      "Epoch 1 [163/340] - Loss: 6.567 [-6.529, 0.038, -0.000]\n",
      "Epoch 1 [164/340] - Loss: 6.160 [-6.128, 0.032, -0.000]\n",
      "Epoch 1 [165/340] - Loss: 5.584 [-5.554, 0.029, -0.000]\n",
      "Epoch 1 [166/340] - Loss: 5.748 [-5.722, 0.027, -0.000]\n",
      "Epoch 1 [167/340] - Loss: 5.491 [-5.466, 0.025, -0.000]\n",
      "Epoch 1 [168/340] - Loss: 5.600 [-5.572, 0.027, -0.000]\n",
      "Epoch 1 [169/340] - Loss: 5.686 [-5.658, 0.027, -0.000]\n",
      "Epoch 1 [170/340] - Loss: 6.033 [-6.006, 0.027, -0.000]\n",
      "Epoch 1 [171/340] - Loss: 5.812 [-5.783, 0.029, -0.000]\n",
      "Epoch 1 [172/340] - Loss: 5.702 [-5.673, 0.029, -0.000]\n",
      "Epoch 1 [173/340] - Loss: 5.678 [-5.649, 0.028, -0.000]\n",
      "Epoch 1 [174/340] - Loss: 5.410 [-5.382, 0.028, -0.000]\n",
      "Epoch 1 [175/340] - Loss: 5.516 [-5.490, 0.027, -0.000]\n",
      "Epoch 1 [176/340] - Loss: 5.631 [-5.604, 0.026, -0.000]\n",
      "Epoch 1 [177/340] - Loss: 5.385 [-5.359, 0.026, -0.000]\n",
      "Epoch 1 [178/340] - Loss: 5.730 [-5.704, 0.026, -0.000]\n",
      "Epoch 1 [179/340] - Loss: 5.789 [-5.764, 0.025, -0.000]\n",
      "Epoch 1 [180/340] - Loss: 5.127 [-5.102, 0.025, -0.000]\n",
      "Epoch 1 [181/340] - Loss: 5.329 [-5.306, 0.024, -0.000]\n",
      "Epoch 1 [182/340] - Loss: 5.285 [-5.260, 0.025, -0.000]\n",
      "Epoch 1 [183/340] - Loss: 5.234 [-5.208, 0.027, -0.000]\n",
      "Epoch 1 [184/340] - Loss: 5.336 [-5.310, 0.026, -0.000]\n",
      "Epoch 1 [185/340] - Loss: 5.394 [-5.370, 0.024, -0.000]\n",
      "Epoch 1 [186/340] - Loss: 5.565 [-5.542, 0.023, -0.000]\n",
      "Epoch 1 [187/340] - Loss: 5.849 [-5.827, 0.022, -0.000]\n",
      "Epoch 1 [188/340] - Loss: 5.531 [-5.510, 0.020, -0.000]\n",
      "Epoch 1 [189/340] - Loss: 5.389 [-5.368, 0.021, -0.000]\n",
      "Epoch 1 [190/340] - Loss: 5.433 [-5.415, 0.019, -0.000]\n",
      "Epoch 1 [191/340] - Loss: 5.434 [-5.416, 0.018, -0.000]\n",
      "Epoch 1 [192/340] - Loss: 5.367 [-5.349, 0.018, -0.000]\n",
      "Epoch 1 [193/340] - Loss: 5.336 [-5.317, 0.020, -0.000]\n",
      "Epoch 1 [194/340] - Loss: 5.085 [-5.067, 0.018, -0.000]\n",
      "Epoch 1 [195/340] - Loss: 5.151 [-5.134, 0.017, -0.000]\n",
      "Epoch 1 [196/340] - Loss: 4.923 [-4.906, 0.017, -0.000]\n",
      "Epoch 1 [197/340] - Loss: 5.262 [-5.244, 0.018, -0.000]\n",
      "Epoch 1 [198/340] - Loss: 5.198 [-5.180, 0.018, -0.000]\n",
      "Epoch 1 [199/340] - Loss: 5.168 [-5.151, 0.018, -0.000]\n",
      "Epoch 1 [200/340] - Loss: 4.922 [-4.904, 0.018, -0.000]\n",
      "Epoch 1 [201/340] - Loss: 4.996 [-4.976, 0.020, -0.000]\n",
      "Epoch 1 [202/340] - Loss: 4.746 [-4.728, 0.018, -0.000]\n",
      "Epoch 1 [203/340] - Loss: 4.974 [-4.958, 0.016, -0.000]\n",
      "Epoch 1 [204/340] - Loss: 4.857 [-4.840, 0.016, -0.000]\n",
      "Epoch 1 [205/340] - Loss: 4.796 [-4.779, 0.016, -0.000]\n",
      "Epoch 1 [206/340] - Loss: 4.974 [-4.957, 0.017, -0.000]\n",
      "Epoch 1 [207/340] - Loss: 4.796 [-4.778, 0.018, -0.000]\n",
      "Epoch 1 [208/340] - Loss: 4.876 [-4.859, 0.016, -0.000]\n",
      "Epoch 1 [209/340] - Loss: 4.978 [-4.963, 0.016, -0.000]\n",
      "Epoch 1 [210/340] - Loss: 4.927 [-4.912, 0.015, -0.000]\n",
      "Epoch 1 [211/340] - Loss: 4.809 [-4.795, 0.014, -0.000]\n",
      "Epoch 1 [212/340] - Loss: 4.711 [-4.699, 0.012, -0.000]\n",
      "Epoch 1 [213/340] - Loss: 4.574 [-4.563, 0.012, -0.000]\n",
      "Epoch 1 [214/340] - Loss: 4.675 [-4.665, 0.011, -0.000]\n",
      "Epoch 1 [215/340] - Loss: 4.625 [-4.615, 0.010, -0.000]\n",
      "Epoch 1 [216/340] - Loss: 4.738 [-4.729, 0.009, -0.000]\n",
      "Epoch 1 [217/340] - Loss: 4.800 [-4.792, 0.008, -0.000]\n",
      "Epoch 1 [218/340] - Loss: 4.779 [-4.771, 0.008, -0.000]\n",
      "Epoch 1 [219/340] - Loss: 4.674 [-4.666, 0.008, -0.000]\n",
      "Epoch 1 [220/340] - Loss: 4.726 [-4.717, 0.008, -0.000]\n",
      "Epoch 1 [221/340] - Loss: 4.469 [-4.461, 0.009, -0.000]\n",
      "Epoch 1 [222/340] - Loss: 4.572 [-4.563, 0.009, -0.000]\n",
      "Epoch 1 [223/340] - Loss: 4.468 [-4.460, 0.008, -0.000]\n",
      "Epoch 1 [224/340] - Loss: 4.500 [-4.492, 0.008, -0.000]\n",
      "Epoch 1 [225/340] - Loss: 4.495 [-4.487, 0.008, -0.000]\n",
      "Epoch 1 [226/340] - Loss: 4.553 [-4.545, 0.008, -0.000]\n",
      "Epoch 1 [227/340] - Loss: 4.470 [-4.461, 0.008, -0.000]\n",
      "Epoch 1 [228/340] - Loss: 4.535 [-4.527, 0.008, -0.000]\n",
      "Epoch 1 [229/340] - Loss: 4.624 [-4.616, 0.008, -0.000]\n",
      "Epoch 1 [230/340] - Loss: 4.571 [-4.563, 0.008, -0.000]\n",
      "Epoch 1 [231/340] - Loss: 4.650 [-4.642, 0.008, -0.000]\n",
      "Epoch 1 [232/340] - Loss: 4.559 [-4.551, 0.007, -0.000]\n",
      "Epoch 1 [233/340] - Loss: 4.657 [-4.649, 0.008, -0.000]\n",
      "Epoch 1 [234/340] - Loss: 4.517 [-4.509, 0.008, -0.000]\n",
      "Epoch 1 [235/340] - Loss: 4.547 [-4.539, 0.008, -0.000]\n",
      "Epoch 1 [236/340] - Loss: 4.426 [-4.418, 0.008, -0.000]\n",
      "Epoch 1 [237/340] - Loss: 4.522 [-4.513, 0.008, -0.000]\n",
      "Epoch 1 [238/340] - Loss: 4.324 [-4.316, 0.008, -0.000]\n",
      "Epoch 1 [239/340] - Loss: 4.492 [-4.484, 0.008, -0.000]\n",
      "Epoch 1 [240/340] - Loss: 4.539 [-4.531, 0.008, -0.000]\n",
      "Epoch 1 [241/340] - Loss: 4.521 [-4.513, 0.008, -0.000]\n",
      "Epoch 1 [242/340] - Loss: 4.473 [-4.465, 0.008, -0.000]\n",
      "Epoch 1 [243/340] - Loss: 4.478 [-4.471, 0.007, -0.000]\n",
      "Epoch 1 [244/340] - Loss: 4.546 [-4.538, 0.007, -0.000]\n",
      "Epoch 1 [245/340] - Loss: 4.530 [-4.523, 0.007, -0.000]\n",
      "Epoch 1 [246/340] - Loss: 4.450 [-4.443, 0.007, -0.000]\n",
      "Epoch 1 [247/340] - Loss: 4.586 [-4.580, 0.006, -0.000]\n",
      "Epoch 1 [248/340] - Loss: 4.515 [-4.509, 0.006, -0.000]\n",
      "Epoch 1 [249/340] - Loss: 4.463 [-4.456, 0.006, -0.000]\n",
      "Epoch 1 [250/340] - Loss: 4.380 [-4.374, 0.006, -0.000]\n",
      "Epoch 1 [251/340] - Loss: 4.410 [-4.404, 0.006, -0.000]\n",
      "Epoch 1 [252/340] - Loss: 4.363 [-4.357, 0.006, -0.000]\n",
      "Epoch 1 [253/340] - Loss: 4.378 [-4.372, 0.006, -0.000]\n",
      "Epoch 1 [254/340] - Loss: 4.356 [-4.349, 0.007, -0.000]\n",
      "Epoch 1 [255/340] - Loss: 4.457 [-4.450, 0.007, -0.000]\n",
      "Epoch 1 [256/340] - Loss: 4.460 [-4.453, 0.007, -0.000]\n",
      "Epoch 1 [257/340] - Loss: 4.280 [-4.273, 0.007, -0.000]\n",
      "Epoch 1 [258/340] - Loss: 4.354 [-4.347, 0.006, -0.000]\n",
      "Epoch 1 [259/340] - Loss: 4.329 [-4.323, 0.006, -0.000]\n",
      "Epoch 1 [260/340] - Loss: 4.333 [-4.328, 0.006, -0.000]\n",
      "Epoch 1 [261/340] - Loss: 4.439 [-4.434, 0.005, -0.000]\n",
      "Epoch 1 [262/340] - Loss: 4.474 [-4.469, 0.005, -0.000]\n",
      "Epoch 1 [263/340] - Loss: 4.325 [-4.320, 0.005, -0.000]\n",
      "Epoch 1 [264/340] - Loss: 4.491 [-4.486, 0.005, -0.000]\n",
      "Epoch 1 [265/340] - Loss: 4.264 [-4.259, 0.005, -0.000]\n",
      "Epoch 1 [266/340] - Loss: 4.430 [-4.424, 0.005, -0.000]\n",
      "Epoch 1 [267/340] - Loss: 4.418 [-4.413, 0.005, -0.000]\n",
      "Epoch 1 [268/340] - Loss: 4.500 [-4.495, 0.005, -0.000]\n",
      "Epoch 1 [269/340] - Loss: 4.435 [-4.430, 0.005, -0.000]\n",
      "Epoch 1 [270/340] - Loss: 4.460 [-4.454, 0.006, -0.000]\n",
      "Epoch 1 [271/340] - Loss: 4.540 [-4.534, 0.006, -0.000]\n",
      "Epoch 1 [272/340] - Loss: 4.953 [-4.947, 0.007, -0.000]\n",
      "Epoch 1 [273/340] - Loss: 4.545 [-4.540, 0.005, -0.000]\n",
      "Epoch 1 [274/340] - Loss: 4.471 [-4.466, 0.005, -0.000]\n",
      "Epoch 1 [275/340] - Loss: 4.492 [-4.487, 0.005, -0.000]\n",
      "Epoch 1 [276/340] - Loss: 4.596 [-4.591, 0.005, -0.000]\n",
      "Epoch 1 [277/340] - Loss: 4.342 [-4.337, 0.005, -0.000]\n",
      "Epoch 1 [278/340] - Loss: 4.771 [-4.766, 0.006, -0.000]\n",
      "Epoch 1 [279/340] - Loss: 4.564 [-4.560, 0.005, -0.000]\n",
      "Epoch 1 [280/340] - Loss: 4.599 [-4.594, 0.005, -0.000]\n",
      "Epoch 1 [281/340] - Loss: 4.576 [-4.571, 0.005, -0.000]\n",
      "Epoch 1 [282/340] - Loss: 4.487 [-4.482, 0.005, -0.000]\n",
      "Epoch 1 [283/340] - Loss: 4.646 [-4.642, 0.004, -0.000]\n",
      "Epoch 1 [284/340] - Loss: 4.399 [-4.394, 0.004, -0.000]\n",
      "Epoch 1 [285/340] - Loss: 4.514 [-4.509, 0.005, -0.000]\n",
      "Epoch 1 [286/340] - Loss: 4.483 [-4.479, 0.005, -0.000]\n",
      "Epoch 1 [287/340] - Loss: 4.415 [-4.411, 0.004, -0.000]\n",
      "Epoch 1 [288/340] - Loss: 4.445 [-4.441, 0.005, -0.000]\n",
      "Epoch 1 [289/340] - Loss: 4.343 [-4.338, 0.005, -0.000]\n",
      "Epoch 1 [290/340] - Loss: 4.242 [-4.237, 0.005, -0.000]\n",
      "Epoch 1 [291/340] - Loss: 4.374 [-4.369, 0.005, -0.000]\n",
      "Epoch 1 [292/340] - Loss: 4.408 [-4.403, 0.005, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [293/340] - Loss: 4.236 [-4.231, 0.005, -0.000]\n",
      "Epoch 1 [294/340] - Loss: 4.310 [-4.305, 0.005, -0.000]\n",
      "Epoch 1 [295/340] - Loss: 4.278 [-4.272, 0.005, -0.000]\n",
      "Epoch 1 [296/340] - Loss: 4.343 [-4.338, 0.005, -0.000]\n",
      "Epoch 1 [297/340] - Loss: 4.299 [-4.293, 0.005, -0.000]\n",
      "Epoch 1 [298/340] - Loss: 4.195 [-4.189, 0.005, -0.000]\n",
      "Epoch 1 [299/340] - Loss: 4.158 [-4.152, 0.006, -0.000]\n",
      "Epoch 1 [300/340] - Loss: 4.264 [-4.258, 0.006, -0.000]\n",
      "Epoch 1 [301/340] - Loss: 4.255 [-4.249, 0.006, -0.000]\n",
      "Epoch 1 [302/340] - Loss: 4.393 [-4.387, 0.006, -0.000]\n",
      "Epoch 1 [303/340] - Loss: 4.290 [-4.283, 0.007, -0.000]\n",
      "Epoch 1 [304/340] - Loss: 4.280 [-4.273, 0.007, -0.000]\n",
      "Epoch 1 [305/340] - Loss: 4.312 [-4.305, 0.007, -0.000]\n",
      "Epoch 1 [306/340] - Loss: 4.549 [-4.542, 0.007, -0.000]\n",
      "Epoch 1 [307/340] - Loss: 4.442 [-4.435, 0.007, -0.000]\n",
      "Epoch 1 [308/340] - Loss: 4.357 [-4.350, 0.006, -0.000]\n",
      "Epoch 1 [309/340] - Loss: 4.431 [-4.425, 0.006, -0.000]\n",
      "Epoch 1 [310/340] - Loss: 4.346 [-4.340, 0.006, -0.000]\n",
      "Epoch 1 [311/340] - Loss: 4.335 [-4.329, 0.006, -0.000]\n",
      "Epoch 1 [312/340] - Loss: 4.394 [-4.388, 0.006, -0.000]\n",
      "Epoch 1 [313/340] - Loss: 4.275 [-4.269, 0.007, -0.000]\n",
      "Epoch 1 [314/340] - Loss: 4.319 [-4.312, 0.007, -0.000]\n",
      "Epoch 1 [315/340] - Loss: 4.300 [-4.292, 0.007, -0.000]\n",
      "Epoch 1 [316/340] - Loss: 4.301 [-4.294, 0.007, -0.000]\n",
      "Epoch 1 [317/340] - Loss: 4.235 [-4.228, 0.007, -0.000]\n",
      "Epoch 1 [318/340] - Loss: 4.242 [-4.235, 0.007, -0.000]\n",
      "Epoch 1 [319/340] - Loss: 4.412 [-4.405, 0.007, -0.000]\n",
      "Epoch 1 [320/340] - Loss: 4.403 [-4.396, 0.007, -0.000]\n",
      "Epoch 1 [321/340] - Loss: 4.338 [-4.332, 0.006, -0.000]\n",
      "Epoch 1 [322/340] - Loss: 4.323 [-4.317, 0.006, -0.000]\n",
      "Epoch 1 [323/340] - Loss: 4.299 [-4.293, 0.006, -0.000]\n",
      "Epoch 1 [324/340] - Loss: 4.260 [-4.255, 0.006, -0.000]\n",
      "Epoch 1 [325/340] - Loss: 4.289 [-4.284, 0.005, -0.000]\n",
      "Epoch 1 [326/340] - Loss: 4.329 [-4.324, 0.005, -0.000]\n",
      "Epoch 1 [327/340] - Loss: 4.226 [-4.221, 0.006, -0.000]\n",
      "Epoch 1 [328/340] - Loss: 4.253 [-4.247, 0.006, -0.000]\n",
      "Epoch 1 [329/340] - Loss: 4.251 [-4.245, 0.006, -0.000]\n",
      "Epoch 1 [330/340] - Loss: 4.175 [-4.168, 0.006, -0.000]\n",
      "Epoch 1 [331/340] - Loss: 4.394 [-4.387, 0.007, -0.000]\n",
      "Epoch 1 [332/340] - Loss: 4.257 [-4.251, 0.006, -0.000]\n",
      "Epoch 1 [333/340] - Loss: 4.218 [-4.212, 0.006, -0.000]\n",
      "Epoch 1 [334/340] - Loss: 4.178 [-4.173, 0.006, -0.000]\n",
      "Epoch 1 [335/340] - Loss: 4.209 [-4.203, 0.006, -0.000]\n",
      "Epoch 1 [336/340] - Loss: 4.358 [-4.353, 0.005, -0.000]\n",
      "Epoch 1 [337/340] - Loss: 4.236 [-4.231, 0.005, -0.000]\n",
      "Epoch 1 [338/340] - Loss: 4.302 [-4.297, 0.005, -0.000]\n",
      "Epoch 1 [339/340] - Loss: 4.137 [-4.132, 0.005, -0.000]\n",
      "Epoch 2 [0/340] - Loss: 4.315 [-4.309, 0.005, -0.000]\n",
      "Epoch 2 [1/340] - Loss: 4.208 [-4.203, 0.005, -0.000]\n",
      "Epoch 2 [2/340] - Loss: 4.177 [-4.172, 0.005, -0.000]\n",
      "Epoch 2 [3/340] - Loss: 4.171 [-4.166, 0.005, -0.000]\n",
      "Epoch 2 [4/340] - Loss: 4.228 [-4.222, 0.005, -0.000]\n",
      "Epoch 2 [5/340] - Loss: 4.173 [-4.167, 0.006, -0.000]\n",
      "Epoch 2 [6/340] - Loss: 4.217 [-4.211, 0.006, -0.000]\n",
      "Epoch 2 [7/340] - Loss: 4.310 [-4.305, 0.006, -0.000]\n",
      "Epoch 2 [8/340] - Loss: 4.231 [-4.225, 0.005, -0.000]\n",
      "Epoch 2 [9/340] - Loss: 4.183 [-4.178, 0.005, -0.000]\n",
      "Epoch 2 [10/340] - Loss: 4.204 [-4.199, 0.005, -0.000]\n",
      "Epoch 2 [11/340] - Loss: 4.264 [-4.259, 0.005, -0.000]\n",
      "Epoch 2 [12/340] - Loss: 4.313 [-4.308, 0.005, -0.000]\n",
      "Epoch 2 [13/340] - Loss: 4.162 [-4.158, 0.004, -0.000]\n",
      "Epoch 2 [14/340] - Loss: 4.273 [-4.268, 0.004, -0.000]\n",
      "Epoch 2 [15/340] - Loss: 4.177 [-4.173, 0.004, -0.000]\n",
      "Epoch 2 [16/340] - Loss: 4.185 [-4.181, 0.004, -0.000]\n",
      "Epoch 2 [17/340] - Loss: 4.181 [-4.177, 0.005, -0.000]\n",
      "Epoch 2 [18/340] - Loss: 4.138 [-4.133, 0.005, -0.000]\n",
      "Epoch 2 [19/340] - Loss: 4.199 [-4.194, 0.004, -0.000]\n",
      "Epoch 2 [20/340] - Loss: 4.299 [-4.295, 0.004, -0.000]\n",
      "Epoch 2 [21/340] - Loss: 4.451 [-4.446, 0.004, -0.000]\n",
      "Epoch 2 [22/340] - Loss: 4.372 [-4.368, 0.004, -0.000]\n",
      "Epoch 2 [23/340] - Loss: 4.471 [-4.467, 0.004, -0.000]\n",
      "Epoch 2 [24/340] - Loss: 4.300 [-4.297, 0.003, -0.000]\n",
      "Epoch 2 [25/340] - Loss: 4.297 [-4.293, 0.003, -0.000]\n",
      "Epoch 2 [26/340] - Loss: 4.378 [-4.375, 0.003, -0.000]\n",
      "Epoch 2 [27/340] - Loss: 4.328 [-4.325, 0.003, -0.000]\n",
      "Epoch 2 [28/340] - Loss: 4.320 [-4.317, 0.003, -0.000]\n",
      "Epoch 2 [29/340] - Loss: 4.186 [-4.183, 0.003, -0.000]\n",
      "Epoch 2 [30/340] - Loss: 4.239 [-4.236, 0.003, -0.000]\n",
      "Epoch 2 [31/340] - Loss: 4.343 [-4.340, 0.004, -0.000]\n",
      "Epoch 2 [32/340] - Loss: 4.224 [-4.220, 0.004, -0.000]\n",
      "Epoch 2 [33/340] - Loss: 4.399 [-4.395, 0.004, -0.000]\n",
      "Epoch 2 [34/340] - Loss: 4.445 [-4.441, 0.004, -0.000]\n",
      "Epoch 2 [35/340] - Loss: 4.305 [-4.301, 0.004, -0.000]\n",
      "Epoch 2 [36/340] - Loss: 4.282 [-4.278, 0.004, -0.000]\n",
      "Epoch 2 [37/340] - Loss: 4.428 [-4.424, 0.004, -0.000]\n",
      "Epoch 2 [38/340] - Loss: 4.193 [-4.189, 0.004, -0.000]\n",
      "Epoch 2 [39/340] - Loss: 4.325 [-4.321, 0.004, -0.000]\n",
      "Epoch 2 [40/340] - Loss: 4.234 [-4.231, 0.004, -0.000]\n",
      "Epoch 2 [41/340] - Loss: 4.110 [-4.106, 0.003, -0.000]\n",
      "Epoch 2 [42/340] - Loss: 4.249 [-4.246, 0.003, -0.000]\n",
      "Epoch 2 [43/340] - Loss: 4.292 [-4.289, 0.003, -0.000]\n",
      "Epoch 2 [44/340] - Loss: 4.160 [-4.157, 0.003, -0.000]\n",
      "Epoch 2 [45/340] - Loss: 4.319 [-4.315, 0.003, -0.000]\n",
      "Epoch 2 [46/340] - Loss: 4.216 [-4.213, 0.003, -0.000]\n",
      "Epoch 2 [47/340] - Loss: 4.218 [-4.214, 0.003, -0.000]\n",
      "Epoch 2 [48/340] - Loss: 4.198 [-4.194, 0.003, -0.000]\n",
      "Epoch 2 [49/340] - Loss: 4.168 [-4.164, 0.003, -0.000]\n",
      "Epoch 2 [50/340] - Loss: 4.057 [-4.054, 0.003, -0.000]\n",
      "Epoch 2 [51/340] - Loss: 4.175 [-4.172, 0.003, -0.000]\n",
      "Epoch 2 [52/340] - Loss: 4.185 [-4.182, 0.003, -0.000]\n",
      "Epoch 2 [53/340] - Loss: 4.193 [-4.190, 0.003, -0.000]\n",
      "Epoch 2 [54/340] - Loss: 4.234 [-4.231, 0.003, -0.000]\n",
      "Epoch 2 [55/340] - Loss: 4.212 [-4.209, 0.003, -0.000]\n",
      "Epoch 2 [56/340] - Loss: 4.184 [-4.181, 0.003, -0.000]\n",
      "Epoch 2 [57/340] - Loss: 4.128 [-4.125, 0.003, -0.000]\n",
      "Epoch 2 [58/340] - Loss: 4.226 [-4.223, 0.003, -0.000]\n",
      "Epoch 2 [59/340] - Loss: 4.398 [-4.395, 0.003, -0.000]\n",
      "Epoch 2 [60/340] - Loss: 4.146 [-4.143, 0.003, -0.000]\n",
      "Epoch 2 [61/340] - Loss: 4.120 [-4.117, 0.003, -0.000]\n",
      "Epoch 2 [62/340] - Loss: 4.177 [-4.173, 0.003, -0.000]\n",
      "Epoch 2 [63/340] - Loss: 4.218 [-4.215, 0.003, -0.000]\n",
      "Epoch 2 [64/340] - Loss: 4.201 [-4.198, 0.003, -0.000]\n",
      "Epoch 2 [65/340] - Loss: 4.291 [-4.288, 0.003, -0.000]\n",
      "Epoch 2 [66/340] - Loss: 4.154 [-4.151, 0.003, -0.000]\n",
      "Epoch 2 [67/340] - Loss: 4.230 [-4.227, 0.003, -0.000]\n",
      "Epoch 2 [68/340] - Loss: 4.187 [-4.185, 0.003, -0.000]\n",
      "Epoch 2 [69/340] - Loss: 4.113 [-4.110, 0.003, -0.000]\n",
      "Epoch 2 [70/340] - Loss: 4.110 [-4.107, 0.003, -0.000]\n",
      "Epoch 2 [71/340] - Loss: 4.124 [-4.121, 0.003, -0.000]\n",
      "Epoch 2 [72/340] - Loss: 4.205 [-4.202, 0.003, -0.000]\n",
      "Epoch 2 [73/340] - Loss: 4.256 [-4.253, 0.003, -0.000]\n",
      "Epoch 2 [74/340] - Loss: 4.231 [-4.228, 0.003, -0.000]\n",
      "Epoch 2 [75/340] - Loss: 4.195 [-4.192, 0.003, -0.000]\n",
      "Epoch 2 [76/340] - Loss: 4.110 [-4.107, 0.003, -0.000]\n",
      "Epoch 2 [77/340] - Loss: 4.176 [-4.173, 0.003, -0.000]\n",
      "Epoch 2 [78/340] - Loss: 4.144 [-4.142, 0.003, -0.000]\n",
      "Epoch 2 [79/340] - Loss: 4.141 [-4.138, 0.003, -0.000]\n",
      "Epoch 2 [80/340] - Loss: 4.167 [-4.164, 0.003, -0.000]\n",
      "Epoch 2 [81/340] - Loss: 4.163 [-4.160, 0.003, -0.000]\n",
      "Epoch 2 [82/340] - Loss: 4.172 [-4.168, 0.003, -0.000]\n",
      "Epoch 2 [83/340] - Loss: 4.166 [-4.162, 0.004, -0.000]\n",
      "Epoch 2 [84/340] - Loss: 4.253 [-4.249, 0.003, -0.000]\n",
      "Epoch 2 [85/340] - Loss: 4.179 [-4.176, 0.003, -0.000]\n",
      "Epoch 2 [86/340] - Loss: 4.215 [-4.212, 0.003, -0.000]\n",
      "Epoch 2 [87/340] - Loss: 4.130 [-4.128, 0.003, -0.000]\n",
      "Epoch 2 [88/340] - Loss: 4.089 [-4.086, 0.003, -0.000]\n",
      "Epoch 2 [89/340] - Loss: 4.062 [-4.059, 0.003, -0.000]\n",
      "Epoch 2 [90/340] - Loss: 4.169 [-4.167, 0.003, -0.000]\n",
      "Epoch 2 [91/340] - Loss: 4.099 [-4.096, 0.003, -0.000]\n",
      "Epoch 2 [92/340] - Loss: 4.086 [-4.083, 0.003, -0.000]\n",
      "Epoch 2 [93/340] - Loss: 4.053 [-4.050, 0.003, -0.000]\n",
      "Epoch 2 [94/340] - Loss: 4.097 [-4.094, 0.003, -0.000]\n",
      "Epoch 2 [95/340] - Loss: 4.103 [-4.099, 0.003, -0.000]\n",
      "Epoch 2 [96/340] - Loss: 4.302 [-4.298, 0.004, -0.000]\n",
      "Epoch 2 [97/340] - Loss: 4.253 [-4.247, 0.005, -0.000]\n",
      "Epoch 2 [98/340] - Loss: 4.203 [-4.198, 0.005, -0.000]\n",
      "Epoch 2 [99/340] - Loss: 4.245 [-4.240, 0.005, -0.000]\n",
      "Epoch 2 [100/340] - Loss: 4.125 [-4.122, 0.004, -0.000]\n",
      "Epoch 2 [101/340] - Loss: 4.229 [-4.226, 0.003, -0.000]\n",
      "Epoch 2 [102/340] - Loss: 4.207 [-4.204, 0.003, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [103/340] - Loss: 4.137 [-4.134, 0.003, -0.000]\n",
      "Epoch 2 [104/340] - Loss: 4.124 [-4.121, 0.003, -0.000]\n",
      "Epoch 2 [105/340] - Loss: 4.191 [-4.189, 0.003, -0.000]\n",
      "Epoch 2 [106/340] - Loss: 4.145 [-4.142, 0.003, -0.000]\n",
      "Epoch 2 [107/340] - Loss: 4.137 [-4.134, 0.003, -0.000]\n",
      "Epoch 2 [108/340] - Loss: 4.210 [-4.208, 0.003, -0.000]\n",
      "Epoch 2 [109/340] - Loss: 4.133 [-4.130, 0.003, -0.000]\n",
      "Epoch 2 [110/340] - Loss: 4.182 [-4.179, 0.003, -0.000]\n",
      "Epoch 2 [111/340] - Loss: 4.194 [-4.191, 0.003, -0.000]\n",
      "Epoch 2 [112/340] - Loss: 4.153 [-4.150, 0.003, -0.000]\n",
      "Epoch 2 [113/340] - Loss: 4.053 [-4.050, 0.003, -0.000]\n",
      "Epoch 2 [114/340] - Loss: 4.145 [-4.141, 0.003, -0.000]\n",
      "Epoch 2 [115/340] - Loss: 4.132 [-4.128, 0.003, -0.000]\n",
      "Epoch 2 [116/340] - Loss: 4.158 [-4.155, 0.004, -0.000]\n",
      "Epoch 2 [117/340] - Loss: 4.135 [-4.131, 0.004, -0.000]\n",
      "Epoch 2 [118/340] - Loss: 4.151 [-4.148, 0.004, -0.000]\n",
      "Epoch 2 [119/340] - Loss: 4.074 [-4.070, 0.003, -0.000]\n",
      "Epoch 2 [120/340] - Loss: 4.203 [-4.199, 0.004, -0.000]\n",
      "Epoch 2 [121/340] - Loss: 4.111 [-4.108, 0.003, -0.000]\n",
      "Epoch 2 [122/340] - Loss: 4.237 [-4.234, 0.003, -0.000]\n",
      "Epoch 2 [123/340] - Loss: 4.108 [-4.105, 0.003, -0.000]\n",
      "Epoch 2 [124/340] - Loss: 4.142 [-4.139, 0.003, -0.000]\n",
      "Epoch 2 [125/340] - Loss: 4.227 [-4.224, 0.003, -0.000]\n",
      "Epoch 2 [126/340] - Loss: 4.175 [-4.172, 0.003, -0.000]\n",
      "Epoch 2 [127/340] - Loss: 4.220 [-4.217, 0.003, -0.000]\n",
      "Epoch 2 [128/340] - Loss: 4.406 [-4.403, 0.003, -0.000]\n",
      "Epoch 2 [129/340] - Loss: 4.196 [-4.193, 0.003, -0.000]\n",
      "Epoch 2 [130/340] - Loss: 4.148 [-4.145, 0.003, -0.000]\n",
      "Epoch 2 [131/340] - Loss: 4.119 [-4.116, 0.003, -0.000]\n",
      "Epoch 2 [132/340] - Loss: 4.201 [-4.198, 0.003, -0.000]\n",
      "Epoch 2 [133/340] - Loss: 4.163 [-4.160, 0.003, -0.000]\n",
      "Epoch 2 [134/340] - Loss: 4.106 [-4.104, 0.003, -0.000]\n",
      "Epoch 2 [135/340] - Loss: 4.165 [-4.163, 0.003, -0.000]\n",
      "Epoch 2 [136/340] - Loss: 4.192 [-4.189, 0.003, -0.000]\n",
      "Epoch 2 [137/340] - Loss: 4.117 [-4.114, 0.003, -0.000]\n",
      "Epoch 2 [138/340] - Loss: 4.217 [-4.214, 0.003, -0.000]\n",
      "Epoch 2 [139/340] - Loss: 4.136 [-4.133, 0.003, -0.000]\n",
      "Epoch 2 [140/340] - Loss: 4.145 [-4.142, 0.003, -0.000]\n",
      "Epoch 2 [141/340] - Loss: 4.197 [-4.193, 0.003, -0.000]\n",
      "Epoch 2 [142/340] - Loss: 4.076 [-4.073, 0.003, -0.000]\n",
      "Epoch 2 [143/340] - Loss: 4.169 [-4.166, 0.004, -0.000]\n",
      "Epoch 2 [144/340] - Loss: 4.094 [-4.090, 0.004, -0.000]\n",
      "Epoch 2 [145/340] - Loss: 4.118 [-4.114, 0.004, -0.000]\n",
      "Epoch 2 [146/340] - Loss: 4.191 [-4.188, 0.004, -0.000]\n",
      "Epoch 2 [147/340] - Loss: 4.284 [-4.281, 0.004, -0.000]\n",
      "Epoch 2 [148/340] - Loss: 4.168 [-4.165, 0.004, -0.000]\n",
      "Epoch 2 [149/340] - Loss: 4.177 [-4.173, 0.004, -0.000]\n",
      "Epoch 2 [150/340] - Loss: 4.131 [-4.128, 0.003, -0.000]\n",
      "Epoch 2 [151/340] - Loss: 4.051 [-4.048, 0.003, -0.000]\n",
      "Epoch 2 [152/340] - Loss: 4.187 [-4.184, 0.003, -0.000]\n",
      "Epoch 2 [153/340] - Loss: 4.185 [-4.182, 0.003, -0.000]\n",
      "Epoch 2 [154/340] - Loss: 4.083 [-4.080, 0.003, -0.000]\n",
      "Epoch 2 [155/340] - Loss: 4.156 [-4.153, 0.003, -0.000]\n",
      "Epoch 2 [156/340] - Loss: 4.052 [-4.050, 0.003, -0.000]\n",
      "Epoch 2 [157/340] - Loss: 4.166 [-4.164, 0.003, -0.000]\n",
      "Epoch 2 [158/340] - Loss: 4.104 [-4.101, 0.003, -0.000]\n",
      "Epoch 2 [159/340] - Loss: 4.031 [-4.028, 0.003, -0.000]\n",
      "Epoch 2 [160/340] - Loss: 4.120 [-4.118, 0.003, -0.000]\n",
      "Epoch 2 [161/340] - Loss: 4.197 [-4.194, 0.003, -0.000]\n",
      "Epoch 2 [162/340] - Loss: 4.310 [-4.307, 0.003, -0.000]\n",
      "Epoch 2 [163/340] - Loss: 4.180 [-4.177, 0.003, -0.000]\n",
      "Epoch 2 [164/340] - Loss: 4.172 [-4.169, 0.003, -0.000]\n",
      "Epoch 2 [165/340] - Loss: 4.187 [-4.184, 0.003, -0.000]\n",
      "Epoch 2 [166/340] - Loss: 4.129 [-4.125, 0.003, -0.000]\n",
      "Epoch 2 [167/340] - Loss: 4.121 [-4.118, 0.003, -0.000]\n",
      "Epoch 2 [168/340] - Loss: 4.220 [-4.216, 0.004, -0.000]\n",
      "Epoch 2 [169/340] - Loss: 4.176 [-4.172, 0.004, -0.000]\n",
      "Epoch 2 [170/340] - Loss: 4.177 [-4.174, 0.003, -0.000]\n",
      "Epoch 2 [171/340] - Loss: 4.135 [-4.132, 0.003, -0.000]\n",
      "Epoch 2 [172/340] - Loss: 4.222 [-4.219, 0.003, -0.000]\n",
      "Epoch 2 [173/340] - Loss: 4.162 [-4.159, 0.003, -0.000]\n",
      "Epoch 2 [174/340] - Loss: 4.130 [-4.127, 0.003, -0.000]\n",
      "Epoch 2 [175/340] - Loss: 4.134 [-4.131, 0.003, -0.000]\n",
      "Epoch 2 [176/340] - Loss: 4.184 [-4.181, 0.003, -0.000]\n",
      "Epoch 2 [177/340] - Loss: 4.149 [-4.146, 0.003, -0.000]\n",
      "Epoch 2 [178/340] - Loss: 4.145 [-4.143, 0.003, -0.000]\n",
      "Epoch 2 [179/340] - Loss: 4.178 [-4.176, 0.003, -0.000]\n",
      "Epoch 2 [180/340] - Loss: 4.176 [-4.173, 0.003, -0.000]\n",
      "Epoch 2 [181/340] - Loss: 4.189 [-4.186, 0.003, -0.000]\n",
      "Epoch 2 [182/340] - Loss: 4.151 [-4.148, 0.003, -0.000]\n",
      "Epoch 2 [183/340] - Loss: 4.067 [-4.064, 0.003, -0.000]\n",
      "Epoch 2 [184/340] - Loss: 4.162 [-4.159, 0.003, -0.000]\n",
      "Epoch 2 [185/340] - Loss: 4.157 [-4.154, 0.003, -0.000]\n",
      "Epoch 2 [186/340] - Loss: 4.197 [-4.194, 0.003, -0.000]\n",
      "Epoch 2 [187/340] - Loss: 4.141 [-4.138, 0.003, -0.000]\n",
      "Epoch 2 [188/340] - Loss: 4.149 [-4.146, 0.003, -0.000]\n",
      "Epoch 2 [189/340] - Loss: 4.181 [-4.178, 0.003, -0.000]\n",
      "Epoch 2 [190/340] - Loss: 4.124 [-4.121, 0.003, -0.000]\n",
      "Epoch 2 [191/340] - Loss: 4.159 [-4.156, 0.003, -0.000]\n",
      "Epoch 2 [192/340] - Loss: 4.098 [-4.095, 0.003, -0.000]\n",
      "Epoch 2 [193/340] - Loss: 4.187 [-4.184, 0.003, -0.000]\n",
      "Epoch 2 [194/340] - Loss: 4.158 [-4.155, 0.003, -0.000]\n",
      "Epoch 2 [195/340] - Loss: 4.154 [-4.151, 0.003, -0.000]\n",
      "Epoch 2 [196/340] - Loss: 4.093 [-4.090, 0.003, -0.000]\n",
      "Epoch 2 [197/340] - Loss: 4.077 [-4.075, 0.003, -0.000]\n",
      "Epoch 2 [198/340] - Loss: 4.063 [-4.060, 0.003, -0.000]\n",
      "Epoch 2 [199/340] - Loss: 4.111 [-4.108, 0.003, -0.000]\n",
      "Epoch 2 [200/340] - Loss: 4.123 [-4.120, 0.003, -0.000]\n",
      "Epoch 2 [201/340] - Loss: 4.190 [-4.187, 0.003, -0.000]\n",
      "Epoch 2 [202/340] - Loss: 4.098 [-4.095, 0.003, -0.000]\n",
      "Epoch 2 [203/340] - Loss: 4.100 [-4.097, 0.003, -0.000]\n",
      "Epoch 2 [204/340] - Loss: 4.121 [-4.118, 0.003, -0.000]\n",
      "Epoch 2 [205/340] - Loss: 4.208 [-4.205, 0.003, -0.000]\n",
      "Epoch 2 [206/340] - Loss: 4.135 [-4.132, 0.003, -0.000]\n",
      "Epoch 2 [207/340] - Loss: 4.067 [-4.064, 0.003, -0.000]\n",
      "Epoch 2 [208/340] - Loss: 4.089 [-4.086, 0.003, -0.000]\n",
      "Epoch 2 [209/340] - Loss: 4.093 [-4.090, 0.003, -0.000]\n",
      "Epoch 2 [210/340] - Loss: 4.045 [-4.042, 0.003, -0.000]\n",
      "Epoch 2 [211/340] - Loss: 4.180 [-4.177, 0.003, -0.000]\n",
      "Epoch 2 [212/340] - Loss: 4.172 [-4.169, 0.003, -0.000]\n",
      "Epoch 2 [213/340] - Loss: 4.076 [-4.073, 0.003, -0.000]\n",
      "Epoch 2 [214/340] - Loss: 4.074 [-4.071, 0.003, -0.000]\n",
      "Epoch 2 [215/340] - Loss: 4.154 [-4.152, 0.003, -0.000]\n",
      "Epoch 2 [216/340] - Loss: 4.082 [-4.079, 0.003, -0.000]\n",
      "Epoch 2 [217/340] - Loss: 4.122 [-4.119, 0.003, -0.000]\n",
      "Epoch 2 [218/340] - Loss: 4.118 [-4.116, 0.003, -0.000]\n",
      "Epoch 2 [219/340] - Loss: 4.140 [-4.137, 0.003, -0.000]\n",
      "Epoch 2 [220/340] - Loss: 4.274 [-4.271, 0.003, -0.000]\n",
      "Epoch 2 [221/340] - Loss: 4.135 [-4.133, 0.003, -0.000]\n",
      "Epoch 2 [222/340] - Loss: 4.145 [-4.142, 0.003, -0.000]\n",
      "Epoch 2 [223/340] - Loss: 4.122 [-4.119, 0.003, -0.000]\n",
      "Epoch 2 [224/340] - Loss: 4.077 [-4.074, 0.003, -0.000]\n",
      "Epoch 2 [225/340] - Loss: 4.097 [-4.095, 0.003, -0.000]\n",
      "Epoch 2 [226/340] - Loss: 4.123 [-4.120, 0.003, -0.000]\n",
      "Epoch 2 [227/340] - Loss: 4.185 [-4.183, 0.002, -0.000]\n",
      "Epoch 2 [228/340] - Loss: 4.141 [-4.139, 0.002, -0.000]\n",
      "Epoch 2 [229/340] - Loss: 4.121 [-4.119, 0.002, -0.000]\n",
      "Epoch 2 [230/340] - Loss: 4.130 [-4.128, 0.002, -0.000]\n",
      "Epoch 2 [231/340] - Loss: 4.153 [-4.151, 0.002, -0.000]\n",
      "Epoch 2 [232/340] - Loss: 4.185 [-4.183, 0.002, -0.000]\n",
      "Epoch 2 [233/340] - Loss: 4.141 [-4.138, 0.003, -0.000]\n",
      "Epoch 2 [234/340] - Loss: 4.099 [-4.097, 0.003, -0.000]\n",
      "Epoch 2 [235/340] - Loss: 4.134 [-4.131, 0.003, -0.000]\n",
      "Epoch 2 [236/340] - Loss: 4.188 [-4.185, 0.003, -0.000]\n",
      "Epoch 2 [237/340] - Loss: 4.131 [-4.128, 0.003, -0.000]\n",
      "Epoch 2 [238/340] - Loss: 4.104 [-4.101, 0.003, -0.000]\n",
      "Epoch 2 [239/340] - Loss: 4.102 [-4.099, 0.003, -0.000]\n",
      "Epoch 2 [240/340] - Loss: 4.176 [-4.173, 0.003, -0.000]\n",
      "Epoch 2 [241/340] - Loss: 4.123 [-4.120, 0.003, -0.000]\n",
      "Epoch 2 [242/340] - Loss: 4.084 [-4.081, 0.003, -0.000]\n",
      "Epoch 2 [243/340] - Loss: 4.172 [-4.169, 0.003, -0.000]\n",
      "Epoch 2 [244/340] - Loss: 4.175 [-4.172, 0.003, -0.000]\n",
      "Epoch 2 [245/340] - Loss: 4.160 [-4.158, 0.002, -0.000]\n",
      "Epoch 2 [246/340] - Loss: 4.105 [-4.103, 0.002, -0.000]\n",
      "Epoch 2 [247/340] - Loss: 4.195 [-4.192, 0.002, -0.000]\n",
      "Epoch 2 [248/340] - Loss: 4.162 [-4.160, 0.002, -0.000]\n",
      "Epoch 2 [249/340] - Loss: 4.104 [-4.102, 0.002, -0.000]\n",
      "Epoch 2 [250/340] - Loss: 4.122 [-4.120, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [251/340] - Loss: 4.107 [-4.105, 0.002, -0.000]\n",
      "Epoch 2 [252/340] - Loss: 4.130 [-4.127, 0.003, -0.000]\n",
      "Epoch 2 [253/340] - Loss: 4.156 [-4.153, 0.003, -0.000]\n",
      "Epoch 2 [254/340] - Loss: 4.151 [-4.148, 0.003, -0.000]\n",
      "Epoch 2 [255/340] - Loss: 4.067 [-4.065, 0.003, -0.000]\n",
      "Epoch 2 [256/340] - Loss: 4.179 [-4.176, 0.003, -0.000]\n",
      "Epoch 2 [257/340] - Loss: 4.173 [-4.170, 0.004, -0.000]\n",
      "Epoch 2 [258/340] - Loss: 4.163 [-4.160, 0.004, -0.000]\n",
      "Epoch 2 [259/340] - Loss: 4.136 [-4.133, 0.003, -0.000]\n",
      "Epoch 2 [260/340] - Loss: 4.144 [-4.141, 0.002, -0.000]\n",
      "Epoch 2 [261/340] - Loss: 4.039 [-4.037, 0.002, -0.000]\n",
      "Epoch 2 [262/340] - Loss: 4.059 [-4.057, 0.002, -0.000]\n",
      "Epoch 2 [263/340] - Loss: 4.181 [-4.179, 0.002, -0.000]\n",
      "Epoch 2 [264/340] - Loss: 4.158 [-4.156, 0.002, -0.000]\n",
      "Epoch 2 [265/340] - Loss: 4.061 [-4.059, 0.002, -0.000]\n",
      "Epoch 2 [266/340] - Loss: 4.207 [-4.205, 0.002, -0.000]\n",
      "Epoch 2 [267/340] - Loss: 4.172 [-4.170, 0.002, -0.000]\n",
      "Epoch 2 [268/340] - Loss: 4.155 [-4.154, 0.002, -0.000]\n",
      "Epoch 2 [269/340] - Loss: 4.201 [-4.198, 0.002, -0.000]\n",
      "Epoch 2 [270/340] - Loss: 4.059 [-4.057, 0.002, -0.000]\n",
      "Epoch 2 [271/340] - Loss: 4.092 [-4.089, 0.002, -0.000]\n",
      "Epoch 2 [272/340] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 2 [273/340] - Loss: 4.106 [-4.104, 0.002, -0.000]\n",
      "Epoch 2 [274/340] - Loss: 4.093 [-4.091, 0.003, -0.000]\n",
      "Epoch 2 [275/340] - Loss: 4.076 [-4.073, 0.003, -0.000]\n",
      "Epoch 2 [276/340] - Loss: 4.115 [-4.112, 0.003, -0.000]\n",
      "Epoch 2 [277/340] - Loss: 4.074 [-4.071, 0.003, -0.000]\n",
      "Epoch 2 [278/340] - Loss: 4.170 [-4.167, 0.003, -0.000]\n",
      "Epoch 2 [279/340] - Loss: 4.175 [-4.172, 0.003, -0.000]\n",
      "Epoch 2 [280/340] - Loss: 4.065 [-4.061, 0.003, -0.000]\n",
      "Epoch 2 [281/340] - Loss: 4.101 [-4.097, 0.003, -0.000]\n",
      "Epoch 2 [282/340] - Loss: 4.151 [-4.148, 0.003, -0.000]\n",
      "Epoch 2 [283/340] - Loss: 4.136 [-4.133, 0.003, -0.000]\n",
      "Epoch 2 [284/340] - Loss: 4.127 [-4.124, 0.003, -0.000]\n",
      "Epoch 2 [285/340] - Loss: 4.117 [-4.114, 0.003, -0.000]\n",
      "Epoch 2 [286/340] - Loss: 4.096 [-4.093, 0.003, -0.000]\n",
      "Epoch 2 [287/340] - Loss: 4.090 [-4.088, 0.003, -0.000]\n",
      "Epoch 2 [288/340] - Loss: 4.066 [-4.063, 0.003, -0.000]\n",
      "Epoch 2 [289/340] - Loss: 4.144 [-4.141, 0.003, -0.000]\n",
      "Epoch 2 [290/340] - Loss: 4.187 [-4.184, 0.003, -0.000]\n",
      "Epoch 2 [291/340] - Loss: 4.252 [-4.249, 0.003, -0.000]\n",
      "Epoch 2 [292/340] - Loss: 4.195 [-4.192, 0.003, -0.000]\n",
      "Epoch 2 [293/340] - Loss: 4.191 [-4.188, 0.003, -0.000]\n",
      "Epoch 2 [294/340] - Loss: 4.094 [-4.091, 0.003, -0.000]\n",
      "Epoch 2 [295/340] - Loss: 4.090 [-4.088, 0.003, -0.000]\n",
      "Epoch 2 [296/340] - Loss: 4.142 [-4.139, 0.003, -0.000]\n",
      "Epoch 2 [297/340] - Loss: 4.089 [-4.086, 0.003, -0.000]\n",
      "Epoch 2 [298/340] - Loss: 4.060 [-4.057, 0.002, -0.000]\n",
      "Epoch 2 [299/340] - Loss: 4.097 [-4.095, 0.002, -0.000]\n",
      "Epoch 2 [300/340] - Loss: 4.114 [-4.112, 0.002, -0.000]\n",
      "Epoch 2 [301/340] - Loss: 4.060 [-4.057, 0.002, -0.000]\n",
      "Epoch 2 [302/340] - Loss: 4.113 [-4.111, 0.002, -0.000]\n",
      "Epoch 2 [303/340] - Loss: 4.115 [-4.113, 0.002, -0.000]\n",
      "Epoch 2 [304/340] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 2 [305/340] - Loss: 4.117 [-4.114, 0.002, -0.000]\n",
      "Epoch 2 [306/340] - Loss: 4.120 [-4.118, 0.002, -0.000]\n",
      "Epoch 2 [307/340] - Loss: 4.102 [-4.099, 0.002, -0.000]\n",
      "Epoch 2 [308/340] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 2 [309/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 2 [310/340] - Loss: 4.104 [-4.101, 0.002, -0.000]\n",
      "Epoch 2 [311/340] - Loss: 4.036 [-4.034, 0.003, -0.000]\n",
      "Epoch 2 [312/340] - Loss: 4.089 [-4.087, 0.003, -0.000]\n",
      "Epoch 2 [313/340] - Loss: 4.097 [-4.095, 0.003, -0.000]\n",
      "Epoch 2 [314/340] - Loss: 4.047 [-4.045, 0.003, -0.000]\n",
      "Epoch 2 [315/340] - Loss: 4.074 [-4.072, 0.003, -0.000]\n",
      "Epoch 2 [316/340] - Loss: 4.104 [-4.102, 0.003, -0.000]\n",
      "Epoch 2 [317/340] - Loss: 4.098 [-4.096, 0.003, -0.000]\n",
      "Epoch 2 [318/340] - Loss: 4.117 [-4.115, 0.002, -0.000]\n",
      "Epoch 2 [319/340] - Loss: 4.038 [-4.035, 0.002, -0.000]\n",
      "Epoch 2 [320/340] - Loss: 4.099 [-4.097, 0.002, -0.000]\n",
      "Epoch 2 [321/340] - Loss: 4.148 [-4.146, 0.002, -0.000]\n",
      "Epoch 2 [322/340] - Loss: 4.220 [-4.217, 0.002, -0.000]\n",
      "Epoch 2 [323/340] - Loss: 4.169 [-4.167, 0.002, -0.000]\n",
      "Epoch 2 [324/340] - Loss: 4.093 [-4.090, 0.002, -0.000]\n",
      "Epoch 2 [325/340] - Loss: 4.185 [-4.183, 0.002, -0.000]\n",
      "Epoch 2 [326/340] - Loss: 4.041 [-4.039, 0.002, -0.000]\n",
      "Epoch 2 [327/340] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 2 [328/340] - Loss: 4.091 [-4.089, 0.002, -0.000]\n",
      "Epoch 2 [329/340] - Loss: 4.085 [-4.083, 0.002, -0.000]\n",
      "Epoch 2 [330/340] - Loss: 4.023 [-4.021, 0.002, -0.000]\n",
      "Epoch 2 [331/340] - Loss: 4.121 [-4.119, 0.002, -0.000]\n",
      "Epoch 2 [332/340] - Loss: 4.128 [-4.126, 0.002, -0.000]\n",
      "Epoch 2 [333/340] - Loss: 4.249 [-4.246, 0.003, -0.000]\n",
      "Epoch 2 [334/340] - Loss: 4.192 [-4.190, 0.003, -0.000]\n",
      "Epoch 2 [335/340] - Loss: 4.035 [-4.033, 0.003, -0.000]\n",
      "Epoch 2 [336/340] - Loss: 4.110 [-4.107, 0.003, -0.000]\n",
      "Epoch 2 [337/340] - Loss: 4.084 [-4.081, 0.003, -0.000]\n",
      "Epoch 2 [338/340] - Loss: 4.001 [-3.998, 0.003, -0.000]\n",
      "Epoch 2 [339/340] - Loss: 4.113 [-4.110, 0.003, -0.000]\n",
      "Epoch 3 [0/340] - Loss: 4.100 [-4.097, 0.003, -0.000]\n",
      "Epoch 3 [1/340] - Loss: 4.111 [-4.108, 0.003, -0.000]\n",
      "Epoch 3 [2/340] - Loss: 4.096 [-4.092, 0.003, -0.000]\n",
      "Epoch 3 [3/340] - Loss: 4.030 [-4.027, 0.003, -0.000]\n",
      "Epoch 3 [4/340] - Loss: 4.128 [-4.125, 0.003, -0.000]\n",
      "Epoch 3 [5/340] - Loss: 4.034 [-4.032, 0.002, -0.000]\n",
      "Epoch 3 [6/340] - Loss: 4.152 [-4.149, 0.002, -0.000]\n",
      "Epoch 3 [7/340] - Loss: 4.070 [-4.068, 0.002, -0.000]\n",
      "Epoch 3 [8/340] - Loss: 4.141 [-4.139, 0.002, -0.000]\n",
      "Epoch 3 [9/340] - Loss: 4.101 [-4.099, 0.002, -0.000]\n",
      "Epoch 3 [10/340] - Loss: 4.202 [-4.200, 0.002, -0.000]\n",
      "Epoch 3 [11/340] - Loss: 4.106 [-4.104, 0.002, -0.000]\n",
      "Epoch 3 [12/340] - Loss: 4.095 [-4.093, 0.002, -0.000]\n",
      "Epoch 3 [13/340] - Loss: 4.116 [-4.114, 0.003, -0.000]\n",
      "Epoch 3 [14/340] - Loss: 4.200 [-4.197, 0.003, -0.000]\n",
      "Epoch 3 [15/340] - Loss: 4.085 [-4.082, 0.003, -0.000]\n",
      "Epoch 3 [16/340] - Loss: 4.120 [-4.117, 0.003, -0.000]\n",
      "Epoch 3 [17/340] - Loss: 4.120 [-4.116, 0.004, -0.000]\n",
      "Epoch 3 [18/340] - Loss: 4.186 [-4.183, 0.003, -0.000]\n",
      "Epoch 3 [19/340] - Loss: 4.205 [-4.203, 0.002, -0.000]\n",
      "Epoch 3 [20/340] - Loss: 4.310 [-4.308, 0.002, -0.000]\n",
      "Epoch 3 [21/340] - Loss: 4.108 [-4.106, 0.002, -0.000]\n",
      "Epoch 3 [22/340] - Loss: 4.188 [-4.186, 0.002, -0.000]\n",
      "Epoch 3 [23/340] - Loss: 4.141 [-4.139, 0.002, -0.000]\n",
      "Epoch 3 [24/340] - Loss: 4.093 [-4.091, 0.002, -0.000]\n",
      "Epoch 3 [25/340] - Loss: 4.078 [-4.076, 0.002, -0.000]\n",
      "Epoch 3 [26/340] - Loss: 4.154 [-4.152, 0.002, -0.000]\n",
      "Epoch 3 [27/340] - Loss: 4.186 [-4.184, 0.002, -0.000]\n",
      "Epoch 3 [28/340] - Loss: 4.118 [-4.116, 0.002, -0.000]\n",
      "Epoch 3 [29/340] - Loss: 4.200 [-4.198, 0.002, -0.000]\n",
      "Epoch 3 [30/340] - Loss: 4.169 [-4.166, 0.002, -0.000]\n",
      "Epoch 3 [31/340] - Loss: 4.075 [-4.072, 0.003, -0.000]\n",
      "Epoch 3 [32/340] - Loss: 4.163 [-4.161, 0.003, -0.000]\n",
      "Epoch 3 [33/340] - Loss: 4.189 [-4.186, 0.003, -0.000]\n",
      "Epoch 3 [34/340] - Loss: 4.145 [-4.142, 0.003, -0.000]\n",
      "Epoch 3 [35/340] - Loss: 4.170 [-4.166, 0.003, -0.000]\n",
      "Epoch 3 [36/340] - Loss: 4.175 [-4.171, 0.004, -0.000]\n",
      "Epoch 3 [37/340] - Loss: 4.093 [-4.090, 0.004, -0.000]\n",
      "Epoch 3 [38/340] - Loss: 4.042 [-4.038, 0.004, -0.000]\n",
      "Epoch 3 [39/340] - Loss: 4.161 [-4.157, 0.004, -0.000]\n",
      "Epoch 3 [40/340] - Loss: 4.117 [-4.114, 0.004, -0.000]\n",
      "Epoch 3 [41/340] - Loss: 4.100 [-4.096, 0.003, -0.000]\n",
      "Epoch 3 [42/340] - Loss: 4.177 [-4.174, 0.003, -0.000]\n",
      "Epoch 3 [43/340] - Loss: 4.121 [-4.118, 0.003, -0.000]\n",
      "Epoch 3 [44/340] - Loss: 4.064 [-4.061, 0.003, -0.000]\n",
      "Epoch 3 [45/340] - Loss: 4.052 [-4.049, 0.003, -0.000]\n",
      "Epoch 3 [46/340] - Loss: 4.101 [-4.098, 0.003, -0.000]\n",
      "Epoch 3 [47/340] - Loss: 4.060 [-4.057, 0.003, -0.000]\n",
      "Epoch 3 [48/340] - Loss: 4.142 [-4.139, 0.003, -0.000]\n",
      "Epoch 3 [49/340] - Loss: 4.093 [-4.090, 0.003, -0.000]\n",
      "Epoch 3 [50/340] - Loss: 4.079 [-4.076, 0.003, -0.000]\n",
      "Epoch 3 [51/340] - Loss: 4.132 [-4.128, 0.003, -0.000]\n",
      "Epoch 3 [52/340] - Loss: 4.133 [-4.129, 0.004, -0.000]\n",
      "Epoch 3 [53/340] - Loss: 4.153 [-4.149, 0.004, -0.000]\n",
      "Epoch 3 [54/340] - Loss: 4.126 [-4.123, 0.004, -0.000]\n",
      "Epoch 3 [55/340] - Loss: 4.085 [-4.081, 0.004, -0.000]\n",
      "Epoch 3 [56/340] - Loss: 4.126 [-4.122, 0.004, -0.000]\n",
      "Epoch 3 [57/340] - Loss: 4.281 [-4.277, 0.004, -0.000]\n",
      "Epoch 3 [58/340] - Loss: 4.345 [-4.342, 0.003, -0.000]\n",
      "Epoch 3 [59/340] - Loss: 4.147 [-4.143, 0.004, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [60/340] - Loss: 4.163 [-4.160, 0.004, -0.000]\n",
      "Epoch 3 [61/340] - Loss: 4.191 [-4.188, 0.003, -0.000]\n",
      "Epoch 3 [62/340] - Loss: 4.122 [-4.118, 0.003, -0.000]\n",
      "Epoch 3 [63/340] - Loss: 4.057 [-4.053, 0.003, -0.000]\n",
      "Epoch 3 [64/340] - Loss: 4.109 [-4.105, 0.004, -0.000]\n",
      "Epoch 3 [65/340] - Loss: 4.151 [-4.148, 0.004, -0.000]\n",
      "Epoch 3 [66/340] - Loss: 4.171 [-4.167, 0.004, -0.000]\n",
      "Epoch 3 [67/340] - Loss: 4.187 [-4.184, 0.004, -0.000]\n",
      "Epoch 3 [68/340] - Loss: 4.101 [-4.097, 0.004, -0.000]\n",
      "Epoch 3 [69/340] - Loss: 4.150 [-4.146, 0.004, -0.000]\n",
      "Epoch 3 [70/340] - Loss: 4.105 [-4.101, 0.003, -0.000]\n",
      "Epoch 3 [71/340] - Loss: 4.152 [-4.149, 0.003, -0.000]\n",
      "Epoch 3 [72/340] - Loss: 4.156 [-4.153, 0.003, -0.000]\n",
      "Epoch 3 [73/340] - Loss: 4.142 [-4.138, 0.003, -0.000]\n",
      "Epoch 3 [74/340] - Loss: 4.169 [-4.165, 0.004, -0.000]\n",
      "Epoch 3 [75/340] - Loss: 4.155 [-4.151, 0.003, -0.000]\n",
      "Epoch 3 [76/340] - Loss: 4.099 [-4.095, 0.004, -0.000]\n",
      "Epoch 3 [77/340] - Loss: 4.122 [-4.118, 0.004, -0.000]\n",
      "Epoch 3 [78/340] - Loss: 4.191 [-4.187, 0.005, -0.000]\n",
      "Epoch 3 [79/340] - Loss: 4.135 [-4.132, 0.004, -0.000]\n",
      "Epoch 3 [80/340] - Loss: 4.139 [-4.136, 0.003, -0.000]\n",
      "Epoch 3 [81/340] - Loss: 4.165 [-4.162, 0.003, -0.000]\n",
      "Epoch 3 [82/340] - Loss: 4.054 [-4.051, 0.003, -0.000]\n",
      "Epoch 3 [83/340] - Loss: 4.153 [-4.151, 0.002, -0.000]\n",
      "Epoch 3 [84/340] - Loss: 4.110 [-4.108, 0.002, -0.000]\n",
      "Epoch 3 [85/340] - Loss: 4.123 [-4.120, 0.002, -0.000]\n",
      "Epoch 3 [86/340] - Loss: 4.171 [-4.168, 0.003, -0.000]\n",
      "Epoch 3 [87/340] - Loss: 4.155 [-4.152, 0.003, -0.000]\n",
      "Epoch 3 [88/340] - Loss: 4.053 [-4.050, 0.003, -0.000]\n",
      "Epoch 3 [89/340] - Loss: 4.101 [-4.097, 0.003, -0.000]\n",
      "Epoch 3 [90/340] - Loss: 4.058 [-4.055, 0.003, -0.000]\n",
      "Epoch 3 [91/340] - Loss: 4.057 [-4.053, 0.003, -0.000]\n",
      "Epoch 3 [92/340] - Loss: 4.066 [-4.063, 0.003, -0.000]\n",
      "Epoch 3 [93/340] - Loss: 4.098 [-4.095, 0.003, -0.000]\n",
      "Epoch 3 [94/340] - Loss: 4.117 [-4.114, 0.003, -0.000]\n",
      "Epoch 3 [95/340] - Loss: 4.101 [-4.098, 0.003, -0.000]\n",
      "Epoch 3 [96/340] - Loss: 4.050 [-4.047, 0.003, -0.000]\n",
      "Epoch 3 [97/340] - Loss: 4.178 [-4.175, 0.003, -0.000]\n",
      "Epoch 3 [98/340] - Loss: 4.077 [-4.075, 0.003, -0.000]\n",
      "Epoch 3 [99/340] - Loss: 4.112 [-4.109, 0.003, -0.000]\n",
      "Epoch 3 [100/340] - Loss: 4.098 [-4.095, 0.003, -0.000]\n",
      "Epoch 3 [101/340] - Loss: 4.125 [-4.122, 0.003, -0.000]\n",
      "Epoch 3 [102/340] - Loss: 4.078 [-4.075, 0.003, -0.000]\n",
      "Epoch 3 [103/340] - Loss: 4.080 [-4.077, 0.003, -0.000]\n",
      "Epoch 3 [104/340] - Loss: 4.103 [-4.100, 0.003, -0.000]\n",
      "Epoch 3 [105/340] - Loss: 4.031 [-4.028, 0.003, -0.000]\n",
      "Epoch 3 [106/340] - Loss: 4.062 [-4.059, 0.003, -0.000]\n",
      "Epoch 3 [107/340] - Loss: 4.025 [-4.022, 0.003, -0.000]\n",
      "Epoch 3 [108/340] - Loss: 4.110 [-4.107, 0.003, -0.000]\n",
      "Epoch 3 [109/340] - Loss: 4.079 [-4.076, 0.003, -0.000]\n",
      "Epoch 3 [110/340] - Loss: 4.037 [-4.034, 0.003, -0.000]\n",
      "Epoch 3 [111/340] - Loss: 4.080 [-4.077, 0.003, -0.000]\n",
      "Epoch 3 [112/340] - Loss: 4.045 [-4.042, 0.003, -0.000]\n",
      "Epoch 3 [113/340] - Loss: 4.043 [-4.040, 0.003, -0.000]\n",
      "Epoch 3 [114/340] - Loss: 4.080 [-4.077, 0.003, -0.000]\n",
      "Epoch 3 [115/340] - Loss: 4.068 [-4.065, 0.003, -0.000]\n",
      "Epoch 3 [116/340] - Loss: 4.055 [-4.052, 0.003, -0.000]\n",
      "Epoch 3 [117/340] - Loss: 4.025 [-4.022, 0.003, -0.000]\n",
      "Epoch 3 [118/340] - Loss: 4.070 [-4.067, 0.003, -0.000]\n",
      "Epoch 3 [119/340] - Loss: 4.045 [-4.042, 0.003, -0.000]\n",
      "Epoch 3 [120/340] - Loss: 4.051 [-4.049, 0.003, -0.000]\n",
      "Epoch 3 [121/340] - Loss: 4.068 [-4.066, 0.003, -0.000]\n",
      "Epoch 3 [122/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 3 [123/340] - Loss: 4.049 [-4.046, 0.003, -0.000]\n",
      "Epoch 3 [124/340] - Loss: 4.100 [-4.097, 0.003, -0.000]\n",
      "Epoch 3 [125/340] - Loss: 4.053 [-4.050, 0.003, -0.000]\n",
      "Epoch 3 [126/340] - Loss: 4.111 [-4.108, 0.003, -0.000]\n",
      "Epoch 3 [127/340] - Loss: 4.078 [-4.075, 0.003, -0.000]\n",
      "Epoch 3 [128/340] - Loss: 4.114 [-4.111, 0.003, -0.000]\n",
      "Epoch 3 [129/340] - Loss: 4.045 [-4.042, 0.003, -0.000]\n",
      "Epoch 3 [130/340] - Loss: 4.128 [-4.125, 0.003, -0.000]\n",
      "Epoch 3 [131/340] - Loss: 4.258 [-4.256, 0.003, -0.000]\n",
      "Epoch 3 [132/340] - Loss: 4.074 [-4.071, 0.003, -0.000]\n",
      "Epoch 3 [133/340] - Loss: 4.115 [-4.112, 0.003, -0.000]\n",
      "Epoch 3 [134/340] - Loss: 4.120 [-4.117, 0.003, -0.000]\n",
      "Epoch 3 [135/340] - Loss: 4.107 [-4.104, 0.003, -0.000]\n",
      "Epoch 3 [136/340] - Loss: 4.108 [-4.105, 0.003, -0.000]\n",
      "Epoch 3 [137/340] - Loss: 4.132 [-4.130, 0.003, -0.000]\n",
      "Epoch 3 [138/340] - Loss: 4.054 [-4.052, 0.003, -0.000]\n",
      "Epoch 3 [139/340] - Loss: 4.101 [-4.098, 0.002, -0.000]\n",
      "Epoch 3 [140/340] - Loss: 4.076 [-4.073, 0.003, -0.000]\n",
      "Epoch 3 [141/340] - Loss: 4.109 [-4.107, 0.003, -0.000]\n",
      "Epoch 3 [142/340] - Loss: 4.125 [-4.122, 0.003, -0.000]\n",
      "Epoch 3 [143/340] - Loss: 4.071 [-4.068, 0.003, -0.000]\n",
      "Epoch 3 [144/340] - Loss: 4.136 [-4.133, 0.003, -0.000]\n",
      "Epoch 3 [145/340] - Loss: 4.089 [-4.086, 0.003, -0.000]\n",
      "Epoch 3 [146/340] - Loss: 4.123 [-4.120, 0.003, -0.000]\n",
      "Epoch 3 [147/340] - Loss: 4.120 [-4.117, 0.003, -0.000]\n",
      "Epoch 3 [148/340] - Loss: 4.064 [-4.061, 0.003, -0.000]\n",
      "Epoch 3 [149/340] - Loss: 4.094 [-4.091, 0.003, -0.000]\n",
      "Epoch 3 [150/340] - Loss: 4.062 [-4.059, 0.003, -0.000]\n",
      "Epoch 3 [151/340] - Loss: 4.133 [-4.130, 0.003, -0.000]\n",
      "Epoch 3 [152/340] - Loss: 4.101 [-4.098, 0.003, -0.000]\n",
      "Epoch 3 [153/340] - Loss: 4.077 [-4.074, 0.003, -0.000]\n",
      "Epoch 3 [154/340] - Loss: 4.059 [-4.056, 0.003, -0.000]\n",
      "Epoch 3 [155/340] - Loss: 4.166 [-4.164, 0.003, -0.000]\n",
      "Epoch 3 [156/340] - Loss: 4.023 [-4.021, 0.003, -0.000]\n",
      "Epoch 3 [157/340] - Loss: 4.079 [-4.076, 0.003, -0.000]\n",
      "Epoch 3 [158/340] - Loss: 4.077 [-4.074, 0.003, -0.000]\n",
      "Epoch 3 [159/340] - Loss: 4.104 [-4.102, 0.003, -0.000]\n",
      "Epoch 3 [160/340] - Loss: 4.215 [-4.212, 0.002, -0.000]\n",
      "Epoch 3 [161/340] - Loss: 4.193 [-4.191, 0.002, -0.000]\n",
      "Epoch 3 [162/340] - Loss: 4.181 [-4.178, 0.003, -0.000]\n",
      "Epoch 3 [163/340] - Loss: 4.088 [-4.085, 0.003, -0.000]\n",
      "Epoch 3 [164/340] - Loss: 4.099 [-4.097, 0.003, -0.000]\n",
      "Epoch 3 [165/340] - Loss: 4.091 [-4.089, 0.003, -0.000]\n",
      "Epoch 3 [166/340] - Loss: 4.062 [-4.060, 0.003, -0.000]\n",
      "Epoch 3 [167/340] - Loss: 4.083 [-4.081, 0.003, -0.000]\n",
      "Epoch 3 [168/340] - Loss: 4.032 [-4.029, 0.003, -0.000]\n",
      "Epoch 3 [169/340] - Loss: 4.137 [-4.134, 0.003, -0.000]\n",
      "Epoch 3 [170/340] - Loss: 4.198 [-4.195, 0.003, -0.000]\n",
      "Epoch 3 [171/340] - Loss: 4.112 [-4.108, 0.003, -0.000]\n",
      "Epoch 3 [172/340] - Loss: 4.063 [-4.060, 0.003, -0.000]\n",
      "Epoch 3 [173/340] - Loss: 4.131 [-4.128, 0.003, -0.000]\n",
      "Epoch 3 [174/340] - Loss: 4.017 [-4.014, 0.003, -0.000]\n",
      "Epoch 3 [175/340] - Loss: 4.095 [-4.092, 0.003, -0.000]\n",
      "Epoch 3 [176/340] - Loss: 4.176 [-4.173, 0.003, -0.000]\n",
      "Epoch 3 [177/340] - Loss: 4.321 [-4.319, 0.003, -0.000]\n",
      "Epoch 3 [178/340] - Loss: 4.181 [-4.178, 0.003, -0.000]\n",
      "Epoch 3 [179/340] - Loss: 4.116 [-4.114, 0.002, -0.000]\n",
      "Epoch 3 [180/340] - Loss: 4.074 [-4.071, 0.002, -0.000]\n",
      "Epoch 3 [181/340] - Loss: 4.132 [-4.129, 0.002, -0.000]\n",
      "Epoch 3 [182/340] - Loss: 4.142 [-4.140, 0.002, -0.000]\n",
      "Epoch 3 [183/340] - Loss: 4.099 [-4.097, 0.002, -0.000]\n",
      "Epoch 3 [184/340] - Loss: 4.105 [-4.103, 0.002, -0.000]\n",
      "Epoch 3 [185/340] - Loss: 4.114 [-4.111, 0.002, -0.000]\n",
      "Epoch 3 [186/340] - Loss: 4.101 [-4.099, 0.002, -0.000]\n",
      "Epoch 3 [187/340] - Loss: 4.157 [-4.155, 0.002, -0.000]\n",
      "Epoch 3 [188/340] - Loss: 4.168 [-4.165, 0.002, -0.000]\n",
      "Epoch 3 [189/340] - Loss: 4.111 [-4.108, 0.002, -0.000]\n",
      "Epoch 3 [190/340] - Loss: 4.083 [-4.081, 0.002, -0.000]\n",
      "Epoch 3 [191/340] - Loss: 4.090 [-4.088, 0.002, -0.000]\n",
      "Epoch 3 [192/340] - Loss: 4.070 [-4.068, 0.002, -0.000]\n",
      "Epoch 3 [193/340] - Loss: 4.051 [-4.049, 0.002, -0.000]\n",
      "Epoch 3 [194/340] - Loss: 4.113 [-4.111, 0.002, -0.000]\n",
      "Epoch 3 [195/340] - Loss: 4.082 [-4.080, 0.002, -0.000]\n",
      "Epoch 3 [196/340] - Loss: 4.091 [-4.088, 0.002, -0.000]\n",
      "Epoch 3 [197/340] - Loss: 4.106 [-4.103, 0.002, -0.000]\n",
      "Epoch 3 [198/340] - Loss: 4.133 [-4.131, 0.002, -0.000]\n",
      "Epoch 3 [199/340] - Loss: 4.004 [-4.001, 0.002, -0.000]\n",
      "Epoch 3 [200/340] - Loss: 4.064 [-4.061, 0.002, -0.000]\n",
      "Epoch 3 [201/340] - Loss: 4.138 [-4.136, 0.002, -0.000]\n",
      "Epoch 3 [202/340] - Loss: 4.125 [-4.123, 0.002, -0.000]\n",
      "Epoch 3 [203/340] - Loss: 4.080 [-4.078, 0.002, -0.000]\n",
      "Epoch 3 [204/340] - Loss: 4.118 [-4.116, 0.002, -0.000]\n",
      "Epoch 3 [205/340] - Loss: 4.061 [-4.059, 0.002, -0.000]\n",
      "Epoch 3 [206/340] - Loss: 4.037 [-4.035, 0.002, -0.000]\n",
      "Epoch 3 [207/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [208/340] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 3 [209/340] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 3 [210/340] - Loss: 4.081 [-4.080, 0.002, -0.000]\n",
      "Epoch 3 [211/340] - Loss: 4.093 [-4.092, 0.002, -0.000]\n",
      "Epoch 3 [212/340] - Loss: 4.052 [-4.050, 0.002, -0.000]\n",
      "Epoch 3 [213/340] - Loss: 4.078 [-4.076, 0.002, -0.000]\n",
      "Epoch 3 [214/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 3 [215/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 3 [216/340] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 3 [217/340] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 3 [218/340] - Loss: 4.095 [-4.093, 0.002, -0.000]\n",
      "Epoch 3 [219/340] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 3 [220/340] - Loss: 4.088 [-4.086, 0.002, -0.000]\n",
      "Epoch 3 [221/340] - Loss: 4.117 [-4.115, 0.002, -0.000]\n",
      "Epoch 3 [222/340] - Loss: 4.033 [-4.031, 0.002, -0.000]\n",
      "Epoch 3 [223/340] - Loss: 4.148 [-4.146, 0.002, -0.000]\n",
      "Epoch 3 [224/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 3 [225/340] - Loss: 4.102 [-4.100, 0.002, -0.000]\n",
      "Epoch 3 [226/340] - Loss: 4.123 [-4.121, 0.002, -0.000]\n",
      "Epoch 3 [227/340] - Loss: 4.102 [-4.100, 0.002, -0.000]\n",
      "Epoch 3 [228/340] - Loss: 4.076 [-4.074, 0.002, -0.000]\n",
      "Epoch 3 [229/340] - Loss: 4.093 [-4.091, 0.002, -0.000]\n",
      "Epoch 3 [230/340] - Loss: 4.046 [-4.045, 0.002, -0.000]\n",
      "Epoch 3 [231/340] - Loss: 4.048 [-4.046, 0.002, -0.000]\n",
      "Epoch 3 [232/340] - Loss: 4.060 [-4.058, 0.002, -0.000]\n",
      "Epoch 3 [233/340] - Loss: 4.087 [-4.085, 0.002, -0.000]\n",
      "Epoch 3 [234/340] - Loss: 4.052 [-4.050, 0.002, -0.000]\n",
      "Epoch 3 [235/340] - Loss: 4.099 [-4.096, 0.002, -0.000]\n",
      "Epoch 3 [236/340] - Loss: 4.056 [-4.054, 0.002, -0.000]\n",
      "Epoch 3 [237/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 3 [238/340] - Loss: 4.108 [-4.106, 0.002, -0.000]\n",
      "Epoch 3 [239/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 3 [240/340] - Loss: 4.038 [-4.036, 0.002, -0.000]\n",
      "Epoch 3 [241/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 3 [242/340] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 3 [243/340] - Loss: 4.058 [-4.057, 0.002, -0.000]\n",
      "Epoch 3 [244/340] - Loss: 4.058 [-4.056, 0.002, -0.000]\n",
      "Epoch 3 [245/340] - Loss: 4.060 [-4.058, 0.002, -0.000]\n",
      "Epoch 3 [246/340] - Loss: 4.080 [-4.078, 0.002, -0.000]\n",
      "Epoch 3 [247/340] - Loss: 4.048 [-4.046, 0.002, -0.000]\n",
      "Epoch 3 [248/340] - Loss: 4.122 [-4.120, 0.002, -0.000]\n",
      "Epoch 3 [249/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 3 [250/340] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 3 [251/340] - Loss: 4.098 [-4.096, 0.002, -0.000]\n",
      "Epoch 3 [252/340] - Loss: 4.083 [-4.081, 0.002, -0.000]\n",
      "Epoch 3 [253/340] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 3 [254/340] - Loss: 3.990 [-3.989, 0.002, -0.000]\n",
      "Epoch 3 [255/340] - Loss: 4.061 [-4.060, 0.002, -0.000]\n",
      "Epoch 3 [256/340] - Loss: 4.010 [-4.009, 0.002, -0.000]\n",
      "Epoch 3 [257/340] - Loss: 4.024 [-4.023, 0.002, -0.000]\n",
      "Epoch 3 [258/340] - Loss: 4.089 [-4.087, 0.002, -0.000]\n",
      "Epoch 3 [259/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 3 [260/340] - Loss: 4.111 [-4.109, 0.002, -0.000]\n",
      "Epoch 3 [261/340] - Loss: 4.084 [-4.082, 0.002, -0.000]\n",
      "Epoch 3 [262/340] - Loss: 4.056 [-4.053, 0.003, -0.000]\n",
      "Epoch 3 [263/340] - Loss: 4.061 [-4.058, 0.003, -0.000]\n",
      "Epoch 3 [264/340] - Loss: 4.029 [-4.026, 0.003, -0.000]\n",
      "Epoch 3 [265/340] - Loss: 4.082 [-4.079, 0.003, -0.000]\n",
      "Epoch 3 [266/340] - Loss: 4.118 [-4.115, 0.003, -0.000]\n",
      "Epoch 3 [267/340] - Loss: 4.001 [-3.998, 0.003, -0.000]\n",
      "Epoch 3 [268/340] - Loss: 4.077 [-4.074, 0.003, -0.000]\n",
      "Epoch 3 [269/340] - Loss: 4.092 [-4.089, 0.002, -0.000]\n",
      "Epoch 3 [270/340] - Loss: 4.126 [-4.124, 0.002, -0.000]\n",
      "Epoch 3 [271/340] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 3 [272/340] - Loss: 4.090 [-4.088, 0.002, -0.000]\n",
      "Epoch 3 [273/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 3 [274/340] - Loss: 4.101 [-4.099, 0.002, -0.000]\n",
      "Epoch 3 [275/340] - Loss: 4.067 [-4.065, 0.002, -0.000]\n",
      "Epoch 3 [276/340] - Loss: 4.055 [-4.053, 0.002, -0.000]\n",
      "Epoch 3 [277/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 3 [278/340] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 3 [279/340] - Loss: 4.087 [-4.086, 0.002, -0.000]\n",
      "Epoch 3 [280/340] - Loss: 4.078 [-4.076, 0.002, -0.000]\n",
      "Epoch 3 [281/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 3 [282/340] - Loss: 4.050 [-4.048, 0.002, -0.000]\n",
      "Epoch 3 [283/340] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 3 [284/340] - Loss: 4.045 [-4.042, 0.002, -0.000]\n",
      "Epoch 3 [285/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 3 [286/340] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 3 [287/340] - Loss: 4.128 [-4.126, 0.002, -0.000]\n",
      "Epoch 3 [288/340] - Loss: 4.113 [-4.111, 0.002, -0.000]\n",
      "Epoch 3 [289/340] - Loss: 4.047 [-4.045, 0.002, -0.000]\n",
      "Epoch 3 [290/340] - Loss: 4.105 [-4.103, 0.002, -0.000]\n",
      "Epoch 3 [291/340] - Loss: 4.042 [-4.040, 0.002, -0.000]\n",
      "Epoch 3 [292/340] - Loss: 4.041 [-4.039, 0.002, -0.000]\n",
      "Epoch 3 [293/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 3 [294/340] - Loss: 4.066 [-4.064, 0.002, -0.000]\n",
      "Epoch 3 [295/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 3 [296/340] - Loss: 4.080 [-4.078, 0.002, -0.000]\n",
      "Epoch 3 [297/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 3 [298/340] - Loss: 4.068 [-4.066, 0.002, -0.000]\n",
      "Epoch 3 [299/340] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 3 [300/340] - Loss: 4.056 [-4.054, 0.002, -0.000]\n",
      "Epoch 3 [301/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 3 [302/340] - Loss: 4.019 [-4.017, 0.002, -0.000]\n",
      "Epoch 3 [303/340] - Loss: 4.001 [-3.999, 0.002, -0.000]\n",
      "Epoch 3 [304/340] - Loss: 4.058 [-4.055, 0.002, -0.000]\n",
      "Epoch 3 [305/340] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 3 [306/340] - Loss: 4.155 [-4.153, 0.002, -0.000]\n",
      "Epoch 3 [307/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 3 [308/340] - Loss: 4.051 [-4.049, 0.002, -0.000]\n",
      "Epoch 3 [309/340] - Loss: 4.027 [-4.025, 0.002, -0.000]\n",
      "Epoch 3 [310/340] - Loss: 4.056 [-4.054, 0.002, -0.000]\n",
      "Epoch 3 [311/340] - Loss: 4.022 [-4.020, 0.002, -0.000]\n",
      "Epoch 3 [312/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 3 [313/340] - Loss: 4.053 [-4.051, 0.002, -0.000]\n",
      "Epoch 3 [314/340] - Loss: 4.048 [-4.046, 0.002, -0.000]\n",
      "Epoch 3 [315/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 3 [316/340] - Loss: 4.048 [-4.045, 0.002, -0.000]\n",
      "Epoch 3 [317/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 3 [318/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 3 [319/340] - Loss: 4.039 [-4.037, 0.002, -0.000]\n",
      "Epoch 3 [320/340] - Loss: 4.047 [-4.045, 0.002, -0.000]\n",
      "Epoch 3 [321/340] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 3 [322/340] - Loss: 4.051 [-4.049, 0.002, -0.000]\n",
      "Epoch 3 [323/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 3 [324/340] - Loss: 4.028 [-4.026, 0.002, -0.000]\n",
      "Epoch 3 [325/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 3 [326/340] - Loss: 4.055 [-4.053, 0.002, -0.000]\n",
      "Epoch 3 [327/340] - Loss: 4.109 [-4.107, 0.002, -0.000]\n",
      "Epoch 3 [328/340] - Loss: 4.066 [-4.064, 0.002, -0.000]\n",
      "Epoch 3 [329/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 3 [330/340] - Loss: 4.090 [-4.088, 0.002, -0.000]\n",
      "Epoch 3 [331/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 3 [332/340] - Loss: 4.028 [-4.026, 0.002, -0.000]\n",
      "Epoch 3 [333/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 3 [334/340] - Loss: 4.104 [-4.102, 0.002, -0.000]\n",
      "Epoch 3 [335/340] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 3 [336/340] - Loss: 4.145 [-4.143, 0.002, -0.000]\n",
      "Epoch 3 [337/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 3 [338/340] - Loss: 4.056 [-4.054, 0.002, -0.000]\n",
      "Epoch 3 [339/340] - Loss: 4.058 [-4.056, 0.002, -0.000]\n",
      "Epoch 4 [0/340] - Loss: 4.128 [-4.126, 0.002, -0.000]\n",
      "Epoch 4 [1/340] - Loss: 4.101 [-4.099, 0.002, -0.000]\n",
      "Epoch 4 [2/340] - Loss: 4.090 [-4.088, 0.002, -0.000]\n",
      "Epoch 4 [3/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [4/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 4 [5/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [6/340] - Loss: 4.059 [-4.057, 0.002, -0.000]\n",
      "Epoch 4 [7/340] - Loss: 4.051 [-4.049, 0.002, -0.000]\n",
      "Epoch 4 [8/340] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 4 [9/340] - Loss: 4.049 [-4.048, 0.002, -0.000]\n",
      "Epoch 4 [10/340] - Loss: 4.092 [-4.090, 0.002, -0.000]\n",
      "Epoch 4 [11/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 4 [12/340] - Loss: 4.049 [-4.047, 0.002, -0.000]\n",
      "Epoch 4 [13/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [14/340] - Loss: 4.038 [-4.036, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [15/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 4 [16/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [17/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [18/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [19/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [20/340] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 4 [21/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 4 [22/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 4 [23/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [24/340] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 4 [25/340] - Loss: 4.039 [-4.037, 0.002, -0.000]\n",
      "Epoch 4 [26/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 4 [27/340] - Loss: 3.938 [-3.936, 0.002, -0.000]\n",
      "Epoch 4 [28/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 4 [29/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [30/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 4 [31/340] - Loss: 4.028 [-4.026, 0.002, -0.000]\n",
      "Epoch 4 [32/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 4 [33/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [34/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [35/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 4 [36/340] - Loss: 4.004 [-4.002, 0.002, -0.000]\n",
      "Epoch 4 [37/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 4 [38/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [39/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [40/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [41/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [42/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 4 [43/340] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 4 [44/340] - Loss: 4.047 [-4.045, 0.002, -0.000]\n",
      "Epoch 4 [45/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 4 [46/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 4 [47/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 4 [48/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [49/340] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 4 [50/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [51/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 4 [52/340] - Loss: 4.001 [-3.999, 0.002, -0.000]\n",
      "Epoch 4 [53/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 4 [54/340] - Loss: 4.015 [-4.013, 0.002, -0.000]\n",
      "Epoch 4 [55/340] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 4 [56/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [57/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 4 [58/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [59/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [60/340] - Loss: 4.015 [-4.013, 0.002, -0.000]\n",
      "Epoch 4 [61/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 4 [62/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 4 [63/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [64/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [65/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 4 [66/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 4 [67/340] - Loss: 4.072 [-4.070, 0.002, -0.000]\n",
      "Epoch 4 [68/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 4 [69/340] - Loss: 4.060 [-4.058, 0.002, -0.000]\n",
      "Epoch 4 [70/340] - Loss: 4.042 [-4.040, 0.002, -0.000]\n",
      "Epoch 4 [71/340] - Loss: 4.017 [-4.015, 0.002, -0.000]\n",
      "Epoch 4 [72/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [73/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [74/340] - Loss: 4.004 [-4.002, 0.002, -0.000]\n",
      "Epoch 4 [75/340] - Loss: 4.022 [-4.019, 0.002, -0.000]\n",
      "Epoch 4 [76/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 4 [77/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 4 [78/340] - Loss: 4.123 [-4.121, 0.002, -0.000]\n",
      "Epoch 4 [79/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 4 [80/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [81/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [82/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [83/340] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 4 [84/340] - Loss: 4.026 [-4.024, 0.002, -0.000]\n",
      "Epoch 4 [85/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 4 [86/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 4 [87/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 4 [88/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 4 [89/340] - Loss: 4.043 [-4.041, 0.002, -0.000]\n",
      "Epoch 4 [90/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 4 [91/340] - Loss: 4.061 [-4.059, 0.002, -0.000]\n",
      "Epoch 4 [92/340] - Loss: 4.027 [-4.025, 0.002, -0.000]\n",
      "Epoch 4 [93/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [94/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [95/340] - Loss: 4.070 [-4.068, 0.002, -0.000]\n",
      "Epoch 4 [96/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [97/340] - Loss: 4.061 [-4.059, 0.002, -0.000]\n",
      "Epoch 4 [98/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [99/340] - Loss: 3.974 [-3.971, 0.002, -0.000]\n",
      "Epoch 4 [100/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [101/340] - Loss: 4.069 [-4.067, 0.002, -0.000]\n",
      "Epoch 4 [102/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [103/340] - Loss: 3.972 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [104/340] - Loss: 4.092 [-4.090, 0.002, -0.000]\n",
      "Epoch 4 [105/340] - Loss: 4.111 [-4.109, 0.002, -0.000]\n",
      "Epoch 4 [106/340] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 4 [107/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [108/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [109/340] - Loss: 3.994 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [110/340] - Loss: 4.006 [-4.003, 0.002, -0.000]\n",
      "Epoch 4 [111/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [112/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [113/340] - Loss: 3.920 [-3.917, 0.002, -0.000]\n",
      "Epoch 4 [114/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 4 [115/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 4 [116/340] - Loss: 4.048 [-4.046, 0.002, -0.000]\n",
      "Epoch 4 [117/340] - Loss: 3.921 [-3.918, 0.002, -0.000]\n",
      "Epoch 4 [118/340] - Loss: 4.023 [-4.021, 0.002, -0.000]\n",
      "Epoch 4 [119/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [120/340] - Loss: 4.045 [-4.043, 0.002, -0.000]\n",
      "Epoch 4 [121/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 4 [122/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 4 [123/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 4 [124/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 4 [125/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [126/340] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 4 [127/340] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 4 [128/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 4 [129/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 4 [130/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [131/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [132/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 4 [133/340] - Loss: 4.027 [-4.025, 0.002, -0.000]\n",
      "Epoch 4 [134/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [135/340] - Loss: 4.031 [-4.029, 0.002, -0.000]\n",
      "Epoch 4 [136/340] - Loss: 4.028 [-4.026, 0.002, -0.000]\n",
      "Epoch 4 [137/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 4 [138/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [139/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 4 [140/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [141/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 4 [142/340] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 4 [143/340] - Loss: 4.045 [-4.043, 0.002, -0.000]\n",
      "Epoch 4 [144/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 4 [145/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 4 [146/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [147/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [148/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [149/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 4 [150/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [151/340] - Loss: 3.962 [-3.959, 0.002, -0.000]\n",
      "Epoch 4 [152/340] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 4 [153/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [154/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [155/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 4 [156/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 4 [157/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 4 [158/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [159/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [160/340] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 4 [161/340] - Loss: 3.980 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [162/340] - Loss: 3.954 [-3.951, 0.002, -0.000]\n",
      "Epoch 4 [163/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 4 [164/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [165/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 4 [166/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 4 [167/340] - Loss: 3.973 [-3.970, 0.002, -0.000]\n",
      "Epoch 4 [168/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 4 [169/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 4 [170/340] - Loss: 4.049 [-4.047, 0.002, -0.000]\n",
      "Epoch 4 [171/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [172/340] - Loss: 4.019 [-4.017, 0.002, -0.000]\n",
      "Epoch 4 [173/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 4 [174/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 4 [175/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 4 [176/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [177/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 4 [178/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 4 [179/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 4 [180/340] - Loss: 3.968 [-3.965, 0.002, -0.000]\n",
      "Epoch 4 [181/340] - Loss: 4.001 [-3.999, 0.002, -0.000]\n",
      "Epoch 4 [182/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 4 [183/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [184/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [185/340] - Loss: 3.993 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [186/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [187/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [188/340] - Loss: 4.051 [-4.049, 0.002, -0.000]\n",
      "Epoch 4 [189/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 4 [190/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 4 [191/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [192/340] - Loss: 3.978 [-3.975, 0.002, -0.000]\n",
      "Epoch 4 [193/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [194/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [195/340] - Loss: 3.986 [-3.983, 0.002, -0.000]\n",
      "Epoch 4 [196/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 4 [197/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [198/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 4 [199/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 4 [200/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 4 [201/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 4 [202/340] - Loss: 4.033 [-4.031, 0.002, -0.000]\n",
      "Epoch 4 [203/340] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 4 [204/340] - Loss: 4.037 [-4.035, 0.002, -0.000]\n",
      "Epoch 4 [205/340] - Loss: 4.030 [-4.028, 0.002, -0.000]\n",
      "Epoch 4 [206/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [207/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [208/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 4 [209/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [210/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 4 [211/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [212/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [213/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 4 [214/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 4 [215/340] - Loss: 4.050 [-4.048, 0.002, -0.000]\n",
      "Epoch 4 [216/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 4 [217/340] - Loss: 4.047 [-4.045, 0.002, -0.000]\n",
      "Epoch 4 [218/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 4 [219/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [220/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [221/340] - Loss: 4.032 [-4.030, 0.002, -0.000]\n",
      "Epoch 4 [222/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [223/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 4 [224/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 4 [225/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 4 [226/340] - Loss: 3.978 [-3.975, 0.002, -0.000]\n",
      "Epoch 4 [227/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 4 [228/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [229/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [230/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 4 [231/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [232/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 4 [233/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [234/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 4 [235/340] - Loss: 3.913 [-3.911, 0.002, -0.000]\n",
      "Epoch 4 [236/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [237/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 4 [238/340] - Loss: 4.034 [-4.032, 0.002, -0.000]\n",
      "Epoch 4 [239/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 4 [240/340] - Loss: 4.047 [-4.044, 0.002, -0.000]\n",
      "Epoch 4 [241/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 4 [242/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 4 [243/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 4 [244/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [245/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 4 [246/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 4 [247/340] - Loss: 4.022 [-4.020, 0.002, -0.000]\n",
      "Epoch 4 [248/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 4 [249/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 4 [250/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 4 [251/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 4 [252/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 4 [253/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 4 [254/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 4 [255/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [256/340] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 4 [257/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 4 [258/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 4 [259/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [260/340] - Loss: 4.005 [-4.002, 0.002, -0.000]\n",
      "Epoch 4 [261/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 4 [262/340] - Loss: 3.913 [-3.911, 0.002, -0.000]\n",
      "Epoch 4 [263/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 4 [264/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 4 [265/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 4 [266/340] - Loss: 4.034 [-4.032, 0.002, -0.000]\n",
      "Epoch 4 [267/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 4 [268/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [269/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [270/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 4 [271/340] - Loss: 3.921 [-3.919, 0.002, -0.000]\n",
      "Epoch 4 [272/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 4 [273/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 4 [274/340] - Loss: 4.012 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [275/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 4 [276/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [277/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 4 [278/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 4 [279/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 4 [280/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 4 [281/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 4 [282/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [283/340] - Loss: 4.010 [-4.007, 0.002, -0.000]\n",
      "Epoch 4 [284/340] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 4 [285/340] - Loss: 3.997 [-3.994, 0.002, -0.000]\n",
      "Epoch 4 [286/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [287/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 4 [288/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [289/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 4 [290/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n",
      "Epoch 4 [291/340] - Loss: 3.966 [-3.963, 0.002, -0.000]\n",
      "Epoch 4 [292/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 4 [293/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 4 [294/340] - Loss: 3.930 [-3.927, 0.002, -0.000]\n",
      "Epoch 4 [295/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [296/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 4 [297/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [298/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 4 [299/340] - Loss: 3.951 [-3.948, 0.002, -0.000]\n",
      "Epoch 4 [300/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 4 [301/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 4 [302/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 4 [303/340] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 4 [304/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [305/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 4 [306/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 4 [307/340] - Loss: 4.011 [-4.009, 0.002, -0.000]\n",
      "Epoch 4 [308/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 4 [309/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 4 [310/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 4 [311/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 4 [312/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [313/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 4 [314/340] - Loss: 3.945 [-3.944, 0.002, -0.000]\n",
      "Epoch 4 [315/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 4 [316/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 4 [317/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 4 [318/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 4 [319/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 4 [320/340] - Loss: 3.956 [-3.955, 0.002, -0.000]\n",
      "Epoch 4 [321/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 4 [322/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 4 [323/340] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 4 [324/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 4 [325/340] - Loss: 4.076 [-4.074, 0.002, -0.000]\n",
      "Epoch 4 [326/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 4 [327/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 4 [328/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 4 [329/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 4 [330/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 4 [331/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 4 [332/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 4 [333/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 4 [334/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 4 [335/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 4 [336/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 4 [337/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 4 [338/340] - Loss: 3.971 [-3.968, 0.002, -0.000]\n",
      "Epoch 4 [339/340] - Loss: 4.057 [-4.055, 0.002, -0.000]\n",
      "Epoch 5 [0/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [1/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [2/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [3/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [4/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [5/340] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 5 [6/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [7/340] - Loss: 3.965 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [8/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [9/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [10/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 5 [11/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [12/340] - Loss: 4.058 [-4.056, 0.002, -0.000]\n",
      "Epoch 5 [13/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 5 [14/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [15/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 5 [16/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 5 [17/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [18/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [19/340] - Loss: 3.977 [-3.974, 0.002, -0.000]\n",
      "Epoch 5 [20/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 5 [21/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [22/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 5 [23/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [24/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 5 [25/340] - Loss: 4.060 [-4.058, 0.002, -0.000]\n",
      "Epoch 5 [26/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [27/340] - Loss: 4.028 [-4.026, 0.002, -0.000]\n",
      "Epoch 5 [28/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [29/340] - Loss: 4.008 [-4.005, 0.002, -0.000]\n",
      "Epoch 5 [30/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [31/340] - Loss: 3.938 [-3.936, 0.002, -0.000]\n",
      "Epoch 5 [32/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 5 [33/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 5 [34/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 5 [35/340] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 5 [36/340] - Loss: 4.044 [-4.042, 0.002, -0.000]\n",
      "Epoch 5 [37/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 5 [38/340] - Loss: 3.957 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [39/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 5 [40/340] - Loss: 3.913 [-3.911, 0.002, -0.000]\n",
      "Epoch 5 [41/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [42/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 5 [43/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [44/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 5 [45/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 5 [46/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 5 [47/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [48/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [49/340] - Loss: 3.995 [-3.993, 0.002, -0.000]\n",
      "Epoch 5 [50/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 5 [51/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [52/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [53/340] - Loss: 3.996 [-3.993, 0.002, -0.000]\n",
      "Epoch 5 [54/340] - Loss: 3.988 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [55/340] - Loss: 3.957 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [56/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 5 [57/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [58/340] - Loss: 3.989 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [59/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [60/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [61/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 5 [62/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [63/340] - Loss: 4.007 [-4.005, 0.002, -0.000]\n",
      "Epoch 5 [64/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 5 [65/340] - Loss: 3.904 [-3.902, 0.002, -0.000]\n",
      "Epoch 5 [66/340] - Loss: 4.038 [-4.036, 0.002, -0.000]\n",
      "Epoch 5 [67/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 5 [68/340] - Loss: 4.046 [-4.044, 0.002, -0.000]\n",
      "Epoch 5 [69/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [70/340] - Loss: 4.008 [-4.006, 0.002, -0.000]\n",
      "Epoch 5 [71/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [72/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 5 [73/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 5 [74/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 5 [75/340] - Loss: 4.084 [-4.082, 0.002, -0.000]\n",
      "Epoch 5 [76/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [77/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 5 [78/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [79/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [80/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [81/340] - Loss: 4.062 [-4.060, 0.002, -0.000]\n",
      "Epoch 5 [82/340] - Loss: 4.013 [-4.011, 0.002, -0.000]\n",
      "Epoch 5 [83/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 5 [84/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 5 [85/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 5 [86/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [87/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 5 [88/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [89/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 5 [90/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 5 [91/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 5 [92/340] - Loss: 4.015 [-4.013, 0.002, -0.000]\n",
      "Epoch 5 [93/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [94/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [95/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [96/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [97/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [98/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [99/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [100/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 5 [101/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [102/340] - Loss: 3.915 [-3.913, 0.002, -0.000]\n",
      "Epoch 5 [103/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [104/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 5 [105/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [106/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [107/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [108/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [109/340] - Loss: 3.920 [-3.918, 0.002, -0.000]\n",
      "Epoch 5 [110/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [111/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 5 [112/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [113/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 5 [114/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 5 [115/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [116/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [117/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 5 [118/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 5 [119/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [120/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 5 [121/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [122/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [123/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [124/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 5 [125/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 5 [126/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 5 [127/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 5 [128/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [129/340] - Loss: 4.064 [-4.062, 0.002, -0.000]\n",
      "Epoch 5 [130/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [131/340] - Loss: 3.992 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [132/340] - Loss: 4.038 [-4.036, 0.002, -0.000]\n",
      "Epoch 5 [133/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [134/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [135/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 5 [136/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [137/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 5 [138/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [139/340] - Loss: 4.031 [-4.029, 0.002, -0.000]\n",
      "Epoch 5 [140/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [141/340] - Loss: 3.975 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [142/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 5 [143/340] - Loss: 3.921 [-3.919, 0.002, -0.000]\n",
      "Epoch 5 [144/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [145/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 5 [146/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 5 [147/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 5 [148/340] - Loss: 4.028 [-4.026, 0.002, -0.000]\n",
      "Epoch 5 [149/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [150/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 5 [151/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 5 [152/340] - Loss: 4.022 [-4.020, 0.002, -0.000]\n",
      "Epoch 5 [153/340] - Loss: 4.002 [-4.000, 0.002, -0.000]\n",
      "Epoch 5 [154/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [155/340] - Loss: 4.059 [-4.057, 0.002, -0.000]\n",
      "Epoch 5 [156/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 5 [157/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [158/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [159/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [160/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [161/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 5 [162/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [163/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [164/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 5 [165/340] - Loss: 4.050 [-4.048, 0.002, -0.000]\n",
      "Epoch 5 [166/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 5 [167/340] - Loss: 3.999 [-3.998, 0.002, -0.000]\n",
      "Epoch 5 [168/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [169/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 5 [170/340] - Loss: 4.042 [-4.040, 0.002, -0.000]\n",
      "Epoch 5 [171/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [172/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [173/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 5 [174/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 5 [175/340] - Loss: 4.010 [-4.008, 0.002, -0.000]\n",
      "Epoch 5 [176/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [177/340] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 5 [178/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [179/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [180/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [181/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [182/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 5 [183/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 5 [184/340] - Loss: 4.067 [-4.065, 0.002, -0.000]\n",
      "Epoch 5 [185/340] - Loss: 4.023 [-4.021, 0.002, -0.000]\n",
      "Epoch 5 [186/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [187/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [188/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 5 [189/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [190/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [191/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [192/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [193/340] - Loss: 4.024 [-4.022, 0.002, -0.000]\n",
      "Epoch 5 [194/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [195/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [196/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [197/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 5 [198/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 5 [199/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [200/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 5 [201/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 5 [202/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 5 [203/340] - Loss: 4.035 [-4.033, 0.002, -0.000]\n",
      "Epoch 5 [204/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 5 [205/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [206/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 5 [207/340] - Loss: 4.000 [-3.998, 0.002, -0.000]\n",
      "Epoch 5 [208/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [209/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [210/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [211/340] - Loss: 4.030 [-4.028, 0.002, -0.000]\n",
      "Epoch 5 [212/340] - Loss: 3.997 [-3.995, 0.002, -0.000]\n",
      "Epoch 5 [213/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [214/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [215/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 5 [216/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [217/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [218/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 5 [219/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 5 [220/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 5 [221/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [222/340] - Loss: 3.985 [-3.983, 0.002, -0.000]\n",
      "Epoch 5 [223/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 5 [224/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 5 [225/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 5 [226/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 5 [227/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [228/340] - Loss: 3.977 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [229/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [230/340] - Loss: 4.036 [-4.034, 0.002, -0.000]\n",
      "Epoch 5 [231/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [232/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 5 [233/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [234/340] - Loss: 3.972 [-3.970, 0.002, -0.000]\n",
      "Epoch 5 [235/340] - Loss: 4.065 [-4.063, 0.002, -0.000]\n",
      "Epoch 5 [236/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 5 [237/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 5 [238/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [239/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 5 [240/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [241/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 5 [242/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 5 [243/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [244/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 5 [245/340] - Loss: 3.998 [-3.996, 0.002, -0.000]\n",
      "Epoch 5 [246/340] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 5 [247/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 5 [248/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 5 [249/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 5 [250/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 5 [251/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 5 [252/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 5 [253/340] - Loss: 4.052 [-4.050, 0.002, -0.000]\n",
      "Epoch 5 [254/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 5 [255/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 5 [256/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 5 [257/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 5 [258/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 5 [259/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 5 [260/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 5 [261/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [262/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 5 [263/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [264/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 5 [265/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 5 [266/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [267/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 5 [268/340] - Loss: 4.031 [-4.029, 0.002, -0.000]\n",
      "Epoch 5 [269/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [270/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [271/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [272/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 5 [273/340] - Loss: 4.015 [-4.013, 0.002, -0.000]\n",
      "Epoch 5 [274/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [275/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 5 [276/340] - Loss: 3.922 [-3.920, 0.002, -0.000]\n",
      "Epoch 5 [277/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [278/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 5 [279/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 5 [280/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 5 [281/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [282/340] - Loss: 4.001 [-3.999, 0.002, -0.000]\n",
      "Epoch 5 [283/340] - Loss: 4.015 [-4.014, 0.002, -0.000]\n",
      "Epoch 5 [284/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [285/340] - Loss: 3.965 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [286/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 5 [287/340] - Loss: 4.021 [-4.020, 0.002, -0.000]\n",
      "Epoch 5 [288/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 5 [289/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 5 [290/340] - Loss: 4.035 [-4.033, 0.002, -0.000]\n",
      "Epoch 5 [291/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [292/340] - Loss: 3.883 [-3.881, 0.002, -0.000]\n",
      "Epoch 5 [293/340] - Loss: 3.961 [-3.959, 0.002, -0.000]\n",
      "Epoch 5 [294/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 5 [295/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 5 [296/340] - Loss: 3.893 [-3.891, 0.002, -0.000]\n",
      "Epoch 5 [297/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 5 [298/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [299/340] - Loss: 4.053 [-4.051, 0.002, -0.000]\n",
      "Epoch 5 [300/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 5 [301/340] - Loss: 4.086 [-4.084, 0.002, -0.000]\n",
      "Epoch 5 [302/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 5 [303/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 5 [304/340] - Loss: 4.025 [-4.023, 0.002, -0.000]\n",
      "Epoch 5 [305/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 5 [306/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 5 [307/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 5 [308/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 5 [309/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 5 [310/340] - Loss: 4.037 [-4.035, 0.002, -0.000]\n",
      "Epoch 5 [311/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 5 [312/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 5 [313/340] - Loss: 3.981 [-3.979, 0.002, -0.000]\n",
      "Epoch 5 [314/340] - Loss: 3.944 [-3.941, 0.002, -0.000]\n",
      "Epoch 5 [315/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 5 [316/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 5 [317/340] - Loss: 4.050 [-4.048, 0.002, -0.000]\n",
      "Epoch 5 [318/340] - Loss: 3.920 [-3.918, 0.002, -0.000]\n",
      "Epoch 5 [319/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 5 [320/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 5 [321/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [322/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 5 [323/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 5 [324/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [325/340] - Loss: 3.907 [-3.905, 0.002, -0.000]\n",
      "Epoch 5 [326/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 5 [327/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 5 [328/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [329/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 5 [330/340] - Loss: 4.016 [-4.014, 0.002, -0.000]\n",
      "Epoch 5 [331/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 5 [332/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 5 [333/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 5 [334/340] - Loss: 4.029 [-4.027, 0.002, -0.000]\n",
      "Epoch 5 [335/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 5 [336/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 5 [337/340] - Loss: 3.988 [-3.986, 0.002, -0.000]\n",
      "Epoch 5 [338/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 5 [339/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 6 [0/340] - Loss: 4.068 [-4.065, 0.002, -0.000]\n",
      "Epoch 6 [1/340] - Loss: 4.066 [-4.064, 0.002, -0.000]\n",
      "Epoch 6 [2/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [3/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 6 [4/340] - Loss: 4.054 [-4.052, 0.002, -0.000]\n",
      "Epoch 6 [5/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [6/340] - Loss: 4.019 [-4.017, 0.002, -0.000]\n",
      "Epoch 6 [7/340] - Loss: 4.074 [-4.072, 0.002, -0.000]\n",
      "Epoch 6 [8/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 6 [9/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 6 [10/340] - Loss: 3.984 [-3.982, 0.002, -0.000]\n",
      "Epoch 6 [11/340] - Loss: 4.021 [-4.019, 0.002, -0.000]\n",
      "Epoch 6 [12/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [13/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [14/340] - Loss: 4.059 [-4.057, 0.002, -0.000]\n",
      "Epoch 6 [15/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [16/340] - Loss: 3.969 [-3.966, 0.002, -0.000]\n",
      "Epoch 6 [17/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 6 [18/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [19/340] - Loss: 3.986 [-3.984, 0.002, -0.000]\n",
      "Epoch 6 [20/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 6 [21/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 6 [22/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [23/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [24/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [25/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [26/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 6 [27/340] - Loss: 3.982 [-3.979, 0.002, -0.000]\n",
      "Epoch 6 [28/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [29/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [30/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 6 [31/340] - Loss: 3.915 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [32/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 6 [33/340] - Loss: 3.967 [-3.964, 0.002, -0.000]\n",
      "Epoch 6 [34/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [35/340] - Loss: 3.914 [-3.911, 0.002, -0.000]\n",
      "Epoch 6 [36/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 6 [37/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [38/340] - Loss: 3.980 [-3.978, 0.002, -0.000]\n",
      "Epoch 6 [39/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 6 [40/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 6 [41/340] - Loss: 3.956 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [42/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [43/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 6 [44/340] - Loss: 4.020 [-4.018, 0.002, -0.000]\n",
      "Epoch 6 [45/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [46/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 6 [47/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [48/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [49/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 6 [50/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [51/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [52/340] - Loss: 3.992 [-3.990, 0.002, -0.000]\n",
      "Epoch 6 [53/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 6 [54/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 6 [55/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [56/340] - Loss: 3.960 [-3.957, 0.002, -0.000]\n",
      "Epoch 6 [57/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [58/340] - Loss: 4.000 [-3.997, 0.002, -0.000]\n",
      "Epoch 6 [59/340] - Loss: 4.006 [-4.004, 0.002, -0.000]\n",
      "Epoch 6 [60/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 6 [61/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 6 [62/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [63/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [64/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [65/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [66/340] - Loss: 4.018 [-4.016, 0.002, -0.000]\n",
      "Epoch 6 [67/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 6 [68/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [69/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 6 [70/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [71/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 6 [72/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [73/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [74/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 6 [75/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [76/340] - Loss: 3.983 [-3.981, 0.002, -0.000]\n",
      "Epoch 6 [77/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [78/340] - Loss: 3.994 [-3.991, 0.002, -0.000]\n",
      "Epoch 6 [79/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [80/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [81/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [82/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 6 [83/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 6 [84/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 6 [85/340] - Loss: 3.963 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [86/340] - Loss: 3.962 [-3.959, 0.002, -0.000]\n",
      "Epoch 6 [87/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 6 [88/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 6 [89/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 6 [90/340] - Loss: 3.897 [-3.895, 0.002, -0.000]\n",
      "Epoch 6 [91/340] - Loss: 4.040 [-4.038, 0.002, -0.000]\n",
      "Epoch 6 [92/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 6 [93/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 6 [94/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 6 [95/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [96/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 6 [97/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [98/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [99/340] - Loss: 3.894 [-3.892, 0.002, -0.000]\n",
      "Epoch 6 [100/340] - Loss: 3.918 [-3.915, 0.002, -0.000]\n",
      "Epoch 6 [101/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [102/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [103/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [104/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 6 [105/340] - Loss: 3.889 [-3.887, 0.002, -0.000]\n",
      "Epoch 6 [106/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [107/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [108/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [109/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [110/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 6 [111/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 6 [112/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [113/340] - Loss: 3.991 [-3.989, 0.002, -0.000]\n",
      "Epoch 6 [114/340] - Loss: 3.951 [-3.948, 0.002, -0.000]\n",
      "Epoch 6 [115/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 6 [116/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 6 [117/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 6 [118/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [119/340] - Loss: 3.946 [-3.944, 0.002, -0.000]\n",
      "Epoch 6 [120/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [121/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [122/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [123/340] - Loss: 3.990 [-3.988, 0.002, -0.000]\n",
      "Epoch 6 [124/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 6 [125/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 6 [126/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 6 [127/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [128/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [129/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [130/340] - Loss: 3.973 [-3.971, 0.002, -0.000]\n",
      "Epoch 6 [131/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 6 [132/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [133/340] - Loss: 3.950 [-3.948, 0.002, -0.000]\n",
      "Epoch 6 [134/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [135/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [136/340] - Loss: 3.931 [-3.928, 0.002, -0.000]\n",
      "Epoch 6 [137/340] - Loss: 3.938 [-3.936, 0.002, -0.000]\n",
      "Epoch 6 [138/340] - Loss: 3.976 [-3.974, 0.002, -0.000]\n",
      "Epoch 6 [139/340] - Loss: 3.908 [-3.905, 0.002, -0.000]\n",
      "Epoch 6 [140/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 6 [141/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 6 [142/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [143/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [144/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [145/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [146/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 6 [147/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [148/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [149/340] - Loss: 3.982 [-3.980, 0.002, -0.000]\n",
      "Epoch 6 [150/340] - Loss: 3.886 [-3.884, 0.002, -0.000]\n",
      "Epoch 6 [151/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 6 [152/340] - Loss: 3.912 [-3.910, 0.002, -0.000]\n",
      "Epoch 6 [153/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [154/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [155/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 6 [156/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [157/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 6 [158/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 6 [159/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 6 [160/340] - Loss: 3.879 [-3.877, 0.002, -0.000]\n",
      "Epoch 6 [161/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [162/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 6 [163/340] - Loss: 3.946 [-3.943, 0.002, -0.000]\n",
      "Epoch 6 [164/340] - Loss: 3.899 [-3.897, 0.002, -0.000]\n",
      "Epoch 6 [165/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [166/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [167/340] - Loss: 3.960 [-3.958, 0.002, -0.000]\n",
      "Epoch 6 [168/340] - Loss: 3.895 [-3.893, 0.002, -0.000]\n",
      "Epoch 6 [169/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 6 [170/340] - Loss: 3.957 [-3.955, 0.002, -0.000]\n",
      "Epoch 6 [171/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 6 [172/340] - Loss: 3.910 [-3.908, 0.002, -0.000]\n",
      "Epoch 6 [173/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [174/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [175/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 6 [176/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 6 [177/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [178/340] - Loss: 4.031 [-4.029, 0.002, -0.000]\n",
      "Epoch 6 [179/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 6 [180/340] - Loss: 3.884 [-3.882, 0.002, -0.000]\n",
      "Epoch 6 [181/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [182/340] - Loss: 3.971 [-3.969, 0.002, -0.000]\n",
      "Epoch 6 [183/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [184/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [185/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [186/340] - Loss: 3.938 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [187/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 6 [188/340] - Loss: 3.926 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [189/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 6 [190/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [191/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 6 [192/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [193/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [194/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [195/340] - Loss: 3.951 [-3.949, 0.002, -0.000]\n",
      "Epoch 6 [196/340] - Loss: 3.908 [-3.906, 0.002, -0.000]\n",
      "Epoch 6 [197/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 6 [198/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 6 [199/340] - Loss: 4.066 [-4.064, 0.002, -0.000]\n",
      "Epoch 6 [200/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [201/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 6 [202/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [203/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [204/340] - Loss: 3.979 [-3.977, 0.002, -0.000]\n",
      "Epoch 6 [205/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 6 [206/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 6 [207/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [208/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 6 [209/340] - Loss: 3.897 [-3.894, 0.002, -0.000]\n",
      "Epoch 6 [210/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [211/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 6 [212/340] - Loss: 3.925 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [213/340] - Loss: 4.012 [-4.010, 0.002, -0.000]\n",
      "Epoch 6 [214/340] - Loss: 3.903 [-3.901, 0.002, -0.000]\n",
      "Epoch 6 [215/340] - Loss: 3.893 [-3.891, 0.002, -0.000]\n",
      "Epoch 6 [216/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [217/340] - Loss: 3.916 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [218/340] - Loss: 3.969 [-3.967, 0.002, -0.000]\n",
      "Epoch 6 [219/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [220/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 6 [221/340] - Loss: 3.999 [-3.997, 0.002, -0.000]\n",
      "Epoch 6 [222/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 6 [223/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [224/340] - Loss: 3.940 [-3.937, 0.002, -0.000]\n",
      "Epoch 6 [225/340] - Loss: 3.929 [-3.927, 0.002, -0.000]\n",
      "Epoch 6 [226/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [227/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [228/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [229/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 6 [230/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 6 [231/340] - Loss: 3.958 [-3.956, 0.002, -0.000]\n",
      "Epoch 6 [232/340] - Loss: 3.940 [-3.937, 0.002, -0.000]\n",
      "Epoch 6 [233/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [234/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 6 [235/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [236/340] - Loss: 3.949 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [237/340] - Loss: 3.967 [-3.965, 0.002, -0.000]\n",
      "Epoch 6 [238/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [239/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 6 [240/340] - Loss: 3.947 [-3.945, 0.002, -0.000]\n",
      "Epoch 6 [241/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [242/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [243/340] - Loss: 3.914 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [244/340] - Loss: 3.937 [-3.935, 0.002, -0.000]\n",
      "Epoch 6 [245/340] - Loss: 3.949 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [246/340] - Loss: 3.955 [-3.953, 0.002, -0.000]\n",
      "Epoch 6 [247/340] - Loss: 3.898 [-3.896, 0.002, -0.000]\n",
      "Epoch 6 [248/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [249/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 6 [250/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [251/340] - Loss: 3.923 [-3.920, 0.002, -0.000]\n",
      "Epoch 6 [252/340] - Loss: 3.935 [-3.933, 0.002, -0.000]\n",
      "Epoch 6 [253/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [254/340] - Loss: 3.922 [-3.919, 0.002, -0.000]\n",
      "Epoch 6 [255/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [256/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [257/340] - Loss: 3.942 [-3.940, 0.002, -0.000]\n",
      "Epoch 6 [258/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [259/340] - Loss: 3.941 [-3.939, 0.002, -0.000]\n",
      "Epoch 6 [260/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 6 [261/340] - Loss: 3.962 [-3.960, 0.002, -0.000]\n",
      "Epoch 6 [262/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 6 [263/340] - Loss: 4.005 [-4.003, 0.002, -0.000]\n",
      "Epoch 6 [264/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 6 [265/340] - Loss: 3.898 [-3.896, 0.002, -0.000]\n",
      "Epoch 6 [266/340] - Loss: 3.993 [-3.991, 0.002, -0.000]\n",
      "Epoch 6 [267/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [268/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 6 [269/340] - Loss: 3.959 [-3.957, 0.002, -0.000]\n",
      "Epoch 6 [270/340] - Loss: 3.883 [-3.881, 0.002, -0.000]\n",
      "Epoch 6 [271/340] - Loss: 3.954 [-3.952, 0.002, -0.000]\n",
      "Epoch 6 [272/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [273/340] - Loss: 3.927 [-3.925, 0.002, -0.000]\n",
      "Epoch 6 [274/340] - Loss: 3.975 [-3.973, 0.002, -0.000]\n",
      "Epoch 6 [275/340] - Loss: 3.933 [-3.931, 0.002, -0.000]\n",
      "Epoch 6 [276/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 6 [277/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [278/340] - Loss: 4.003 [-4.001, 0.002, -0.000]\n",
      "Epoch 6 [279/340] - Loss: 3.917 [-3.915, 0.002, -0.000]\n",
      "Epoch 6 [280/340] - Loss: 3.921 [-3.919, 0.002, -0.000]\n",
      "Epoch 6 [281/340] - Loss: 3.926 [-3.924, 0.002, -0.000]\n",
      "Epoch 6 [282/340] - Loss: 3.891 [-3.889, 0.002, -0.000]\n",
      "Epoch 6 [283/340] - Loss: 3.949 [-3.947, 0.002, -0.000]\n",
      "Epoch 6 [284/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 6 [285/340] - Loss: 3.970 [-3.968, 0.002, -0.000]\n",
      "Epoch 6 [286/340] - Loss: 3.945 [-3.943, 0.002, -0.000]\n",
      "Epoch 6 [287/340] - Loss: 3.965 [-3.963, 0.002, -0.000]\n",
      "Epoch 6 [288/340] - Loss: 3.948 [-3.946, 0.002, -0.000]\n",
      "Epoch 6 [289/340] - Loss: 3.930 [-3.928, 0.002, -0.000]\n",
      "Epoch 6 [290/340] - Loss: 3.884 [-3.882, 0.002, -0.000]\n",
      "Epoch 6 [291/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 6 [292/340] - Loss: 3.936 [-3.934, 0.002, -0.000]\n",
      "Epoch 6 [293/340] - Loss: 3.952 [-3.950, 0.002, -0.000]\n",
      "Epoch 6 [294/340] - Loss: 3.896 [-3.894, 0.002, -0.000]\n",
      "Epoch 6 [295/340] - Loss: 3.902 [-3.900, 0.002, -0.000]\n",
      "Epoch 6 [296/340] - Loss: 3.906 [-3.904, 0.002, -0.000]\n",
      "Epoch 6 [297/340] - Loss: 3.987 [-3.985, 0.002, -0.000]\n",
      "Epoch 6 [298/340] - Loss: 3.925 [-3.923, 0.002, -0.000]\n",
      "Epoch 6 [299/340] - Loss: 3.924 [-3.922, 0.002, -0.000]\n",
      "Epoch 6 [300/340] - Loss: 3.994 [-3.992, 0.002, -0.000]\n",
      "Epoch 6 [301/340] - Loss: 3.913 [-3.911, 0.002, -0.000]\n",
      "Epoch 6 [302/340] - Loss: 3.932 [-3.930, 0.002, -0.000]\n",
      "Epoch 6 [303/340] - Loss: 3.939 [-3.937, 0.002, -0.000]\n",
      "Epoch 6 [304/340] - Loss: 4.017 [-4.014, 0.002, -0.000]\n",
      "Epoch 6 [305/340] - Loss: 3.953 [-3.951, 0.002, -0.000]\n",
      "Epoch 6 [306/340] - Loss: 3.944 [-3.942, 0.002, -0.000]\n",
      "Epoch 6 [307/340] - Loss: 3.964 [-3.962, 0.002, -0.000]\n",
      "Epoch 6 [308/340] - Loss: 3.968 [-3.966, 0.002, -0.000]\n",
      "Epoch 6 [309/340] - Loss: 3.977 [-3.975, 0.002, -0.000]\n",
      "Epoch 6 [310/340] - Loss: 3.922 [-3.919, 0.002, -0.000]\n",
      "Epoch 6 [311/340] - Loss: 4.009 [-4.007, 0.002, -0.000]\n",
      "Epoch 6 [312/340] - Loss: 3.923 [-3.921, 0.002, -0.000]\n",
      "Epoch 6 [313/340] - Loss: 3.910 [-3.908, 0.002, -0.000]\n",
      "Epoch 6 [314/340] - Loss: 4.014 [-4.012, 0.002, -0.000]\n",
      "Epoch 6 [315/340] - Loss: 3.928 [-3.926, 0.002, -0.000]\n",
      "Epoch 6 [316/340] - Loss: 3.918 [-3.916, 0.002, -0.000]\n",
      "Epoch 6 [317/340] - Loss: 3.963 [-3.961, 0.002, -0.000]\n",
      "Epoch 6 [318/340] - Loss: 3.916 [-3.914, 0.002, -0.000]\n",
      "Epoch 6 [319/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 6 [320/340] - Loss: 3.890 [-3.888, 0.002, -0.000]\n",
      "Epoch 6 [321/340] - Loss: 3.966 [-3.964, 0.002, -0.000]\n",
      "Epoch 6 [322/340] - Loss: 3.989 [-3.987, 0.002, -0.000]\n",
      "Epoch 6 [323/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 6 [324/340] - Loss: 3.911 [-3.909, 0.002, -0.000]\n",
      "Epoch 6 [325/340] - Loss: 4.017 [-4.014, 0.002, -0.000]\n",
      "Epoch 6 [326/340] - Loss: 3.996 [-3.994, 0.002, -0.000]\n",
      "Epoch 6 [327/340] - Loss: 3.901 [-3.899, 0.002, -0.000]\n",
      "Epoch 6 [328/340] - Loss: 3.910 [-3.908, 0.002, -0.000]\n",
      "Epoch 6 [329/340] - Loss: 3.931 [-3.929, 0.002, -0.000]\n",
      "Epoch 6 [330/340] - Loss: 3.943 [-3.941, 0.002, -0.000]\n",
      "Epoch 6 [331/340] - Loss: 3.978 [-3.976, 0.002, -0.000]\n",
      "Epoch 6 [332/340] - Loss: 3.956 [-3.954, 0.002, -0.000]\n",
      "Epoch 6 [333/340] - Loss: 3.915 [-3.912, 0.002, -0.000]\n",
      "Epoch 6 [334/340] - Loss: 3.940 [-3.938, 0.002, -0.000]\n",
      "Epoch 6 [335/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [336/340] - Loss: 3.974 [-3.972, 0.002, -0.000]\n",
      "Epoch 6 [337/340] - Loss: 3.910 [-3.908, 0.002, -0.000]\n",
      "Epoch 6 [338/340] - Loss: 3.934 [-3.932, 0.002, -0.000]\n",
      "Epoch 6 [339/340] - Loss: 3.919 [-3.917, 0.002, -0.000]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 1 epochs of training in this tutorial\n",
    "num_epochs = 6\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Because the grid is relatively small, we turn off the Toeplitz matrix multiplication and just perform them directly\n",
    "        # We find this to be more efficient when the grid is very small.\n",
    "        with gpytorch.settings.use_toeplitz(False):\n",
    "            output = model(x_batch)\n",
    "            log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "            loss = -(log_lik - kl_div + log_prior)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader), loss.item(), log_lik.item(), kl_div.item(), log_prior.item()))\n",
    "\n",
    "        # The actual optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`) using the standard SKI testing code, with no acceleration or precomputation. Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although our other tutorials demonstrate how to do this (for example, see the CIFAR tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrg365/gpytorch/gpytorch/utils/cholesky.py:14: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  potrf_list = [sub_mat.potrf() for sub_mat in mat.view(-1, *mat.shape[-2:])]\n",
      "/home/jrg365/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py:74: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  ld_one = lr_flipped.potrf().diag().log().sum() * 2\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 8.996102333068848\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
