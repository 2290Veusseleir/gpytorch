{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA) (w/ SVGP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use Deep Kernel Learning with SVGP stochastic variational regression to rapidly train using minibatches on the `3droad` UCI dataset with hundreds of thousands of training examples. \n",
    "\n",
    "In contrast to the SVDKL_Regression_GridInterp_CUDA notebook, we'll be using SVGP (https://arxiv.org/pdf/1411.2005.pdf) here to learn the inducing point locations. Our implementation of SVGP is modified to be efficient with the inference techniques used in GPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `song` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~136 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Feature Extractor\n",
    "\n",
    "Next, we define the neural network feature extractor used to define the deep kernel. In this case, we use a fully connected network with the architecture `d -> 1000 -> 500 -> 50 -> 2`, as described in the original DKL paper. All of the code below uses standard PyTorch implementations of neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('bn1', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 1000))\n",
    "        self.add_module('bn2', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu2', torch.nn.ReLU())                       \n",
    "        self.add_module('linear3', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('bn3', torch.nn.BatchNorm1d(500))\n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('bn4', torch.nn.BatchNorm1d(50))\n",
    "        self.add_module('relu4', torch.nn.ReLU())                  \n",
    "        self.add_module('linear5', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Regression Layer\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. In this example, because we will be learning the inducing point locations, we'll be using a base `VariationalStrategy` with `learn_inducing_locations=True`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "softplus = torch.functional.F.softplus\n",
    "\n",
    "class GPRegressionLayer(AbstractVariationalGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPRegressionLayer, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(param_transform=softplus), param_transform=softplus\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Model\n",
    "\n",
    "With the feature extractor and GP regression layer defined, we can now define our full model. To do this, we simply create a module whose `forward()` method passes the data first through the feature extractor, and then through the GP regression layer.\n",
    "\n",
    "The only other interesting feature of the model below is that we use a helper function, `scale_to_bounds`, to ensure that the features extracted by the neural network fit within the grid bounds used for SKI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, inducing_points, feature_extractor, num_features, grid_bounds=(-1., 1.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GPRegressionLayer(inducing_points)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "inducing_points = gpytorch.utils.grid.scale_to_bounds(feature_extractor(train_x[:500, :]), -1, 1)\n",
    "model = DKLModel(inducing_points=inducing_points, feature_extractor=feature_extractor, num_features=num_features).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(param_transform=softplus).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the DKL model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalMarginalLogLikelihood` or ELBO), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/340] - Loss: 231.386 [-231.386, 0.000, 0.000]\n",
      "Epoch 1 [1/340] - Loss: 224.000 [-223.660, 0.340, 0.000]\n",
      "Epoch 1 [2/340] - Loss: 215.339 [-214.659, 0.680, 0.000]\n",
      "Epoch 1 [3/340] - Loss: 238.089 [-237.241, 0.848, 0.000]\n",
      "Epoch 1 [4/340] - Loss: 219.715 [-218.678, 1.037, 0.000]\n",
      "Epoch 1 [5/340] - Loss: 200.558 [-199.142, 1.416, 0.000]\n",
      "Epoch 1 [6/340] - Loss: 177.126 [-175.359, 1.767, 0.000]\n",
      "Epoch 1 [7/340] - Loss: 168.512 [-166.541, 1.971, 0.000]\n",
      "Epoch 1 [8/340] - Loss: 154.651 [-152.548, 2.103, 0.000]\n",
      "Epoch 1 [9/340] - Loss: 125.872 [-123.504, 2.368, 0.000]\n",
      "Epoch 1 [10/340] - Loss: 137.950 [-135.491, 2.460, 0.000]\n",
      "Epoch 1 [11/340] - Loss: 127.375 [-124.782, 2.593, 0.000]\n",
      "Epoch 1 [12/340] - Loss: 123.587 [-120.999, 2.588, 0.000]\n",
      "Epoch 1 [13/340] - Loss: 119.784 [-117.043, 2.741, 0.000]\n",
      "Epoch 1 [14/340] - Loss: 116.733 [-113.707, 3.026, 0.000]\n",
      "Epoch 1 [15/340] - Loss: 110.453 [-107.430, 3.023, 0.000]\n",
      "Epoch 1 [16/340] - Loss: 104.404 [-101.405, 2.999, 0.000]\n",
      "Epoch 1 [17/340] - Loss: 106.833 [-103.820, 3.014, 0.000]\n",
      "Epoch 1 [18/340] - Loss: 101.867 [-98.832, 3.035, 0.000]\n",
      "Epoch 1 [19/340] - Loss: 98.269 [-95.270, 2.999, 0.000]\n",
      "Epoch 1 [20/340] - Loss: 88.536 [-85.676, 2.861, 0.000]\n",
      "Epoch 1 [21/340] - Loss: 95.490 [-92.534, 2.955, 0.000]\n",
      "Epoch 1 [22/340] - Loss: 90.651 [-87.916, 2.735, 0.000]\n",
      "Epoch 1 [23/340] - Loss: 89.219 [-86.537, 2.683, 0.000]\n",
      "Epoch 1 [24/340] - Loss: 91.622 [-88.933, 2.689, 0.000]\n",
      "Epoch 1 [25/340] - Loss: 78.015 [-75.347, 2.668, 0.000]\n",
      "Epoch 1 [26/340] - Loss: 86.313 [-83.710, 2.604, 0.000]\n",
      "Epoch 1 [27/340] - Loss: 79.874 [-77.312, 2.561, 0.000]\n",
      "Epoch 1 [28/340] - Loss: 78.152 [-75.584, 2.569, 0.000]\n",
      "Epoch 1 [29/340] - Loss: 80.537 [-77.715, 2.822, 0.000]\n",
      "Epoch 1 [30/340] - Loss: 76.545 [-73.858, 2.687, 0.000]\n",
      "Epoch 1 [31/340] - Loss: 74.960 [-72.420, 2.540, 0.000]\n",
      "Epoch 1 [32/340] - Loss: 73.308 [-70.832, 2.476, 0.000]\n",
      "Epoch 1 [33/340] - Loss: 69.507 [-67.055, 2.452, 0.000]\n",
      "Epoch 1 [34/340] - Loss: 72.892 [-70.460, 2.431, 0.000]\n",
      "Epoch 1 [35/340] - Loss: 69.721 [-67.305, 2.416, 0.000]\n",
      "Epoch 1 [36/340] - Loss: 63.309 [-60.892, 2.417, 0.000]\n",
      "Epoch 1 [37/340] - Loss: 70.593 [-68.170, 2.423, 0.000]\n",
      "Epoch 1 [38/340] - Loss: 65.635 [-63.212, 2.423, 0.000]\n",
      "Epoch 1 [39/340] - Loss: 65.976 [-63.579, 2.398, 0.000]\n",
      "Epoch 1 [40/340] - Loss: 63.815 [-61.475, 2.340, 0.000]\n",
      "Epoch 1 [41/340] - Loss: 65.438 [-63.228, 2.209, 0.000]\n",
      "Epoch 1 [42/340] - Loss: 66.597 [-64.461, 2.136, 0.000]\n",
      "Epoch 1 [43/340] - Loss: 63.192 [-61.120, 2.073, 0.000]\n",
      "Epoch 1 [44/340] - Loss: 59.044 [-57.033, 2.010, 0.000]\n",
      "Epoch 1 [45/340] - Loss: 58.356 [-56.415, 1.941, 0.000]\n",
      "Epoch 1 [46/340] - Loss: 55.854 [-53.988, 1.867, 0.000]\n",
      "Epoch 1 [47/340] - Loss: 56.427 [-54.645, 1.782, 0.000]\n",
      "Epoch 1 [48/340] - Loss: 62.113 [-60.419, 1.694, 0.000]\n",
      "Epoch 1 [49/340] - Loss: 53.201 [-51.579, 1.622, 0.000]\n",
      "Epoch 1 [50/340] - Loss: 56.106 [-54.552, 1.554, 0.000]\n",
      "Epoch 1 [51/340] - Loss: 55.712 [-54.216, 1.496, 0.000]\n",
      "Epoch 1 [52/340] - Loss: 58.372 [-56.928, 1.444, 0.000]\n",
      "Epoch 1 [53/340] - Loss: 60.815 [-59.429, 1.386, 0.000]\n",
      "Epoch 1 [54/340] - Loss: 56.065 [-54.728, 1.337, 0.000]\n",
      "Epoch 1 [55/340] - Loss: 52.497 [-51.204, 1.293, 0.000]\n",
      "Epoch 1 [56/340] - Loss: 53.851 [-52.604, 1.247, 0.000]\n",
      "Epoch 1 [57/340] - Loss: 47.766 [-46.563, 1.203, 0.000]\n",
      "Epoch 1 [58/340] - Loss: 54.175 [-53.015, 1.161, 0.000]\n",
      "Epoch 1 [59/340] - Loss: 47.816 [-46.699, 1.118, 0.000]\n",
      "Epoch 1 [60/340] - Loss: 52.385 [-51.303, 1.081, 0.000]\n",
      "Epoch 1 [61/340] - Loss: 51.091 [-50.043, 1.048, 0.000]\n",
      "Epoch 1 [62/340] - Loss: 55.020 [-54.002, 1.018, 0.000]\n",
      "Epoch 1 [63/340] - Loss: 49.439 [-48.455, 0.985, 0.000]\n",
      "Epoch 1 [64/340] - Loss: 50.411 [-49.459, 0.953, 0.000]\n",
      "Epoch 1 [65/340] - Loss: 48.444 [-47.521, 0.922, 0.000]\n",
      "Epoch 1 [66/340] - Loss: 49.879 [-48.984, 0.894, 0.000]\n",
      "Epoch 1 [67/340] - Loss: 52.471 [-51.606, 0.866, 0.000]\n",
      "Epoch 1 [68/340] - Loss: 47.800 [-46.965, 0.834, 0.000]\n",
      "Epoch 1 [69/340] - Loss: 52.072 [-51.265, 0.807, 0.000]\n",
      "Epoch 1 [70/340] - Loss: 48.040 [-47.262, 0.778, 0.000]\n",
      "Epoch 1 [71/340] - Loss: 48.103 [-47.350, 0.753, 0.000]\n",
      "Epoch 1 [72/340] - Loss: 48.086 [-47.358, 0.728, 0.000]\n",
      "Epoch 1 [73/340] - Loss: 45.709 [-45.006, 0.703, 0.000]\n",
      "Epoch 1 [74/340] - Loss: 49.896 [-49.217, 0.679, 0.000]\n",
      "Epoch 1 [75/340] - Loss: 44.571 [-43.914, 0.657, 0.000]\n",
      "Epoch 1 [76/340] - Loss: 43.301 [-42.669, 0.632, 0.000]\n",
      "Epoch 1 [77/340] - Loss: 43.508 [-42.899, 0.609, 0.000]\n",
      "Epoch 1 [78/340] - Loss: 45.961 [-45.373, 0.588, 0.000]\n",
      "Epoch 1 [79/340] - Loss: 42.370 [-41.803, 0.567, 0.000]\n",
      "Epoch 1 [80/340] - Loss: 41.170 [-40.621, 0.550, 0.000]\n",
      "Epoch 1 [81/340] - Loss: 40.932 [-40.399, 0.533, 0.000]\n",
      "Epoch 1 [82/340] - Loss: 41.177 [-40.659, 0.517, 0.000]\n",
      "Epoch 1 [83/340] - Loss: 38.442 [-37.939, 0.503, 0.000]\n",
      "Epoch 1 [84/340] - Loss: 44.061 [-43.572, 0.488, 0.000]\n",
      "Epoch 1 [85/340] - Loss: 34.639 [-34.167, 0.472, 0.000]\n",
      "Epoch 1 [86/340] - Loss: 47.891 [-47.433, 0.458, 0.000]\n",
      "Epoch 1 [87/340] - Loss: 41.358 [-40.911, 0.447, 0.000]\n",
      "Epoch 1 [88/340] - Loss: 35.419 [-34.984, 0.435, 0.000]\n",
      "Epoch 1 [89/340] - Loss: 40.324 [-39.901, 0.423, 0.000]\n",
      "Epoch 1 [90/340] - Loss: 41.531 [-41.121, 0.410, 0.000]\n",
      "Epoch 1 [91/340] - Loss: 35.276 [-34.880, 0.396, 0.000]\n",
      "Epoch 1 [92/340] - Loss: 38.249 [-37.868, 0.381, 0.000]\n",
      "Epoch 1 [93/340] - Loss: 39.025 [-38.655, 0.370, 0.000]\n",
      "Epoch 1 [94/340] - Loss: 39.433 [-39.072, 0.361, 0.000]\n",
      "Epoch 1 [95/340] - Loss: 34.929 [-34.575, 0.353, 0.000]\n",
      "Epoch 1 [96/340] - Loss: 35.330 [-34.977, 0.354, 0.000]\n",
      "Epoch 1 [97/340] - Loss: 38.860 [-38.489, 0.372, 0.000]\n",
      "Epoch 1 [98/340] - Loss: 35.887 [-35.515, 0.371, 0.000]\n",
      "Epoch 1 [99/340] - Loss: 32.922 [-32.559, 0.363, 0.000]\n",
      "Epoch 1 [100/340] - Loss: 39.584 [-39.245, 0.340, 0.000]\n",
      "Epoch 1 [101/340] - Loss: 37.164 [-36.826, 0.338, 0.000]\n",
      "Epoch 1 [102/340] - Loss: 37.115 [-36.776, 0.339, 0.000]\n",
      "Epoch 1 [103/340] - Loss: 38.615 [-38.281, 0.334, 0.000]\n",
      "Epoch 1 [104/340] - Loss: 33.451 [-33.125, 0.327, 0.000]\n",
      "Epoch 1 [105/340] - Loss: 31.681 [-31.364, 0.318, 0.000]\n",
      "Epoch 1 [106/340] - Loss: 31.065 [-30.754, 0.311, 0.000]\n",
      "Epoch 1 [107/340] - Loss: 33.112 [-32.802, 0.310, 0.000]\n",
      "Epoch 1 [108/340] - Loss: 28.247 [-27.936, 0.311, 0.000]\n",
      "Epoch 1 [109/340] - Loss: 33.066 [-32.745, 0.321, 0.000]\n",
      "Epoch 1 [110/340] - Loss: 35.433 [-35.110, 0.323, 0.000]\n",
      "Epoch 1 [111/340] - Loss: 33.524 [-33.214, 0.310, 0.000]\n",
      "Epoch 1 [112/340] - Loss: 33.909 [-33.609, 0.299, 0.000]\n",
      "Epoch 1 [113/340] - Loss: 32.663 [-32.373, 0.290, 0.000]\n",
      "Epoch 1 [114/340] - Loss: 32.915 [-32.628, 0.287, 0.000]\n",
      "Epoch 1 [115/340] - Loss: 35.926 [-35.639, 0.288, 0.000]\n",
      "Epoch 1 [116/340] - Loss: 33.395 [-33.103, 0.292, 0.000]\n",
      "Epoch 1 [117/340] - Loss: 32.245 [-31.948, 0.297, 0.000]\n",
      "Epoch 1 [118/340] - Loss: 36.052 [-35.751, 0.301, 0.000]\n",
      "Epoch 1 [119/340] - Loss: 33.259 [-32.961, 0.298, 0.000]\n",
      "Epoch 1 [120/340] - Loss: 36.447 [-36.151, 0.296, 0.000]\n",
      "Epoch 1 [121/340] - Loss: 37.873 [-37.581, 0.292, 0.000]\n",
      "Epoch 1 [122/340] - Loss: 37.034 [-36.746, 0.288, 0.000]\n",
      "Epoch 1 [123/340] - Loss: 31.547 [-31.266, 0.282, 0.000]\n",
      "Epoch 1 [124/340] - Loss: 31.159 [-30.886, 0.273, 0.000]\n",
      "Epoch 1 [125/340] - Loss: 30.306 [-30.047, 0.259, 0.000]\n",
      "Epoch 1 [126/340] - Loss: 32.956 [-32.710, 0.246, 0.000]\n",
      "Epoch 1 [127/340] - Loss: 29.388 [-29.154, 0.234, 0.000]\n",
      "Epoch 1 [128/340] - Loss: 30.462 [-30.237, 0.226, 0.000]\n",
      "Epoch 1 [129/340] - Loss: 29.898 [-29.672, 0.225, 0.000]\n",
      "Epoch 1 [130/340] - Loss: 29.772 [-29.543, 0.229, 0.000]\n",
      "Epoch 1 [131/340] - Loss: 33.165 [-32.934, 0.230, 0.000]\n",
      "Epoch 1 [132/340] - Loss: 32.548 [-32.336, 0.212, 0.000]\n",
      "Epoch 1 [133/340] - Loss: 29.071 [-28.878, 0.193, 0.000]\n",
      "Epoch 1 [134/340] - Loss: 29.525 [-29.341, 0.184, 0.000]\n",
      "Epoch 1 [135/340] - Loss: 31.804 [-31.625, 0.179, 0.000]\n",
      "Epoch 1 [136/340] - Loss: 32.724 [-32.547, 0.177, 0.000]\n",
      "Epoch 1 [137/340] - Loss: 30.332 [-30.155, 0.177, 0.000]\n",
      "Epoch 1 [138/340] - Loss: 31.106 [-30.921, 0.186, 0.000]\n",
      "Epoch 1 [139/340] - Loss: 27.957 [-27.769, 0.188, 0.000]\n",
      "Epoch 1 [140/340] - Loss: 30.984 [-30.791, 0.193, 0.000]\n",
      "Epoch 1 [141/340] - Loss: 29.949 [-29.755, 0.194, 0.000]\n",
      "Epoch 1 [142/340] - Loss: 37.060 [-36.857, 0.203, 0.000]\n",
      "Epoch 1 [143/340] - Loss: 30.509 [-30.305, 0.204, 0.000]\n",
      "Epoch 1 [144/340] - Loss: 29.562 [-29.359, 0.203, 0.000]\n",
      "Epoch 1 [145/340] - Loss: 29.829 [-29.627, 0.202, 0.000]\n",
      "Epoch 1 [146/340] - Loss: 29.725 [-29.527, 0.199, 0.000]\n",
      "Epoch 1 [147/340] - Loss: 27.284 [-27.086, 0.199, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [148/340] - Loss: 26.419 [-26.223, 0.196, 0.000]\n",
      "Epoch 1 [149/340] - Loss: 29.123 [-28.923, 0.199, 0.000]\n",
      "Epoch 1 [150/340] - Loss: 29.041 [-28.836, 0.205, 0.000]\n",
      "Epoch 1 [151/340] - Loss: 27.033 [-26.834, 0.199, 0.000]\n",
      "Epoch 1 [152/340] - Loss: 27.389 [-27.191, 0.198, 0.000]\n",
      "Epoch 1 [153/340] - Loss: 30.345 [-30.148, 0.198, 0.000]\n",
      "Epoch 1 [154/340] - Loss: 29.007 [-28.806, 0.200, 0.000]\n",
      "Epoch 1 [155/340] - Loss: 25.998 [-25.798, 0.200, 0.000]\n",
      "Epoch 1 [156/340] - Loss: 26.288 [-26.086, 0.201, 0.000]\n",
      "Epoch 1 [157/340] - Loss: 25.736 [-25.532, 0.204, 0.000]\n",
      "Epoch 1 [158/340] - Loss: 26.964 [-26.755, 0.210, 0.000]\n",
      "Epoch 1 [159/340] - Loss: 24.904 [-24.691, 0.213, 0.000]\n",
      "Epoch 1 [160/340] - Loss: 27.456 [-27.247, 0.209, 0.000]\n",
      "Epoch 1 [161/340] - Loss: 27.573 [-27.366, 0.207, 0.000]\n",
      "Epoch 1 [162/340] - Loss: 24.242 [-24.043, 0.200, 0.000]\n",
      "Epoch 1 [163/340] - Loss: 25.195 [-24.996, 0.199, 0.000]\n",
      "Epoch 1 [164/340] - Loss: 24.416 [-24.219, 0.197, 0.000]\n",
      "Epoch 1 [165/340] - Loss: 24.053 [-23.860, 0.193, 0.000]\n",
      "Epoch 1 [166/340] - Loss: 25.481 [-25.291, 0.190, 0.000]\n",
      "Epoch 1 [167/340] - Loss: 23.580 [-23.393, 0.186, 0.000]\n",
      "Epoch 1 [168/340] - Loss: 24.261 [-24.074, 0.187, 0.000]\n",
      "Epoch 1 [169/340] - Loss: 23.244 [-23.051, 0.193, 0.000]\n",
      "Epoch 1 [170/340] - Loss: 22.430 [-22.231, 0.199, 0.000]\n",
      "Epoch 1 [171/340] - Loss: 26.043 [-25.842, 0.202, 0.000]\n",
      "Epoch 1 [172/340] - Loss: 23.206 [-23.003, 0.203, 0.000]\n",
      "Epoch 1 [173/340] - Loss: 24.266 [-24.060, 0.206, 0.000]\n",
      "Epoch 1 [174/340] - Loss: 23.346 [-23.136, 0.210, 0.000]\n",
      "Epoch 1 [175/340] - Loss: 21.662 [-21.449, 0.212, 0.000]\n",
      "Epoch 1 [176/340] - Loss: 23.168 [-22.956, 0.212, 0.000]\n",
      "Epoch 1 [177/340] - Loss: 20.820 [-20.607, 0.213, 0.000]\n",
      "Epoch 1 [178/340] - Loss: 24.739 [-24.523, 0.216, 0.000]\n",
      "Epoch 1 [179/340] - Loss: 24.994 [-24.771, 0.223, 0.000]\n",
      "Epoch 1 [180/340] - Loss: 25.017 [-24.786, 0.231, 0.000]\n",
      "Epoch 1 [181/340] - Loss: 24.618 [-24.379, 0.238, 0.000]\n",
      "Epoch 1 [182/340] - Loss: 24.417 [-24.174, 0.243, 0.000]\n",
      "Epoch 1 [183/340] - Loss: 25.544 [-25.300, 0.244, 0.000]\n",
      "Epoch 1 [184/340] - Loss: 29.172 [-28.928, 0.244, 0.000]\n",
      "Epoch 1 [185/340] - Loss: 30.811 [-30.564, 0.248, 0.000]\n",
      "Epoch 1 [186/340] - Loss: 30.189 [-29.943, 0.246, 0.000]\n",
      "Epoch 1 [187/340] - Loss: 30.476 [-30.232, 0.243, 0.000]\n",
      "Epoch 1 [188/340] - Loss: 27.366 [-27.125, 0.242, 0.000]\n",
      "Epoch 1 [189/340] - Loss: 25.762 [-25.524, 0.238, 0.000]\n",
      "Epoch 1 [190/340] - Loss: 24.569 [-24.333, 0.235, 0.000]\n",
      "Epoch 1 [191/340] - Loss: 27.166 [-26.932, 0.234, 0.000]\n",
      "Epoch 1 [192/340] - Loss: 26.271 [-26.040, 0.232, 0.000]\n",
      "Epoch 1 [193/340] - Loss: 24.469 [-24.242, 0.227, 0.000]\n",
      "Epoch 1 [194/340] - Loss: 24.515 [-24.295, 0.221, 0.000]\n",
      "Epoch 1 [195/340] - Loss: 25.027 [-24.813, 0.214, 0.000]\n",
      "Epoch 1 [196/340] - Loss: 25.101 [-24.893, 0.208, 0.000]\n",
      "Epoch 1 [197/340] - Loss: 23.638 [-23.435, 0.203, 0.000]\n",
      "Epoch 1 [198/340] - Loss: 22.256 [-22.059, 0.196, 0.000]\n",
      "Epoch 1 [199/340] - Loss: 24.481 [-24.289, 0.192, 0.000]\n",
      "Epoch 1 [200/340] - Loss: 22.774 [-22.583, 0.192, 0.000]\n",
      "Epoch 1 [201/340] - Loss: 25.712 [-25.519, 0.192, 0.000]\n",
      "Epoch 1 [202/340] - Loss: 21.694 [-21.503, 0.191, 0.000]\n",
      "Epoch 1 [203/340] - Loss: 20.631 [-20.442, 0.188, 0.000]\n",
      "Epoch 1 [204/340] - Loss: 22.306 [-22.120, 0.186, 0.000]\n",
      "Epoch 1 [205/340] - Loss: 21.759 [-21.573, 0.187, 0.000]\n",
      "Epoch 1 [206/340] - Loss: 24.013 [-23.831, 0.182, 0.000]\n",
      "Epoch 1 [207/340] - Loss: 21.771 [-21.591, 0.180, 0.000]\n",
      "Epoch 1 [208/340] - Loss: 24.256 [-24.077, 0.179, 0.000]\n",
      "Epoch 1 [209/340] - Loss: 23.135 [-22.956, 0.179, 0.000]\n",
      "Epoch 1 [210/340] - Loss: 21.915 [-21.736, 0.179, 0.000]\n",
      "Epoch 1 [211/340] - Loss: 23.456 [-23.278, 0.178, 0.000]\n",
      "Epoch 1 [212/340] - Loss: 21.780 [-21.605, 0.175, 0.000]\n",
      "Epoch 1 [213/340] - Loss: 21.609 [-21.438, 0.171, 0.000]\n",
      "Epoch 1 [214/340] - Loss: 21.418 [-21.247, 0.171, 0.000]\n",
      "Epoch 1 [215/340] - Loss: 19.722 [-19.550, 0.172, 0.000]\n",
      "Epoch 1 [216/340] - Loss: 21.977 [-21.804, 0.174, 0.000]\n",
      "Epoch 1 [217/340] - Loss: 22.206 [-22.031, 0.175, 0.000]\n",
      "Epoch 1 [218/340] - Loss: 22.111 [-21.941, 0.170, 0.000]\n",
      "Epoch 1 [219/340] - Loss: 19.861 [-19.698, 0.163, 0.000]\n",
      "Epoch 1 [220/340] - Loss: 22.551 [-22.393, 0.158, 0.000]\n",
      "Epoch 1 [221/340] - Loss: 22.171 [-22.016, 0.154, 0.000]\n",
      "Epoch 1 [222/340] - Loss: 20.354 [-20.203, 0.151, 0.000]\n",
      "Epoch 1 [223/340] - Loss: 21.935 [-21.784, 0.150, 0.000]\n",
      "Epoch 1 [224/340] - Loss: 23.544 [-23.395, 0.149, 0.000]\n",
      "Epoch 1 [225/340] - Loss: 21.576 [-21.426, 0.149, 0.000]\n",
      "Epoch 1 [226/340] - Loss: 20.419 [-20.269, 0.149, 0.000]\n",
      "Epoch 1 [227/340] - Loss: 20.218 [-20.068, 0.150, 0.000]\n",
      "Epoch 1 [228/340] - Loss: 18.969 [-18.819, 0.150, 0.000]\n",
      "Epoch 1 [229/340] - Loss: 20.248 [-20.099, 0.149, 0.000]\n",
      "Epoch 1 [230/340] - Loss: 20.494 [-20.344, 0.150, 0.000]\n",
      "Epoch 1 [231/340] - Loss: 19.409 [-19.255, 0.153, 0.000]\n",
      "Epoch 1 [232/340] - Loss: 21.669 [-21.512, 0.157, 0.000]\n",
      "Epoch 1 [233/340] - Loss: 20.501 [-20.342, 0.159, 0.000]\n",
      "Epoch 1 [234/340] - Loss: 20.214 [-20.056, 0.159, 0.000]\n",
      "Epoch 1 [235/340] - Loss: 21.282 [-21.124, 0.158, 0.000]\n",
      "Epoch 1 [236/340] - Loss: 21.601 [-21.449, 0.152, 0.000]\n",
      "Epoch 1 [237/340] - Loss: 21.752 [-21.598, 0.154, 0.000]\n",
      "Epoch 1 [238/340] - Loss: 18.177 [-18.031, 0.146, 0.000]\n",
      "Epoch 1 [239/340] - Loss: 18.934 [-18.795, 0.139, 0.000]\n",
      "Epoch 1 [240/340] - Loss: 19.714 [-19.579, 0.134, 0.000]\n",
      "Epoch 1 [241/340] - Loss: 21.482 [-21.351, 0.132, 0.000]\n",
      "Epoch 1 [242/340] - Loss: 20.181 [-20.050, 0.131, 0.000]\n",
      "Epoch 1 [243/340] - Loss: 21.471 [-21.341, 0.130, 0.000]\n",
      "Epoch 1 [244/340] - Loss: 20.567 [-20.437, 0.130, 0.000]\n",
      "Epoch 1 [245/340] - Loss: 20.143 [-20.014, 0.129, 0.000]\n",
      "Epoch 1 [246/340] - Loss: 21.479 [-21.352, 0.127, 0.000]\n",
      "Epoch 1 [247/340] - Loss: 21.772 [-21.647, 0.125, 0.000]\n",
      "Epoch 1 [248/340] - Loss: 23.021 [-22.898, 0.124, 0.000]\n",
      "Epoch 1 [249/340] - Loss: 19.694 [-19.571, 0.123, 0.000]\n",
      "Epoch 1 [250/340] - Loss: 19.226 [-19.106, 0.121, 0.000]\n",
      "Epoch 1 [251/340] - Loss: 19.938 [-19.819, 0.118, 0.000]\n",
      "Epoch 1 [252/340] - Loss: 21.322 [-21.206, 0.116, 0.000]\n",
      "Epoch 1 [253/340] - Loss: 21.067 [-20.953, 0.113, 0.000]\n",
      "Epoch 1 [254/340] - Loss: 19.680 [-19.568, 0.112, 0.000]\n",
      "Epoch 1 [255/340] - Loss: 18.077 [-17.967, 0.110, 0.000]\n",
      "Epoch 1 [256/340] - Loss: 20.103 [-19.994, 0.109, 0.000]\n",
      "Epoch 1 [257/340] - Loss: 19.521 [-19.412, 0.110, 0.000]\n",
      "Epoch 1 [258/340] - Loss: 19.187 [-19.078, 0.109, 0.000]\n",
      "Epoch 1 [259/340] - Loss: 19.729 [-19.621, 0.108, 0.000]\n",
      "Epoch 1 [260/340] - Loss: 18.309 [-18.202, 0.107, 0.000]\n",
      "Epoch 1 [261/340] - Loss: 19.839 [-19.734, 0.105, 0.000]\n",
      "Epoch 1 [262/340] - Loss: 19.066 [-18.963, 0.103, 0.000]\n",
      "Epoch 1 [263/340] - Loss: 21.976 [-21.875, 0.101, 0.000]\n",
      "Epoch 1 [264/340] - Loss: 19.583 [-19.484, 0.099, 0.000]\n",
      "Epoch 1 [265/340] - Loss: 17.588 [-17.490, 0.099, 0.000]\n",
      "Epoch 1 [266/340] - Loss: 17.533 [-17.435, 0.098, 0.000]\n",
      "Epoch 1 [267/340] - Loss: 21.296 [-21.199, 0.097, 0.000]\n",
      "Epoch 1 [268/340] - Loss: 18.748 [-18.653, 0.095, 0.000]\n",
      "Epoch 1 [269/340] - Loss: 19.449 [-19.355, 0.095, 0.000]\n",
      "Epoch 1 [270/340] - Loss: 19.555 [-19.461, 0.094, 0.000]\n",
      "Epoch 1 [271/340] - Loss: 18.333 [-18.240, 0.093, 0.000]\n",
      "Epoch 1 [272/340] - Loss: 20.145 [-20.052, 0.093, 0.000]\n",
      "Epoch 1 [273/340] - Loss: 17.594 [-17.501, 0.093, 0.000]\n",
      "Epoch 1 [274/340] - Loss: 19.466 [-19.373, 0.093, 0.000]\n",
      "Epoch 1 [275/340] - Loss: 18.912 [-18.820, 0.092, 0.000]\n",
      "Epoch 1 [276/340] - Loss: 19.176 [-19.084, 0.092, 0.000]\n",
      "Epoch 1 [277/340] - Loss: 18.489 [-18.398, 0.091, 0.000]\n",
      "Epoch 1 [278/340] - Loss: 16.881 [-16.791, 0.090, 0.000]\n",
      "Epoch 1 [279/340] - Loss: 18.653 [-18.563, 0.089, 0.000]\n",
      "Epoch 1 [280/340] - Loss: 17.263 [-17.174, 0.089, 0.000]\n",
      "Epoch 1 [281/340] - Loss: 17.114 [-17.026, 0.088, 0.000]\n",
      "Epoch 1 [282/340] - Loss: 18.815 [-18.727, 0.088, 0.000]\n",
      "Epoch 1 [283/340] - Loss: 22.364 [-22.276, 0.088, 0.000]\n",
      "Epoch 1 [284/340] - Loss: 17.830 [-17.743, 0.087, 0.000]\n",
      "Epoch 1 [285/340] - Loss: 17.099 [-17.013, 0.086, 0.000]\n",
      "Epoch 1 [286/340] - Loss: 17.721 [-17.637, 0.084, 0.000]\n",
      "Epoch 1 [287/340] - Loss: 19.709 [-19.626, 0.082, 0.000]\n",
      "Epoch 1 [288/340] - Loss: 18.404 [-18.324, 0.080, 0.000]\n",
      "Epoch 1 [289/340] - Loss: 17.550 [-17.472, 0.079, 0.000]\n",
      "Epoch 1 [290/340] - Loss: 18.742 [-18.665, 0.077, 0.000]\n",
      "Epoch 1 [291/340] - Loss: 18.362 [-18.285, 0.077, 0.000]\n",
      "Epoch 1 [292/340] - Loss: 18.052 [-17.975, 0.077, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [293/340] - Loss: 18.354 [-18.277, 0.077, 0.000]\n",
      "Epoch 1 [294/340] - Loss: 18.551 [-18.474, 0.077, 0.000]\n",
      "Epoch 1 [295/340] - Loss: 18.300 [-18.223, 0.077, 0.000]\n",
      "Epoch 1 [296/340] - Loss: 18.642 [-18.566, 0.076, 0.000]\n",
      "Epoch 1 [297/340] - Loss: 18.463 [-18.387, 0.076, 0.000]\n",
      "Epoch 1 [298/340] - Loss: 18.188 [-18.113, 0.075, 0.000]\n",
      "Epoch 1 [299/340] - Loss: 18.245 [-18.171, 0.074, 0.000]\n",
      "Epoch 1 [300/340] - Loss: 18.958 [-18.884, 0.073, 0.000]\n",
      "Epoch 1 [301/340] - Loss: 18.261 [-18.188, 0.073, 0.000]\n",
      "Epoch 1 [302/340] - Loss: 18.895 [-18.821, 0.073, 0.000]\n",
      "Epoch 1 [303/340] - Loss: 17.181 [-17.108, 0.073, 0.000]\n",
      "Epoch 1 [304/340] - Loss: 15.932 [-15.857, 0.074, 0.000]\n",
      "Epoch 1 [305/340] - Loss: 17.802 [-17.729, 0.074, 0.000]\n",
      "Epoch 1 [306/340] - Loss: 17.160 [-17.086, 0.074, 0.000]\n",
      "Epoch 1 [307/340] - Loss: 16.328 [-16.254, 0.074, 0.000]\n",
      "Epoch 1 [308/340] - Loss: 19.513 [-19.438, 0.074, 0.000]\n",
      "Epoch 1 [309/340] - Loss: 17.287 [-17.213, 0.074, 0.000]\n",
      "Epoch 1 [310/340] - Loss: 19.242 [-19.169, 0.073, 0.000]\n",
      "Epoch 1 [311/340] - Loss: 19.569 [-19.498, 0.072, 0.000]\n",
      "Epoch 1 [312/340] - Loss: 16.670 [-16.600, 0.070, 0.000]\n",
      "Epoch 1 [313/340] - Loss: 21.097 [-21.028, 0.069, 0.000]\n",
      "Epoch 1 [314/340] - Loss: 18.315 [-18.247, 0.069, 0.000]\n",
      "Epoch 1 [315/340] - Loss: 18.711 [-18.642, 0.069, 0.000]\n",
      "Epoch 1 [316/340] - Loss: 19.230 [-19.161, 0.069, 0.000]\n",
      "Epoch 1 [317/340] - Loss: 19.373 [-19.304, 0.069, 0.000]\n",
      "Epoch 1 [318/340] - Loss: 18.428 [-18.358, 0.070, 0.000]\n",
      "Epoch 1 [319/340] - Loss: 17.838 [-17.768, 0.070, 0.000]\n",
      "Epoch 1 [320/340] - Loss: 18.167 [-18.097, 0.071, 0.000]\n",
      "Epoch 1 [321/340] - Loss: 20.450 [-20.380, 0.071, 0.000]\n",
      "Epoch 1 [322/340] - Loss: 16.812 [-16.741, 0.070, 0.000]\n",
      "Epoch 1 [323/340] - Loss: 18.377 [-18.307, 0.070, 0.000]\n",
      "Epoch 1 [324/340] - Loss: 17.245 [-17.176, 0.070, 0.000]\n",
      "Epoch 1 [325/340] - Loss: 18.629 [-18.559, 0.069, 0.000]\n",
      "Epoch 1 [326/340] - Loss: 19.264 [-19.195, 0.069, 0.000]\n",
      "Epoch 1 [327/340] - Loss: 17.514 [-17.445, 0.069, 0.000]\n",
      "Epoch 1 [328/340] - Loss: 19.007 [-18.938, 0.069, 0.000]\n",
      "Epoch 1 [329/340] - Loss: 17.298 [-17.229, 0.069, 0.000]\n",
      "Epoch 1 [330/340] - Loss: 17.520 [-17.450, 0.070, 0.000]\n",
      "Epoch 1 [331/340] - Loss: 19.822 [-19.753, 0.068, 0.000]\n",
      "Epoch 1 [332/340] - Loss: 17.065 [-16.997, 0.068, 0.000]\n",
      "Epoch 1 [333/340] - Loss: 17.862 [-17.795, 0.067, 0.000]\n",
      "Epoch 1 [334/340] - Loss: 16.223 [-16.156, 0.067, 0.000]\n",
      "Epoch 1 [335/340] - Loss: 17.280 [-17.214, 0.066, 0.000]\n",
      "Epoch 1 [336/340] - Loss: 19.093 [-19.027, 0.066, 0.000]\n",
      "Epoch 1 [337/340] - Loss: 16.916 [-16.852, 0.064, 0.000]\n",
      "Epoch 1 [338/340] - Loss: 18.213 [-18.149, 0.063, 0.000]\n",
      "Epoch 1 [339/340] - Loss: 16.476 [-16.414, 0.063, 0.000]\n",
      "Epoch 2 [0/340] - Loss: 16.930 [-16.868, 0.063, 0.000]\n",
      "Epoch 2 [1/340] - Loss: 18.293 [-18.232, 0.062, 0.000]\n",
      "Epoch 2 [2/340] - Loss: 16.259 [-16.198, 0.061, 0.000]\n",
      "Epoch 2 [3/340] - Loss: 22.712 [-22.652, 0.060, 0.000]\n",
      "Epoch 2 [4/340] - Loss: 16.194 [-16.135, 0.059, 0.000]\n",
      "Epoch 2 [5/340] - Loss: 18.976 [-18.918, 0.058, 0.000]\n",
      "Epoch 2 [6/340] - Loss: 17.535 [-17.477, 0.057, 0.000]\n",
      "Epoch 2 [7/340] - Loss: 18.298 [-18.242, 0.056, 0.000]\n",
      "Epoch 2 [8/340] - Loss: 17.134 [-17.078, 0.056, 0.000]\n",
      "Epoch 2 [9/340] - Loss: 15.719 [-15.664, 0.055, 0.000]\n",
      "Epoch 2 [10/340] - Loss: 18.255 [-18.201, 0.055, 0.000]\n",
      "Epoch 2 [11/340] - Loss: 18.056 [-18.001, 0.054, 0.000]\n",
      "Epoch 2 [12/340] - Loss: 18.504 [-18.449, 0.055, 0.000]\n",
      "Epoch 2 [13/340] - Loss: 16.484 [-16.429, 0.055, 0.000]\n",
      "Epoch 2 [14/340] - Loss: 17.530 [-17.475, 0.055, 0.000]\n",
      "Epoch 2 [15/340] - Loss: 16.778 [-16.722, 0.056, 0.000]\n",
      "Epoch 2 [16/340] - Loss: 16.351 [-16.295, 0.056, 0.000]\n",
      "Epoch 2 [17/340] - Loss: 16.948 [-16.891, 0.057, 0.000]\n",
      "Epoch 2 [18/340] - Loss: 17.245 [-17.188, 0.057, 0.000]\n",
      "Epoch 2 [19/340] - Loss: 15.069 [-15.013, 0.056, 0.000]\n",
      "Epoch 2 [20/340] - Loss: 16.064 [-16.009, 0.055, 0.000]\n",
      "Epoch 2 [21/340] - Loss: 17.672 [-17.618, 0.054, 0.000]\n",
      "Epoch 2 [22/340] - Loss: 18.328 [-18.275, 0.052, 0.000]\n",
      "Epoch 2 [23/340] - Loss: 19.388 [-19.337, 0.052, 0.000]\n",
      "Epoch 2 [24/340] - Loss: 17.033 [-16.982, 0.051, 0.000]\n",
      "Epoch 2 [25/340] - Loss: 16.985 [-16.935, 0.051, 0.000]\n",
      "Epoch 2 [26/340] - Loss: 17.289 [-17.238, 0.051, 0.000]\n",
      "Epoch 2 [27/340] - Loss: 16.482 [-16.431, 0.051, 0.000]\n",
      "Epoch 2 [28/340] - Loss: 17.070 [-17.020, 0.051, 0.000]\n",
      "Epoch 2 [29/340] - Loss: 17.529 [-17.478, 0.051, 0.000]\n",
      "Epoch 2 [30/340] - Loss: 16.216 [-16.165, 0.051, 0.000]\n",
      "Epoch 2 [31/340] - Loss: 16.895 [-16.843, 0.051, 0.000]\n",
      "Epoch 2 [32/340] - Loss: 15.886 [-15.835, 0.051, 0.000]\n",
      "Epoch 2 [33/340] - Loss: 16.196 [-16.144, 0.051, 0.000]\n",
      "Epoch 2 [34/340] - Loss: 16.832 [-16.780, 0.052, 0.000]\n",
      "Epoch 2 [35/340] - Loss: 16.753 [-16.701, 0.052, 0.000]\n",
      "Epoch 2 [36/340] - Loss: 17.554 [-17.503, 0.051, 0.000]\n",
      "Epoch 2 [37/340] - Loss: 16.752 [-16.700, 0.051, 0.000]\n",
      "Epoch 2 [38/340] - Loss: 17.882 [-17.831, 0.050, 0.000]\n",
      "Epoch 2 [39/340] - Loss: 15.910 [-15.860, 0.050, 0.000]\n",
      "Epoch 2 [40/340] - Loss: 16.725 [-16.676, 0.050, 0.000]\n",
      "Epoch 2 [41/340] - Loss: 19.023 [-18.973, 0.050, 0.000]\n",
      "Epoch 2 [42/340] - Loss: 15.761 [-15.712, 0.050, 0.000]\n",
      "Epoch 2 [43/340] - Loss: 16.164 [-16.115, 0.049, 0.000]\n",
      "Epoch 2 [44/340] - Loss: 18.419 [-18.370, 0.049, 0.000]\n",
      "Epoch 2 [45/340] - Loss: 16.228 [-16.180, 0.049, 0.000]\n",
      "Epoch 2 [46/340] - Loss: 17.340 [-17.292, 0.048, 0.000]\n",
      "Epoch 2 [47/340] - Loss: 16.881 [-16.833, 0.047, 0.000]\n",
      "Epoch 2 [48/340] - Loss: 17.409 [-17.362, 0.047, 0.000]\n",
      "Epoch 2 [49/340] - Loss: 16.383 [-16.337, 0.047, 0.000]\n",
      "Epoch 2 [50/340] - Loss: 17.882 [-17.835, 0.047, 0.000]\n",
      "Epoch 2 [51/340] - Loss: 16.987 [-16.941, 0.046, 0.000]\n",
      "Epoch 2 [52/340] - Loss: 16.086 [-16.039, 0.046, 0.000]\n",
      "Epoch 2 [53/340] - Loss: 16.526 [-16.479, 0.046, 0.000]\n",
      "Epoch 2 [54/340] - Loss: 15.949 [-15.903, 0.046, 0.000]\n",
      "Epoch 2 [55/340] - Loss: 17.011 [-16.965, 0.046, 0.000]\n",
      "Epoch 2 [56/340] - Loss: 17.192 [-17.146, 0.046, 0.000]\n",
      "Epoch 2 [57/340] - Loss: 17.030 [-16.983, 0.046, 0.000]\n",
      "Epoch 2 [58/340] - Loss: 17.855 [-17.810, 0.046, 0.000]\n",
      "Epoch 2 [59/340] - Loss: 18.572 [-18.526, 0.046, 0.000]\n",
      "Epoch 2 [60/340] - Loss: 17.086 [-17.040, 0.046, 0.000]\n",
      "Epoch 2 [61/340] - Loss: 16.825 [-16.778, 0.046, 0.000]\n",
      "Epoch 2 [62/340] - Loss: 15.381 [-15.334, 0.046, 0.000]\n",
      "Epoch 2 [63/340] - Loss: 17.219 [-17.173, 0.046, 0.000]\n",
      "Epoch 2 [64/340] - Loss: 18.788 [-18.742, 0.046, 0.000]\n",
      "Epoch 2 [65/340] - Loss: 17.177 [-17.130, 0.047, 0.000]\n",
      "Epoch 2 [66/340] - Loss: 18.762 [-18.709, 0.053, 0.000]\n",
      "Epoch 2 [67/340] - Loss: 20.401 [-20.341, 0.060, 0.000]\n",
      "Epoch 2 [68/340] - Loss: 24.155 [-24.090, 0.065, 0.000]\n",
      "Epoch 2 [69/340] - Loss: 21.117 [-21.048, 0.069, 0.000]\n",
      "Epoch 2 [70/340] - Loss: 20.701 [-20.622, 0.079, 0.000]\n",
      "Epoch 2 [71/340] - Loss: 21.350 [-21.288, 0.061, 0.000]\n",
      "Epoch 2 [72/340] - Loss: 19.571 [-19.519, 0.051, 0.000]\n",
      "Epoch 2 [73/340] - Loss: 19.812 [-19.762, 0.050, 0.000]\n",
      "Epoch 2 [74/340] - Loss: 19.188 [-19.137, 0.051, 0.000]\n",
      "Epoch 2 [75/340] - Loss: 20.786 [-20.728, 0.058, 0.000]\n",
      "Epoch 2 [76/340] - Loss: 21.731 [-21.669, 0.062, 0.000]\n",
      "Epoch 2 [77/340] - Loss: 20.367 [-20.306, 0.062, 0.000]\n",
      "Epoch 2 [78/340] - Loss: 21.902 [-21.837, 0.065, 0.000]\n",
      "Epoch 2 [79/340] - Loss: 21.224 [-21.160, 0.064, 0.000]\n",
      "Epoch 2 [80/340] - Loss: 23.617 [-23.554, 0.063, 0.000]\n",
      "Epoch 2 [81/340] - Loss: 27.667 [-27.598, 0.069, 0.000]\n",
      "Epoch 2 [82/340] - Loss: 25.832 [-25.763, 0.069, 0.000]\n",
      "Epoch 2 [83/340] - Loss: 21.403 [-21.330, 0.073, 0.000]\n",
      "Epoch 2 [84/340] - Loss: 26.163 [-26.091, 0.071, 0.000]\n",
      "Epoch 2 [85/340] - Loss: 26.628 [-26.558, 0.070, 0.000]\n",
      "Epoch 2 [86/340] - Loss: 25.188 [-25.113, 0.075, 0.000]\n",
      "Epoch 2 [87/340] - Loss: 28.998 [-28.920, 0.078, 0.000]\n",
      "Epoch 2 [88/340] - Loss: 31.920 [-31.836, 0.084, 0.000]\n",
      "Epoch 2 [89/340] - Loss: 24.257 [-24.166, 0.091, 0.000]\n",
      "Epoch 2 [90/340] - Loss: 23.933 [-23.841, 0.093, 0.000]\n",
      "Epoch 2 [91/340] - Loss: 21.983 [-21.894, 0.088, 0.000]\n",
      "Epoch 2 [92/340] - Loss: 22.502 [-22.415, 0.087, 0.000]\n",
      "Epoch 2 [93/340] - Loss: 22.395 [-22.308, 0.086, 0.000]\n",
      "Epoch 2 [94/340] - Loss: 23.506 [-23.420, 0.086, 0.000]\n",
      "Epoch 2 [95/340] - Loss: 17.384 [-17.296, 0.088, 0.000]\n",
      "Epoch 2 [96/340] - Loss: 19.226 [-19.135, 0.091, 0.000]\n",
      "Epoch 2 [97/340] - Loss: 19.646 [-19.551, 0.095, 0.000]\n",
      "Epoch 2 [98/340] - Loss: 20.218 [-20.118, 0.100, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [99/340] - Loss: 20.669 [-20.561, 0.109, 0.000]\n",
      "Epoch 2 [100/340] - Loss: 19.873 [-19.755, 0.117, 0.000]\n",
      "Epoch 2 [101/340] - Loss: 19.939 [-19.811, 0.128, 0.000]\n",
      "Epoch 2 [102/340] - Loss: 19.191 [-19.062, 0.128, 0.000]\n",
      "Epoch 2 [103/340] - Loss: 19.372 [-19.250, 0.123, 0.000]\n",
      "Epoch 2 [104/340] - Loss: 19.072 [-18.948, 0.124, 0.000]\n",
      "Epoch 2 [105/340] - Loss: 20.560 [-20.438, 0.122, 0.000]\n",
      "Epoch 2 [106/340] - Loss: 19.183 [-19.059, 0.123, 0.000]\n",
      "Epoch 2 [107/340] - Loss: 18.336 [-18.231, 0.105, 0.000]\n",
      "Epoch 2 [108/340] - Loss: 19.943 [-19.843, 0.100, 0.000]\n",
      "Epoch 2 [109/340] - Loss: 21.090 [-20.994, 0.096, 0.000]\n",
      "Epoch 2 [110/340] - Loss: 19.748 [-19.654, 0.094, 0.000]\n",
      "Epoch 2 [111/340] - Loss: 19.492 [-19.397, 0.095, 0.000]\n",
      "Epoch 2 [112/340] - Loss: 21.237 [-21.142, 0.096, 0.000]\n",
      "Epoch 2 [113/340] - Loss: 20.960 [-20.862, 0.098, 0.000]\n",
      "Epoch 2 [114/340] - Loss: 22.070 [-21.972, 0.098, 0.000]\n",
      "Epoch 2 [115/340] - Loss: 19.819 [-19.719, 0.100, 0.000]\n",
      "Epoch 2 [116/340] - Loss: 20.464 [-20.361, 0.103, 0.000]\n",
      "Epoch 2 [117/340] - Loss: 18.282 [-18.175, 0.107, 0.000]\n",
      "Epoch 2 [118/340] - Loss: 21.250 [-21.142, 0.108, 0.000]\n",
      "Epoch 2 [119/340] - Loss: 19.595 [-19.485, 0.110, 0.000]\n",
      "Epoch 2 [120/340] - Loss: 18.522 [-18.414, 0.108, 0.000]\n",
      "Epoch 2 [121/340] - Loss: 19.493 [-19.386, 0.108, 0.000]\n",
      "Epoch 2 [122/340] - Loss: 18.687 [-18.581, 0.106, 0.000]\n",
      "Epoch 2 [123/340] - Loss: 20.835 [-20.728, 0.108, 0.000]\n",
      "Epoch 2 [124/340] - Loss: 20.052 [-19.944, 0.108, 0.000]\n",
      "Epoch 2 [125/340] - Loss: 19.315 [-19.205, 0.110, 0.000]\n",
      "Epoch 2 [126/340] - Loss: 18.916 [-18.807, 0.109, 0.000]\n",
      "Epoch 2 [127/340] - Loss: 17.622 [-17.515, 0.107, 0.000]\n",
      "Epoch 2 [128/340] - Loss: 17.882 [-17.775, 0.107, 0.000]\n",
      "Epoch 2 [129/340] - Loss: 20.332 [-20.225, 0.108, 0.000]\n",
      "Epoch 2 [130/340] - Loss: 20.342 [-20.235, 0.107, 0.000]\n",
      "Epoch 2 [131/340] - Loss: 19.874 [-19.769, 0.105, 0.000]\n",
      "Epoch 2 [132/340] - Loss: 18.219 [-18.117, 0.102, 0.000]\n",
      "Epoch 2 [133/340] - Loss: 19.260 [-19.163, 0.097, 0.000]\n",
      "Epoch 2 [134/340] - Loss: 18.916 [-18.822, 0.094, 0.000]\n",
      "Epoch 2 [135/340] - Loss: 18.441 [-18.348, 0.092, 0.000]\n",
      "Epoch 2 [136/340] - Loss: 19.065 [-18.973, 0.092, 0.000]\n",
      "Epoch 2 [137/340] - Loss: 17.982 [-17.890, 0.092, 0.000]\n",
      "Epoch 2 [138/340] - Loss: 19.435 [-19.341, 0.093, 0.000]\n",
      "Epoch 2 [139/340] - Loss: 17.723 [-17.628, 0.095, 0.000]\n",
      "Epoch 2 [140/340] - Loss: 18.502 [-18.405, 0.097, 0.000]\n",
      "Epoch 2 [141/340] - Loss: 18.067 [-17.969, 0.097, 0.000]\n",
      "Epoch 2 [142/340] - Loss: 18.032 [-17.934, 0.098, 0.000]\n",
      "Epoch 2 [143/340] - Loss: 18.380 [-18.282, 0.098, 0.000]\n",
      "Epoch 2 [144/340] - Loss: 19.177 [-19.079, 0.097, 0.000]\n",
      "Epoch 2 [145/340] - Loss: 20.436 [-20.340, 0.096, 0.000]\n",
      "Epoch 2 [146/340] - Loss: 18.141 [-18.048, 0.092, 0.000]\n",
      "Epoch 2 [147/340] - Loss: 19.333 [-19.244, 0.088, 0.000]\n",
      "Epoch 2 [148/340] - Loss: 17.607 [-17.522, 0.085, 0.000]\n",
      "Epoch 2 [149/340] - Loss: 19.264 [-19.179, 0.085, 0.000]\n",
      "Epoch 2 [150/340] - Loss: 17.419 [-17.337, 0.081, 0.000]\n",
      "Epoch 2 [151/340] - Loss: 20.504 [-20.424, 0.080, 0.000]\n",
      "Epoch 2 [152/340] - Loss: 17.714 [-17.636, 0.078, 0.000]\n",
      "Epoch 2 [153/340] - Loss: 18.742 [-18.666, 0.076, 0.000]\n",
      "Epoch 2 [154/340] - Loss: 17.439 [-17.365, 0.074, 0.000]\n",
      "Epoch 2 [155/340] - Loss: 20.549 [-20.477, 0.073, 0.000]\n",
      "Epoch 2 [156/340] - Loss: 18.355 [-18.284, 0.071, 0.000]\n",
      "Epoch 2 [157/340] - Loss: 18.043 [-17.972, 0.070, 0.000]\n",
      "Epoch 2 [158/340] - Loss: 18.452 [-18.382, 0.069, 0.000]\n",
      "Epoch 2 [159/340] - Loss: 18.087 [-18.017, 0.070, 0.000]\n",
      "Epoch 2 [160/340] - Loss: 18.554 [-18.484, 0.071, 0.000]\n",
      "Epoch 2 [161/340] - Loss: 19.828 [-19.755, 0.073, 0.000]\n",
      "Epoch 2 [162/340] - Loss: 18.804 [-18.728, 0.075, 0.000]\n",
      "Epoch 2 [163/340] - Loss: 18.607 [-18.529, 0.078, 0.000]\n",
      "Epoch 2 [164/340] - Loss: 18.465 [-18.385, 0.081, 0.000]\n",
      "Epoch 2 [165/340] - Loss: 18.557 [-18.474, 0.083, 0.000]\n",
      "Epoch 2 [166/340] - Loss: 19.321 [-19.237, 0.084, 0.000]\n",
      "Epoch 2 [167/340] - Loss: 18.430 [-18.346, 0.084, 0.000]\n",
      "Epoch 2 [168/340] - Loss: 19.910 [-19.826, 0.084, 0.000]\n",
      "Epoch 2 [169/340] - Loss: 17.320 [-17.235, 0.085, 0.000]\n",
      "Epoch 2 [170/340] - Loss: 17.750 [-17.663, 0.087, 0.000]\n",
      "Epoch 2 [171/340] - Loss: 17.577 [-17.488, 0.089, 0.000]\n",
      "Epoch 2 [172/340] - Loss: 18.093 [-18.001, 0.091, 0.000]\n",
      "Epoch 2 [173/340] - Loss: 17.707 [-17.615, 0.092, 0.000]\n",
      "Epoch 2 [174/340] - Loss: 16.909 [-16.816, 0.093, 0.000]\n",
      "Epoch 2 [175/340] - Loss: 18.888 [-18.798, 0.090, 0.000]\n",
      "Epoch 2 [176/340] - Loss: 18.590 [-18.501, 0.089, 0.000]\n",
      "Epoch 2 [177/340] - Loss: 17.810 [-17.723, 0.086, 0.000]\n",
      "Epoch 2 [178/340] - Loss: 16.964 [-16.880, 0.084, 0.000]\n",
      "Epoch 2 [179/340] - Loss: 18.567 [-18.485, 0.083, 0.000]\n",
      "Epoch 2 [180/340] - Loss: 16.425 [-16.342, 0.083, 0.000]\n",
      "Epoch 2 [181/340] - Loss: 19.418 [-19.335, 0.084, 0.000]\n",
      "Epoch 2 [182/340] - Loss: 16.944 [-16.859, 0.086, 0.000]\n",
      "Epoch 2 [183/340] - Loss: 19.558 [-19.471, 0.087, 0.000]\n",
      "Epoch 2 [184/340] - Loss: 16.767 [-16.679, 0.088, 0.000]\n",
      "Epoch 2 [185/340] - Loss: 19.216 [-19.128, 0.088, 0.000]\n",
      "Epoch 2 [186/340] - Loss: 17.029 [-16.940, 0.089, 0.000]\n",
      "Epoch 2 [187/340] - Loss: 17.156 [-17.068, 0.088, 0.000]\n",
      "Epoch 2 [188/340] - Loss: 18.389 [-18.305, 0.085, 0.000]\n",
      "Epoch 2 [189/340] - Loss: 18.611 [-18.528, 0.082, 0.000]\n",
      "Epoch 2 [190/340] - Loss: 17.315 [-17.234, 0.081, 0.000]\n",
      "Epoch 2 [191/340] - Loss: 17.052 [-16.972, 0.081, 0.000]\n",
      "Epoch 2 [192/340] - Loss: 15.895 [-15.813, 0.082, 0.000]\n",
      "Epoch 2 [193/340] - Loss: 16.885 [-16.800, 0.085, 0.000]\n",
      "Epoch 2 [194/340] - Loss: 20.892 [-20.807, 0.085, 0.000]\n",
      "Epoch 2 [195/340] - Loss: 17.041 [-16.956, 0.085, 0.000]\n",
      "Epoch 2 [196/340] - Loss: 19.533 [-19.449, 0.084, 0.000]\n",
      "Epoch 2 [197/340] - Loss: 18.601 [-18.520, 0.081, 0.000]\n",
      "Epoch 2 [198/340] - Loss: 18.972 [-18.892, 0.080, 0.000]\n",
      "Epoch 2 [199/340] - Loss: 19.883 [-19.801, 0.082, 0.000]\n",
      "Epoch 2 [200/340] - Loss: 19.526 [-19.439, 0.087, 0.000]\n",
      "Epoch 2 [201/340] - Loss: 16.281 [-16.200, 0.080, 0.000]\n",
      "Epoch 2 [202/340] - Loss: 19.578 [-19.503, 0.076, 0.000]\n",
      "Epoch 2 [203/340] - Loss: 16.820 [-16.748, 0.072, 0.000]\n",
      "Epoch 2 [204/340] - Loss: 17.004 [-16.934, 0.070, 0.000]\n",
      "Epoch 2 [205/340] - Loss: 16.098 [-16.028, 0.070, 0.000]\n",
      "Epoch 2 [206/340] - Loss: 17.157 [-17.087, 0.070, 0.000]\n",
      "Epoch 2 [207/340] - Loss: 17.622 [-17.552, 0.071, 0.000]\n",
      "Epoch 2 [208/340] - Loss: 17.759 [-17.686, 0.072, 0.000]\n",
      "Epoch 2 [209/340] - Loss: 18.104 [-18.030, 0.074, 0.000]\n",
      "Epoch 2 [210/340] - Loss: 15.859 [-15.783, 0.076, 0.000]\n",
      "Epoch 2 [211/340] - Loss: 16.231 [-16.154, 0.077, 0.000]\n",
      "Epoch 2 [212/340] - Loss: 16.136 [-16.058, 0.077, 0.000]\n",
      "Epoch 2 [213/340] - Loss: 15.895 [-15.818, 0.077, 0.000]\n",
      "Epoch 2 [214/340] - Loss: 16.171 [-16.095, 0.076, 0.000]\n",
      "Epoch 2 [215/340] - Loss: 16.605 [-16.529, 0.076, 0.000]\n",
      "Epoch 2 [216/340] - Loss: 16.452 [-16.377, 0.076, 0.000]\n",
      "Epoch 2 [217/340] - Loss: 16.937 [-16.862, 0.076, 0.000]\n",
      "Epoch 2 [218/340] - Loss: 16.790 [-16.714, 0.076, 0.000]\n",
      "Epoch 2 [219/340] - Loss: 16.550 [-16.474, 0.076, 0.000]\n",
      "Epoch 2 [220/340] - Loss: 16.378 [-16.301, 0.077, 0.000]\n",
      "Epoch 2 [221/340] - Loss: 17.245 [-17.167, 0.078, 0.000]\n",
      "Epoch 2 [222/340] - Loss: 17.355 [-17.278, 0.077, 0.000]\n",
      "Epoch 2 [223/340] - Loss: 18.402 [-18.326, 0.076, 0.000]\n",
      "Epoch 2 [224/340] - Loss: 18.427 [-18.352, 0.075, 0.000]\n",
      "Epoch 2 [225/340] - Loss: 17.544 [-17.472, 0.072, 0.000]\n",
      "Epoch 2 [226/340] - Loss: 17.137 [-17.067, 0.070, 0.000]\n",
      "Epoch 2 [227/340] - Loss: 18.913 [-18.845, 0.068, 0.000]\n",
      "Epoch 2 [228/340] - Loss: 18.011 [-17.943, 0.068, 0.000]\n",
      "Epoch 2 [229/340] - Loss: 15.409 [-15.340, 0.069, 0.000]\n",
      "Epoch 2 [230/340] - Loss: 15.951 [-15.881, 0.070, 0.000]\n",
      "Epoch 2 [231/340] - Loss: 16.520 [-16.450, 0.070, 0.000]\n",
      "Epoch 2 [232/340] - Loss: 16.588 [-16.519, 0.069, 0.000]\n",
      "Epoch 2 [233/340] - Loss: 16.253 [-16.185, 0.069, 0.000]\n",
      "Epoch 2 [234/340] - Loss: 15.735 [-15.667, 0.067, 0.000]\n",
      "Epoch 2 [235/340] - Loss: 15.606 [-15.538, 0.068, 0.000]\n",
      "Epoch 2 [236/340] - Loss: 15.526 [-15.458, 0.068, 0.000]\n",
      "Epoch 2 [237/340] - Loss: 16.532 [-16.464, 0.068, 0.000]\n",
      "Epoch 2 [238/340] - Loss: 15.666 [-15.596, 0.070, 0.000]\n",
      "Epoch 2 [239/340] - Loss: 16.353 [-16.284, 0.070, 0.000]\n",
      "Epoch 2 [240/340] - Loss: 16.575 [-16.506, 0.069, 0.000]\n",
      "Epoch 2 [241/340] - Loss: 15.746 [-15.677, 0.070, 0.000]\n",
      "Epoch 2 [242/340] - Loss: 16.049 [-15.980, 0.068, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [243/340] - Loss: 15.820 [-15.754, 0.066, 0.000]\n",
      "Epoch 2 [244/340] - Loss: 16.167 [-16.103, 0.064, 0.000]\n",
      "Epoch 2 [245/340] - Loss: 15.611 [-15.549, 0.061, 0.000]\n",
      "Epoch 2 [246/340] - Loss: 16.774 [-16.714, 0.060, 0.000]\n",
      "Epoch 2 [247/340] - Loss: 15.938 [-15.880, 0.058, 0.000]\n",
      "Epoch 2 [248/340] - Loss: 15.428 [-15.372, 0.056, 0.000]\n",
      "Epoch 2 [249/340] - Loss: 15.548 [-15.493, 0.055, 0.000]\n",
      "Epoch 2 [250/340] - Loss: 14.927 [-14.873, 0.054, 0.000]\n",
      "Epoch 2 [251/340] - Loss: 17.893 [-17.840, 0.053, 0.000]\n",
      "Epoch 2 [252/340] - Loss: 16.159 [-16.106, 0.053, 0.000]\n",
      "Epoch 2 [253/340] - Loss: 14.125 [-14.072, 0.053, 0.000]\n",
      "Epoch 2 [254/340] - Loss: 14.751 [-14.699, 0.053, 0.000]\n",
      "Epoch 2 [255/340] - Loss: 14.690 [-14.638, 0.052, 0.000]\n",
      "Epoch 2 [256/340] - Loss: 16.026 [-15.974, 0.052, 0.000]\n",
      "Epoch 2 [257/340] - Loss: 14.982 [-14.930, 0.052, 0.000]\n",
      "Epoch 2 [258/340] - Loss: 16.640 [-16.587, 0.053, 0.000]\n",
      "Epoch 2 [259/340] - Loss: 14.597 [-14.544, 0.053, 0.000]\n",
      "Epoch 2 [260/340] - Loss: 15.433 [-15.378, 0.054, 0.000]\n",
      "Epoch 2 [261/340] - Loss: 15.480 [-15.424, 0.056, 0.000]\n",
      "Epoch 2 [262/340] - Loss: 14.625 [-14.572, 0.053, 0.000]\n",
      "Epoch 2 [263/340] - Loss: 15.455 [-15.403, 0.052, 0.000]\n",
      "Epoch 2 [264/340] - Loss: 16.266 [-16.215, 0.051, 0.000]\n",
      "Epoch 2 [265/340] - Loss: 17.052 [-17.001, 0.051, 0.000]\n",
      "Epoch 2 [266/340] - Loss: 15.780 [-15.729, 0.051, 0.000]\n",
      "Epoch 2 [267/340] - Loss: 14.802 [-14.751, 0.051, 0.000]\n",
      "Epoch 2 [268/340] - Loss: 14.712 [-14.660, 0.052, 0.000]\n",
      "Epoch 2 [269/340] - Loss: 16.323 [-16.270, 0.053, 0.000]\n",
      "Epoch 2 [270/340] - Loss: 14.761 [-14.707, 0.053, 0.000]\n",
      "Epoch 2 [271/340] - Loss: 15.060 [-15.008, 0.051, 0.000]\n",
      "Epoch 2 [272/340] - Loss: 15.768 [-15.718, 0.051, 0.000]\n",
      "Epoch 2 [273/340] - Loss: 15.225 [-15.174, 0.051, 0.000]\n",
      "Epoch 2 [274/340] - Loss: 15.058 [-15.007, 0.051, 0.000]\n",
      "Epoch 2 [275/340] - Loss: 17.075 [-17.024, 0.050, 0.000]\n",
      "Epoch 2 [276/340] - Loss: 15.948 [-15.898, 0.050, 0.000]\n",
      "Epoch 2 [277/340] - Loss: 15.967 [-15.918, 0.048, 0.000]\n",
      "Epoch 2 [278/340] - Loss: 15.624 [-15.576, 0.048, 0.000]\n",
      "Epoch 2 [279/340] - Loss: 16.225 [-16.177, 0.048, 0.000]\n",
      "Epoch 2 [280/340] - Loss: 14.569 [-14.521, 0.048, 0.000]\n",
      "Epoch 2 [281/340] - Loss: 14.836 [-14.787, 0.049, 0.000]\n",
      "Epoch 2 [282/340] - Loss: 15.751 [-15.702, 0.048, 0.000]\n",
      "Epoch 2 [283/340] - Loss: 15.868 [-15.821, 0.047, 0.000]\n",
      "Epoch 2 [284/340] - Loss: 14.763 [-14.716, 0.047, 0.000]\n",
      "Epoch 2 [285/340] - Loss: 15.010 [-14.962, 0.048, 0.000]\n",
      "Epoch 2 [286/340] - Loss: 17.463 [-17.416, 0.047, 0.000]\n",
      "Epoch 2 [287/340] - Loss: 15.326 [-15.280, 0.045, 0.000]\n",
      "Epoch 2 [288/340] - Loss: 15.697 [-15.651, 0.045, 0.000]\n",
      "Epoch 2 [289/340] - Loss: 16.722 [-16.677, 0.045, 0.000]\n",
      "Epoch 2 [290/340] - Loss: 14.763 [-14.717, 0.046, 0.000]\n",
      "Epoch 2 [291/340] - Loss: 16.472 [-16.425, 0.047, 0.000]\n",
      "Epoch 2 [292/340] - Loss: 16.681 [-16.635, 0.046, 0.000]\n",
      "Epoch 2 [293/340] - Loss: 15.581 [-15.536, 0.045, 0.000]\n",
      "Epoch 2 [294/340] - Loss: 16.169 [-16.124, 0.044, 0.000]\n",
      "Epoch 2 [295/340] - Loss: 18.835 [-18.791, 0.043, 0.000]\n",
      "Epoch 2 [296/340] - Loss: 14.909 [-14.866, 0.043, 0.000]\n",
      "Epoch 2 [297/340] - Loss: 15.630 [-15.587, 0.043, 0.000]\n",
      "Epoch 2 [298/340] - Loss: 14.823 [-14.780, 0.043, 0.000]\n",
      "Epoch 2 [299/340] - Loss: 14.719 [-14.675, 0.044, 0.000]\n",
      "Epoch 2 [300/340] - Loss: 14.329 [-14.285, 0.044, 0.000]\n",
      "Epoch 2 [301/340] - Loss: 15.696 [-15.652, 0.045, 0.000]\n",
      "Epoch 2 [302/340] - Loss: 14.571 [-14.526, 0.046, 0.000]\n",
      "Epoch 2 [303/340] - Loss: 15.952 [-15.906, 0.046, 0.000]\n",
      "Epoch 2 [304/340] - Loss: 14.031 [-13.985, 0.046, 0.000]\n",
      "Epoch 2 [305/340] - Loss: 14.611 [-14.565, 0.046, 0.000]\n",
      "Epoch 2 [306/340] - Loss: 16.726 [-16.681, 0.045, 0.000]\n",
      "Epoch 2 [307/340] - Loss: 15.500 [-15.456, 0.044, 0.000]\n",
      "Epoch 2 [308/340] - Loss: 15.876 [-15.834, 0.041, 0.000]\n",
      "Epoch 2 [309/340] - Loss: 16.597 [-16.558, 0.039, 0.000]\n",
      "Epoch 2 [310/340] - Loss: 15.941 [-15.903, 0.038, 0.000]\n",
      "Epoch 2 [311/340] - Loss: 15.912 [-15.875, 0.037, 0.000]\n",
      "Epoch 2 [312/340] - Loss: 15.832 [-15.795, 0.037, 0.000]\n",
      "Epoch 2 [313/340] - Loss: 14.848 [-14.811, 0.037, 0.000]\n",
      "Epoch 2 [314/340] - Loss: 16.916 [-16.878, 0.038, 0.000]\n",
      "Epoch 2 [315/340] - Loss: 14.985 [-14.946, 0.039, 0.000]\n",
      "Epoch 2 [316/340] - Loss: 15.208 [-15.169, 0.040, 0.000]\n",
      "Epoch 2 [317/340] - Loss: 15.323 [-15.283, 0.040, 0.000]\n",
      "Epoch 2 [318/340] - Loss: 14.271 [-14.231, 0.040, 0.000]\n",
      "Epoch 2 [319/340] - Loss: 14.924 [-14.885, 0.040, 0.000]\n",
      "Epoch 2 [320/340] - Loss: 16.495 [-16.455, 0.040, 0.000]\n",
      "Epoch 2 [321/340] - Loss: 15.628 [-15.588, 0.040, 0.000]\n",
      "Epoch 2 [322/340] - Loss: 14.684 [-14.643, 0.041, 0.000]\n",
      "Epoch 2 [323/340] - Loss: 14.671 [-14.629, 0.042, 0.000]\n",
      "Epoch 2 [324/340] - Loss: 14.479 [-14.437, 0.042, 0.000]\n",
      "Epoch 2 [325/340] - Loss: 13.908 [-13.866, 0.043, 0.000]\n",
      "Epoch 2 [326/340] - Loss: 14.544 [-14.501, 0.044, 0.000]\n",
      "Epoch 2 [327/340] - Loss: 15.354 [-15.312, 0.042, 0.000]\n",
      "Epoch 2 [328/340] - Loss: 14.706 [-14.665, 0.041, 0.000]\n",
      "Epoch 2 [329/340] - Loss: 14.788 [-14.747, 0.041, 0.000]\n",
      "Epoch 2 [330/340] - Loss: 14.893 [-14.851, 0.041, 0.000]\n",
      "Epoch 2 [331/340] - Loss: 15.400 [-15.358, 0.042, 0.000]\n",
      "Epoch 2 [332/340] - Loss: 14.826 [-14.784, 0.042, 0.000]\n",
      "Epoch 2 [333/340] - Loss: 14.911 [-14.869, 0.043, 0.000]\n",
      "Epoch 2 [334/340] - Loss: 14.128 [-14.085, 0.043, 0.000]\n",
      "Epoch 2 [335/340] - Loss: 14.067 [-14.023, 0.043, 0.000]\n",
      "Epoch 2 [336/340] - Loss: 16.307 [-16.263, 0.044, 0.000]\n",
      "Epoch 2 [337/340] - Loss: 15.045 [-15.000, 0.045, 0.000]\n",
      "Epoch 2 [338/340] - Loss: 15.011 [-14.964, 0.046, 0.000]\n",
      "Epoch 2 [339/340] - Loss: 14.179 [-14.133, 0.046, 0.000]\n",
      "Epoch 3 [0/340] - Loss: 15.418 [-15.372, 0.046, 0.000]\n",
      "Epoch 3 [1/340] - Loss: 14.329 [-14.285, 0.044, 0.000]\n",
      "Epoch 3 [2/340] - Loss: 14.426 [-14.384, 0.042, 0.000]\n",
      "Epoch 3 [3/340] - Loss: 14.318 [-14.276, 0.042, 0.000]\n",
      "Epoch 3 [4/340] - Loss: 15.085 [-15.043, 0.042, 0.000]\n",
      "Epoch 3 [5/340] - Loss: 14.681 [-14.639, 0.042, 0.000]\n",
      "Epoch 3 [6/340] - Loss: 16.234 [-16.193, 0.041, 0.000]\n",
      "Epoch 3 [7/340] - Loss: 15.308 [-15.267, 0.041, 0.000]\n",
      "Epoch 3 [8/340] - Loss: 16.312 [-16.272, 0.040, 0.000]\n",
      "Epoch 3 [9/340] - Loss: 14.410 [-14.370, 0.040, 0.000]\n",
      "Epoch 3 [10/340] - Loss: 13.790 [-13.750, 0.040, 0.000]\n",
      "Epoch 3 [11/340] - Loss: 14.733 [-14.693, 0.040, 0.000]\n",
      "Epoch 3 [12/340] - Loss: 15.318 [-15.278, 0.040, 0.000]\n",
      "Epoch 3 [13/340] - Loss: 14.761 [-14.721, 0.040, 0.000]\n",
      "Epoch 3 [14/340] - Loss: 15.896 [-15.856, 0.040, 0.000]\n",
      "Epoch 3 [15/340] - Loss: 15.762 [-15.722, 0.040, 0.000]\n",
      "Epoch 3 [16/340] - Loss: 13.998 [-13.958, 0.040, 0.000]\n",
      "Epoch 3 [17/340] - Loss: 13.464 [-13.422, 0.042, 0.000]\n",
      "Epoch 3 [18/340] - Loss: 14.665 [-14.623, 0.042, 0.000]\n",
      "Epoch 3 [19/340] - Loss: 14.736 [-14.694, 0.042, 0.000]\n",
      "Epoch 3 [20/340] - Loss: 14.572 [-14.531, 0.042, 0.000]\n",
      "Epoch 3 [21/340] - Loss: 13.493 [-13.451, 0.042, 0.000]\n",
      "Epoch 3 [22/340] - Loss: 14.436 [-14.394, 0.042, 0.000]\n",
      "Epoch 3 [23/340] - Loss: 14.790 [-14.748, 0.042, 0.000]\n",
      "Epoch 3 [24/340] - Loss: 14.967 [-14.925, 0.041, 0.000]\n",
      "Epoch 3 [25/340] - Loss: 17.381 [-17.340, 0.041, 0.000]\n",
      "Epoch 3 [26/340] - Loss: 14.850 [-14.808, 0.041, 0.000]\n",
      "Epoch 3 [27/340] - Loss: 14.474 [-14.433, 0.041, 0.000]\n",
      "Epoch 3 [28/340] - Loss: 16.867 [-16.826, 0.041, 0.000]\n",
      "Epoch 3 [29/340] - Loss: 15.661 [-15.621, 0.041, 0.000]\n",
      "Epoch 3 [30/340] - Loss: 14.458 [-14.418, 0.040, 0.000]\n",
      "Epoch 3 [31/340] - Loss: 13.483 [-13.443, 0.040, 0.000]\n",
      "Epoch 3 [32/340] - Loss: 15.066 [-15.026, 0.040, 0.000]\n",
      "Epoch 3 [33/340] - Loss: 15.273 [-15.233, 0.040, 0.000]\n",
      "Epoch 3 [34/340] - Loss: 14.689 [-14.649, 0.039, 0.000]\n",
      "Epoch 3 [35/340] - Loss: 16.214 [-16.175, 0.039, 0.000]\n",
      "Epoch 3 [36/340] - Loss: 15.585 [-15.546, 0.039, 0.000]\n",
      "Epoch 3 [37/340] - Loss: 15.138 [-15.100, 0.039, 0.000]\n",
      "Epoch 3 [38/340] - Loss: 14.388 [-14.350, 0.038, 0.000]\n",
      "Epoch 3 [39/340] - Loss: 14.302 [-14.264, 0.038, 0.000]\n",
      "Epoch 3 [40/340] - Loss: 13.938 [-13.901, 0.037, 0.000]\n",
      "Epoch 3 [41/340] - Loss: 14.693 [-14.655, 0.037, 0.000]\n",
      "Epoch 3 [42/340] - Loss: 14.464 [-14.426, 0.038, 0.000]\n",
      "Epoch 3 [43/340] - Loss: 15.868 [-15.830, 0.038, 0.000]\n",
      "Epoch 3 [44/340] - Loss: 14.195 [-14.156, 0.038, 0.000]\n",
      "Epoch 3 [45/340] - Loss: 13.969 [-13.931, 0.038, 0.000]\n",
      "Epoch 3 [46/340] - Loss: 15.435 [-15.396, 0.038, 0.000]\n",
      "Epoch 3 [47/340] - Loss: 15.579 [-15.542, 0.037, 0.000]\n",
      "Epoch 3 [48/340] - Loss: 16.068 [-16.031, 0.036, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [49/340] - Loss: 14.821 [-14.786, 0.035, 0.000]\n",
      "Epoch 3 [50/340] - Loss: 14.663 [-14.628, 0.035, 0.000]\n",
      "Epoch 3 [51/340] - Loss: 14.297 [-14.263, 0.034, 0.000]\n",
      "Epoch 3 [52/340] - Loss: 15.329 [-15.295, 0.034, 0.000]\n",
      "Epoch 3 [53/340] - Loss: 15.778 [-15.743, 0.035, 0.000]\n",
      "Epoch 3 [54/340] - Loss: 14.286 [-14.251, 0.035, 0.000]\n",
      "Epoch 3 [55/340] - Loss: 14.660 [-14.626, 0.035, 0.000]\n",
      "Epoch 3 [56/340] - Loss: 14.173 [-14.138, 0.034, 0.000]\n",
      "Epoch 3 [57/340] - Loss: 15.402 [-15.368, 0.034, 0.000]\n",
      "Epoch 3 [58/340] - Loss: 13.976 [-13.942, 0.035, 0.000]\n",
      "Epoch 3 [59/340] - Loss: 15.432 [-15.397, 0.035, 0.000]\n",
      "Epoch 3 [60/340] - Loss: 14.506 [-14.471, 0.035, 0.000]\n",
      "Epoch 3 [61/340] - Loss: 14.985 [-14.950, 0.035, 0.000]\n",
      "Epoch 3 [62/340] - Loss: 14.791 [-14.756, 0.035, 0.000]\n",
      "Epoch 3 [63/340] - Loss: 15.187 [-15.152, 0.035, 0.000]\n",
      "Epoch 3 [64/340] - Loss: 15.221 [-15.185, 0.036, 0.000]\n",
      "Epoch 3 [65/340] - Loss: 13.928 [-13.892, 0.036, 0.000]\n",
      "Epoch 3 [66/340] - Loss: 14.596 [-14.560, 0.036, 0.000]\n",
      "Epoch 3 [67/340] - Loss: 14.930 [-14.893, 0.037, 0.000]\n",
      "Epoch 3 [68/340] - Loss: 14.879 [-14.843, 0.036, 0.000]\n",
      "Epoch 3 [69/340] - Loss: 14.510 [-14.473, 0.036, 0.000]\n",
      "Epoch 3 [70/340] - Loss: 14.687 [-14.650, 0.036, 0.000]\n",
      "Epoch 3 [71/340] - Loss: 16.828 [-16.792, 0.037, 0.000]\n",
      "Epoch 3 [72/340] - Loss: 14.741 [-14.704, 0.037, 0.000]\n",
      "Epoch 3 [73/340] - Loss: 15.702 [-15.665, 0.037, 0.000]\n",
      "Epoch 3 [74/340] - Loss: 13.649 [-13.612, 0.037, 0.000]\n",
      "Epoch 3 [75/340] - Loss: 13.825 [-13.787, 0.037, 0.000]\n",
      "Epoch 3 [76/340] - Loss: 14.319 [-14.281, 0.037, 0.000]\n",
      "Epoch 3 [77/340] - Loss: 16.039 [-16.003, 0.036, 0.000]\n",
      "Epoch 3 [78/340] - Loss: 14.464 [-14.429, 0.035, 0.000]\n",
      "Epoch 3 [79/340] - Loss: 13.869 [-13.836, 0.033, 0.000]\n",
      "Epoch 3 [80/340] - Loss: 16.064 [-16.031, 0.033, 0.000]\n",
      "Epoch 3 [81/340] - Loss: 15.245 [-15.212, 0.033, 0.000]\n",
      "Epoch 3 [82/340] - Loss: 15.692 [-15.660, 0.032, 0.000]\n",
      "Epoch 3 [83/340] - Loss: 14.668 [-14.635, 0.032, 0.000]\n",
      "Epoch 3 [84/340] - Loss: 14.524 [-14.492, 0.033, 0.000]\n",
      "Epoch 3 [85/340] - Loss: 15.379 [-15.346, 0.033, 0.000]\n",
      "Epoch 3 [86/340] - Loss: 14.155 [-14.121, 0.034, 0.000]\n",
      "Epoch 3 [87/340] - Loss: 14.858 [-14.823, 0.035, 0.000]\n",
      "Epoch 3 [88/340] - Loss: 14.470 [-14.434, 0.036, 0.000]\n",
      "Epoch 3 [89/340] - Loss: 15.321 [-15.285, 0.036, 0.000]\n",
      "Epoch 3 [90/340] - Loss: 16.018 [-15.982, 0.036, 0.000]\n",
      "Epoch 3 [91/340] - Loss: 16.039 [-16.002, 0.037, 0.000]\n",
      "Epoch 3 [92/340] - Loss: 15.488 [-15.451, 0.037, 0.000]\n",
      "Epoch 3 [93/340] - Loss: 15.618 [-15.582, 0.036, 0.000]\n",
      "Epoch 3 [94/340] - Loss: 16.317 [-16.282, 0.035, 0.000]\n",
      "Epoch 3 [95/340] - Loss: 16.310 [-16.276, 0.034, 0.000]\n",
      "Epoch 3 [96/340] - Loss: 16.843 [-16.810, 0.033, 0.000]\n",
      "Epoch 3 [97/340] - Loss: 16.804 [-16.771, 0.032, 0.000]\n",
      "Epoch 3 [98/340] - Loss: 14.936 [-14.904, 0.032, 0.000]\n",
      "Epoch 3 [99/340] - Loss: 15.885 [-15.854, 0.032, 0.000]\n",
      "Epoch 3 [100/340] - Loss: 16.056 [-16.024, 0.032, 0.000]\n",
      "Epoch 3 [101/340] - Loss: 14.779 [-14.747, 0.032, 0.000]\n",
      "Epoch 3 [102/340] - Loss: 14.920 [-14.887, 0.033, 0.000]\n",
      "Epoch 3 [103/340] - Loss: 14.491 [-14.457, 0.034, 0.000]\n",
      "Epoch 3 [104/340] - Loss: 15.601 [-15.568, 0.033, 0.000]\n",
      "Epoch 3 [105/340] - Loss: 13.717 [-13.684, 0.033, 0.000]\n",
      "Epoch 3 [106/340] - Loss: 13.653 [-13.619, 0.033, 0.000]\n",
      "Epoch 3 [107/340] - Loss: 15.120 [-15.087, 0.033, 0.000]\n",
      "Epoch 3 [108/340] - Loss: 14.853 [-14.818, 0.034, 0.000]\n",
      "Epoch 3 [109/340] - Loss: 14.017 [-13.981, 0.035, 0.000]\n",
      "Epoch 3 [110/340] - Loss: 15.320 [-15.284, 0.036, 0.000]\n",
      "Epoch 3 [111/340] - Loss: 15.530 [-15.493, 0.037, 0.000]\n",
      "Epoch 3 [112/340] - Loss: 15.100 [-15.062, 0.038, 0.000]\n",
      "Epoch 3 [113/340] - Loss: 14.438 [-14.399, 0.039, 0.000]\n",
      "Epoch 3 [114/340] - Loss: 14.428 [-14.389, 0.039, 0.000]\n",
      "Epoch 3 [115/340] - Loss: 13.823 [-13.785, 0.038, 0.000]\n",
      "Epoch 3 [116/340] - Loss: 15.166 [-15.128, 0.037, 0.000]\n",
      "Epoch 3 [117/340] - Loss: 13.004 [-12.967, 0.037, 0.000]\n",
      "Epoch 3 [118/340] - Loss: 14.294 [-14.258, 0.036, 0.000]\n",
      "Epoch 3 [119/340] - Loss: 13.630 [-13.596, 0.034, 0.000]\n",
      "Epoch 3 [120/340] - Loss: 15.283 [-15.250, 0.033, 0.000]\n",
      "Epoch 3 [121/340] - Loss: 15.740 [-15.707, 0.033, 0.000]\n",
      "Epoch 3 [122/340] - Loss: 14.757 [-14.724, 0.033, 0.000]\n",
      "Epoch 3 [123/340] - Loss: 16.125 [-16.092, 0.033, 0.000]\n",
      "Epoch 3 [124/340] - Loss: 14.723 [-14.690, 0.033, 0.000]\n",
      "Epoch 3 [125/340] - Loss: 14.698 [-14.665, 0.033, 0.000]\n",
      "Epoch 3 [126/340] - Loss: 14.655 [-14.623, 0.032, 0.000]\n",
      "Epoch 3 [127/340] - Loss: 13.837 [-13.805, 0.032, 0.000]\n",
      "Epoch 3 [128/340] - Loss: 15.686 [-15.654, 0.032, 0.000]\n",
      "Epoch 3 [129/340] - Loss: 16.059 [-16.028, 0.031, 0.000]\n",
      "Epoch 3 [130/340] - Loss: 14.146 [-14.116, 0.030, 0.000]\n",
      "Epoch 3 [131/340] - Loss: 15.189 [-15.160, 0.029, 0.000]\n",
      "Epoch 3 [132/340] - Loss: 17.052 [-17.023, 0.028, 0.000]\n",
      "Epoch 3 [133/340] - Loss: 14.754 [-14.726, 0.028, 0.000]\n",
      "Epoch 3 [134/340] - Loss: 17.823 [-17.796, 0.028, 0.000]\n",
      "Epoch 3 [135/340] - Loss: 17.111 [-17.083, 0.028, 0.000]\n",
      "Epoch 3 [136/340] - Loss: 15.370 [-15.343, 0.027, 0.000]\n",
      "Epoch 3 [137/340] - Loss: 14.939 [-14.912, 0.027, 0.000]\n",
      "Epoch 3 [138/340] - Loss: 14.258 [-14.231, 0.027, 0.000]\n",
      "Epoch 3 [139/340] - Loss: 16.874 [-16.847, 0.027, 0.000]\n",
      "Epoch 3 [140/340] - Loss: 14.127 [-14.099, 0.028, 0.000]\n",
      "Epoch 3 [141/340] - Loss: 14.892 [-14.863, 0.029, 0.000]\n",
      "Epoch 3 [142/340] - Loss: 13.585 [-13.555, 0.030, 0.000]\n",
      "Epoch 3 [143/340] - Loss: 14.197 [-14.167, 0.030, 0.000]\n",
      "Epoch 3 [144/340] - Loss: 16.226 [-16.196, 0.031, 0.000]\n",
      "Epoch 3 [145/340] - Loss: 14.741 [-14.711, 0.030, 0.000]\n",
      "Epoch 3 [146/340] - Loss: 15.605 [-15.575, 0.031, 0.000]\n",
      "Epoch 3 [147/340] - Loss: 14.120 [-14.089, 0.031, 0.000]\n",
      "Epoch 3 [148/340] - Loss: 14.372 [-14.340, 0.032, 0.000]\n",
      "Epoch 3 [149/340] - Loss: 14.765 [-14.732, 0.033, 0.000]\n",
      "Epoch 3 [150/340] - Loss: 14.010 [-13.976, 0.034, 0.000]\n",
      "Epoch 3 [151/340] - Loss: 15.290 [-15.255, 0.035, 0.000]\n",
      "Epoch 3 [152/340] - Loss: 14.709 [-14.673, 0.035, 0.000]\n",
      "Epoch 3 [153/340] - Loss: 14.563 [-14.529, 0.034, 0.000]\n",
      "Epoch 3 [154/340] - Loss: 14.148 [-14.115, 0.033, 0.000]\n",
      "Epoch 3 [155/340] - Loss: 15.541 [-15.509, 0.033, 0.000]\n",
      "Epoch 3 [156/340] - Loss: 14.393 [-14.361, 0.032, 0.000]\n",
      "Epoch 3 [157/340] - Loss: 13.458 [-13.426, 0.032, 0.000]\n",
      "Epoch 3 [158/340] - Loss: 15.405 [-15.373, 0.033, 0.000]\n",
      "Epoch 3 [159/340] - Loss: 14.993 [-14.960, 0.033, 0.000]\n",
      "Epoch 3 [160/340] - Loss: 14.547 [-14.514, 0.033, 0.000]\n",
      "Epoch 3 [161/340] - Loss: 14.247 [-14.214, 0.033, 0.000]\n",
      "Epoch 3 [162/340] - Loss: 14.953 [-14.920, 0.034, 0.000]\n",
      "Epoch 3 [163/340] - Loss: 14.096 [-14.063, 0.034, 0.000]\n",
      "Epoch 3 [164/340] - Loss: 16.986 [-16.952, 0.033, 0.000]\n",
      "Epoch 3 [165/340] - Loss: 13.691 [-13.657, 0.034, 0.000]\n",
      "Epoch 3 [166/340] - Loss: 14.533 [-14.500, 0.033, 0.000]\n",
      "Epoch 3 [167/340] - Loss: 14.458 [-14.425, 0.034, 0.000]\n",
      "Epoch 3 [168/340] - Loss: 13.479 [-13.445, 0.034, 0.000]\n",
      "Epoch 3 [169/340] - Loss: 14.323 [-14.289, 0.034, 0.000]\n",
      "Epoch 3 [170/340] - Loss: 14.338 [-14.304, 0.034, 0.000]\n",
      "Epoch 3 [171/340] - Loss: 15.013 [-14.979, 0.033, 0.000]\n",
      "Epoch 3 [172/340] - Loss: 15.578 [-15.545, 0.033, 0.000]\n",
      "Epoch 3 [173/340] - Loss: 14.789 [-14.756, 0.033, 0.000]\n",
      "Epoch 3 [174/340] - Loss: 14.950 [-14.917, 0.032, 0.000]\n",
      "Epoch 3 [175/340] - Loss: 15.898 [-15.866, 0.032, 0.000]\n",
      "Epoch 3 [176/340] - Loss: 14.687 [-14.655, 0.032, 0.000]\n",
      "Epoch 3 [177/340] - Loss: 15.079 [-15.046, 0.033, 0.000]\n",
      "Epoch 3 [178/340] - Loss: 15.107 [-15.075, 0.033, 0.000]\n",
      "Epoch 3 [179/340] - Loss: 14.335 [-14.302, 0.033, 0.000]\n",
      "Epoch 3 [180/340] - Loss: 14.985 [-14.951, 0.034, 0.000]\n",
      "Epoch 3 [181/340] - Loss: 13.885 [-13.853, 0.033, 0.000]\n",
      "Epoch 3 [182/340] - Loss: 14.357 [-14.325, 0.031, 0.000]\n",
      "Epoch 3 [183/340] - Loss: 13.799 [-13.769, 0.030, 0.000]\n",
      "Epoch 3 [184/340] - Loss: 14.875 [-14.845, 0.030, 0.000]\n",
      "Epoch 3 [185/340] - Loss: 14.001 [-13.971, 0.030, 0.000]\n",
      "Epoch 3 [186/340] - Loss: 13.590 [-13.560, 0.030, 0.000]\n",
      "Epoch 3 [187/340] - Loss: 14.297 [-14.267, 0.030, 0.000]\n",
      "Epoch 3 [188/340] - Loss: 14.318 [-14.289, 0.030, 0.000]\n",
      "Epoch 3 [189/340] - Loss: 12.897 [-12.868, 0.029, 0.000]\n",
      "Epoch 3 [190/340] - Loss: 14.944 [-14.916, 0.029, 0.000]\n",
      "Epoch 3 [191/340] - Loss: 13.708 [-13.679, 0.028, 0.000]\n",
      "Epoch 3 [192/340] - Loss: 14.275 [-14.247, 0.028, 0.000]\n",
      "Epoch 3 [193/340] - Loss: 13.911 [-13.883, 0.028, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [194/340] - Loss: 13.854 [-13.825, 0.029, 0.000]\n",
      "Epoch 3 [195/340] - Loss: 14.350 [-14.320, 0.030, 0.000]\n",
      "Epoch 3 [196/340] - Loss: 14.112 [-14.082, 0.031, 0.000]\n",
      "Epoch 3 [197/340] - Loss: 13.756 [-13.724, 0.032, 0.000]\n",
      "Epoch 3 [198/340] - Loss: 15.335 [-15.304, 0.031, 0.000]\n",
      "Epoch 3 [199/340] - Loss: 14.386 [-14.357, 0.029, 0.000]\n",
      "Epoch 3 [200/340] - Loss: 13.899 [-13.872, 0.027, 0.000]\n",
      "Epoch 3 [201/340] - Loss: 16.311 [-16.285, 0.025, 0.000]\n",
      "Epoch 3 [202/340] - Loss: 14.156 [-14.132, 0.025, 0.000]\n",
      "Epoch 3 [203/340] - Loss: 14.600 [-14.576, 0.024, 0.000]\n",
      "Epoch 3 [204/340] - Loss: 15.274 [-15.250, 0.024, 0.000]\n",
      "Epoch 3 [205/340] - Loss: 13.958 [-13.933, 0.025, 0.000]\n",
      "Epoch 3 [206/340] - Loss: 14.509 [-14.483, 0.025, 0.000]\n",
      "Epoch 3 [207/340] - Loss: 14.209 [-14.183, 0.026, 0.000]\n",
      "Epoch 3 [208/340] - Loss: 13.633 [-13.606, 0.027, 0.000]\n",
      "Epoch 3 [209/340] - Loss: 15.251 [-15.223, 0.028, 0.000]\n",
      "Epoch 3 [210/340] - Loss: 14.586 [-14.557, 0.029, 0.000]\n",
      "Epoch 3 [211/340] - Loss: 14.671 [-14.641, 0.029, 0.000]\n",
      "Epoch 3 [212/340] - Loss: 13.843 [-13.813, 0.030, 0.000]\n",
      "Epoch 3 [213/340] - Loss: 14.618 [-14.588, 0.030, 0.000]\n",
      "Epoch 3 [214/340] - Loss: 13.818 [-13.787, 0.030, 0.000]\n",
      "Epoch 3 [215/340] - Loss: 14.261 [-14.230, 0.031, 0.000]\n",
      "Epoch 3 [216/340] - Loss: 14.228 [-14.196, 0.032, 0.000]\n",
      "Epoch 3 [217/340] - Loss: 14.237 [-14.204, 0.033, 0.000]\n",
      "Epoch 3 [218/340] - Loss: 14.709 [-14.676, 0.034, 0.000]\n",
      "Epoch 3 [219/340] - Loss: 16.050 [-16.016, 0.034, 0.000]\n",
      "Epoch 3 [220/340] - Loss: 14.732 [-14.698, 0.034, 0.000]\n",
      "Epoch 3 [221/340] - Loss: 13.762 [-13.727, 0.034, 0.000]\n",
      "Epoch 3 [222/340] - Loss: 15.882 [-15.847, 0.035, 0.000]\n",
      "Epoch 3 [223/340] - Loss: 13.936 [-13.900, 0.035, 0.000]\n",
      "Epoch 3 [224/340] - Loss: 14.522 [-14.485, 0.037, 0.000]\n",
      "Epoch 3 [225/340] - Loss: 15.221 [-15.184, 0.037, 0.000]\n",
      "Epoch 3 [226/340] - Loss: 14.414 [-14.377, 0.038, 0.000]\n",
      "Epoch 3 [227/340] - Loss: 14.082 [-14.043, 0.038, 0.000]\n",
      "Epoch 3 [228/340] - Loss: 14.418 [-14.379, 0.039, 0.000]\n",
      "Epoch 3 [229/340] - Loss: 13.776 [-13.737, 0.040, 0.000]\n",
      "Epoch 3 [230/340] - Loss: 14.133 [-14.094, 0.040, 0.000]\n",
      "Epoch 3 [231/340] - Loss: 15.299 [-15.259, 0.040, 0.000]\n",
      "Epoch 3 [232/340] - Loss: 13.346 [-13.307, 0.040, 0.000]\n",
      "Epoch 3 [233/340] - Loss: 14.319 [-14.280, 0.040, 0.000]\n",
      "Epoch 3 [234/340] - Loss: 13.073 [-13.034, 0.040, 0.000]\n",
      "Epoch 3 [235/340] - Loss: 13.772 [-13.732, 0.040, 0.000]\n",
      "Epoch 3 [236/340] - Loss: 14.127 [-14.088, 0.039, 0.000]\n",
      "Epoch 3 [237/340] - Loss: 13.571 [-13.531, 0.039, 0.000]\n",
      "Epoch 3 [238/340] - Loss: 13.126 [-13.088, 0.038, 0.000]\n",
      "Epoch 3 [239/340] - Loss: 14.698 [-14.660, 0.038, 0.000]\n",
      "Epoch 3 [240/340] - Loss: 13.951 [-13.912, 0.038, 0.000]\n",
      "Epoch 3 [241/340] - Loss: 13.048 [-13.011, 0.037, 0.000]\n",
      "Epoch 3 [242/340] - Loss: 13.592 [-13.555, 0.037, 0.000]\n",
      "Epoch 3 [243/340] - Loss: 14.107 [-14.070, 0.037, 0.000]\n",
      "Epoch 3 [244/340] - Loss: 13.620 [-13.583, 0.037, 0.000]\n",
      "Epoch 3 [245/340] - Loss: 13.369 [-13.332, 0.037, 0.000]\n",
      "Epoch 3 [246/340] - Loss: 13.864 [-13.826, 0.038, 0.000]\n",
      "Epoch 3 [247/340] - Loss: 14.424 [-14.386, 0.038, 0.000]\n",
      "Epoch 3 [248/340] - Loss: 12.938 [-12.900, 0.038, 0.000]\n",
      "Epoch 3 [249/340] - Loss: 13.200 [-13.162, 0.039, 0.000]\n",
      "Epoch 3 [250/340] - Loss: 14.439 [-14.400, 0.039, 0.000]\n",
      "Epoch 3 [251/340] - Loss: 13.410 [-13.371, 0.038, 0.000]\n",
      "Epoch 3 [252/340] - Loss: 14.343 [-14.304, 0.038, 0.000]\n",
      "Epoch 3 [253/340] - Loss: 13.087 [-13.048, 0.039, 0.000]\n",
      "Epoch 3 [254/340] - Loss: 14.022 [-13.984, 0.038, 0.000]\n",
      "Epoch 3 [255/340] - Loss: 14.246 [-14.208, 0.038, 0.000]\n",
      "Epoch 3 [256/340] - Loss: 12.475 [-12.437, 0.038, 0.000]\n",
      "Epoch 3 [257/340] - Loss: 14.971 [-14.933, 0.038, 0.000]\n",
      "Epoch 3 [258/340] - Loss: 14.852 [-14.813, 0.039, 0.000]\n",
      "Epoch 3 [259/340] - Loss: 14.940 [-14.900, 0.040, 0.000]\n",
      "Epoch 3 [260/340] - Loss: 13.667 [-13.630, 0.037, 0.000]\n",
      "Epoch 3 [261/340] - Loss: 13.504 [-13.468, 0.036, 0.000]\n",
      "Epoch 3 [262/340] - Loss: 14.514 [-14.480, 0.034, 0.000]\n",
      "Epoch 3 [263/340] - Loss: 14.076 [-14.043, 0.033, 0.000]\n",
      "Epoch 3 [264/340] - Loss: 14.775 [-14.741, 0.034, 0.000]\n",
      "Epoch 3 [265/340] - Loss: 14.918 [-14.884, 0.033, 0.000]\n",
      "Epoch 3 [266/340] - Loss: 14.700 [-14.667, 0.033, 0.000]\n",
      "Epoch 3 [267/340] - Loss: 15.777 [-15.744, 0.033, 0.000]\n",
      "Epoch 3 [268/340] - Loss: 14.724 [-14.691, 0.033, 0.000]\n",
      "Epoch 3 [269/340] - Loss: 14.240 [-14.205, 0.034, 0.000]\n",
      "Epoch 3 [270/340] - Loss: 13.883 [-13.849, 0.034, 0.000]\n",
      "Epoch 3 [271/340] - Loss: 14.282 [-14.248, 0.035, 0.000]\n",
      "Epoch 3 [272/340] - Loss: 15.384 [-15.349, 0.035, 0.000]\n",
      "Epoch 3 [273/340] - Loss: 15.746 [-15.710, 0.035, 0.000]\n",
      "Epoch 3 [274/340] - Loss: 14.533 [-14.497, 0.035, 0.000]\n",
      "Epoch 3 [275/340] - Loss: 14.448 [-14.413, 0.036, 0.000]\n",
      "Epoch 3 [276/340] - Loss: 13.840 [-13.805, 0.035, 0.000]\n",
      "Epoch 3 [277/340] - Loss: 14.728 [-14.693, 0.035, 0.000]\n",
      "Epoch 3 [278/340] - Loss: 15.142 [-15.106, 0.035, 0.000]\n",
      "Epoch 3 [279/340] - Loss: 15.748 [-15.712, 0.036, 0.000]\n",
      "Epoch 3 [280/340] - Loss: 16.833 [-16.795, 0.038, 0.000]\n",
      "Epoch 3 [281/340] - Loss: 15.617 [-15.579, 0.038, 0.000]\n",
      "Epoch 3 [282/340] - Loss: 15.004 [-14.965, 0.039, 0.000]\n",
      "Epoch 3 [283/340] - Loss: 14.970 [-14.931, 0.039, 0.000]\n",
      "Epoch 3 [284/340] - Loss: 15.585 [-15.545, 0.039, 0.000]\n",
      "Epoch 3 [285/340] - Loss: 14.863 [-14.822, 0.040, 0.000]\n",
      "Epoch 3 [286/340] - Loss: 15.894 [-15.854, 0.039, 0.000]\n",
      "Epoch 3 [287/340] - Loss: 14.762 [-14.725, 0.038, 0.000]\n",
      "Epoch 3 [288/340] - Loss: 15.125 [-15.089, 0.036, 0.000]\n",
      "Epoch 3 [289/340] - Loss: 15.896 [-15.861, 0.035, 0.000]\n",
      "Epoch 3 [290/340] - Loss: 13.449 [-13.415, 0.034, 0.000]\n",
      "Epoch 3 [291/340] - Loss: 14.528 [-14.495, 0.033, 0.000]\n",
      "Epoch 3 [292/340] - Loss: 14.941 [-14.909, 0.032, 0.000]\n",
      "Epoch 3 [293/340] - Loss: 15.811 [-15.780, 0.032, 0.000]\n",
      "Epoch 3 [294/340] - Loss: 13.325 [-13.294, 0.031, 0.000]\n",
      "Epoch 3 [295/340] - Loss: 14.658 [-14.626, 0.031, 0.000]\n",
      "Epoch 3 [296/340] - Loss: 14.002 [-13.971, 0.031, 0.000]\n",
      "Epoch 3 [297/340] - Loss: 14.701 [-14.670, 0.032, 0.000]\n",
      "Epoch 3 [298/340] - Loss: 14.949 [-14.916, 0.032, 0.000]\n",
      "Epoch 3 [299/340] - Loss: 13.879 [-13.847, 0.033, 0.000]\n",
      "Epoch 3 [300/340] - Loss: 14.935 [-14.902, 0.033, 0.000]\n",
      "Epoch 3 [301/340] - Loss: 15.797 [-15.764, 0.033, 0.000]\n",
      "Epoch 3 [302/340] - Loss: 13.813 [-13.780, 0.034, 0.000]\n",
      "Epoch 3 [303/340] - Loss: 13.901 [-13.866, 0.035, 0.000]\n",
      "Epoch 3 [304/340] - Loss: 14.757 [-14.721, 0.036, 0.000]\n",
      "Epoch 3 [305/340] - Loss: 13.796 [-13.760, 0.036, 0.000]\n",
      "Epoch 3 [306/340] - Loss: 14.844 [-14.808, 0.037, 0.000]\n",
      "Epoch 3 [307/340] - Loss: 13.936 [-13.900, 0.036, 0.000]\n",
      "Epoch 3 [308/340] - Loss: 14.429 [-14.394, 0.036, 0.000]\n",
      "Epoch 3 [309/340] - Loss: 15.959 [-15.924, 0.035, 0.000]\n",
      "Epoch 3 [310/340] - Loss: 13.342 [-13.307, 0.035, 0.000]\n",
      "Epoch 3 [311/340] - Loss: 13.794 [-13.758, 0.036, 0.000]\n",
      "Epoch 3 [312/340] - Loss: 16.112 [-16.075, 0.036, 0.000]\n",
      "Epoch 3 [313/340] - Loss: 14.240 [-14.203, 0.037, 0.000]\n",
      "Epoch 3 [314/340] - Loss: 15.204 [-15.167, 0.037, 0.000]\n",
      "Epoch 3 [315/340] - Loss: 13.175 [-13.138, 0.037, 0.000]\n",
      "Epoch 3 [316/340] - Loss: 14.108 [-14.072, 0.036, 0.000]\n",
      "Epoch 3 [317/340] - Loss: 13.579 [-13.544, 0.034, 0.000]\n",
      "Epoch 3 [318/340] - Loss: 14.653 [-14.619, 0.034, 0.000]\n",
      "Epoch 3 [319/340] - Loss: 13.709 [-13.675, 0.034, 0.000]\n",
      "Epoch 3 [320/340] - Loss: 14.171 [-14.137, 0.034, 0.000]\n",
      "Epoch 3 [321/340] - Loss: 14.228 [-14.194, 0.034, 0.000]\n",
      "Epoch 3 [322/340] - Loss: 13.723 [-13.689, 0.033, 0.000]\n",
      "Epoch 3 [323/340] - Loss: 13.442 [-13.409, 0.033, 0.000]\n",
      "Epoch 3 [324/340] - Loss: 14.281 [-14.248, 0.033, 0.000]\n",
      "Epoch 3 [325/340] - Loss: 14.664 [-14.632, 0.032, 0.000]\n",
      "Epoch 3 [326/340] - Loss: 14.514 [-14.482, 0.032, 0.000]\n",
      "Epoch 3 [327/340] - Loss: 14.885 [-14.854, 0.031, 0.000]\n",
      "Epoch 3 [328/340] - Loss: 13.553 [-13.522, 0.031, 0.000]\n",
      "Epoch 3 [329/340] - Loss: 17.207 [-17.176, 0.031, 0.000]\n",
      "Epoch 3 [330/340] - Loss: 16.403 [-16.371, 0.031, 0.000]\n",
      "Epoch 3 [331/340] - Loss: 14.818 [-14.786, 0.032, 0.000]\n",
      "Epoch 3 [332/340] - Loss: 14.323 [-14.290, 0.033, 0.000]\n",
      "Epoch 3 [333/340] - Loss: 14.403 [-14.370, 0.034, 0.000]\n",
      "Epoch 3 [334/340] - Loss: 14.467 [-14.433, 0.034, 0.000]\n",
      "Epoch 3 [335/340] - Loss: 13.496 [-13.461, 0.035, 0.000]\n",
      "Epoch 3 [336/340] - Loss: 16.481 [-16.446, 0.035, 0.000]\n",
      "Epoch 3 [337/340] - Loss: 15.203 [-15.169, 0.034, 0.000]\n",
      "Epoch 3 [338/340] - Loss: 14.386 [-14.353, 0.032, 0.000]\n",
      "Epoch 3 [339/340] - Loss: 14.201 [-14.169, 0.032, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [0/340] - Loss: 14.045 [-14.013, 0.032, 0.000]\n",
      "Epoch 4 [1/340] - Loss: 14.150 [-14.118, 0.032, 0.000]\n",
      "Epoch 4 [2/340] - Loss: 13.417 [-13.385, 0.032, 0.000]\n",
      "Epoch 4 [3/340] - Loss: 14.035 [-14.003, 0.032, 0.000]\n",
      "Epoch 4 [4/340] - Loss: 15.362 [-15.331, 0.032, 0.000]\n",
      "Epoch 4 [5/340] - Loss: 16.526 [-16.494, 0.032, 0.000]\n",
      "Epoch 4 [6/340] - Loss: 13.355 [-13.324, 0.031, 0.000]\n",
      "Epoch 4 [7/340] - Loss: 15.333 [-15.302, 0.032, 0.000]\n",
      "Epoch 4 [8/340] - Loss: 14.797 [-14.766, 0.031, 0.000]\n",
      "Epoch 4 [9/340] - Loss: 15.137 [-15.106, 0.031, 0.000]\n",
      "Epoch 4 [10/340] - Loss: 12.931 [-12.900, 0.031, 0.000]\n",
      "Epoch 4 [11/340] - Loss: 13.745 [-13.714, 0.031, 0.000]\n",
      "Epoch 4 [12/340] - Loss: 13.517 [-13.486, 0.031, 0.000]\n",
      "Epoch 4 [13/340] - Loss: 13.448 [-13.418, 0.030, 0.000]\n",
      "Epoch 4 [14/340] - Loss: 12.991 [-12.960, 0.030, 0.000]\n",
      "Epoch 4 [15/340] - Loss: 13.431 [-13.400, 0.030, 0.000]\n",
      "Epoch 4 [16/340] - Loss: 13.532 [-13.502, 0.030, 0.000]\n",
      "Epoch 4 [17/340] - Loss: 12.888 [-12.858, 0.030, 0.000]\n",
      "Epoch 4 [18/340] - Loss: 13.124 [-13.094, 0.030, 0.000]\n",
      "Epoch 4 [19/340] - Loss: 13.309 [-13.279, 0.030, 0.000]\n",
      "Epoch 4 [20/340] - Loss: 14.090 [-14.060, 0.030, 0.000]\n",
      "Epoch 4 [21/340] - Loss: 13.555 [-13.525, 0.030, 0.000]\n",
      "Epoch 4 [22/340] - Loss: 13.093 [-13.062, 0.031, 0.000]\n",
      "Epoch 4 [23/340] - Loss: 13.394 [-13.364, 0.030, 0.000]\n",
      "Epoch 4 [24/340] - Loss: 13.472 [-13.442, 0.030, 0.000]\n",
      "Epoch 4 [25/340] - Loss: 13.728 [-13.698, 0.031, 0.000]\n",
      "Epoch 4 [26/340] - Loss: 13.546 [-13.515, 0.031, 0.000]\n",
      "Epoch 4 [27/340] - Loss: 13.423 [-13.393, 0.031, 0.000]\n",
      "Epoch 4 [28/340] - Loss: 13.084 [-13.054, 0.031, 0.000]\n",
      "Epoch 4 [29/340] - Loss: 13.982 [-13.951, 0.031, 0.000]\n",
      "Epoch 4 [30/340] - Loss: 13.918 [-13.887, 0.031, 0.000]\n",
      "Epoch 4 [31/340] - Loss: 13.367 [-13.336, 0.031, 0.000]\n",
      "Epoch 4 [32/340] - Loss: 13.125 [-13.093, 0.031, 0.000]\n",
      "Epoch 4 [33/340] - Loss: 12.996 [-12.964, 0.032, 0.000]\n",
      "Epoch 4 [34/340] - Loss: 14.494 [-14.462, 0.032, 0.000]\n",
      "Epoch 4 [35/340] - Loss: 13.143 [-13.111, 0.032, 0.000]\n",
      "Epoch 4 [36/340] - Loss: 12.907 [-12.875, 0.032, 0.000]\n",
      "Epoch 4 [37/340] - Loss: 13.841 [-13.809, 0.032, 0.000]\n",
      "Epoch 4 [38/340] - Loss: 12.218 [-12.186, 0.032, 0.000]\n",
      "Epoch 4 [39/340] - Loss: 13.311 [-13.279, 0.032, 0.000]\n",
      "Epoch 4 [40/340] - Loss: 13.760 [-13.728, 0.032, 0.000]\n",
      "Epoch 4 [41/340] - Loss: 14.883 [-14.850, 0.032, 0.000]\n",
      "Epoch 4 [42/340] - Loss: 13.804 [-13.771, 0.033, 0.000]\n",
      "Epoch 4 [43/340] - Loss: 13.661 [-13.628, 0.033, 0.000]\n",
      "Epoch 4 [44/340] - Loss: 13.268 [-13.235, 0.033, 0.000]\n",
      "Epoch 4 [45/340] - Loss: 13.262 [-13.229, 0.033, 0.000]\n",
      "Epoch 4 [46/340] - Loss: 12.758 [-12.726, 0.033, 0.000]\n",
      "Epoch 4 [47/340] - Loss: 13.103 [-13.071, 0.033, 0.000]\n",
      "Epoch 4 [48/340] - Loss: 13.284 [-13.252, 0.033, 0.000]\n",
      "Epoch 4 [49/340] - Loss: 13.979 [-13.947, 0.033, 0.000]\n",
      "Epoch 4 [50/340] - Loss: 13.156 [-13.123, 0.033, 0.000]\n",
      "Epoch 4 [51/340] - Loss: 13.980 [-13.948, 0.033, 0.000]\n",
      "Epoch 4 [52/340] - Loss: 12.955 [-12.923, 0.033, 0.000]\n",
      "Epoch 4 [53/340] - Loss: 13.087 [-13.054, 0.033, 0.000]\n",
      "Epoch 4 [54/340] - Loss: 12.626 [-12.594, 0.033, 0.000]\n",
      "Epoch 4 [55/340] - Loss: 13.820 [-13.787, 0.033, 0.000]\n",
      "Epoch 4 [56/340] - Loss: 13.187 [-13.154, 0.033, 0.000]\n",
      "Epoch 4 [57/340] - Loss: 12.874 [-12.841, 0.033, 0.000]\n",
      "Epoch 4 [58/340] - Loss: 12.958 [-12.925, 0.033, 0.000]\n",
      "Epoch 4 [59/340] - Loss: 13.653 [-13.621, 0.033, 0.000]\n",
      "Epoch 4 [60/340] - Loss: 12.945 [-12.913, 0.032, 0.000]\n",
      "Epoch 4 [61/340] - Loss: 12.905 [-12.872, 0.033, 0.000]\n",
      "Epoch 4 [62/340] - Loss: 12.146 [-12.114, 0.032, 0.000]\n",
      "Epoch 4 [63/340] - Loss: 12.275 [-12.243, 0.032, 0.000]\n",
      "Epoch 4 [64/340] - Loss: 13.747 [-13.715, 0.032, 0.000]\n",
      "Epoch 4 [65/340] - Loss: 14.021 [-13.989, 0.032, 0.000]\n",
      "Epoch 4 [66/340] - Loss: 12.925 [-12.893, 0.032, 0.000]\n",
      "Epoch 4 [67/340] - Loss: 13.111 [-13.079, 0.032, 0.000]\n",
      "Epoch 4 [68/340] - Loss: 13.782 [-13.750, 0.032, 0.000]\n",
      "Epoch 4 [69/340] - Loss: 12.572 [-12.539, 0.032, 0.000]\n",
      "Epoch 4 [70/340] - Loss: 12.789 [-12.756, 0.032, 0.000]\n",
      "Epoch 4 [71/340] - Loss: 13.189 [-13.157, 0.032, 0.000]\n",
      "Epoch 4 [72/340] - Loss: 12.418 [-12.386, 0.033, 0.000]\n",
      "Epoch 4 [73/340] - Loss: 12.546 [-12.513, 0.033, 0.000]\n",
      "Epoch 4 [74/340] - Loss: 13.817 [-13.785, 0.033, 0.000]\n",
      "Epoch 4 [75/340] - Loss: 14.077 [-14.045, 0.033, 0.000]\n",
      "Epoch 4 [76/340] - Loss: 13.723 [-13.691, 0.033, 0.000]\n",
      "Epoch 4 [77/340] - Loss: 12.866 [-12.834, 0.032, 0.000]\n",
      "Epoch 4 [78/340] - Loss: 12.609 [-12.576, 0.033, 0.000]\n",
      "Epoch 4 [79/340] - Loss: 13.375 [-13.343, 0.032, 0.000]\n",
      "Epoch 4 [80/340] - Loss: 12.856 [-12.824, 0.032, 0.000]\n",
      "Epoch 4 [81/340] - Loss: 13.439 [-13.407, 0.032, 0.000]\n",
      "Epoch 4 [82/340] - Loss: 13.275 [-13.243, 0.032, 0.000]\n",
      "Epoch 4 [83/340] - Loss: 12.954 [-12.922, 0.032, 0.000]\n",
      "Epoch 4 [84/340] - Loss: 14.999 [-14.967, 0.032, 0.000]\n",
      "Epoch 4 [85/340] - Loss: 12.777 [-12.744, 0.032, 0.000]\n",
      "Epoch 4 [86/340] - Loss: 12.754 [-12.722, 0.032, 0.000]\n",
      "Epoch 4 [87/340] - Loss: 13.639 [-13.607, 0.032, 0.000]\n",
      "Epoch 4 [88/340] - Loss: 12.423 [-12.391, 0.033, 0.000]\n",
      "Epoch 4 [89/340] - Loss: 14.054 [-14.022, 0.032, 0.000]\n",
      "Epoch 4 [90/340] - Loss: 11.795 [-11.762, 0.032, 0.000]\n",
      "Epoch 4 [91/340] - Loss: 13.137 [-13.105, 0.032, 0.000]\n",
      "Epoch 4 [92/340] - Loss: 13.519 [-13.487, 0.032, 0.000]\n",
      "Epoch 4 [93/340] - Loss: 13.173 [-13.141, 0.032, 0.000]\n",
      "Epoch 4 [94/340] - Loss: 13.033 [-13.001, 0.032, 0.000]\n",
      "Epoch 4 [95/340] - Loss: 13.560 [-13.528, 0.032, 0.000]\n",
      "Epoch 4 [96/340] - Loss: 13.906 [-13.874, 0.032, 0.000]\n",
      "Epoch 4 [97/340] - Loss: 13.362 [-13.330, 0.032, 0.000]\n",
      "Epoch 4 [98/340] - Loss: 12.807 [-12.774, 0.032, 0.000]\n",
      "Epoch 4 [99/340] - Loss: 12.765 [-12.733, 0.032, 0.000]\n",
      "Epoch 4 [100/340] - Loss: 13.816 [-13.784, 0.032, 0.000]\n",
      "Epoch 4 [101/340] - Loss: 12.854 [-12.822, 0.032, 0.000]\n",
      "Epoch 4 [102/340] - Loss: 13.074 [-13.043, 0.032, 0.000]\n",
      "Epoch 4 [103/340] - Loss: 12.622 [-12.590, 0.032, 0.000]\n",
      "Epoch 4 [104/340] - Loss: 12.998 [-12.967, 0.032, 0.000]\n",
      "Epoch 4 [105/340] - Loss: 12.490 [-12.459, 0.032, 0.000]\n",
      "Epoch 4 [106/340] - Loss: 12.486 [-12.454, 0.032, 0.000]\n",
      "Epoch 4 [107/340] - Loss: 13.618 [-13.586, 0.032, 0.000]\n",
      "Epoch 4 [108/340] - Loss: 13.004 [-12.972, 0.032, 0.000]\n",
      "Epoch 4 [109/340] - Loss: 13.573 [-13.541, 0.032, 0.000]\n",
      "Epoch 4 [110/340] - Loss: 13.258 [-13.226, 0.032, 0.000]\n",
      "Epoch 4 [111/340] - Loss: 12.799 [-12.768, 0.032, 0.000]\n",
      "Epoch 4 [112/340] - Loss: 12.646 [-12.614, 0.032, 0.000]\n",
      "Epoch 4 [113/340] - Loss: 12.592 [-12.560, 0.032, 0.000]\n",
      "Epoch 4 [114/340] - Loss: 13.461 [-13.429, 0.032, 0.000]\n",
      "Epoch 4 [115/340] - Loss: 14.015 [-13.983, 0.032, 0.000]\n",
      "Epoch 4 [116/340] - Loss: 13.595 [-13.564, 0.032, 0.000]\n",
      "Epoch 4 [117/340] - Loss: 14.124 [-14.093, 0.032, 0.000]\n",
      "Epoch 4 [118/340] - Loss: 13.490 [-13.458, 0.032, 0.000]\n",
      "Epoch 4 [119/340] - Loss: 13.377 [-13.346, 0.031, 0.000]\n",
      "Epoch 4 [120/340] - Loss: 12.766 [-12.735, 0.031, 0.000]\n",
      "Epoch 4 [121/340] - Loss: 12.855 [-12.824, 0.031, 0.000]\n",
      "Epoch 4 [122/340] - Loss: 12.432 [-12.401, 0.031, 0.000]\n",
      "Epoch 4 [123/340] - Loss: 13.037 [-13.006, 0.031, 0.000]\n",
      "Epoch 4 [124/340] - Loss: 13.285 [-13.254, 0.031, 0.000]\n",
      "Epoch 4 [125/340] - Loss: 12.689 [-12.658, 0.031, 0.000]\n",
      "Epoch 4 [126/340] - Loss: 13.132 [-13.101, 0.031, 0.000]\n",
      "Epoch 4 [127/340] - Loss: 12.905 [-12.874, 0.031, 0.000]\n",
      "Epoch 4 [128/340] - Loss: 13.161 [-13.130, 0.031, 0.000]\n",
      "Epoch 4 [129/340] - Loss: 13.245 [-13.214, 0.031, 0.000]\n",
      "Epoch 4 [130/340] - Loss: 14.078 [-14.048, 0.031, 0.000]\n",
      "Epoch 4 [131/340] - Loss: 13.532 [-13.501, 0.031, 0.000]\n",
      "Epoch 4 [132/340] - Loss: 13.805 [-13.774, 0.031, 0.000]\n",
      "Epoch 4 [133/340] - Loss: 12.888 [-12.858, 0.031, 0.000]\n",
      "Epoch 4 [134/340] - Loss: 13.681 [-13.650, 0.031, 0.000]\n",
      "Epoch 4 [135/340] - Loss: 12.778 [-12.748, 0.031, 0.000]\n",
      "Epoch 4 [136/340] - Loss: 12.305 [-12.274, 0.031, 0.000]\n",
      "Epoch 4 [137/340] - Loss: 12.427 [-12.396, 0.031, 0.000]\n",
      "Epoch 4 [138/340] - Loss: 13.046 [-13.015, 0.031, 0.000]\n",
      "Epoch 4 [139/340] - Loss: 12.717 [-12.686, 0.031, 0.000]\n",
      "Epoch 4 [140/340] - Loss: 13.299 [-13.268, 0.031, 0.000]\n",
      "Epoch 4 [141/340] - Loss: 12.545 [-12.514, 0.031, 0.000]\n",
      "Epoch 4 [142/340] - Loss: 12.553 [-12.522, 0.032, 0.000]\n",
      "Epoch 4 [143/340] - Loss: 14.331 [-14.300, 0.031, 0.000]\n",
      "Epoch 4 [144/340] - Loss: 12.698 [-12.667, 0.032, 0.000]\n",
      "Epoch 4 [145/340] - Loss: 13.347 [-13.316, 0.031, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [146/340] - Loss: 12.426 [-12.395, 0.032, 0.000]\n",
      "Epoch 4 [147/340] - Loss: 14.537 [-14.505, 0.032, 0.000]\n",
      "Epoch 4 [148/340] - Loss: 12.665 [-12.634, 0.031, 0.000]\n",
      "Epoch 4 [149/340] - Loss: 13.793 [-13.762, 0.031, 0.000]\n",
      "Epoch 4 [150/340] - Loss: 13.891 [-13.860, 0.031, 0.000]\n",
      "Epoch 4 [151/340] - Loss: 13.357 [-13.326, 0.032, 0.000]\n",
      "Epoch 4 [152/340] - Loss: 13.527 [-13.496, 0.031, 0.000]\n",
      "Epoch 4 [153/340] - Loss: 12.634 [-12.603, 0.031, 0.000]\n",
      "Epoch 4 [154/340] - Loss: 14.091 [-14.060, 0.031, 0.000]\n",
      "Epoch 4 [155/340] - Loss: 12.875 [-12.844, 0.031, 0.000]\n",
      "Epoch 4 [156/340] - Loss: 13.118 [-13.087, 0.031, 0.000]\n",
      "Epoch 4 [157/340] - Loss: 12.758 [-12.726, 0.032, 0.000]\n",
      "Epoch 4 [158/340] - Loss: 12.401 [-12.369, 0.032, 0.000]\n",
      "Epoch 4 [159/340] - Loss: 12.809 [-12.777, 0.032, 0.000]\n",
      "Epoch 4 [160/340] - Loss: 13.077 [-13.045, 0.032, 0.000]\n",
      "Epoch 4 [161/340] - Loss: 13.145 [-13.113, 0.032, 0.000]\n",
      "Epoch 4 [162/340] - Loss: 12.730 [-12.698, 0.032, 0.000]\n",
      "Epoch 4 [163/340] - Loss: 12.530 [-12.498, 0.032, 0.000]\n",
      "Epoch 4 [164/340] - Loss: 12.720 [-12.689, 0.032, 0.000]\n",
      "Epoch 4 [165/340] - Loss: 13.165 [-13.133, 0.032, 0.000]\n",
      "Epoch 4 [166/340] - Loss: 12.582 [-12.551, 0.032, 0.000]\n",
      "Epoch 4 [167/340] - Loss: 12.927 [-12.896, 0.031, 0.000]\n",
      "Epoch 4 [168/340] - Loss: 12.782 [-12.750, 0.032, 0.000]\n",
      "Epoch 4 [169/340] - Loss: 13.772 [-13.741, 0.032, 0.000]\n",
      "Epoch 4 [170/340] - Loss: 12.446 [-12.414, 0.032, 0.000]\n",
      "Epoch 4 [171/340] - Loss: 13.343 [-13.312, 0.032, 0.000]\n",
      "Epoch 4 [172/340] - Loss: 14.978 [-14.946, 0.032, 0.000]\n",
      "Epoch 4 [173/340] - Loss: 12.805 [-12.774, 0.032, 0.000]\n",
      "Epoch 4 [174/340] - Loss: 12.598 [-12.566, 0.032, 0.000]\n",
      "Epoch 4 [175/340] - Loss: 12.969 [-12.938, 0.032, 0.000]\n",
      "Epoch 4 [176/340] - Loss: 12.394 [-12.363, 0.032, 0.000]\n",
      "Epoch 4 [177/340] - Loss: 13.272 [-13.241, 0.032, 0.000]\n",
      "Epoch 4 [178/340] - Loss: 12.677 [-12.645, 0.032, 0.000]\n",
      "Epoch 4 [179/340] - Loss: 13.165 [-13.134, 0.031, 0.000]\n",
      "Epoch 4 [180/340] - Loss: 13.389 [-13.358, 0.031, 0.000]\n",
      "Epoch 4 [181/340] - Loss: 12.637 [-12.606, 0.031, 0.000]\n",
      "Epoch 4 [182/340] - Loss: 11.663 [-11.631, 0.031, 0.000]\n",
      "Epoch 4 [183/340] - Loss: 12.117 [-12.086, 0.031, 0.000]\n",
      "Epoch 4 [184/340] - Loss: 13.712 [-13.681, 0.031, 0.000]\n",
      "Epoch 4 [185/340] - Loss: 12.880 [-12.849, 0.031, 0.000]\n",
      "Epoch 4 [186/340] - Loss: 13.454 [-13.423, 0.031, 0.000]\n",
      "Epoch 4 [187/340] - Loss: 11.943 [-11.912, 0.031, 0.000]\n",
      "Epoch 4 [188/340] - Loss: 12.885 [-12.854, 0.031, 0.000]\n",
      "Epoch 4 [189/340] - Loss: 12.884 [-12.853, 0.031, 0.000]\n",
      "Epoch 4 [190/340] - Loss: 13.111 [-13.080, 0.031, 0.000]\n",
      "Epoch 4 [191/340] - Loss: 13.917 [-13.886, 0.031, 0.000]\n",
      "Epoch 4 [192/340] - Loss: 14.078 [-14.047, 0.031, 0.000]\n",
      "Epoch 4 [193/340] - Loss: 14.253 [-14.222, 0.031, 0.000]\n",
      "Epoch 4 [194/340] - Loss: 13.634 [-13.603, 0.031, 0.000]\n",
      "Epoch 4 [195/340] - Loss: 12.772 [-12.741, 0.031, 0.000]\n",
      "Epoch 4 [196/340] - Loss: 13.602 [-13.571, 0.031, 0.000]\n",
      "Epoch 4 [197/340] - Loss: 12.503 [-12.473, 0.031, 0.000]\n",
      "Epoch 4 [198/340] - Loss: 12.403 [-12.373, 0.030, 0.000]\n",
      "Epoch 4 [199/340] - Loss: 12.786 [-12.755, 0.030, 0.000]\n",
      "Epoch 4 [200/340] - Loss: 12.624 [-12.594, 0.030, 0.000]\n",
      "Epoch 4 [201/340] - Loss: 13.586 [-13.556, 0.030, 0.000]\n",
      "Epoch 4 [202/340] - Loss: 12.875 [-12.845, 0.030, 0.000]\n",
      "Epoch 4 [203/340] - Loss: 13.355 [-13.325, 0.030, 0.000]\n",
      "Epoch 4 [204/340] - Loss: 13.454 [-13.425, 0.030, 0.000]\n",
      "Epoch 4 [205/340] - Loss: 12.431 [-12.402, 0.030, 0.000]\n",
      "Epoch 4 [206/340] - Loss: 12.459 [-12.429, 0.030, 0.000]\n",
      "Epoch 4 [207/340] - Loss: 12.991 [-12.961, 0.030, 0.000]\n",
      "Epoch 4 [208/340] - Loss: 13.006 [-12.977, 0.030, 0.000]\n",
      "Epoch 4 [209/340] - Loss: 12.969 [-12.939, 0.029, 0.000]\n",
      "Epoch 4 [210/340] - Loss: 13.247 [-13.218, 0.029, 0.000]\n",
      "Epoch 4 [211/340] - Loss: 13.837 [-13.808, 0.029, 0.000]\n",
      "Epoch 4 [212/340] - Loss: 13.160 [-13.131, 0.029, 0.000]\n",
      "Epoch 4 [213/340] - Loss: 12.700 [-12.670, 0.029, 0.000]\n",
      "Epoch 4 [214/340] - Loss: 12.845 [-12.816, 0.029, 0.000]\n",
      "Epoch 4 [215/340] - Loss: 13.715 [-13.686, 0.029, 0.000]\n",
      "Epoch 4 [216/340] - Loss: 13.917 [-13.888, 0.029, 0.000]\n",
      "Epoch 4 [217/340] - Loss: 12.370 [-12.341, 0.029, 0.000]\n",
      "Epoch 4 [218/340] - Loss: 12.779 [-12.750, 0.029, 0.000]\n",
      "Epoch 4 [219/340] - Loss: 13.198 [-13.169, 0.029, 0.000]\n",
      "Epoch 4 [220/340] - Loss: 12.756 [-12.727, 0.029, 0.000]\n",
      "Epoch 4 [221/340] - Loss: 13.011 [-12.982, 0.029, 0.000]\n",
      "Epoch 4 [222/340] - Loss: 12.410 [-12.381, 0.029, 0.000]\n",
      "Epoch 4 [223/340] - Loss: 12.432 [-12.403, 0.029, 0.000]\n",
      "Epoch 4 [224/340] - Loss: 12.161 [-12.132, 0.029, 0.000]\n",
      "Epoch 4 [225/340] - Loss: 12.726 [-12.697, 0.029, 0.000]\n",
      "Epoch 4 [226/340] - Loss: 13.065 [-13.035, 0.030, 0.000]\n",
      "Epoch 4 [227/340] - Loss: 13.869 [-13.839, 0.029, 0.000]\n",
      "Epoch 4 [228/340] - Loss: 13.025 [-12.995, 0.029, 0.000]\n",
      "Epoch 4 [229/340] - Loss: 12.366 [-12.337, 0.029, 0.000]\n",
      "Epoch 4 [230/340] - Loss: 12.803 [-12.774, 0.029, 0.000]\n",
      "Epoch 4 [231/340] - Loss: 15.189 [-15.160, 0.029, 0.000]\n",
      "Epoch 4 [232/340] - Loss: 12.861 [-12.832, 0.029, 0.000]\n",
      "Epoch 4 [233/340] - Loss: 13.108 [-13.079, 0.029, 0.000]\n",
      "Epoch 4 [234/340] - Loss: 12.799 [-12.770, 0.029, 0.000]\n",
      "Epoch 4 [235/340] - Loss: 12.441 [-12.412, 0.029, 0.000]\n",
      "Epoch 4 [236/340] - Loss: 13.006 [-12.977, 0.029, 0.000]\n",
      "Epoch 4 [237/340] - Loss: 13.497 [-13.467, 0.029, 0.000]\n",
      "Epoch 4 [238/340] - Loss: 13.300 [-13.271, 0.029, 0.000]\n",
      "Epoch 4 [239/340] - Loss: 12.571 [-12.542, 0.029, 0.000]\n",
      "Epoch 4 [240/340] - Loss: 14.487 [-14.458, 0.029, 0.000]\n",
      "Epoch 4 [241/340] - Loss: 12.311 [-12.282, 0.029, 0.000]\n",
      "Epoch 4 [242/340] - Loss: 14.024 [-13.996, 0.029, 0.000]\n",
      "Epoch 4 [243/340] - Loss: 14.093 [-14.064, 0.029, 0.000]\n",
      "Epoch 4 [244/340] - Loss: 12.772 [-12.744, 0.028, 0.000]\n",
      "Epoch 4 [245/340] - Loss: 13.005 [-12.978, 0.028, 0.000]\n",
      "Epoch 4 [246/340] - Loss: 12.742 [-12.713, 0.028, 0.000]\n",
      "Epoch 4 [247/340] - Loss: 13.091 [-13.064, 0.028, 0.000]\n",
      "Epoch 4 [248/340] - Loss: 11.890 [-11.863, 0.028, 0.000]\n",
      "Epoch 4 [249/340] - Loss: 13.361 [-13.334, 0.028, 0.000]\n",
      "Epoch 4 [250/340] - Loss: 12.842 [-12.814, 0.028, 0.000]\n",
      "Epoch 4 [251/340] - Loss: 14.063 [-14.036, 0.028, 0.000]\n",
      "Epoch 4 [252/340] - Loss: 12.692 [-12.664, 0.028, 0.000]\n",
      "Epoch 4 [253/340] - Loss: 12.752 [-12.725, 0.028, 0.000]\n",
      "Epoch 4 [254/340] - Loss: 12.904 [-12.877, 0.028, 0.000]\n",
      "Epoch 4 [255/340] - Loss: 13.060 [-13.032, 0.028, 0.000]\n",
      "Epoch 4 [256/340] - Loss: 12.817 [-12.788, 0.028, 0.000]\n",
      "Epoch 4 [257/340] - Loss: 13.933 [-13.905, 0.028, 0.000]\n",
      "Epoch 4 [258/340] - Loss: 13.765 [-13.737, 0.028, 0.000]\n",
      "Epoch 4 [259/340] - Loss: 12.704 [-12.676, 0.028, 0.000]\n",
      "Epoch 4 [260/340] - Loss: 13.703 [-13.675, 0.028, 0.000]\n",
      "Epoch 4 [261/340] - Loss: 12.996 [-12.969, 0.028, 0.000]\n",
      "Epoch 4 [262/340] - Loss: 13.525 [-13.498, 0.028, 0.000]\n",
      "Epoch 4 [263/340] - Loss: 12.936 [-12.909, 0.028, 0.000]\n",
      "Epoch 4 [264/340] - Loss: 12.655 [-12.627, 0.028, 0.000]\n",
      "Epoch 4 [265/340] - Loss: 13.041 [-13.013, 0.028, 0.000]\n",
      "Epoch 4 [266/340] - Loss: 12.427 [-12.399, 0.028, 0.000]\n",
      "Epoch 4 [267/340] - Loss: 12.710 [-12.682, 0.028, 0.000]\n",
      "Epoch 4 [268/340] - Loss: 12.734 [-12.705, 0.028, 0.000]\n",
      "Epoch 4 [269/340] - Loss: 12.175 [-12.147, 0.028, 0.000]\n",
      "Epoch 4 [270/340] - Loss: 11.964 [-11.936, 0.029, 0.000]\n",
      "Epoch 4 [271/340] - Loss: 13.425 [-13.396, 0.029, 0.000]\n",
      "Epoch 4 [272/340] - Loss: 12.814 [-12.785, 0.029, 0.000]\n",
      "Epoch 4 [273/340] - Loss: 13.698 [-13.669, 0.029, 0.000]\n",
      "Epoch 4 [274/340] - Loss: 13.350 [-13.322, 0.029, 0.000]\n",
      "Epoch 4 [275/340] - Loss: 13.786 [-13.758, 0.029, 0.000]\n",
      "Epoch 4 [276/340] - Loss: 13.291 [-13.263, 0.028, 0.000]\n",
      "Epoch 4 [277/340] - Loss: 13.056 [-13.028, 0.028, 0.000]\n",
      "Epoch 4 [278/340] - Loss: 12.472 [-12.444, 0.028, 0.000]\n",
      "Epoch 4 [279/340] - Loss: 13.385 [-13.357, 0.028, 0.000]\n",
      "Epoch 4 [280/340] - Loss: 13.175 [-13.147, 0.028, 0.000]\n",
      "Epoch 4 [281/340] - Loss: 12.590 [-12.562, 0.028, 0.000]\n",
      "Epoch 4 [282/340] - Loss: 12.199 [-12.171, 0.028, 0.000]\n",
      "Epoch 4 [283/340] - Loss: 12.875 [-12.847, 0.028, 0.000]\n",
      "Epoch 4 [284/340] - Loss: 12.767 [-12.740, 0.028, 0.000]\n",
      "Epoch 4 [285/340] - Loss: 12.884 [-12.857, 0.028, 0.000]\n",
      "Epoch 4 [286/340] - Loss: 13.212 [-13.185, 0.028, 0.000]\n",
      "Epoch 4 [287/340] - Loss: 12.320 [-12.293, 0.028, 0.000]\n",
      "Epoch 4 [288/340] - Loss: 12.454 [-12.426, 0.028, 0.000]\n",
      "Epoch 4 [289/340] - Loss: 13.227 [-13.199, 0.028, 0.000]\n",
      "Epoch 4 [290/340] - Loss: 13.433 [-13.405, 0.027, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [291/340] - Loss: 12.952 [-12.924, 0.027, 0.000]\n",
      "Epoch 4 [292/340] - Loss: 12.806 [-12.779, 0.027, 0.000]\n",
      "Epoch 4 [293/340] - Loss: 12.306 [-12.279, 0.027, 0.000]\n",
      "Epoch 4 [294/340] - Loss: 13.280 [-13.253, 0.027, 0.000]\n",
      "Epoch 4 [295/340] - Loss: 13.592 [-13.565, 0.027, 0.000]\n",
      "Epoch 4 [296/340] - Loss: 13.043 [-13.016, 0.027, 0.000]\n",
      "Epoch 4 [297/340] - Loss: 13.619 [-13.592, 0.027, 0.000]\n",
      "Epoch 4 [298/340] - Loss: 13.290 [-13.263, 0.027, 0.000]\n",
      "Epoch 4 [299/340] - Loss: 12.552 [-12.524, 0.027, 0.000]\n",
      "Epoch 4 [300/340] - Loss: 12.227 [-12.200, 0.027, 0.000]\n",
      "Epoch 4 [301/340] - Loss: 12.515 [-12.488, 0.027, 0.000]\n",
      "Epoch 4 [302/340] - Loss: 13.322 [-13.294, 0.027, 0.000]\n",
      "Epoch 4 [303/340] - Loss: 13.307 [-13.280, 0.027, 0.000]\n",
      "Epoch 4 [304/340] - Loss: 13.677 [-13.650, 0.027, 0.000]\n",
      "Epoch 4 [305/340] - Loss: 13.219 [-13.192, 0.027, 0.000]\n",
      "Epoch 4 [306/340] - Loss: 13.188 [-13.161, 0.027, 0.000]\n",
      "Epoch 4 [307/340] - Loss: 14.022 [-13.995, 0.027, 0.000]\n",
      "Epoch 4 [308/340] - Loss: 12.405 [-12.378, 0.027, 0.000]\n",
      "Epoch 4 [309/340] - Loss: 13.080 [-13.053, 0.027, 0.000]\n",
      "Epoch 4 [310/340] - Loss: 12.411 [-12.384, 0.027, 0.000]\n",
      "Epoch 4 [311/340] - Loss: 12.612 [-12.585, 0.027, 0.000]\n",
      "Epoch 4 [312/340] - Loss: 12.432 [-12.405, 0.027, 0.000]\n",
      "Epoch 4 [313/340] - Loss: 12.765 [-12.738, 0.027, 0.000]\n",
      "Epoch 4 [314/340] - Loss: 12.839 [-12.812, 0.027, 0.000]\n",
      "Epoch 4 [315/340] - Loss: 12.922 [-12.895, 0.027, 0.000]\n",
      "Epoch 4 [316/340] - Loss: 13.884 [-13.857, 0.028, 0.000]\n",
      "Epoch 4 [317/340] - Loss: 13.978 [-13.950, 0.028, 0.000]\n",
      "Epoch 4 [318/340] - Loss: 13.414 [-13.386, 0.028, 0.000]\n",
      "Epoch 4 [319/340] - Loss: 12.519 [-12.491, 0.028, 0.000]\n",
      "Epoch 4 [320/340] - Loss: 13.218 [-13.190, 0.028, 0.000]\n",
      "Epoch 4 [321/340] - Loss: 13.261 [-13.233, 0.028, 0.000]\n",
      "Epoch 4 [322/340] - Loss: 11.903 [-11.875, 0.028, 0.000]\n",
      "Epoch 4 [323/340] - Loss: 11.968 [-11.939, 0.028, 0.000]\n",
      "Epoch 4 [324/340] - Loss: 14.822 [-14.794, 0.028, 0.000]\n",
      "Epoch 4 [325/340] - Loss: 13.363 [-13.335, 0.028, 0.000]\n",
      "Epoch 4 [326/340] - Loss: 13.629 [-13.601, 0.028, 0.000]\n",
      "Epoch 4 [327/340] - Loss: 14.274 [-14.246, 0.028, 0.000]\n",
      "Epoch 4 [328/340] - Loss: 12.163 [-12.135, 0.028, 0.000]\n",
      "Epoch 4 [329/340] - Loss: 12.826 [-12.798, 0.028, 0.000]\n",
      "Epoch 4 [330/340] - Loss: 12.807 [-12.780, 0.028, 0.000]\n",
      "Epoch 4 [331/340] - Loss: 12.634 [-12.606, 0.028, 0.000]\n",
      "Epoch 4 [332/340] - Loss: 13.994 [-13.966, 0.028, 0.000]\n",
      "Epoch 4 [333/340] - Loss: 15.681 [-15.653, 0.028, 0.000]\n",
      "Epoch 4 [334/340] - Loss: 12.841 [-12.813, 0.028, 0.000]\n",
      "Epoch 4 [335/340] - Loss: 13.293 [-13.266, 0.028, 0.000]\n",
      "Epoch 4 [336/340] - Loss: 13.413 [-13.385, 0.028, 0.000]\n",
      "Epoch 4 [337/340] - Loss: 12.556 [-12.528, 0.028, 0.000]\n",
      "Epoch 4 [338/340] - Loss: 13.757 [-13.729, 0.028, 0.000]\n",
      "Epoch 4 [339/340] - Loss: 14.538 [-14.510, 0.028, 0.000]\n",
      "Epoch 5 [0/340] - Loss: 12.945 [-12.917, 0.028, 0.000]\n",
      "Epoch 5 [1/340] - Loss: 13.386 [-13.357, 0.028, 0.000]\n",
      "Epoch 5 [2/340] - Loss: 12.584 [-12.556, 0.028, 0.000]\n",
      "Epoch 5 [3/340] - Loss: 14.410 [-14.382, 0.028, 0.000]\n",
      "Epoch 5 [4/340] - Loss: 14.661 [-14.632, 0.028, 0.000]\n",
      "Epoch 5 [5/340] - Loss: 12.139 [-12.111, 0.028, 0.000]\n",
      "Epoch 5 [6/340] - Loss: 13.419 [-13.391, 0.028, 0.000]\n",
      "Epoch 5 [7/340] - Loss: 13.103 [-13.075, 0.028, 0.000]\n",
      "Epoch 5 [8/340] - Loss: 13.423 [-13.394, 0.029, 0.000]\n",
      "Epoch 5 [9/340] - Loss: 13.403 [-13.374, 0.028, 0.000]\n",
      "Epoch 5 [10/340] - Loss: 14.249 [-14.220, 0.029, 0.000]\n",
      "Epoch 5 [11/340] - Loss: 12.995 [-12.966, 0.029, 0.000]\n",
      "Epoch 5 [12/340] - Loss: 12.711 [-12.683, 0.029, 0.000]\n",
      "Epoch 5 [13/340] - Loss: 12.309 [-12.280, 0.029, 0.000]\n",
      "Epoch 5 [14/340] - Loss: 13.336 [-13.307, 0.029, 0.000]\n",
      "Epoch 5 [15/340] - Loss: 12.884 [-12.855, 0.029, 0.000]\n",
      "Epoch 5 [16/340] - Loss: 12.530 [-12.501, 0.029, 0.000]\n",
      "Epoch 5 [17/340] - Loss: 12.689 [-12.660, 0.029, 0.000]\n",
      "Epoch 5 [18/340] - Loss: 12.660 [-12.631, 0.029, 0.000]\n",
      "Epoch 5 [19/340] - Loss: 13.495 [-13.466, 0.029, 0.000]\n",
      "Epoch 5 [20/340] - Loss: 11.938 [-11.909, 0.029, 0.000]\n",
      "Epoch 5 [21/340] - Loss: 12.457 [-12.428, 0.029, 0.000]\n",
      "Epoch 5 [22/340] - Loss: 12.701 [-12.671, 0.029, 0.000]\n",
      "Epoch 5 [23/340] - Loss: 13.492 [-13.463, 0.029, 0.000]\n",
      "Epoch 5 [24/340] - Loss: 13.286 [-13.257, 0.029, 0.000]\n",
      "Epoch 5 [25/340] - Loss: 13.774 [-13.744, 0.029, 0.000]\n",
      "Epoch 5 [26/340] - Loss: 13.010 [-12.980, 0.029, 0.000]\n",
      "Epoch 5 [27/340] - Loss: 12.473 [-12.444, 0.029, 0.000]\n",
      "Epoch 5 [28/340] - Loss: 14.120 [-14.091, 0.029, 0.000]\n",
      "Epoch 5 [29/340] - Loss: 12.345 [-12.315, 0.029, 0.000]\n",
      "Epoch 5 [30/340] - Loss: 13.022 [-12.993, 0.029, 0.000]\n",
      "Epoch 5 [31/340] - Loss: 12.672 [-12.642, 0.029, 0.000]\n",
      "Epoch 5 [32/340] - Loss: 12.338 [-12.309, 0.029, 0.000]\n",
      "Epoch 5 [33/340] - Loss: 12.260 [-12.231, 0.029, 0.000]\n",
      "Epoch 5 [34/340] - Loss: 13.212 [-13.182, 0.029, 0.000]\n",
      "Epoch 5 [35/340] - Loss: 13.312 [-13.283, 0.029, 0.000]\n",
      "Epoch 5 [36/340] - Loss: 13.272 [-13.243, 0.029, 0.000]\n",
      "Epoch 5 [37/340] - Loss: 12.984 [-12.955, 0.029, 0.000]\n",
      "Epoch 5 [38/340] - Loss: 14.569 [-14.540, 0.029, 0.000]\n",
      "Epoch 5 [39/340] - Loss: 13.498 [-13.470, 0.029, 0.000]\n",
      "Epoch 5 [40/340] - Loss: 12.276 [-12.247, 0.029, 0.000]\n",
      "Epoch 5 [41/340] - Loss: 12.584 [-12.556, 0.028, 0.000]\n",
      "Epoch 5 [42/340] - Loss: 12.581 [-12.552, 0.028, 0.000]\n",
      "Epoch 5 [43/340] - Loss: 12.595 [-12.567, 0.028, 0.000]\n",
      "Epoch 5 [44/340] - Loss: 12.855 [-12.827, 0.028, 0.000]\n",
      "Epoch 5 [45/340] - Loss: 12.686 [-12.658, 0.028, 0.000]\n",
      "Epoch 5 [46/340] - Loss: 12.655 [-12.627, 0.028, 0.000]\n",
      "Epoch 5 [47/340] - Loss: 13.042 [-13.013, 0.028, 0.000]\n",
      "Epoch 5 [48/340] - Loss: 12.843 [-12.815, 0.028, 0.000]\n",
      "Epoch 5 [49/340] - Loss: 12.229 [-12.201, 0.028, 0.000]\n",
      "Epoch 5 [50/340] - Loss: 13.348 [-13.319, 0.028, 0.000]\n",
      "Epoch 5 [51/340] - Loss: 12.509 [-12.481, 0.028, 0.000]\n",
      "Epoch 5 [52/340] - Loss: 12.982 [-12.954, 0.028, 0.000]\n",
      "Epoch 5 [53/340] - Loss: 11.909 [-11.880, 0.029, 0.000]\n",
      "Epoch 5 [54/340] - Loss: 12.312 [-12.284, 0.028, 0.000]\n",
      "Epoch 5 [55/340] - Loss: 13.026 [-12.998, 0.028, 0.000]\n",
      "Epoch 5 [56/340] - Loss: 12.588 [-12.559, 0.028, 0.000]\n",
      "Epoch 5 [57/340] - Loss: 12.443 [-12.415, 0.028, 0.000]\n",
      "Epoch 5 [58/340] - Loss: 12.857 [-12.829, 0.028, 0.000]\n",
      "Epoch 5 [59/340] - Loss: 13.069 [-13.041, 0.028, 0.000]\n",
      "Epoch 5 [60/340] - Loss: 12.479 [-12.450, 0.028, 0.000]\n",
      "Epoch 5 [61/340] - Loss: 14.061 [-14.033, 0.028, 0.000]\n",
      "Epoch 5 [62/340] - Loss: 13.233 [-13.205, 0.028, 0.000]\n",
      "Epoch 5 [63/340] - Loss: 15.161 [-15.133, 0.028, 0.000]\n",
      "Epoch 5 [64/340] - Loss: 13.478 [-13.450, 0.028, 0.000]\n",
      "Epoch 5 [65/340] - Loss: 13.689 [-13.661, 0.028, 0.000]\n",
      "Epoch 5 [66/340] - Loss: 14.154 [-14.126, 0.028, 0.000]\n",
      "Epoch 5 [67/340] - Loss: 13.067 [-13.039, 0.028, 0.000]\n",
      "Epoch 5 [68/340] - Loss: 12.926 [-12.898, 0.028, 0.000]\n",
      "Epoch 5 [69/340] - Loss: 13.086 [-13.058, 0.028, 0.000]\n",
      "Epoch 5 [70/340] - Loss: 11.392 [-11.364, 0.028, 0.000]\n",
      "Epoch 5 [71/340] - Loss: 12.851 [-12.823, 0.028, 0.000]\n",
      "Epoch 5 [72/340] - Loss: 12.915 [-12.887, 0.028, 0.000]\n",
      "Epoch 5 [73/340] - Loss: 13.518 [-13.491, 0.028, 0.000]\n",
      "Epoch 5 [74/340] - Loss: 12.479 [-12.452, 0.028, 0.000]\n",
      "Epoch 5 [75/340] - Loss: 12.828 [-12.800, 0.028, 0.000]\n",
      "Epoch 5 [76/340] - Loss: 12.356 [-12.329, 0.028, 0.000]\n",
      "Epoch 5 [77/340] - Loss: 11.770 [-11.742, 0.028, 0.000]\n",
      "Epoch 5 [78/340] - Loss: 13.314 [-13.286, 0.028, 0.000]\n",
      "Epoch 5 [79/340] - Loss: 12.307 [-12.279, 0.028, 0.000]\n",
      "Epoch 5 [80/340] - Loss: 12.289 [-12.262, 0.028, 0.000]\n",
      "Epoch 5 [81/340] - Loss: 12.797 [-12.769, 0.028, 0.000]\n",
      "Epoch 5 [82/340] - Loss: 13.149 [-13.121, 0.028, 0.000]\n",
      "Epoch 5 [83/340] - Loss: 13.755 [-13.727, 0.028, 0.000]\n",
      "Epoch 5 [84/340] - Loss: 13.536 [-13.507, 0.028, 0.000]\n",
      "Epoch 5 [85/340] - Loss: 12.702 [-12.674, 0.028, 0.000]\n",
      "Epoch 5 [86/340] - Loss: 13.567 [-13.539, 0.028, 0.000]\n",
      "Epoch 5 [87/340] - Loss: 12.408 [-12.380, 0.028, 0.000]\n",
      "Epoch 5 [88/340] - Loss: 13.422 [-13.393, 0.028, 0.000]\n",
      "Epoch 5 [89/340] - Loss: 13.388 [-13.360, 0.028, 0.000]\n",
      "Epoch 5 [90/340] - Loss: 12.551 [-12.523, 0.028, 0.000]\n",
      "Epoch 5 [91/340] - Loss: 12.563 [-12.534, 0.028, 0.000]\n",
      "Epoch 5 [92/340] - Loss: 12.917 [-12.889, 0.028, 0.000]\n",
      "Epoch 5 [93/340] - Loss: 12.239 [-12.211, 0.029, 0.000]\n",
      "Epoch 5 [94/340] - Loss: 13.478 [-13.449, 0.029, 0.000]\n",
      "Epoch 5 [95/340] - Loss: 13.074 [-13.046, 0.029, 0.000]\n",
      "Epoch 5 [96/340] - Loss: 13.629 [-13.601, 0.029, 0.000]\n",
      "Epoch 5 [97/340] - Loss: 12.190 [-12.161, 0.029, 0.000]\n",
      "Epoch 5 [98/340] - Loss: 12.546 [-12.518, 0.029, 0.000]\n",
      "Epoch 5 [99/340] - Loss: 12.501 [-12.472, 0.028, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [100/340] - Loss: 13.365 [-13.337, 0.028, 0.000]\n",
      "Epoch 5 [101/340] - Loss: 12.903 [-12.874, 0.028, 0.000]\n",
      "Epoch 5 [102/340] - Loss: 12.849 [-12.821, 0.028, 0.000]\n",
      "Epoch 5 [103/340] - Loss: 12.004 [-11.976, 0.028, 0.000]\n",
      "Epoch 5 [104/340] - Loss: 12.388 [-12.360, 0.028, 0.000]\n",
      "Epoch 5 [105/340] - Loss: 13.585 [-13.557, 0.028, 0.000]\n",
      "Epoch 5 [106/340] - Loss: 12.205 [-12.177, 0.028, 0.000]\n",
      "Epoch 5 [107/340] - Loss: 11.884 [-11.857, 0.028, 0.000]\n",
      "Epoch 5 [108/340] - Loss: 13.347 [-13.319, 0.028, 0.000]\n",
      "Epoch 5 [109/340] - Loss: 13.563 [-13.535, 0.028, 0.000]\n",
      "Epoch 5 [110/340] - Loss: 11.751 [-11.723, 0.028, 0.000]\n",
      "Epoch 5 [111/340] - Loss: 12.299 [-12.271, 0.028, 0.000]\n",
      "Epoch 5 [112/340] - Loss: 12.474 [-12.446, 0.028, 0.000]\n",
      "Epoch 5 [113/340] - Loss: 11.942 [-11.914, 0.028, 0.000]\n",
      "Epoch 5 [114/340] - Loss: 13.135 [-13.107, 0.028, 0.000]\n",
      "Epoch 5 [115/340] - Loss: 12.432 [-12.404, 0.028, 0.000]\n",
      "Epoch 5 [116/340] - Loss: 13.028 [-13.000, 0.028, 0.000]\n",
      "Epoch 5 [117/340] - Loss: 14.038 [-14.010, 0.028, 0.000]\n",
      "Epoch 5 [118/340] - Loss: 12.960 [-12.932, 0.028, 0.000]\n",
      "Epoch 5 [119/340] - Loss: 12.554 [-12.526, 0.028, 0.000]\n",
      "Epoch 5 [120/340] - Loss: 12.624 [-12.596, 0.028, 0.000]\n",
      "Epoch 5 [121/340] - Loss: 12.291 [-12.264, 0.028, 0.000]\n",
      "Epoch 5 [122/340] - Loss: 12.283 [-12.255, 0.028, 0.000]\n",
      "Epoch 5 [123/340] - Loss: 12.125 [-12.097, 0.028, 0.000]\n",
      "Epoch 5 [124/340] - Loss: 12.805 [-12.777, 0.028, 0.000]\n",
      "Epoch 5 [125/340] - Loss: 12.385 [-12.357, 0.028, 0.000]\n",
      "Epoch 5 [126/340] - Loss: 13.720 [-13.692, 0.028, 0.000]\n",
      "Epoch 5 [127/340] - Loss: 12.533 [-12.505, 0.028, 0.000]\n",
      "Epoch 5 [128/340] - Loss: 13.114 [-13.086, 0.028, 0.000]\n",
      "Epoch 5 [129/340] - Loss: 12.199 [-12.171, 0.028, 0.000]\n",
      "Epoch 5 [130/340] - Loss: 12.515 [-12.486, 0.029, 0.000]\n",
      "Epoch 5 [131/340] - Loss: 12.524 [-12.495, 0.029, 0.000]\n",
      "Epoch 5 [132/340] - Loss: 12.492 [-12.463, 0.029, 0.000]\n",
      "Epoch 5 [133/340] - Loss: 12.475 [-12.446, 0.029, 0.000]\n",
      "Epoch 5 [134/340] - Loss: 12.292 [-12.262, 0.030, 0.000]\n",
      "Epoch 5 [135/340] - Loss: 12.598 [-12.569, 0.029, 0.000]\n",
      "Epoch 5 [136/340] - Loss: 12.521 [-12.491, 0.029, 0.000]\n",
      "Epoch 5 [137/340] - Loss: 12.636 [-12.607, 0.029, 0.000]\n",
      "Epoch 5 [138/340] - Loss: 13.548 [-13.519, 0.029, 0.000]\n",
      "Epoch 5 [139/340] - Loss: 12.487 [-12.458, 0.029, 0.000]\n",
      "Epoch 5 [140/340] - Loss: 13.048 [-13.019, 0.030, 0.000]\n",
      "Epoch 5 [141/340] - Loss: 12.735 [-12.705, 0.029, 0.000]\n",
      "Epoch 5 [142/340] - Loss: 12.878 [-12.849, 0.029, 0.000]\n",
      "Epoch 5 [143/340] - Loss: 13.922 [-13.893, 0.029, 0.000]\n",
      "Epoch 5 [144/340] - Loss: 12.482 [-12.453, 0.029, 0.000]\n",
      "Epoch 5 [145/340] - Loss: 13.129 [-13.100, 0.029, 0.000]\n",
      "Epoch 5 [146/340] - Loss: 13.702 [-13.673, 0.029, 0.000]\n",
      "Epoch 5 [147/340] - Loss: 13.383 [-13.355, 0.029, 0.000]\n",
      "Epoch 5 [148/340] - Loss: 13.275 [-13.247, 0.029, 0.000]\n",
      "Epoch 5 [149/340] - Loss: 13.134 [-13.105, 0.029, 0.000]\n",
      "Epoch 5 [150/340] - Loss: 12.456 [-12.427, 0.029, 0.000]\n",
      "Epoch 5 [151/340] - Loss: 13.507 [-13.478, 0.029, 0.000]\n",
      "Epoch 5 [152/340] - Loss: 12.508 [-12.479, 0.029, 0.000]\n",
      "Epoch 5 [153/340] - Loss: 12.920 [-12.891, 0.029, 0.000]\n",
      "Epoch 5 [154/340] - Loss: 12.792 [-12.762, 0.029, 0.000]\n",
      "Epoch 5 [155/340] - Loss: 12.604 [-12.575, 0.029, 0.000]\n",
      "Epoch 5 [156/340] - Loss: 13.786 [-13.757, 0.029, 0.000]\n",
      "Epoch 5 [157/340] - Loss: 11.775 [-11.746, 0.029, 0.000]\n",
      "Epoch 5 [158/340] - Loss: 12.196 [-12.167, 0.030, 0.000]\n",
      "Epoch 5 [159/340] - Loss: 12.378 [-12.348, 0.030, 0.000]\n",
      "Epoch 5 [160/340] - Loss: 12.609 [-12.579, 0.030, 0.000]\n",
      "Epoch 5 [161/340] - Loss: 12.271 [-12.241, 0.030, 0.000]\n",
      "Epoch 5 [162/340] - Loss: 12.024 [-11.994, 0.030, 0.000]\n",
      "Epoch 5 [163/340] - Loss: 13.570 [-13.541, 0.030, 0.000]\n",
      "Epoch 5 [164/340] - Loss: 13.174 [-13.144, 0.030, 0.000]\n",
      "Epoch 5 [165/340] - Loss: 13.422 [-13.392, 0.030, 0.000]\n",
      "Epoch 5 [166/340] - Loss: 12.393 [-12.363, 0.030, 0.000]\n",
      "Epoch 5 [167/340] - Loss: 12.690 [-12.660, 0.030, 0.000]\n",
      "Epoch 5 [168/340] - Loss: 13.622 [-13.592, 0.030, 0.000]\n",
      "Epoch 5 [169/340] - Loss: 12.584 [-12.554, 0.030, 0.000]\n",
      "Epoch 5 [170/340] - Loss: 13.429 [-13.399, 0.030, 0.000]\n",
      "Epoch 5 [171/340] - Loss: 12.647 [-12.617, 0.030, 0.000]\n",
      "Epoch 5 [172/340] - Loss: 12.246 [-12.216, 0.030, 0.000]\n",
      "Epoch 5 [173/340] - Loss: 12.101 [-12.071, 0.030, 0.000]\n",
      "Epoch 5 [174/340] - Loss: 12.678 [-12.648, 0.030, 0.000]\n",
      "Epoch 5 [175/340] - Loss: 11.963 [-11.932, 0.030, 0.000]\n",
      "Epoch 5 [176/340] - Loss: 13.582 [-13.551, 0.030, 0.000]\n",
      "Epoch 5 [177/340] - Loss: 12.871 [-12.841, 0.030, 0.000]\n",
      "Epoch 5 [178/340] - Loss: 14.115 [-14.085, 0.030, 0.000]\n",
      "Epoch 5 [179/340] - Loss: 12.701 [-12.671, 0.030, 0.000]\n",
      "Epoch 5 [180/340] - Loss: 12.036 [-12.005, 0.031, 0.000]\n",
      "Epoch 5 [181/340] - Loss: 12.645 [-12.615, 0.030, 0.000]\n",
      "Epoch 5 [182/340] - Loss: 12.252 [-12.222, 0.030, 0.000]\n",
      "Epoch 5 [183/340] - Loss: 11.795 [-11.765, 0.030, 0.000]\n",
      "Epoch 5 [184/340] - Loss: 13.584 [-13.554, 0.030, 0.000]\n",
      "Epoch 5 [185/340] - Loss: 13.188 [-13.158, 0.030, 0.000]\n",
      "Epoch 5 [186/340] - Loss: 13.915 [-13.886, 0.030, 0.000]\n",
      "Epoch 5 [187/340] - Loss: 12.635 [-12.605, 0.030, 0.000]\n",
      "Epoch 5 [188/340] - Loss: 12.315 [-12.286, 0.030, 0.000]\n",
      "Epoch 5 [189/340] - Loss: 12.070 [-12.040, 0.029, 0.000]\n",
      "Epoch 5 [190/340] - Loss: 12.640 [-12.610, 0.029, 0.000]\n",
      "Epoch 5 [191/340] - Loss: 12.738 [-12.709, 0.029, 0.000]\n",
      "Epoch 5 [192/340] - Loss: 13.045 [-13.016, 0.029, 0.000]\n",
      "Epoch 5 [193/340] - Loss: 14.659 [-14.630, 0.029, 0.000]\n",
      "Epoch 5 [194/340] - Loss: 12.680 [-12.652, 0.029, 0.000]\n",
      "Epoch 5 [195/340] - Loss: 13.019 [-12.991, 0.028, 0.000]\n",
      "Epoch 5 [196/340] - Loss: 13.452 [-13.424, 0.028, 0.000]\n",
      "Epoch 5 [197/340] - Loss: 13.107 [-13.079, 0.028, 0.000]\n",
      "Epoch 5 [198/340] - Loss: 12.182 [-12.155, 0.027, 0.000]\n",
      "Epoch 5 [199/340] - Loss: 13.184 [-13.157, 0.027, 0.000]\n",
      "Epoch 5 [200/340] - Loss: 12.863 [-12.836, 0.027, 0.000]\n",
      "Epoch 5 [201/340] - Loss: 12.140 [-12.114, 0.026, 0.000]\n",
      "Epoch 5 [202/340] - Loss: 12.686 [-12.660, 0.026, 0.000]\n",
      "Epoch 5 [203/340] - Loss: 12.796 [-12.769, 0.027, 0.000]\n",
      "Epoch 5 [204/340] - Loss: 12.591 [-12.564, 0.027, 0.000]\n",
      "Epoch 5 [205/340] - Loss: 13.186 [-13.160, 0.027, 0.000]\n",
      "Epoch 5 [206/340] - Loss: 14.531 [-14.504, 0.027, 0.000]\n",
      "Epoch 5 [207/340] - Loss: 13.420 [-13.393, 0.027, 0.000]\n",
      "Epoch 5 [208/340] - Loss: 12.256 [-12.229, 0.027, 0.000]\n",
      "Epoch 5 [209/340] - Loss: 12.395 [-12.368, 0.027, 0.000]\n",
      "Epoch 5 [210/340] - Loss: 13.123 [-13.096, 0.027, 0.000]\n",
      "Epoch 5 [211/340] - Loss: 12.803 [-12.776, 0.027, 0.000]\n",
      "Epoch 5 [212/340] - Loss: 12.790 [-12.762, 0.028, 0.000]\n",
      "Epoch 5 [213/340] - Loss: 13.088 [-13.060, 0.028, 0.000]\n",
      "Epoch 5 [214/340] - Loss: 13.198 [-13.170, 0.027, 0.000]\n",
      "Epoch 5 [215/340] - Loss: 13.094 [-13.067, 0.027, 0.000]\n",
      "Epoch 5 [216/340] - Loss: 13.223 [-13.196, 0.027, 0.000]\n",
      "Epoch 5 [217/340] - Loss: 12.401 [-12.374, 0.027, 0.000]\n",
      "Epoch 5 [218/340] - Loss: 11.713 [-11.687, 0.027, 0.000]\n",
      "Epoch 5 [219/340] - Loss: 13.759 [-13.732, 0.027, 0.000]\n",
      "Epoch 5 [220/340] - Loss: 12.171 [-12.144, 0.026, 0.000]\n",
      "Epoch 5 [221/340] - Loss: 14.630 [-14.603, 0.027, 0.000]\n",
      "Epoch 5 [222/340] - Loss: 13.791 [-13.765, 0.026, 0.000]\n",
      "Epoch 5 [223/340] - Loss: 12.789 [-12.762, 0.027, 0.000]\n",
      "Epoch 5 [224/340] - Loss: 13.765 [-13.739, 0.026, 0.000]\n",
      "Epoch 5 [225/340] - Loss: 13.045 [-13.019, 0.026, 0.000]\n",
      "Epoch 5 [226/340] - Loss: 13.273 [-13.247, 0.026, 0.000]\n",
      "Epoch 5 [227/340] - Loss: 12.167 [-12.141, 0.026, 0.000]\n",
      "Epoch 5 [228/340] - Loss: 13.918 [-13.892, 0.026, 0.000]\n",
      "Epoch 5 [229/340] - Loss: 12.740 [-12.714, 0.026, 0.000]\n",
      "Epoch 5 [230/340] - Loss: 13.382 [-13.356, 0.026, 0.000]\n",
      "Epoch 5 [231/340] - Loss: 13.411 [-13.385, 0.025, 0.000]\n",
      "Epoch 5 [232/340] - Loss: 13.257 [-13.231, 0.025, 0.000]\n",
      "Epoch 5 [233/340] - Loss: 13.926 [-13.900, 0.025, 0.000]\n",
      "Epoch 5 [234/340] - Loss: 12.233 [-12.207, 0.025, 0.000]\n",
      "Epoch 5 [235/340] - Loss: 12.664 [-12.638, 0.025, 0.000]\n",
      "Epoch 5 [236/340] - Loss: 12.406 [-12.381, 0.025, 0.000]\n",
      "Epoch 5 [237/340] - Loss: 12.845 [-12.819, 0.025, 0.000]\n",
      "Epoch 5 [238/340] - Loss: 12.670 [-12.644, 0.026, 0.000]\n",
      "Epoch 5 [239/340] - Loss: 11.415 [-11.389, 0.026, 0.000]\n",
      "Epoch 5 [240/340] - Loss: 12.152 [-12.126, 0.026, 0.000]\n",
      "Epoch 5 [241/340] - Loss: 12.377 [-12.351, 0.026, 0.000]\n",
      "Epoch 5 [242/340] - Loss: 12.759 [-12.733, 0.026, 0.000]\n",
      "Epoch 5 [243/340] - Loss: 12.726 [-12.699, 0.027, 0.000]\n",
      "Epoch 5 [244/340] - Loss: 11.924 [-11.897, 0.026, 0.000]\n",
      "Epoch 5 [245/340] - Loss: 13.006 [-12.980, 0.026, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [246/340] - Loss: 11.755 [-11.729, 0.026, 0.000]\n",
      "Epoch 5 [247/340] - Loss: 14.237 [-14.210, 0.027, 0.000]\n",
      "Epoch 5 [248/340] - Loss: 12.758 [-12.731, 0.027, 0.000]\n",
      "Epoch 5 [249/340] - Loss: 13.292 [-13.265, 0.027, 0.000]\n",
      "Epoch 5 [250/340] - Loss: 12.635 [-12.607, 0.027, 0.000]\n",
      "Epoch 5 [251/340] - Loss: 13.644 [-13.617, 0.027, 0.000]\n",
      "Epoch 5 [252/340] - Loss: 13.271 [-13.244, 0.027, 0.000]\n",
      "Epoch 5 [253/340] - Loss: 13.399 [-13.371, 0.027, 0.000]\n",
      "Epoch 5 [254/340] - Loss: 12.840 [-12.812, 0.028, 0.000]\n",
      "Epoch 5 [255/340] - Loss: 13.903 [-13.876, 0.028, 0.000]\n",
      "Epoch 5 [256/340] - Loss: 12.903 [-12.875, 0.027, 0.000]\n",
      "Epoch 5 [257/340] - Loss: 13.768 [-13.741, 0.027, 0.000]\n",
      "Epoch 5 [258/340] - Loss: 12.376 [-12.349, 0.027, 0.000]\n",
      "Epoch 5 [259/340] - Loss: 12.652 [-12.625, 0.027, 0.000]\n",
      "Epoch 5 [260/340] - Loss: 12.615 [-12.588, 0.027, 0.000]\n",
      "Epoch 5 [261/340] - Loss: 12.891 [-12.864, 0.027, 0.000]\n",
      "Epoch 5 [262/340] - Loss: 12.764 [-12.737, 0.027, 0.000]\n",
      "Epoch 5 [263/340] - Loss: 12.775 [-12.748, 0.027, 0.000]\n",
      "Epoch 5 [264/340] - Loss: 13.017 [-12.990, 0.027, 0.000]\n",
      "Epoch 5 [265/340] - Loss: 12.734 [-12.707, 0.027, 0.000]\n",
      "Epoch 5 [266/340] - Loss: 13.003 [-12.976, 0.027, 0.000]\n",
      "Epoch 5 [267/340] - Loss: 12.563 [-12.536, 0.027, 0.000]\n",
      "Epoch 5 [268/340] - Loss: 12.653 [-12.626, 0.027, 0.000]\n",
      "Epoch 5 [269/340] - Loss: 12.622 [-12.595, 0.027, 0.000]\n",
      "Epoch 5 [270/340] - Loss: 12.572 [-12.545, 0.027, 0.000]\n",
      "Epoch 5 [271/340] - Loss: 13.777 [-13.750, 0.027, 0.000]\n",
      "Epoch 5 [272/340] - Loss: 12.447 [-12.421, 0.027, 0.000]\n",
      "Epoch 5 [273/340] - Loss: 13.549 [-13.522, 0.027, 0.000]\n",
      "Epoch 5 [274/340] - Loss: 12.783 [-12.756, 0.027, 0.000]\n",
      "Epoch 5 [275/340] - Loss: 13.421 [-13.394, 0.027, 0.000]\n",
      "Epoch 5 [276/340] - Loss: 13.120 [-13.094, 0.027, 0.000]\n",
      "Epoch 5 [277/340] - Loss: 13.554 [-13.527, 0.027, 0.000]\n",
      "Epoch 5 [278/340] - Loss: 12.723 [-12.697, 0.027, 0.000]\n",
      "Epoch 5 [279/340] - Loss: 11.825 [-11.799, 0.027, 0.000]\n",
      "Epoch 5 [280/340] - Loss: 12.605 [-12.579, 0.027, 0.000]\n",
      "Epoch 5 [281/340] - Loss: 12.366 [-12.339, 0.027, 0.000]\n",
      "Epoch 5 [282/340] - Loss: 12.396 [-12.369, 0.027, 0.000]\n",
      "Epoch 5 [283/340] - Loss: 12.925 [-12.899, 0.027, 0.000]\n",
      "Epoch 5 [284/340] - Loss: 13.063 [-13.037, 0.027, 0.000]\n",
      "Epoch 5 [285/340] - Loss: 12.628 [-12.601, 0.027, 0.000]\n",
      "Epoch 5 [286/340] - Loss: 12.891 [-12.864, 0.027, 0.000]\n",
      "Epoch 5 [287/340] - Loss: 12.265 [-12.238, 0.027, 0.000]\n",
      "Epoch 5 [288/340] - Loss: 13.356 [-13.329, 0.027, 0.000]\n",
      "Epoch 5 [289/340] - Loss: 12.323 [-12.295, 0.027, 0.000]\n",
      "Epoch 5 [290/340] - Loss: 12.040 [-12.013, 0.027, 0.000]\n",
      "Epoch 5 [291/340] - Loss: 12.219 [-12.192, 0.027, 0.000]\n",
      "Epoch 5 [292/340] - Loss: 13.067 [-13.039, 0.028, 0.000]\n",
      "Epoch 5 [293/340] - Loss: 13.403 [-13.375, 0.028, 0.000]\n",
      "Epoch 5 [294/340] - Loss: 12.330 [-12.302, 0.028, 0.000]\n",
      "Epoch 5 [295/340] - Loss: 13.060 [-13.032, 0.028, 0.000]\n",
      "Epoch 5 [296/340] - Loss: 13.582 [-13.554, 0.028, 0.000]\n",
      "Epoch 5 [297/340] - Loss: 12.637 [-12.609, 0.028, 0.000]\n",
      "Epoch 5 [298/340] - Loss: 13.077 [-13.049, 0.028, 0.000]\n",
      "Epoch 5 [299/340] - Loss: 12.195 [-12.167, 0.028, 0.000]\n",
      "Epoch 5 [300/340] - Loss: 12.869 [-12.841, 0.028, 0.000]\n",
      "Epoch 5 [301/340] - Loss: 12.564 [-12.536, 0.028, 0.000]\n",
      "Epoch 5 [302/340] - Loss: 14.167 [-14.139, 0.028, 0.000]\n",
      "Epoch 5 [303/340] - Loss: 12.507 [-12.480, 0.028, 0.000]\n",
      "Epoch 5 [304/340] - Loss: 12.473 [-12.445, 0.028, 0.000]\n",
      "Epoch 5 [305/340] - Loss: 12.667 [-12.640, 0.027, 0.000]\n",
      "Epoch 5 [306/340] - Loss: 12.601 [-12.573, 0.028, 0.000]\n",
      "Epoch 5 [307/340] - Loss: 13.564 [-13.538, 0.027, 0.000]\n",
      "Epoch 5 [308/340] - Loss: 12.680 [-12.654, 0.027, 0.000]\n",
      "Epoch 5 [309/340] - Loss: 13.913 [-13.886, 0.027, 0.000]\n",
      "Epoch 5 [310/340] - Loss: 12.727 [-12.700, 0.027, 0.000]\n",
      "Epoch 5 [311/340] - Loss: 11.799 [-11.773, 0.026, 0.000]\n",
      "Epoch 5 [312/340] - Loss: 12.875 [-12.849, 0.026, 0.000]\n",
      "Epoch 5 [313/340] - Loss: 13.080 [-13.054, 0.026, 0.000]\n",
      "Epoch 5 [314/340] - Loss: 12.297 [-12.271, 0.026, 0.000]\n",
      "Epoch 5 [315/340] - Loss: 12.637 [-12.611, 0.026, 0.000]\n",
      "Epoch 5 [316/340] - Loss: 12.438 [-12.412, 0.026, 0.000]\n",
      "Epoch 5 [317/340] - Loss: 12.426 [-12.401, 0.026, 0.000]\n",
      "Epoch 5 [318/340] - Loss: 13.757 [-13.731, 0.026, 0.000]\n",
      "Epoch 5 [319/340] - Loss: 12.059 [-12.032, 0.026, 0.000]\n",
      "Epoch 5 [320/340] - Loss: 13.035 [-13.009, 0.026, 0.000]\n",
      "Epoch 5 [321/340] - Loss: 12.925 [-12.899, 0.026, 0.000]\n",
      "Epoch 5 [322/340] - Loss: 12.295 [-12.269, 0.026, 0.000]\n",
      "Epoch 5 [323/340] - Loss: 12.372 [-12.346, 0.026, 0.000]\n",
      "Epoch 5 [324/340] - Loss: 12.718 [-12.692, 0.026, 0.000]\n",
      "Epoch 5 [325/340] - Loss: 12.937 [-12.911, 0.026, 0.000]\n",
      "Epoch 5 [326/340] - Loss: 13.933 [-13.907, 0.026, 0.000]\n",
      "Epoch 5 [327/340] - Loss: 13.331 [-13.305, 0.026, 0.000]\n",
      "Epoch 5 [328/340] - Loss: 14.120 [-14.094, 0.026, 0.000]\n",
      "Epoch 5 [329/340] - Loss: 12.889 [-12.863, 0.026, 0.000]\n",
      "Epoch 5 [330/340] - Loss: 12.898 [-12.872, 0.026, 0.000]\n",
      "Epoch 5 [331/340] - Loss: 11.628 [-11.602, 0.026, 0.000]\n",
      "Epoch 5 [332/340] - Loss: 12.302 [-12.276, 0.026, 0.000]\n",
      "Epoch 5 [333/340] - Loss: 12.961 [-12.935, 0.026, 0.000]\n",
      "Epoch 5 [334/340] - Loss: 12.917 [-12.891, 0.026, 0.000]\n",
      "Epoch 5 [335/340] - Loss: 13.040 [-13.014, 0.026, 0.000]\n",
      "Epoch 5 [336/340] - Loss: 13.680 [-13.654, 0.025, 0.000]\n",
      "Epoch 5 [337/340] - Loss: 13.257 [-13.231, 0.025, 0.000]\n",
      "Epoch 5 [338/340] - Loss: 13.969 [-13.944, 0.025, 0.000]\n",
      "Epoch 5 [339/340] - Loss: 12.488 [-12.463, 0.025, 0.000]\n",
      "Epoch 6 [0/340] - Loss: 12.897 [-12.871, 0.025, 0.000]\n",
      "Epoch 6 [1/340] - Loss: 12.403 [-12.378, 0.025, 0.000]\n",
      "Epoch 6 [2/340] - Loss: 13.388 [-13.363, 0.025, 0.000]\n",
      "Epoch 6 [3/340] - Loss: 12.891 [-12.866, 0.025, 0.000]\n",
      "Epoch 6 [4/340] - Loss: 13.422 [-13.396, 0.025, 0.000]\n",
      "Epoch 6 [5/340] - Loss: 12.778 [-12.752, 0.025, 0.000]\n",
      "Epoch 6 [6/340] - Loss: 12.935 [-12.910, 0.025, 0.000]\n",
      "Epoch 6 [7/340] - Loss: 12.151 [-12.125, 0.025, 0.000]\n",
      "Epoch 6 [8/340] - Loss: 12.747 [-12.721, 0.025, 0.000]\n",
      "Epoch 6 [9/340] - Loss: 12.334 [-12.308, 0.026, 0.000]\n",
      "Epoch 6 [10/340] - Loss: 12.115 [-12.090, 0.025, 0.000]\n",
      "Epoch 6 [11/340] - Loss: 12.772 [-12.747, 0.025, 0.000]\n",
      "Epoch 6 [12/340] - Loss: 11.823 [-11.798, 0.025, 0.000]\n",
      "Epoch 6 [13/340] - Loss: 12.619 [-12.593, 0.025, 0.000]\n",
      "Epoch 6 [14/340] - Loss: 13.258 [-13.233, 0.025, 0.000]\n",
      "Epoch 6 [15/340] - Loss: 13.184 [-13.159, 0.025, 0.000]\n",
      "Epoch 6 [16/340] - Loss: 11.829 [-11.804, 0.025, 0.000]\n",
      "Epoch 6 [17/340] - Loss: 12.615 [-12.590, 0.025, 0.000]\n",
      "Epoch 6 [18/340] - Loss: 12.951 [-12.925, 0.025, 0.000]\n",
      "Epoch 6 [19/340] - Loss: 12.241 [-12.216, 0.025, 0.000]\n",
      "Epoch 6 [20/340] - Loss: 13.053 [-13.028, 0.025, 0.000]\n",
      "Epoch 6 [21/340] - Loss: 13.102 [-13.077, 0.025, 0.000]\n",
      "Epoch 6 [22/340] - Loss: 12.142 [-12.117, 0.025, 0.000]\n",
      "Epoch 6 [23/340] - Loss: 12.863 [-12.838, 0.025, 0.000]\n",
      "Epoch 6 [24/340] - Loss: 12.282 [-12.257, 0.025, 0.000]\n",
      "Epoch 6 [25/340] - Loss: 12.913 [-12.887, 0.026, 0.000]\n",
      "Epoch 6 [26/340] - Loss: 13.011 [-12.986, 0.025, 0.000]\n",
      "Epoch 6 [27/340] - Loss: 12.673 [-12.648, 0.025, 0.000]\n",
      "Epoch 6 [28/340] - Loss: 12.266 [-12.241, 0.025, 0.000]\n",
      "Epoch 6 [29/340] - Loss: 12.396 [-12.371, 0.025, 0.000]\n",
      "Epoch 6 [30/340] - Loss: 12.861 [-12.835, 0.025, 0.000]\n",
      "Epoch 6 [31/340] - Loss: 12.876 [-12.851, 0.026, 0.000]\n",
      "Epoch 6 [32/340] - Loss: 12.644 [-12.618, 0.026, 0.000]\n",
      "Epoch 6 [33/340] - Loss: 13.194 [-13.169, 0.026, 0.000]\n",
      "Epoch 6 [34/340] - Loss: 13.485 [-13.460, 0.026, 0.000]\n",
      "Epoch 6 [35/340] - Loss: 12.250 [-12.225, 0.026, 0.000]\n",
      "Epoch 6 [36/340] - Loss: 12.806 [-12.780, 0.026, 0.000]\n",
      "Epoch 6 [37/340] - Loss: 12.545 [-12.520, 0.026, 0.000]\n",
      "Epoch 6 [38/340] - Loss: 11.733 [-11.707, 0.026, 0.000]\n",
      "Epoch 6 [39/340] - Loss: 12.913 [-12.888, 0.026, 0.000]\n",
      "Epoch 6 [40/340] - Loss: 13.171 [-13.145, 0.026, 0.000]\n",
      "Epoch 6 [41/340] - Loss: 12.283 [-12.257, 0.026, 0.000]\n",
      "Epoch 6 [42/340] - Loss: 11.982 [-11.956, 0.026, 0.000]\n",
      "Epoch 6 [43/340] - Loss: 12.965 [-12.939, 0.026, 0.000]\n",
      "Epoch 6 [44/340] - Loss: 12.526 [-12.500, 0.026, 0.000]\n",
      "Epoch 6 [45/340] - Loss: 12.521 [-12.495, 0.026, 0.000]\n",
      "Epoch 6 [46/340] - Loss: 12.443 [-12.417, 0.026, 0.000]\n",
      "Epoch 6 [47/340] - Loss: 13.216 [-13.190, 0.026, 0.000]\n",
      "Epoch 6 [48/340] - Loss: 12.372 [-12.346, 0.026, 0.000]\n",
      "Epoch 6 [49/340] - Loss: 11.715 [-11.688, 0.026, 0.000]\n",
      "Epoch 6 [50/340] - Loss: 11.991 [-11.965, 0.026, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [51/340] - Loss: 13.185 [-13.159, 0.026, 0.000]\n",
      "Epoch 6 [52/340] - Loss: 12.518 [-12.491, 0.026, 0.000]\n",
      "Epoch 6 [53/340] - Loss: 12.571 [-12.545, 0.026, 0.000]\n",
      "Epoch 6 [54/340] - Loss: 12.258 [-12.232, 0.026, 0.000]\n",
      "Epoch 6 [55/340] - Loss: 12.716 [-12.691, 0.026, 0.000]\n",
      "Epoch 6 [56/340] - Loss: 12.322 [-12.296, 0.026, 0.000]\n",
      "Epoch 6 [57/340] - Loss: 12.851 [-12.825, 0.026, 0.000]\n",
      "Epoch 6 [58/340] - Loss: 12.403 [-12.378, 0.026, 0.000]\n",
      "Epoch 6 [59/340] - Loss: 12.406 [-12.380, 0.026, 0.000]\n",
      "Epoch 6 [60/340] - Loss: 12.402 [-12.376, 0.026, 0.000]\n",
      "Epoch 6 [61/340] - Loss: 12.913 [-12.887, 0.026, 0.000]\n",
      "Epoch 6 [62/340] - Loss: 12.477 [-12.451, 0.026, 0.000]\n",
      "Epoch 6 [63/340] - Loss: 12.276 [-12.250, 0.026, 0.000]\n",
      "Epoch 6 [64/340] - Loss: 12.153 [-12.127, 0.026, 0.000]\n",
      "Epoch 6 [65/340] - Loss: 12.892 [-12.866, 0.026, 0.000]\n",
      "Epoch 6 [66/340] - Loss: 12.269 [-12.243, 0.026, 0.000]\n",
      "Epoch 6 [67/340] - Loss: 13.347 [-13.322, 0.026, 0.000]\n",
      "Epoch 6 [68/340] - Loss: 12.587 [-12.561, 0.026, 0.000]\n",
      "Epoch 6 [69/340] - Loss: 12.344 [-12.318, 0.026, 0.000]\n",
      "Epoch 6 [70/340] - Loss: 12.544 [-12.518, 0.026, 0.000]\n",
      "Epoch 6 [71/340] - Loss: 12.026 [-12.000, 0.026, 0.000]\n",
      "Epoch 6 [72/340] - Loss: 11.912 [-11.886, 0.026, 0.000]\n",
      "Epoch 6 [73/340] - Loss: 12.437 [-12.412, 0.026, 0.000]\n",
      "Epoch 6 [74/340] - Loss: 12.077 [-12.051, 0.026, 0.000]\n",
      "Epoch 6 [75/340] - Loss: 11.334 [-11.308, 0.026, 0.000]\n",
      "Epoch 6 [76/340] - Loss: 12.066 [-12.040, 0.026, 0.000]\n",
      "Epoch 6 [77/340] - Loss: 14.468 [-14.443, 0.026, 0.000]\n",
      "Epoch 6 [78/340] - Loss: 12.125 [-12.099, 0.026, 0.000]\n",
      "Epoch 6 [79/340] - Loss: 13.340 [-13.314, 0.026, 0.000]\n",
      "Epoch 6 [80/340] - Loss: 11.873 [-11.846, 0.027, 0.000]\n",
      "Epoch 6 [81/340] - Loss: 12.115 [-12.089, 0.026, 0.000]\n",
      "Epoch 6 [82/340] - Loss: 12.283 [-12.258, 0.026, 0.000]\n",
      "Epoch 6 [83/340] - Loss: 12.961 [-12.935, 0.026, 0.000]\n",
      "Epoch 6 [84/340] - Loss: 12.207 [-12.181, 0.026, 0.000]\n",
      "Epoch 6 [85/340] - Loss: 11.773 [-11.747, 0.026, 0.000]\n",
      "Epoch 6 [86/340] - Loss: 12.984 [-12.958, 0.026, 0.000]\n",
      "Epoch 6 [87/340] - Loss: 13.092 [-13.066, 0.026, 0.000]\n",
      "Epoch 6 [88/340] - Loss: 13.211 [-13.185, 0.026, 0.000]\n",
      "Epoch 6 [89/340] - Loss: 12.010 [-11.983, 0.026, 0.000]\n",
      "Epoch 6 [90/340] - Loss: 13.333 [-13.307, 0.026, 0.000]\n",
      "Epoch 6 [91/340] - Loss: 12.448 [-12.422, 0.026, 0.000]\n",
      "Epoch 6 [92/340] - Loss: 12.021 [-11.995, 0.026, 0.000]\n",
      "Epoch 6 [93/340] - Loss: 12.602 [-12.576, 0.026, 0.000]\n",
      "Epoch 6 [94/340] - Loss: 12.575 [-12.549, 0.026, 0.000]\n",
      "Epoch 6 [95/340] - Loss: 12.539 [-12.513, 0.026, 0.000]\n",
      "Epoch 6 [96/340] - Loss: 12.232 [-12.206, 0.026, 0.000]\n",
      "Epoch 6 [97/340] - Loss: 12.239 [-12.213, 0.026, 0.000]\n",
      "Epoch 6 [98/340] - Loss: 13.007 [-12.981, 0.026, 0.000]\n",
      "Epoch 6 [99/340] - Loss: 12.787 [-12.761, 0.026, 0.000]\n",
      "Epoch 6 [100/340] - Loss: 13.210 [-13.184, 0.026, 0.000]\n",
      "Epoch 6 [101/340] - Loss: 12.493 [-12.467, 0.026, 0.000]\n",
      "Epoch 6 [102/340] - Loss: 12.632 [-12.606, 0.026, 0.000]\n",
      "Epoch 6 [103/340] - Loss: 12.338 [-12.312, 0.026, 0.000]\n",
      "Epoch 6 [104/340] - Loss: 12.402 [-12.376, 0.026, 0.000]\n",
      "Epoch 6 [105/340] - Loss: 12.919 [-12.892, 0.026, 0.000]\n",
      "Epoch 6 [106/340] - Loss: 11.954 [-11.928, 0.026, 0.000]\n",
      "Epoch 6 [107/340] - Loss: 12.513 [-12.487, 0.026, 0.000]\n",
      "Epoch 6 [108/340] - Loss: 12.992 [-12.966, 0.026, 0.000]\n",
      "Epoch 6 [109/340] - Loss: 12.876 [-12.849, 0.026, 0.000]\n",
      "Epoch 6 [110/340] - Loss: 12.935 [-12.908, 0.027, 0.000]\n",
      "Epoch 6 [111/340] - Loss: 13.241 [-13.215, 0.026, 0.000]\n",
      "Epoch 6 [112/340] - Loss: 12.162 [-12.136, 0.026, 0.000]\n",
      "Epoch 6 [113/340] - Loss: 12.559 [-12.532, 0.026, 0.000]\n",
      "Epoch 6 [114/340] - Loss: 13.018 [-12.991, 0.026, 0.000]\n",
      "Epoch 6 [115/340] - Loss: 12.469 [-12.443, 0.026, 0.000]\n",
      "Epoch 6 [116/340] - Loss: 13.205 [-13.179, 0.026, 0.000]\n",
      "Epoch 6 [117/340] - Loss: 12.538 [-12.512, 0.026, 0.000]\n",
      "Epoch 6 [118/340] - Loss: 11.872 [-11.846, 0.026, 0.000]\n",
      "Epoch 6 [119/340] - Loss: 11.725 [-11.699, 0.026, 0.000]\n",
      "Epoch 6 [120/340] - Loss: 12.246 [-12.220, 0.026, 0.000]\n",
      "Epoch 6 [121/340] - Loss: 12.710 [-12.684, 0.026, 0.000]\n",
      "Epoch 6 [122/340] - Loss: 12.824 [-12.797, 0.027, 0.000]\n",
      "Epoch 6 [123/340] - Loss: 11.859 [-11.833, 0.026, 0.000]\n",
      "Epoch 6 [124/340] - Loss: 13.198 [-13.172, 0.026, 0.000]\n",
      "Epoch 6 [125/340] - Loss: 13.800 [-13.774, 0.026, 0.000]\n",
      "Epoch 6 [126/340] - Loss: 13.088 [-13.062, 0.026, 0.000]\n",
      "Epoch 6 [127/340] - Loss: 12.625 [-12.599, 0.026, 0.000]\n",
      "Epoch 6 [128/340] - Loss: 12.351 [-12.325, 0.026, 0.000]\n",
      "Epoch 6 [129/340] - Loss: 13.451 [-13.425, 0.026, 0.000]\n",
      "Epoch 6 [130/340] - Loss: 13.079 [-13.052, 0.026, 0.000]\n",
      "Epoch 6 [131/340] - Loss: 12.276 [-12.250, 0.026, 0.000]\n",
      "Epoch 6 [132/340] - Loss: 12.030 [-12.003, 0.026, 0.000]\n",
      "Epoch 6 [133/340] - Loss: 12.338 [-12.311, 0.026, 0.000]\n",
      "Epoch 6 [134/340] - Loss: 13.584 [-13.557, 0.026, 0.000]\n",
      "Epoch 6 [135/340] - Loss: 13.541 [-13.514, 0.026, 0.000]\n",
      "Epoch 6 [136/340] - Loss: 13.152 [-13.126, 0.026, 0.000]\n",
      "Epoch 6 [137/340] - Loss: 11.802 [-11.776, 0.026, 0.000]\n",
      "Epoch 6 [138/340] - Loss: 12.798 [-12.772, 0.026, 0.000]\n",
      "Epoch 6 [139/340] - Loss: 13.536 [-13.509, 0.027, 0.000]\n",
      "Epoch 6 [140/340] - Loss: 12.781 [-12.754, 0.026, 0.000]\n",
      "Epoch 6 [141/340] - Loss: 12.802 [-12.775, 0.026, 0.000]\n",
      "Epoch 6 [142/340] - Loss: 12.218 [-12.192, 0.027, 0.000]\n",
      "Epoch 6 [143/340] - Loss: 12.891 [-12.865, 0.026, 0.000]\n",
      "Epoch 6 [144/340] - Loss: 12.883 [-12.856, 0.026, 0.000]\n",
      "Epoch 6 [145/340] - Loss: 13.067 [-13.041, 0.026, 0.000]\n",
      "Epoch 6 [146/340] - Loss: 13.620 [-13.594, 0.026, 0.000]\n",
      "Epoch 6 [147/340] - Loss: 12.168 [-12.142, 0.026, 0.000]\n",
      "Epoch 6 [148/340] - Loss: 12.426 [-12.400, 0.026, 0.000]\n",
      "Epoch 6 [149/340] - Loss: 12.549 [-12.523, 0.026, 0.000]\n",
      "Epoch 6 [150/340] - Loss: 14.196 [-14.170, 0.026, 0.000]\n",
      "Epoch 6 [151/340] - Loss: 12.500 [-12.473, 0.027, 0.000]\n",
      "Epoch 6 [152/340] - Loss: 12.417 [-12.391, 0.026, 0.000]\n",
      "Epoch 6 [153/340] - Loss: 12.219 [-12.193, 0.026, 0.000]\n",
      "Epoch 6 [154/340] - Loss: 12.175 [-12.149, 0.026, 0.000]\n",
      "Epoch 6 [155/340] - Loss: 12.049 [-12.023, 0.027, 0.000]\n",
      "Epoch 6 [156/340] - Loss: 12.606 [-12.580, 0.026, 0.000]\n",
      "Epoch 6 [157/340] - Loss: 12.765 [-12.739, 0.026, 0.000]\n",
      "Epoch 6 [158/340] - Loss: 12.831 [-12.805, 0.026, 0.000]\n",
      "Epoch 6 [159/340] - Loss: 12.428 [-12.402, 0.026, 0.000]\n",
      "Epoch 6 [160/340] - Loss: 13.519 [-13.493, 0.026, 0.000]\n",
      "Epoch 6 [161/340] - Loss: 12.050 [-12.024, 0.026, 0.000]\n",
      "Epoch 6 [162/340] - Loss: 12.584 [-12.558, 0.026, 0.000]\n",
      "Epoch 6 [163/340] - Loss: 12.355 [-12.329, 0.026, 0.000]\n",
      "Epoch 6 [164/340] - Loss: 12.539 [-12.513, 0.026, 0.000]\n",
      "Epoch 6 [165/340] - Loss: 12.335 [-12.309, 0.026, 0.000]\n",
      "Epoch 6 [166/340] - Loss: 13.598 [-13.571, 0.026, 0.000]\n",
      "Epoch 6 [167/340] - Loss: 11.583 [-11.557, 0.026, 0.000]\n",
      "Epoch 6 [168/340] - Loss: 12.583 [-12.557, 0.026, 0.000]\n",
      "Epoch 6 [169/340] - Loss: 13.610 [-13.583, 0.026, 0.000]\n",
      "Epoch 6 [170/340] - Loss: 12.168 [-12.141, 0.026, 0.000]\n",
      "Epoch 6 [171/340] - Loss: 13.378 [-13.352, 0.026, 0.000]\n",
      "Epoch 6 [172/340] - Loss: 12.315 [-12.289, 0.026, 0.000]\n",
      "Epoch 6 [173/340] - Loss: 13.807 [-13.781, 0.026, 0.000]\n",
      "Epoch 6 [174/340] - Loss: 13.829 [-13.803, 0.026, 0.000]\n",
      "Epoch 6 [175/340] - Loss: 12.179 [-12.152, 0.027, 0.000]\n",
      "Epoch 6 [176/340] - Loss: 13.277 [-13.251, 0.026, 0.000]\n",
      "Epoch 6 [177/340] - Loss: 14.564 [-14.538, 0.026, 0.000]\n",
      "Epoch 6 [178/340] - Loss: 12.006 [-11.980, 0.026, 0.000]\n",
      "Epoch 6 [179/340] - Loss: 12.298 [-12.272, 0.026, 0.000]\n",
      "Epoch 6 [180/340] - Loss: 12.778 [-12.752, 0.026, 0.000]\n",
      "Epoch 6 [181/340] - Loss: 13.151 [-13.125, 0.026, 0.000]\n",
      "Epoch 6 [182/340] - Loss: 12.577 [-12.551, 0.026, 0.000]\n",
      "Epoch 6 [183/340] - Loss: 13.363 [-13.337, 0.026, 0.000]\n",
      "Epoch 6 [184/340] - Loss: 13.206 [-13.180, 0.026, 0.000]\n",
      "Epoch 6 [185/340] - Loss: 11.419 [-11.393, 0.026, 0.000]\n",
      "Epoch 6 [186/340] - Loss: 11.896 [-11.870, 0.026, 0.000]\n",
      "Epoch 6 [187/340] - Loss: 12.972 [-12.945, 0.026, 0.000]\n",
      "Epoch 6 [188/340] - Loss: 14.993 [-14.967, 0.026, 0.000]\n",
      "Epoch 6 [189/340] - Loss: 11.993 [-11.967, 0.026, 0.000]\n",
      "Epoch 6 [190/340] - Loss: 12.286 [-12.260, 0.026, 0.000]\n",
      "Epoch 6 [191/340] - Loss: 12.244 [-12.217, 0.027, 0.000]\n",
      "Epoch 6 [192/340] - Loss: 12.718 [-12.692, 0.026, 0.000]\n",
      "Epoch 6 [193/340] - Loss: 11.882 [-11.856, 0.026, 0.000]\n",
      "Epoch 6 [194/340] - Loss: 12.368 [-12.342, 0.026, 0.000]\n",
      "Epoch 6 [195/340] - Loss: 11.971 [-11.945, 0.026, 0.000]\n",
      "Epoch 6 [196/340] - Loss: 12.375 [-12.349, 0.026, 0.000]\n",
      "Epoch 6 [197/340] - Loss: 12.639 [-12.613, 0.026, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [198/340] - Loss: 12.351 [-12.325, 0.026, 0.000]\n",
      "Epoch 6 [199/340] - Loss: 12.576 [-12.550, 0.026, 0.000]\n",
      "Epoch 6 [200/340] - Loss: 13.028 [-13.002, 0.026, 0.000]\n",
      "Epoch 6 [201/340] - Loss: 12.523 [-12.497, 0.026, 0.000]\n",
      "Epoch 6 [202/340] - Loss: 13.621 [-13.595, 0.026, 0.000]\n",
      "Epoch 6 [203/340] - Loss: 12.466 [-12.440, 0.026, 0.000]\n",
      "Epoch 6 [204/340] - Loss: 13.231 [-13.205, 0.026, 0.000]\n",
      "Epoch 6 [205/340] - Loss: 12.857 [-12.831, 0.026, 0.000]\n",
      "Epoch 6 [206/340] - Loss: 12.429 [-12.403, 0.026, 0.000]\n",
      "Epoch 6 [207/340] - Loss: 12.896 [-12.870, 0.026, 0.000]\n",
      "Epoch 6 [208/340] - Loss: 12.450 [-12.423, 0.026, 0.000]\n",
      "Epoch 6 [209/340] - Loss: 12.141 [-12.115, 0.026, 0.000]\n",
      "Epoch 6 [210/340] - Loss: 12.646 [-12.620, 0.026, 0.000]\n",
      "Epoch 6 [211/340] - Loss: 12.611 [-12.585, 0.026, 0.000]\n",
      "Epoch 6 [212/340] - Loss: 12.229 [-12.203, 0.026, 0.000]\n",
      "Epoch 6 [213/340] - Loss: 12.090 [-12.063, 0.026, 0.000]\n",
      "Epoch 6 [214/340] - Loss: 11.643 [-11.617, 0.026, 0.000]\n",
      "Epoch 6 [215/340] - Loss: 13.334 [-13.308, 0.026, 0.000]\n",
      "Epoch 6 [216/340] - Loss: 12.517 [-12.491, 0.026, 0.000]\n",
      "Epoch 6 [217/340] - Loss: 11.978 [-11.952, 0.026, 0.000]\n",
      "Epoch 6 [218/340] - Loss: 12.701 [-12.675, 0.026, 0.000]\n",
      "Epoch 6 [219/340] - Loss: 12.507 [-12.481, 0.026, 0.000]\n",
      "Epoch 6 [220/340] - Loss: 14.084 [-14.058, 0.026, 0.000]\n",
      "Epoch 6 [221/340] - Loss: 13.752 [-13.726, 0.026, 0.000]\n",
      "Epoch 6 [222/340] - Loss: 12.969 [-12.942, 0.027, 0.000]\n",
      "Epoch 6 [223/340] - Loss: 12.286 [-12.260, 0.026, 0.000]\n",
      "Epoch 6 [224/340] - Loss: 12.543 [-12.517, 0.026, 0.000]\n",
      "Epoch 6 [225/340] - Loss: 14.056 [-14.030, 0.026, 0.000]\n",
      "Epoch 6 [226/340] - Loss: 12.509 [-12.483, 0.026, 0.000]\n",
      "Epoch 6 [227/340] - Loss: 12.329 [-12.303, 0.026, 0.000]\n",
      "Epoch 6 [228/340] - Loss: 12.699 [-12.672, 0.026, 0.000]\n",
      "Epoch 6 [229/340] - Loss: 12.518 [-12.492, 0.026, 0.000]\n",
      "Epoch 6 [230/340] - Loss: 11.561 [-11.535, 0.026, 0.000]\n",
      "Epoch 6 [231/340] - Loss: 11.784 [-11.758, 0.026, 0.000]\n",
      "Epoch 6 [232/340] - Loss: 12.310 [-12.284, 0.026, 0.000]\n",
      "Epoch 6 [233/340] - Loss: 12.571 [-12.545, 0.026, 0.000]\n",
      "Epoch 6 [234/340] - Loss: 12.494 [-12.467, 0.026, 0.000]\n",
      "Epoch 6 [235/340] - Loss: 12.610 [-12.583, 0.026, 0.000]\n",
      "Epoch 6 [236/340] - Loss: 12.306 [-12.279, 0.026, 0.000]\n",
      "Epoch 6 [237/340] - Loss: 11.612 [-11.586, 0.026, 0.000]\n",
      "Epoch 6 [238/340] - Loss: 13.189 [-13.162, 0.026, 0.000]\n",
      "Epoch 6 [239/340] - Loss: 13.249 [-13.223, 0.027, 0.000]\n",
      "Epoch 6 [240/340] - Loss: 13.227 [-13.201, 0.027, 0.000]\n",
      "Epoch 6 [241/340] - Loss: 11.731 [-11.704, 0.026, 0.000]\n",
      "Epoch 6 [242/340] - Loss: 12.505 [-12.478, 0.027, 0.000]\n",
      "Epoch 6 [243/340] - Loss: 12.599 [-12.572, 0.027, 0.000]\n",
      "Epoch 6 [244/340] - Loss: 12.524 [-12.498, 0.026, 0.000]\n",
      "Epoch 6 [245/340] - Loss: 12.902 [-12.876, 0.026, 0.000]\n",
      "Epoch 6 [246/340] - Loss: 12.878 [-12.851, 0.027, 0.000]\n",
      "Epoch 6 [247/340] - Loss: 12.177 [-12.151, 0.027, 0.000]\n",
      "Epoch 6 [248/340] - Loss: 11.572 [-11.545, 0.027, 0.000]\n",
      "Epoch 6 [249/340] - Loss: 12.622 [-12.595, 0.027, 0.000]\n",
      "Epoch 6 [250/340] - Loss: 12.363 [-12.335, 0.027, 0.000]\n",
      "Epoch 6 [251/340] - Loss: 12.188 [-12.161, 0.027, 0.000]\n",
      "Epoch 6 [252/340] - Loss: 12.905 [-12.878, 0.027, 0.000]\n",
      "Epoch 6 [253/340] - Loss: 12.439 [-12.412, 0.027, 0.000]\n",
      "Epoch 6 [254/340] - Loss: 12.561 [-12.535, 0.027, 0.000]\n",
      "Epoch 6 [255/340] - Loss: 12.846 [-12.819, 0.027, 0.000]\n",
      "Epoch 6 [256/340] - Loss: 11.975 [-11.949, 0.027, 0.000]\n",
      "Epoch 6 [257/340] - Loss: 12.817 [-12.790, 0.027, 0.000]\n",
      "Epoch 6 [258/340] - Loss: 12.562 [-12.535, 0.027, 0.000]\n",
      "Epoch 6 [259/340] - Loss: 12.205 [-12.178, 0.027, 0.000]\n",
      "Epoch 6 [260/340] - Loss: 12.327 [-12.300, 0.027, 0.000]\n",
      "Epoch 6 [261/340] - Loss: 13.339 [-13.311, 0.027, 0.000]\n",
      "Epoch 6 [262/340] - Loss: 12.952 [-12.925, 0.027, 0.000]\n",
      "Epoch 6 [263/340] - Loss: 12.515 [-12.488, 0.027, 0.000]\n",
      "Epoch 6 [264/340] - Loss: 12.175 [-12.148, 0.027, 0.000]\n",
      "Epoch 6 [265/340] - Loss: 11.810 [-11.783, 0.027, 0.000]\n",
      "Epoch 6 [266/340] - Loss: 13.689 [-13.662, 0.027, 0.000]\n",
      "Epoch 6 [267/340] - Loss: 12.135 [-12.108, 0.027, 0.000]\n",
      "Epoch 6 [268/340] - Loss: 12.238 [-12.211, 0.027, 0.000]\n",
      "Epoch 6 [269/340] - Loss: 12.820 [-12.793, 0.027, 0.000]\n",
      "Epoch 6 [270/340] - Loss: 11.617 [-11.590, 0.027, 0.000]\n",
      "Epoch 6 [271/340] - Loss: 12.097 [-12.071, 0.027, 0.000]\n",
      "Epoch 6 [272/340] - Loss: 12.605 [-12.578, 0.027, 0.000]\n",
      "Epoch 6 [273/340] - Loss: 12.653 [-12.626, 0.027, 0.000]\n",
      "Epoch 6 [274/340] - Loss: 12.150 [-12.123, 0.027, 0.000]\n",
      "Epoch 6 [275/340] - Loss: 14.497 [-14.470, 0.027, 0.000]\n",
      "Epoch 6 [276/340] - Loss: 12.036 [-12.009, 0.027, 0.000]\n",
      "Epoch 6 [277/340] - Loss: 13.226 [-13.199, 0.027, 0.000]\n",
      "Epoch 6 [278/340] - Loss: 12.153 [-12.126, 0.027, 0.000]\n",
      "Epoch 6 [279/340] - Loss: 12.340 [-12.313, 0.027, 0.000]\n",
      "Epoch 6 [280/340] - Loss: 11.836 [-11.809, 0.027, 0.000]\n",
      "Epoch 6 [281/340] - Loss: 13.358 [-13.331, 0.027, 0.000]\n",
      "Epoch 6 [282/340] - Loss: 12.722 [-12.695, 0.027, 0.000]\n",
      "Epoch 6 [283/340] - Loss: 12.077 [-12.050, 0.027, 0.000]\n",
      "Epoch 6 [284/340] - Loss: 14.447 [-14.420, 0.027, 0.000]\n",
      "Epoch 6 [285/340] - Loss: 11.912 [-11.885, 0.027, 0.000]\n",
      "Epoch 6 [286/340] - Loss: 13.268 [-13.241, 0.027, 0.000]\n",
      "Epoch 6 [287/340] - Loss: 13.087 [-13.060, 0.027, 0.000]\n",
      "Epoch 6 [288/340] - Loss: 12.109 [-12.083, 0.027, 0.000]\n",
      "Epoch 6 [289/340] - Loss: 12.330 [-12.303, 0.027, 0.000]\n",
      "Epoch 6 [290/340] - Loss: 13.052 [-13.025, 0.027, 0.000]\n",
      "Epoch 6 [291/340] - Loss: 12.413 [-12.386, 0.027, 0.000]\n",
      "Epoch 6 [292/340] - Loss: 12.732 [-12.705, 0.027, 0.000]\n",
      "Epoch 6 [293/340] - Loss: 12.363 [-12.336, 0.027, 0.000]\n",
      "Epoch 6 [294/340] - Loss: 12.218 [-12.191, 0.027, 0.000]\n",
      "Epoch 6 [295/340] - Loss: 11.885 [-11.858, 0.027, 0.000]\n",
      "Epoch 6 [296/340] - Loss: 12.818 [-12.791, 0.027, 0.000]\n",
      "Epoch 6 [297/340] - Loss: 13.148 [-13.122, 0.027, 0.000]\n",
      "Epoch 6 [298/340] - Loss: 12.119 [-12.092, 0.027, 0.000]\n",
      "Epoch 6 [299/340] - Loss: 13.410 [-13.383, 0.027, 0.000]\n",
      "Epoch 6 [300/340] - Loss: 12.415 [-12.389, 0.027, 0.000]\n",
      "Epoch 6 [301/340] - Loss: 11.959 [-11.932, 0.027, 0.000]\n",
      "Epoch 6 [302/340] - Loss: 12.139 [-12.112, 0.027, 0.000]\n",
      "Epoch 6 [303/340] - Loss: 14.071 [-14.044, 0.027, 0.000]\n",
      "Epoch 6 [304/340] - Loss: 12.101 [-12.074, 0.027, 0.000]\n",
      "Epoch 6 [305/340] - Loss: 12.803 [-12.776, 0.027, 0.000]\n",
      "Epoch 6 [306/340] - Loss: 12.535 [-12.509, 0.027, 0.000]\n",
      "Epoch 6 [307/340] - Loss: 12.942 [-12.915, 0.027, 0.000]\n",
      "Epoch 6 [308/340] - Loss: 13.571 [-13.544, 0.027, 0.000]\n",
      "Epoch 6 [309/340] - Loss: 12.205 [-12.178, 0.027, 0.000]\n",
      "Epoch 6 [310/340] - Loss: 12.494 [-12.467, 0.027, 0.000]\n",
      "Epoch 6 [311/340] - Loss: 11.954 [-11.927, 0.027, 0.000]\n",
      "Epoch 6 [312/340] - Loss: 12.401 [-12.374, 0.027, 0.000]\n",
      "Epoch 6 [313/340] - Loss: 12.716 [-12.689, 0.027, 0.000]\n",
      "Epoch 6 [314/340] - Loss: 12.072 [-12.045, 0.027, 0.000]\n",
      "Epoch 6 [315/340] - Loss: 11.409 [-11.382, 0.027, 0.000]\n",
      "Epoch 6 [316/340] - Loss: 12.746 [-12.720, 0.027, 0.000]\n",
      "Epoch 6 [317/340] - Loss: 13.125 [-13.098, 0.027, 0.000]\n",
      "Epoch 6 [318/340] - Loss: 13.032 [-13.005, 0.027, 0.000]\n",
      "Epoch 6 [319/340] - Loss: 11.730 [-11.703, 0.027, 0.000]\n",
      "Epoch 6 [320/340] - Loss: 14.813 [-14.786, 0.027, 0.000]\n",
      "Epoch 6 [321/340] - Loss: 11.726 [-11.699, 0.027, 0.000]\n",
      "Epoch 6 [322/340] - Loss: 12.780 [-12.752, 0.028, 0.000]\n",
      "Epoch 6 [323/340] - Loss: 13.701 [-13.674, 0.027, 0.000]\n",
      "Epoch 6 [324/340] - Loss: 13.782 [-13.755, 0.027, 0.000]\n",
      "Epoch 6 [325/340] - Loss: 11.286 [-11.259, 0.027, 0.000]\n",
      "Epoch 6 [326/340] - Loss: 12.609 [-12.582, 0.027, 0.000]\n",
      "Epoch 6 [327/340] - Loss: 13.036 [-13.009, 0.027, 0.000]\n",
      "Epoch 6 [328/340] - Loss: 12.146 [-12.119, 0.027, 0.000]\n",
      "Epoch 6 [329/340] - Loss: 12.426 [-12.399, 0.027, 0.000]\n",
      "Epoch 6 [330/340] - Loss: 12.537 [-12.510, 0.027, 0.000]\n",
      "Epoch 6 [331/340] - Loss: 12.152 [-12.125, 0.027, 0.000]\n",
      "Epoch 6 [332/340] - Loss: 11.750 [-11.723, 0.027, 0.000]\n",
      "Epoch 6 [333/340] - Loss: 12.740 [-12.713, 0.027, 0.000]\n",
      "Epoch 6 [334/340] - Loss: 13.476 [-13.448, 0.027, 0.000]\n",
      "Epoch 6 [335/340] - Loss: 12.847 [-12.819, 0.027, 0.000]\n",
      "Epoch 6 [336/340] - Loss: 12.434 [-12.406, 0.027, 0.000]\n",
      "Epoch 6 [337/340] - Loss: 14.323 [-14.295, 0.027, 0.000]\n",
      "Epoch 6 [338/340] - Loss: 12.132 [-12.104, 0.027, 0.000]\n",
      "Epoch 6 [339/340] - Loss: 12.791 [-12.764, 0.027, 0.000]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 1 epochs of training in this tutorial\n",
    "num_epochs = 6\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Because the grid is relatively small, we turn off the Toeplitz matrix multiplication and just perform them directly\n",
    "        # We find this to be more efficient when the grid is very small.\n",
    "        with gpytorch.settings.use_toeplitz(False):\n",
    "            output = model(x_batch)\n",
    "            log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "            loss = -(log_lik - kl_div + log_prior)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader), loss.item(), log_lik.item(), kl_div.item(), log_prior.item()))\n",
    "\n",
    "        # The actual optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`) using the standard SKI testing code, with no acceleration or precomputation. Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although our other tutorials demonstrate how to do this (for example, see the CIFAR tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 8.008056640625\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
