{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use SVGP stochastic variational regression ((https://arxiv.org/pdf/1411.2005.pdf)) to rapidly train using minibatches on the `3droad` UCI dataset with hundreds of thousands of training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `song` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~136 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the SVGP Model\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. In this example, because we will be learning the inducing point locations, we'll be using a base `VariationalStrategy` with `learn_inducing_locations=True`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import HalfWhitenedVariationalStrategy\n",
    "\n",
    "class GPModel(AbstractVariationalGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = HalfWhitenedVariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "inducing_points = train_x[:500, :]\n",
    "model = GPModel(inducing_points=inducing_points).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalMarginalLogLikelihood` or ELBO), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NormBackward0>)\n",
      "tensor(6.1922e-05, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "Epoch 1 [0/340] - Loss: 233.928 [-233.928, 0.000, 0.000]- Time: 0.648\n",
      "Epoch 1 [1/340] - Loss: 269.385 [-269.384, 0.000, 0.000]- Time: 0.023\n",
      "Epoch 1 [2/340] - Loss: 227.632 [-227.632, 0.000, 0.000]- Time: 0.026\n",
      "Epoch 1 [3/340] - Loss: 254.816 [-254.816, 0.000, 0.000]- Time: 0.023\n",
      "Epoch 1 [4/340] - Loss: 245.082 [-245.082, 0.000, 0.000]- Time: 0.024\n",
      "Epoch 1 [5/340] - Loss: 237.467 [-237.467, 0.000, 0.000]- Time: 0.025\n",
      "Epoch 1 [6/340] - Loss: 244.266 [-244.266, 0.000, 0.000]- Time: 0.023\n",
      "Epoch 1 [7/340] - Loss: 229.941 [-229.941, 0.000, 0.000]- Time: 0.028\n",
      "Epoch 1 [8/340] - Loss: 252.439 [-252.439, 0.000, 0.000]- Time: 0.027\n",
      "Epoch 1 [9/340] - Loss: 213.721 [-213.721, 0.000, 0.000]- Time: 0.026\n",
      "Epoch 1 [10/340] - Loss: 239.035 [-239.035, 0.000, 0.000]- Time: 0.027\n",
      "Epoch 1 [11/340] - Loss: 227.559 [-227.559, 0.000, 0.000]- Time: 0.023\n",
      "Epoch 1 [12/340] - Loss: 230.745 [-230.745, 0.000, 0.000]- Time: 0.025\n",
      "Epoch 1 [13/340] - Loss: 250.312 [-250.311, 0.000, 0.000]- Time: 0.026\n",
      "Epoch 1 [14/340] - Loss: 231.086 [-231.085, 0.000, 0.000]- Time: 0.022\n",
      "Epoch 1 [15/340] - Loss: 213.797 [-213.797, 0.000, 0.000]- Time: 0.026\n",
      "Epoch 1 [16/340] - Loss: 230.358 [-230.357, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [17/340] - Loss: 206.769 [-206.768, 0.001, 0.000]- Time: 0.024\n",
      "Epoch 1 [18/340] - Loss: 205.451 [-205.450, 0.001, 0.000]- Time: 0.024\n",
      "Epoch 1 [19/340] - Loss: 229.056 [-229.055, 0.001, 0.000]- Time: 0.024\n",
      "Epoch 1 [20/340] - Loss: 204.084 [-204.083, 0.001, 0.000]- Time: 0.025\n",
      "Epoch 1 [21/340] - Loss: 222.125 [-222.125, 0.001, 0.000]- Time: 0.027\n",
      "Epoch 1 [22/340] - Loss: 210.821 [-210.820, 0.001, 0.000]- Time: 0.025\n",
      "Epoch 1 [23/340] - Loss: 198.012 [-198.011, 0.001, 0.000]- Time: 0.028\n",
      "Epoch 1 [24/340] - Loss: 216.220 [-216.220, 0.001, 0.000]- Time: 0.025\n",
      "Epoch 1 [25/340] - Loss: 221.759 [-221.758, 0.001, 0.000]- Time: 0.026\n",
      "Epoch 1 [26/340] - Loss: 205.631 [-205.630, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [27/340] - Loss: 194.125 [-194.124, 0.001, 0.000]- Time: 0.028\n",
      "Epoch 1 [28/340] - Loss: 182.792 [-182.791, 0.001, 0.000]- Time: 0.027\n",
      "Epoch 1 [29/340] - Loss: 176.908 [-176.907, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [30/340] - Loss: 182.166 [-182.165, 0.001, 0.000]- Time: 0.024\n",
      "Epoch 1 [31/340] - Loss: 185.529 [-185.528, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [32/340] - Loss: 211.631 [-211.630, 0.001, 0.000]- Time: 0.024\n",
      "Epoch 1 [33/340] - Loss: 192.619 [-192.618, 0.001, 0.000]- Time: 0.028\n",
      "Epoch 1 [34/340] - Loss: 182.733 [-182.732, 0.001, 0.000]- Time: 0.025\n",
      "Epoch 1 [35/340] - Loss: 193.728 [-193.727, 0.001, 0.000]- Time: 0.028\n",
      "Epoch 1 [36/340] - Loss: 193.924 [-193.923, 0.001, 0.000]- Time: 0.026\n",
      "Epoch 1 [37/340] - Loss: 198.603 [-198.602, 0.001, 0.000]- Time: 0.028\n",
      "Epoch 1 [38/340] - Loss: 171.242 [-171.241, 0.001, 0.000]- Time: 0.027\n",
      "Epoch 1 [39/340] - Loss: 190.994 [-190.992, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [40/340] - Loss: 178.288 [-178.287, 0.001, 0.000]- Time: 0.028\n",
      "Epoch 1 [41/340] - Loss: 183.661 [-183.659, 0.001, 0.000]- Time: 0.024\n",
      "Epoch 1 [42/340] - Loss: 178.948 [-178.947, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [43/340] - Loss: 164.850 [-164.849, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [44/340] - Loss: 171.369 [-171.368, 0.001, 0.000]- Time: 0.027\n",
      "Epoch 1 [45/340] - Loss: 162.279 [-162.277, 0.001, 0.000]- Time: 0.024\n",
      "Epoch 1 [46/340] - Loss: 173.770 [-173.769, 0.001, 0.000]- Time: 0.022\n",
      "Epoch 1 [47/340] - Loss: 161.936 [-161.935, 0.001, 0.000]- Time: 0.023\n",
      "Epoch 1 [48/340] - Loss: 170.400 [-170.398, 0.001, 0.000]- Time: 0.026\n",
      "Epoch 1 [49/340] - Loss: 148.030 [-148.029, 0.002, 0.000]- Time: 0.027\n",
      "Epoch 1 [50/340] - Loss: 163.663 [-163.662, 0.002, 0.000]- Time: 0.022\n",
      "Epoch 1 [51/340] - Loss: 165.677 [-165.676, 0.002, 0.000]- Time: 0.021\n",
      "Epoch 1 [52/340] - Loss: 162.828 [-162.826, 0.002, 0.000]- Time: 0.022\n",
      "Epoch 1 [53/340] - Loss: 153.904 [-153.902, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [54/340] - Loss: 171.579 [-171.577, 0.002, 0.000]- Time: 0.027\n",
      "Epoch 1 [55/340] - Loss: 156.454 [-156.452, 0.002, 0.000]- Time: 0.023\n",
      "Epoch 1 [56/340] - Loss: 153.705 [-153.703, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [57/340] - Loss: 160.920 [-160.919, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [58/340] - Loss: 148.741 [-148.740, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [59/340] - Loss: 141.430 [-141.428, 0.002, 0.000]- Time: 0.026\n",
      "Epoch 1 [60/340] - Loss: 155.623 [-155.622, 0.002, 0.000]- Time: 0.024\n",
      "Epoch 1 [61/340] - Loss: 160.097 [-160.096, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [62/340] - Loss: 152.257 [-152.255, 0.002, 0.000]- Time: 0.023\n",
      "Epoch 1 [63/340] - Loss: 140.015 [-140.013, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [64/340] - Loss: 134.845 [-134.843, 0.002, 0.000]- Time: 0.023\n",
      "Epoch 1 [65/340] - Loss: 150.165 [-150.163, 0.002, 0.000]- Time: 0.028\n",
      "Epoch 1 [66/340] - Loss: 140.156 [-140.154, 0.002, 0.000]- Time: 0.029\n",
      "Epoch 1 [67/340] - Loss: 126.881 [-126.879, 0.002, 0.000]- Time: 0.024\n",
      "Epoch 1 [68/340] - Loss: 130.107 [-130.105, 0.002, 0.000]- Time: 0.026\n",
      "Epoch 1 [69/340] - Loss: 134.628 [-134.626, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [70/340] - Loss: 125.704 [-125.702, 0.002, 0.000]- Time: 0.029\n",
      "Epoch 1 [71/340] - Loss: 146.022 [-146.020, 0.002, 0.000]- Time: 0.026\n",
      "Epoch 1 [72/340] - Loss: 120.732 [-120.730, 0.002, 0.000]- Time: 0.022\n",
      "Epoch 1 [73/340] - Loss: 143.857 [-143.855, 0.002, 0.000]- Time: 0.027\n",
      "Epoch 1 [74/340] - Loss: 136.225 [-136.222, 0.002, 0.000]- Time: 0.024\n",
      "Epoch 1 [75/340] - Loss: 138.308 [-138.306, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [76/340] - Loss: 121.802 [-121.800, 0.002, 0.000]- Time: 0.024\n",
      "Epoch 1 [77/340] - Loss: 130.222 [-130.220, 0.002, 0.000]- Time: 0.023\n",
      "Epoch 1 [78/340] - Loss: 117.684 [-117.681, 0.002, 0.000]- Time: 0.023\n",
      "Epoch 1 [79/340] - Loss: 136.271 [-136.269, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [80/340] - Loss: 123.272 [-123.270, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [81/340] - Loss: 108.149 [-108.146, 0.002, 0.000]- Time: 0.022\n",
      "Epoch 1 [82/340] - Loss: 122.442 [-122.440, 0.002, 0.000]- Time: 0.026\n",
      "Epoch 1 [83/340] - Loss: 128.284 [-128.282, 0.002, 0.000]- Time: 0.026\n",
      "Epoch 1 [84/340] - Loss: 106.483 [-106.480, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [85/340] - Loss: 120.896 [-120.894, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [86/340] - Loss: 111.347 [-111.344, 0.002, 0.000]- Time: 0.026\n",
      "Epoch 1 [87/340] - Loss: 110.026 [-110.023, 0.002, 0.000]- Time: 0.024\n",
      "Epoch 1 [88/340] - Loss: 114.485 [-114.483, 0.002, 0.000]- Time: 0.025\n",
      "Epoch 1 [89/340] - Loss: 120.656 [-120.654, 0.002, 0.000]- Time: 0.022\n",
      "Epoch 1 [90/340] - Loss: 114.068 [-114.065, 0.002, 0.000]- Time: 0.021\n",
      "Epoch 1 [91/340] - Loss: 112.034 [-112.031, 0.002, 0.000]- Time: 0.022\n",
      "Epoch 1 [92/340] - Loss: 109.372 [-109.370, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [93/340] - Loss: 101.167 [-101.164, 0.003, 0.000]- Time: 0.026\n",
      "Epoch 1 [94/340] - Loss: 103.878 [-103.875, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [95/340] - Loss: 112.009 [-112.006, 0.003, 0.000]- Time: 0.026\n",
      "Epoch 1 [96/340] - Loss: 105.575 [-105.573, 0.003, 0.000]- Time: 0.025\n",
      "Epoch 1 [97/340] - Loss: 119.472 [-119.469, 0.003, 0.000]- Time: 0.025\n",
      "Epoch 1 [98/340] - Loss: 114.251 [-114.249, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [99/340] - Loss: 107.872 [-107.870, 0.003, 0.000]- Time: 0.022\n",
      "Epoch 1 [100/340] - Loss: 104.143 [-104.141, 0.003, 0.000]- Time: 0.025\n",
      "Epoch 1 [101/340] - Loss: 115.913 [-115.911, 0.003, 0.000]- Time: 0.026\n",
      "Epoch 1 [102/340] - Loss: 100.020 [-100.017, 0.003, 0.000]- Time: 0.026\n",
      "Epoch 1 [103/340] - Loss: 101.943 [-101.940, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [104/340] - Loss: 99.076 [-99.073, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [105/340] - Loss: 105.054 [-105.051, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [106/340] - Loss: 103.707 [-103.704, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [107/340] - Loss: 91.555 [-91.552, 0.003, 0.000]- Time: 0.026\n",
      "Epoch 1 [108/340] - Loss: 96.873 [-96.870, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [109/340] - Loss: 98.364 [-98.361, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [110/340] - Loss: 101.067 [-101.064, 0.003, 0.000]- Time: 0.022\n",
      "Epoch 1 [111/340] - Loss: 96.141 [-96.138, 0.003, 0.000]- Time: 0.022\n",
      "Epoch 1 [112/340] - Loss: 85.859 [-85.856, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [113/340] - Loss: 89.301 [-89.298, 0.003, 0.000]- Time: 0.022\n",
      "Epoch 1 [114/340] - Loss: 87.776 [-87.773, 0.003, 0.000]- Time: 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [115/340] - Loss: 90.308 [-90.305, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [116/340] - Loss: 98.171 [-98.168, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [117/340] - Loss: 92.716 [-92.713, 0.003, 0.000]- Time: 0.022\n",
      "Epoch 1 [118/340] - Loss: 100.603 [-100.600, 0.003, 0.000]- Time: 0.025\n",
      "Epoch 1 [119/340] - Loss: 86.759 [-86.756, 0.003, 0.000]- Time: 0.025\n",
      "Epoch 1 [120/340] - Loss: 93.563 [-93.559, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [121/340] - Loss: 86.456 [-86.453, 0.003, 0.000]- Time: 0.026\n",
      "Epoch 1 [122/340] - Loss: 91.948 [-91.945, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [123/340] - Loss: 89.140 [-89.137, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [124/340] - Loss: 84.679 [-84.675, 0.003, 0.000]- Time: 0.023\n",
      "Epoch 1 [125/340] - Loss: 85.214 [-85.211, 0.003, 0.000]- Time: 0.024\n",
      "Epoch 1 [126/340] - Loss: 84.209 [-84.205, 0.004, 0.000]- Time: 0.026\n",
      "Epoch 1 [127/340] - Loss: 86.704 [-86.701, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [128/340] - Loss: 87.766 [-87.763, 0.004, 0.000]- Time: 0.031\n",
      "Epoch 1 [129/340] - Loss: 78.669 [-78.666, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [130/340] - Loss: 88.429 [-88.426, 0.004, 0.000]- Time: 0.022\n",
      "Epoch 1 [131/340] - Loss: 86.365 [-86.361, 0.004, 0.000]- Time: 0.025\n",
      "Epoch 1 [132/340] - Loss: 87.457 [-87.453, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [133/340] - Loss: 79.751 [-79.747, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [134/340] - Loss: 84.427 [-84.423, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [135/340] - Loss: 84.391 [-84.387, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [136/340] - Loss: 93.214 [-93.211, 0.004, 0.000]- Time: 0.029\n",
      "Epoch 1 [137/340] - Loss: 76.785 [-76.781, 0.004, 0.000]- Time: 0.025\n",
      "Epoch 1 [138/340] - Loss: 86.145 [-86.141, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [139/340] - Loss: 83.910 [-83.906, 0.004, 0.000]- Time: 0.022\n",
      "Epoch 1 [140/340] - Loss: 76.864 [-76.860, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [141/340] - Loss: 82.467 [-82.463, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [142/340] - Loss: 74.404 [-74.400, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [143/340] - Loss: 75.378 [-75.374, 0.004, 0.000]- Time: 0.022\n",
      "Epoch 1 [144/340] - Loss: 78.764 [-78.760, 0.004, 0.000]- Time: 0.022\n",
      "Epoch 1 [145/340] - Loss: 77.013 [-77.009, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [146/340] - Loss: 80.493 [-80.489, 0.004, 0.000]- Time: 0.022\n",
      "Epoch 1 [147/340] - Loss: 78.864 [-78.860, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [148/340] - Loss: 77.131 [-77.127, 0.004, 0.000]- Time: 0.022\n",
      "Epoch 1 [149/340] - Loss: 78.521 [-78.516, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [150/340] - Loss: 83.484 [-83.480, 0.004, 0.000]- Time: 0.026\n",
      "Epoch 1 [151/340] - Loss: 78.762 [-78.758, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [152/340] - Loss: 79.618 [-79.614, 0.004, 0.000]- Time: 0.025\n",
      "Epoch 1 [153/340] - Loss: 77.942 [-77.938, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [154/340] - Loss: 75.775 [-75.771, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [155/340] - Loss: 64.265 [-64.261, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [156/340] - Loss: 75.678 [-75.674, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [157/340] - Loss: 77.540 [-77.536, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [158/340] - Loss: 76.963 [-76.959, 0.004, 0.000]- Time: 0.023\n",
      "Epoch 1 [159/340] - Loss: 76.007 [-76.003, 0.004, 0.000]- Time: 0.028\n",
      "Epoch 1 [160/340] - Loss: 74.888 [-74.884, 0.004, 0.000]- Time: 0.027\n",
      "Epoch 1 [161/340] - Loss: 64.575 [-64.570, 0.004, 0.000]- Time: 0.028\n",
      "Epoch 1 [162/340] - Loss: 72.298 [-72.293, 0.004, 0.000]- Time: 0.024\n",
      "Epoch 1 [163/340] - Loss: 82.724 [-82.719, 0.004, 0.000]- Time: 0.025\n",
      "Epoch 1 [164/340] - Loss: 74.699 [-74.694, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [165/340] - Loss: 70.842 [-70.838, 0.005, 0.000]- Time: 0.026\n",
      "Epoch 1 [166/340] - Loss: 68.392 [-68.387, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [167/340] - Loss: 69.824 [-69.820, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [168/340] - Loss: 66.182 [-66.178, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [169/340] - Loss: 65.343 [-65.338, 0.005, 0.000]- Time: 0.031\n",
      "Epoch 1 [170/340] - Loss: 78.255 [-78.250, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [171/340] - Loss: 76.156 [-76.152, 0.005, 0.000]- Time: 0.028\n",
      "Epoch 1 [172/340] - Loss: 68.618 [-68.613, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [173/340] - Loss: 73.696 [-73.692, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [174/340] - Loss: 66.595 [-66.590, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [175/340] - Loss: 65.271 [-65.266, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [176/340] - Loss: 69.446 [-69.442, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [177/340] - Loss: 69.388 [-69.383, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [178/340] - Loss: 70.556 [-70.551, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [179/340] - Loss: 68.037 [-68.032, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [180/340] - Loss: 67.148 [-67.143, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [181/340] - Loss: 69.674 [-69.669, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [182/340] - Loss: 70.561 [-70.556, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [183/340] - Loss: 70.210 [-70.205, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [184/340] - Loss: 68.878 [-68.873, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [185/340] - Loss: 75.359 [-75.355, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [186/340] - Loss: 63.939 [-63.934, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [187/340] - Loss: 70.455 [-70.450, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [188/340] - Loss: 67.324 [-67.319, 0.005, 0.000]- Time: 0.028\n",
      "Epoch 1 [189/340] - Loss: 67.511 [-67.506, 0.005, 0.000]- Time: 0.028\n",
      "Epoch 1 [190/340] - Loss: 69.412 [-69.407, 0.005, 0.000]- Time: 0.027\n",
      "Epoch 1 [191/340] - Loss: 63.619 [-63.614, 0.005, 0.000]- Time: 0.027\n",
      "Epoch 1 [192/340] - Loss: 70.956 [-70.951, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [193/340] - Loss: 59.498 [-59.493, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [194/340] - Loss: 60.170 [-60.165, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [195/340] - Loss: 68.587 [-68.582, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [196/340] - Loss: 70.983 [-70.978, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [197/340] - Loss: 67.878 [-67.873, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [198/340] - Loss: 61.469 [-61.463, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [199/340] - Loss: 63.589 [-63.583, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [200/340] - Loss: 65.358 [-65.353, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [201/340] - Loss: 69.987 [-69.982, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [202/340] - Loss: 59.463 [-59.458, 0.005, 0.000]- Time: 0.026\n",
      "Epoch 1 [203/340] - Loss: 69.368 [-69.363, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [204/340] - Loss: 65.337 [-65.332, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [205/340] - Loss: 59.719 [-59.714, 0.005, 0.000]- Time: 0.022\n",
      "Epoch 1 [206/340] - Loss: 63.570 [-63.565, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [207/340] - Loss: 57.479 [-57.474, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [208/340] - Loss: 66.336 [-66.330, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [209/340] - Loss: 63.512 [-63.506, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [210/340] - Loss: 57.992 [-57.986, 0.005, 0.000]- Time: 0.025\n",
      "Epoch 1 [211/340] - Loss: 59.701 [-59.696, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [212/340] - Loss: 62.160 [-62.155, 0.005, 0.000]- Time: 0.024\n",
      "Epoch 1 [213/340] - Loss: 61.558 [-61.553, 0.005, 0.000]- Time: 0.023\n",
      "Epoch 1 [214/340] - Loss: 62.813 [-62.807, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [215/340] - Loss: 60.448 [-60.443, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [216/340] - Loss: 61.329 [-61.324, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [217/340] - Loss: 66.438 [-66.432, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [218/340] - Loss: 65.285 [-65.280, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [219/340] - Loss: 59.255 [-59.249, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [220/340] - Loss: 59.106 [-59.100, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [221/340] - Loss: 58.199 [-58.194, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [222/340] - Loss: 60.782 [-60.776, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [223/340] - Loss: 61.534 [-61.528, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [224/340] - Loss: 56.509 [-56.503, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [225/340] - Loss: 63.167 [-63.162, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [226/340] - Loss: 56.937 [-56.931, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [227/340] - Loss: 58.205 [-58.199, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [228/340] - Loss: 59.077 [-59.072, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [229/340] - Loss: 53.009 [-53.003, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [230/340] - Loss: 50.427 [-50.421, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [231/340] - Loss: 57.565 [-57.559, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [232/340] - Loss: 56.742 [-56.736, 0.006, 0.000]- Time: 0.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [233/340] - Loss: 59.752 [-59.746, 0.006, 0.000]- Time: 0.026\n",
      "Epoch 1 [234/340] - Loss: 59.141 [-59.136, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [235/340] - Loss: 57.082 [-57.076, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [236/340] - Loss: 57.988 [-57.982, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [237/340] - Loss: 54.716 [-54.710, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [238/340] - Loss: 57.793 [-57.787, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [239/340] - Loss: 56.254 [-56.248, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [240/340] - Loss: 57.257 [-57.251, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [241/340] - Loss: 54.162 [-54.156, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [242/340] - Loss: 63.951 [-63.945, 0.006, 0.000]- Time: 0.026\n",
      "Epoch 1 [243/340] - Loss: 56.490 [-56.484, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [244/340] - Loss: 52.663 [-52.657, 0.006, 0.000]- Time: 0.024\n",
      "Epoch 1 [245/340] - Loss: 61.246 [-61.240, 0.006, 0.000]- Time: 0.028\n",
      "Epoch 1 [246/340] - Loss: 49.133 [-49.127, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [247/340] - Loss: 51.671 [-51.665, 0.006, 0.000]- Time: 0.026\n",
      "Epoch 1 [248/340] - Loss: 58.276 [-58.270, 0.006, 0.000]- Time: 0.024\n",
      "Epoch 1 [249/340] - Loss: 50.809 [-50.803, 0.006, 0.000]- Time: 0.026\n",
      "Epoch 1 [250/340] - Loss: 52.762 [-52.756, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [251/340] - Loss: 56.829 [-56.823, 0.006, 0.000]- Time: 0.027\n",
      "Epoch 1 [252/340] - Loss: 52.037 [-52.031, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [253/340] - Loss: 54.804 [-54.798, 0.006, 0.000]- Time: 0.027\n",
      "Epoch 1 [254/340] - Loss: 50.943 [-50.937, 0.006, 0.000]- Time: 0.024\n",
      "Epoch 1 [255/340] - Loss: 54.374 [-54.368, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [256/340] - Loss: 57.328 [-57.322, 0.006, 0.000]- Time: 0.024\n",
      "Epoch 1 [257/340] - Loss: 48.141 [-48.135, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [258/340] - Loss: 49.673 [-49.667, 0.006, 0.000]- Time: 0.021\n",
      "Epoch 1 [259/340] - Loss: 60.146 [-60.140, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [260/340] - Loss: 57.146 [-57.140, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [261/340] - Loss: 53.766 [-53.760, 0.006, 0.000]- Time: 0.021\n",
      "Epoch 1 [262/340] - Loss: 56.251 [-56.244, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [263/340] - Loss: 51.721 [-51.714, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [264/340] - Loss: 53.095 [-53.088, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [265/340] - Loss: 51.745 [-51.739, 0.006, 0.000]- Time: 0.026\n",
      "Epoch 1 [266/340] - Loss: 57.698 [-57.692, 0.006, 0.000]- Time: 0.027\n",
      "Epoch 1 [267/340] - Loss: 49.655 [-49.649, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [268/340] - Loss: 52.050 [-52.044, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [269/340] - Loss: 56.459 [-56.452, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [270/340] - Loss: 48.312 [-48.306, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [271/340] - Loss: 50.309 [-50.303, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [272/340] - Loss: 57.221 [-57.215, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [273/340] - Loss: 53.081 [-53.075, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [274/340] - Loss: 52.786 [-52.780, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [275/340] - Loss: 53.162 [-53.156, 0.006, 0.000]- Time: 0.026\n",
      "Epoch 1 [276/340] - Loss: 53.903 [-53.897, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [277/340] - Loss: 52.932 [-52.925, 0.006, 0.000]- Time: 0.021\n",
      "Epoch 1 [278/340] - Loss: 46.006 [-46.000, 0.006, 0.000]- Time: 0.022\n",
      "Epoch 1 [279/340] - Loss: 55.240 [-55.234, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [280/340] - Loss: 57.291 [-57.284, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [281/340] - Loss: 49.250 [-49.244, 0.006, 0.000]- Time: 0.024\n",
      "Epoch 1 [282/340] - Loss: 56.206 [-56.199, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [283/340] - Loss: 52.180 [-52.174, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [284/340] - Loss: 53.276 [-53.269, 0.006, 0.000]- Time: 0.025\n",
      "Epoch 1 [285/340] - Loss: 49.124 [-49.117, 0.006, 0.000]- Time: 0.023\n",
      "Epoch 1 [286/340] - Loss: 57.576 [-57.570, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [287/340] - Loss: 47.633 [-47.626, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [288/340] - Loss: 55.205 [-55.198, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [289/340] - Loss: 52.774 [-52.768, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 1 [290/340] - Loss: 52.740 [-52.734, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 1 [291/340] - Loss: 49.028 [-49.021, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 1 [292/340] - Loss: 46.272 [-46.265, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [293/340] - Loss: 43.653 [-43.646, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [294/340] - Loss: 49.904 [-49.897, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [295/340] - Loss: 47.709 [-47.702, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [296/340] - Loss: 49.761 [-49.755, 0.007, 0.000]- Time: 0.027\n",
      "Epoch 1 [297/340] - Loss: 44.717 [-44.710, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [298/340] - Loss: 58.241 [-58.234, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [299/340] - Loss: 46.139 [-46.132, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [300/340] - Loss: 47.368 [-47.361, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [301/340] - Loss: 50.724 [-50.717, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 1 [302/340] - Loss: 47.025 [-47.018, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [303/340] - Loss: 50.229 [-50.223, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [304/340] - Loss: 50.553 [-50.546, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [305/340] - Loss: 46.769 [-46.762, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [306/340] - Loss: 44.068 [-44.062, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [307/340] - Loss: 45.907 [-45.900, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [308/340] - Loss: 49.253 [-49.247, 0.007, 0.000]- Time: 0.027\n",
      "Epoch 1 [309/340] - Loss: 59.328 [-59.321, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 1 [310/340] - Loss: 48.569 [-48.562, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [311/340] - Loss: 45.575 [-45.568, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [312/340] - Loss: 46.748 [-46.742, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [313/340] - Loss: 44.189 [-44.182, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [314/340] - Loss: 47.267 [-47.261, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [315/340] - Loss: 47.068 [-47.061, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 1 [316/340] - Loss: 50.401 [-50.395, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [317/340] - Loss: 51.711 [-51.704, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [318/340] - Loss: 51.734 [-51.727, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [319/340] - Loss: 50.578 [-50.571, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 1 [320/340] - Loss: 43.390 [-43.383, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [321/340] - Loss: 48.600 [-48.593, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [322/340] - Loss: 43.318 [-43.311, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [323/340] - Loss: 40.604 [-40.597, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 1 [324/340] - Loss: 47.599 [-47.592, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [325/340] - Loss: 52.458 [-52.451, 0.007, 0.000]- Time: 0.027\n",
      "Epoch 1 [326/340] - Loss: 42.067 [-42.060, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [327/340] - Loss: 50.340 [-50.333, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [328/340] - Loss: 44.658 [-44.651, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [329/340] - Loss: 48.193 [-48.186, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [330/340] - Loss: 46.459 [-46.452, 0.007, 0.000]- Time: 0.034\n",
      "Epoch 1 [331/340] - Loss: 44.967 [-44.960, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 1 [332/340] - Loss: 47.358 [-47.351, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [333/340] - Loss: 46.314 [-46.307, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [334/340] - Loss: 45.373 [-45.366, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 1 [335/340] - Loss: 41.820 [-41.813, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 1 [336/340] - Loss: 50.699 [-50.692, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [337/340] - Loss: 45.268 [-45.261, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 1 [338/340] - Loss: 43.181 [-43.174, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 1 [339/340] - Loss: 44.260 [-44.253, 0.007, 0.000]- Time: 0.028\n",
      "Epoch 2 [0/340] - Loss: 45.837 [-45.830, 0.007, 0.000]- Time: 0.028\n",
      "Epoch 2 [1/340] - Loss: 44.157 [-44.150, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 2 [2/340] - Loss: 43.805 [-43.798, 0.007, 0.000]- Time: 0.027\n",
      "Epoch 2 [3/340] - Loss: 42.947 [-42.940, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [4/340] - Loss: 46.850 [-46.843, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 2 [5/340] - Loss: 46.106 [-46.099, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 2 [6/340] - Loss: 52.287 [-52.280, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [7/340] - Loss: 46.295 [-46.288, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [8/340] - Loss: 47.872 [-47.865, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 2 [9/340] - Loss: 42.428 [-42.421, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [10/340] - Loss: 47.058 [-47.051, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [11/340] - Loss: 45.733 [-45.726, 0.007, 0.000]- Time: 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [12/340] - Loss: 43.317 [-43.310, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 2 [13/340] - Loss: 40.206 [-40.199, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [14/340] - Loss: 39.262 [-39.254, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [15/340] - Loss: 45.068 [-45.061, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 2 [16/340] - Loss: 44.956 [-44.949, 0.007, 0.000]- Time: 0.027\n",
      "Epoch 2 [17/340] - Loss: 46.324 [-46.317, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 2 [18/340] - Loss: 44.176 [-44.169, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [19/340] - Loss: 43.007 [-43.000, 0.007, 0.000]- Time: 0.026\n",
      "Epoch 2 [20/340] - Loss: 45.275 [-45.268, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [21/340] - Loss: 42.845 [-42.838, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [22/340] - Loss: 39.850 [-39.842, 0.007, 0.000]- Time: 0.027\n",
      "Epoch 2 [23/340] - Loss: 46.319 [-46.312, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [24/340] - Loss: 42.549 [-42.541, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 2 [25/340] - Loss: 41.131 [-41.123, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [26/340] - Loss: 44.172 [-44.164, 0.007, 0.000]- Time: 0.028\n",
      "Epoch 2 [27/340] - Loss: 43.912 [-43.905, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 2 [28/340] - Loss: 41.584 [-41.576, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [29/340] - Loss: 43.184 [-43.177, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [30/340] - Loss: 40.141 [-40.134, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [31/340] - Loss: 42.521 [-42.513, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [32/340] - Loss: 43.271 [-43.264, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 2 [33/340] - Loss: 37.567 [-37.560, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [34/340] - Loss: 41.550 [-41.543, 0.007, 0.000]- Time: 0.025\n",
      "Epoch 2 [35/340] - Loss: 47.298 [-47.290, 0.007, 0.000]- Time: 0.022\n",
      "Epoch 2 [36/340] - Loss: 40.637 [-40.629, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [37/340] - Loss: 39.988 [-39.980, 0.007, 0.000]- Time: 0.023\n",
      "Epoch 2 [38/340] - Loss: 36.612 [-36.605, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [39/340] - Loss: 43.493 [-43.486, 0.007, 0.000]- Time: 0.024\n",
      "Epoch 2 [40/340] - Loss: 39.807 [-39.799, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [41/340] - Loss: 41.829 [-41.821, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [42/340] - Loss: 43.970 [-43.962, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [43/340] - Loss: 40.973 [-40.966, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [44/340] - Loss: 36.315 [-36.307, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [45/340] - Loss: 43.770 [-43.762, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [46/340] - Loss: 43.231 [-43.224, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [47/340] - Loss: 39.521 [-39.513, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [48/340] - Loss: 45.589 [-45.582, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [49/340] - Loss: 40.049 [-40.041, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [50/340] - Loss: 42.246 [-42.239, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [51/340] - Loss: 44.136 [-44.128, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [52/340] - Loss: 39.362 [-39.354, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [53/340] - Loss: 37.877 [-37.869, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [54/340] - Loss: 40.128 [-40.121, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [55/340] - Loss: 38.278 [-38.271, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [56/340] - Loss: 40.487 [-40.479, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [57/340] - Loss: 42.349 [-42.342, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [58/340] - Loss: 40.637 [-40.629, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [59/340] - Loss: 43.022 [-43.014, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [60/340] - Loss: 43.136 [-43.128, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [61/340] - Loss: 41.916 [-41.908, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [62/340] - Loss: 38.120 [-38.112, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [63/340] - Loss: 44.528 [-44.520, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [64/340] - Loss: 41.390 [-41.382, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [65/340] - Loss: 43.022 [-43.015, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [66/340] - Loss: 41.706 [-41.698, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [67/340] - Loss: 44.108 [-44.100, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [68/340] - Loss: 38.520 [-38.512, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [69/340] - Loss: 38.684 [-38.676, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [70/340] - Loss: 45.275 [-45.267, 0.008, 0.000]- Time: 0.028\n",
      "Epoch 2 [71/340] - Loss: 38.219 [-38.211, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [72/340] - Loss: 44.352 [-44.344, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [73/340] - Loss: 38.291 [-38.284, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [74/340] - Loss: 41.392 [-41.384, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [75/340] - Loss: 42.107 [-42.099, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [76/340] - Loss: 38.471 [-38.463, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [77/340] - Loss: 36.275 [-36.267, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [78/340] - Loss: 37.488 [-37.480, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [79/340] - Loss: 38.670 [-38.662, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [80/340] - Loss: 37.332 [-37.325, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [81/340] - Loss: 37.896 [-37.889, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [82/340] - Loss: 38.239 [-38.231, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [83/340] - Loss: 40.224 [-40.216, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [84/340] - Loss: 36.796 [-36.788, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [85/340] - Loss: 39.538 [-39.531, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [86/340] - Loss: 40.906 [-40.898, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [87/340] - Loss: 40.620 [-40.612, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [88/340] - Loss: 42.405 [-42.397, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [89/340] - Loss: 38.961 [-38.953, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [90/340] - Loss: 40.289 [-40.281, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [91/340] - Loss: 38.988 [-38.980, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [92/340] - Loss: 41.669 [-41.661, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [93/340] - Loss: 37.961 [-37.953, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [94/340] - Loss: 38.645 [-38.637, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [95/340] - Loss: 38.374 [-38.366, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [96/340] - Loss: 40.811 [-40.803, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [97/340] - Loss: 39.653 [-39.645, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [98/340] - Loss: 34.530 [-34.522, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [99/340] - Loss: 36.060 [-36.052, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [100/340] - Loss: 38.475 [-38.467, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [101/340] - Loss: 35.711 [-35.703, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [102/340] - Loss: 37.592 [-37.584, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [103/340] - Loss: 41.444 [-41.436, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [104/340] - Loss: 40.327 [-40.319, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [105/340] - Loss: 43.480 [-43.472, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [106/340] - Loss: 38.393 [-38.385, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [107/340] - Loss: 39.124 [-39.116, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [108/340] - Loss: 35.845 [-35.837, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [109/340] - Loss: 39.385 [-39.377, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [110/340] - Loss: 38.241 [-38.233, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [111/340] - Loss: 38.724 [-38.716, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [112/340] - Loss: 34.106 [-34.098, 0.008, 0.000]- Time: 0.027\n",
      "Epoch 2 [113/340] - Loss: 37.865 [-37.856, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [114/340] - Loss: 42.594 [-42.585, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [115/340] - Loss: 36.187 [-36.179, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [116/340] - Loss: 37.089 [-37.081, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [117/340] - Loss: 33.291 [-33.283, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [118/340] - Loss: 36.694 [-36.686, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [119/340] - Loss: 38.597 [-38.589, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [120/340] - Loss: 41.824 [-41.815, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [121/340] - Loss: 37.005 [-36.997, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [122/340] - Loss: 36.510 [-36.502, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [123/340] - Loss: 39.434 [-39.426, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [124/340] - Loss: 36.842 [-36.834, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [125/340] - Loss: 33.713 [-33.704, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [126/340] - Loss: 37.430 [-37.422, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [127/340] - Loss: 36.414 [-36.406, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [128/340] - Loss: 39.766 [-39.757, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [129/340] - Loss: 36.194 [-36.185, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [130/340] - Loss: 35.781 [-35.773, 0.008, 0.000]- Time: 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [131/340] - Loss: 34.932 [-34.924, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [132/340] - Loss: 36.563 [-36.555, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [133/340] - Loss: 35.927 [-35.919, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [134/340] - Loss: 35.672 [-35.663, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [135/340] - Loss: 38.008 [-38.000, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [136/340] - Loss: 36.324 [-36.316, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [137/340] - Loss: 34.023 [-34.014, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [138/340] - Loss: 34.854 [-34.845, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [139/340] - Loss: 34.062 [-34.053, 0.008, 0.000]- Time: 0.025\n",
      "Epoch 2 [140/340] - Loss: 36.162 [-36.154, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [141/340] - Loss: 34.205 [-34.196, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [142/340] - Loss: 35.150 [-35.142, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [143/340] - Loss: 39.445 [-39.437, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [144/340] - Loss: 35.458 [-35.450, 0.008, 0.000]- Time: 0.029\n",
      "Epoch 2 [145/340] - Loss: 32.407 [-32.398, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [146/340] - Loss: 33.882 [-33.873, 0.008, 0.000]- Time: 0.026\n",
      "Epoch 2 [147/340] - Loss: 39.324 [-39.316, 0.008, 0.000]- Time: 0.024\n",
      "Epoch 2 [148/340] - Loss: 38.253 [-38.245, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [149/340] - Loss: 36.001 [-35.993, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [150/340] - Loss: 35.558 [-35.550, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [151/340] - Loss: 33.423 [-33.415, 0.008, 0.000]- Time: 0.023\n",
      "Epoch 2 [152/340] - Loss: 36.174 [-36.166, 0.008, 0.000]- Time: 0.022\n",
      "Epoch 2 [153/340] - Loss: 38.203 [-38.195, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [154/340] - Loss: 34.713 [-34.704, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [155/340] - Loss: 35.783 [-35.774, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [156/340] - Loss: 39.711 [-39.703, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [157/340] - Loss: 32.676 [-32.667, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [158/340] - Loss: 38.814 [-38.805, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [159/340] - Loss: 33.988 [-33.979, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [160/340] - Loss: 35.671 [-35.663, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [161/340] - Loss: 32.765 [-32.757, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [162/340] - Loss: 33.915 [-33.907, 0.009, 0.000]- Time: 0.021\n",
      "Epoch 2 [163/340] - Loss: 34.686 [-34.678, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [164/340] - Loss: 32.709 [-32.701, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [165/340] - Loss: 33.925 [-33.917, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [166/340] - Loss: 33.193 [-33.185, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [167/340] - Loss: 35.021 [-35.012, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [168/340] - Loss: 33.243 [-33.235, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [169/340] - Loss: 35.488 [-35.479, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [170/340] - Loss: 41.697 [-41.689, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [171/340] - Loss: 36.442 [-36.433, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [172/340] - Loss: 36.907 [-36.898, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [173/340] - Loss: 33.333 [-33.325, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [174/340] - Loss: 33.877 [-33.868, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [175/340] - Loss: 31.332 [-31.323, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [176/340] - Loss: 37.862 [-37.853, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [177/340] - Loss: 35.468 [-35.460, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [178/340] - Loss: 34.200 [-34.192, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [179/340] - Loss: 33.879 [-33.870, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [180/340] - Loss: 38.705 [-38.696, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [181/340] - Loss: 37.941 [-37.932, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [182/340] - Loss: 36.237 [-36.228, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [183/340] - Loss: 36.167 [-36.158, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [184/340] - Loss: 35.388 [-35.379, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [185/340] - Loss: 34.700 [-34.691, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [186/340] - Loss: 32.794 [-32.785, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [187/340] - Loss: 33.176 [-33.167, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [188/340] - Loss: 31.420 [-31.411, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [189/340] - Loss: 35.955 [-35.946, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [190/340] - Loss: 32.631 [-32.622, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [191/340] - Loss: 34.934 [-34.925, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [192/340] - Loss: 30.290 [-30.281, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [193/340] - Loss: 35.701 [-35.692, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [194/340] - Loss: 30.524 [-30.515, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [195/340] - Loss: 32.380 [-32.371, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [196/340] - Loss: 32.100 [-32.092, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [197/340] - Loss: 37.169 [-37.161, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [198/340] - Loss: 32.388 [-32.379, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [199/340] - Loss: 35.572 [-35.563, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [200/340] - Loss: 31.623 [-31.614, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [201/340] - Loss: 32.957 [-32.948, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [202/340] - Loss: 31.885 [-31.876, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [203/340] - Loss: 35.543 [-35.535, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [204/340] - Loss: 34.252 [-34.243, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [205/340] - Loss: 32.456 [-32.447, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [206/340] - Loss: 35.630 [-35.621, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [207/340] - Loss: 36.401 [-36.392, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [208/340] - Loss: 34.051 [-34.042, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [209/340] - Loss: 35.849 [-35.840, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [210/340] - Loss: 31.339 [-31.330, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [211/340] - Loss: 30.651 [-30.642, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [212/340] - Loss: 31.442 [-31.433, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [213/340] - Loss: 34.748 [-34.739, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [214/340] - Loss: 34.079 [-34.070, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [215/340] - Loss: 34.341 [-34.332, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [216/340] - Loss: 30.470 [-30.461, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [217/340] - Loss: 36.841 [-36.832, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [218/340] - Loss: 32.911 [-32.902, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [219/340] - Loss: 32.758 [-32.749, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [220/340] - Loss: 33.557 [-33.548, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [221/340] - Loss: 30.792 [-30.783, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [222/340] - Loss: 35.026 [-35.017, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [223/340] - Loss: 34.259 [-34.250, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [224/340] - Loss: 29.991 [-29.982, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [225/340] - Loss: 32.959 [-32.950, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [226/340] - Loss: 35.044 [-35.035, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [227/340] - Loss: 29.180 [-29.171, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [228/340] - Loss: 34.320 [-34.311, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [229/340] - Loss: 35.516 [-35.507, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [230/340] - Loss: 35.402 [-35.392, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [231/340] - Loss: 35.051 [-35.042, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [232/340] - Loss: 31.848 [-31.839, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [233/340] - Loss: 33.375 [-33.366, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [234/340] - Loss: 34.952 [-34.943, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [235/340] - Loss: 34.753 [-34.744, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [236/340] - Loss: 35.565 [-35.556, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [237/340] - Loss: 30.765 [-30.756, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [238/340] - Loss: 32.550 [-32.541, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [239/340] - Loss: 35.627 [-35.618, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [240/340] - Loss: 31.333 [-31.323, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [241/340] - Loss: 31.742 [-31.733, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [242/340] - Loss: 36.167 [-36.158, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [243/340] - Loss: 30.628 [-30.619, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [244/340] - Loss: 31.356 [-31.347, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [245/340] - Loss: 34.612 [-34.602, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [246/340] - Loss: 32.581 [-32.572, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [247/340] - Loss: 35.041 [-35.032, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [248/340] - Loss: 29.436 [-29.427, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [249/340] - Loss: 33.414 [-33.405, 0.009, 0.000]- Time: 0.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [250/340] - Loss: 28.824 [-28.815, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [251/340] - Loss: 33.220 [-33.211, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [252/340] - Loss: 33.410 [-33.401, 0.009, 0.000]- Time: 0.022\n",
      "Epoch 2 [253/340] - Loss: 35.368 [-35.359, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [254/340] - Loss: 31.816 [-31.807, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [255/340] - Loss: 32.536 [-32.527, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [256/340] - Loss: 32.131 [-32.122, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [257/340] - Loss: 35.063 [-35.054, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [258/340] - Loss: 31.521 [-31.512, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [259/340] - Loss: 32.428 [-32.419, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [260/340] - Loss: 31.248 [-31.239, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [261/340] - Loss: 31.892 [-31.883, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [262/340] - Loss: 29.535 [-29.525, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [263/340] - Loss: 33.939 [-33.930, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [264/340] - Loss: 32.443 [-32.433, 0.009, 0.000]- Time: 0.031\n",
      "Epoch 2 [265/340] - Loss: 32.498 [-32.489, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [266/340] - Loss: 27.759 [-27.749, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [267/340] - Loss: 35.141 [-35.132, 0.009, 0.000]- Time: 0.027\n",
      "Epoch 2 [268/340] - Loss: 35.231 [-35.222, 0.009, 0.000]- Time: 0.029\n",
      "Epoch 2 [269/340] - Loss: 31.784 [-31.774, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [270/340] - Loss: 31.444 [-31.434, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [271/340] - Loss: 29.408 [-29.399, 0.009, 0.000]- Time: 0.040\n",
      "Epoch 2 [272/340] - Loss: 31.965 [-31.955, 0.009, 0.000]- Time: 0.025\n",
      "Epoch 2 [273/340] - Loss: 31.971 [-31.961, 0.009, 0.000]- Time: 0.034\n",
      "Epoch 2 [274/340] - Loss: 30.847 [-30.838, 0.009, 0.000]- Time: 0.023\n",
      "Epoch 2 [275/340] - Loss: 32.241 [-32.231, 0.009, 0.000]- Time: 0.037\n",
      "Epoch 2 [276/340] - Loss: 33.368 [-33.359, 0.009, 0.000]- Time: 0.030\n",
      "Epoch 2 [277/340] - Loss: 32.686 [-32.677, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [278/340] - Loss: 34.125 [-34.115, 0.009, 0.000]- Time: 0.028\n",
      "Epoch 2 [279/340] - Loss: 28.310 [-28.301, 0.009, 0.000]- Time: 0.024\n",
      "Epoch 2 [280/340] - Loss: 30.182 [-30.173, 0.009, 0.000]- Time: 0.026\n",
      "Epoch 2 [281/340] - Loss: 28.132 [-28.122, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [282/340] - Loss: 29.972 [-29.963, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [283/340] - Loss: 33.476 [-33.466, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [284/340] - Loss: 30.828 [-30.818, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 2 [285/340] - Loss: 27.266 [-27.257, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 2 [286/340] - Loss: 30.601 [-30.591, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [287/340] - Loss: 28.682 [-28.673, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [288/340] - Loss: 29.588 [-29.579, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [289/340] - Loss: 31.663 [-31.653, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [290/340] - Loss: 28.489 [-28.479, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 2 [291/340] - Loss: 33.991 [-33.981, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [292/340] - Loss: 28.881 [-28.871, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 2 [293/340] - Loss: 32.334 [-32.324, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [294/340] - Loss: 29.174 [-29.165, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 2 [295/340] - Loss: 32.031 [-32.022, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [296/340] - Loss: 31.060 [-31.051, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 2 [297/340] - Loss: 27.881 [-27.871, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [298/340] - Loss: 31.492 [-31.482, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 2 [299/340] - Loss: 31.302 [-31.293, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [300/340] - Loss: 30.307 [-30.298, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [301/340] - Loss: 30.298 [-30.288, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [302/340] - Loss: 31.037 [-31.027, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [303/340] - Loss: 31.392 [-31.383, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [304/340] - Loss: 31.220 [-31.210, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [305/340] - Loss: 29.504 [-29.494, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 2 [306/340] - Loss: 30.163 [-30.154, 0.010, 0.000]- Time: 0.029\n",
      "Epoch 2 [307/340] - Loss: 28.343 [-28.334, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 2 [308/340] - Loss: 30.718 [-30.708, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [309/340] - Loss: 32.353 [-32.344, 0.010, 0.000]- Time: 0.028\n",
      "Epoch 2 [310/340] - Loss: 31.949 [-31.940, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [311/340] - Loss: 32.719 [-32.709, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 2 [312/340] - Loss: 32.857 [-32.847, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [313/340] - Loss: 28.602 [-28.592, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 2 [314/340] - Loss: 27.114 [-27.104, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 2 [315/340] - Loss: 32.671 [-32.661, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [316/340] - Loss: 29.700 [-29.690, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [317/340] - Loss: 30.910 [-30.900, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [318/340] - Loss: 29.823 [-29.813, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 2 [319/340] - Loss: 30.377 [-30.368, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 2 [320/340] - Loss: 29.905 [-29.896, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [321/340] - Loss: 29.618 [-29.608, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [322/340] - Loss: 26.628 [-26.618, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [323/340] - Loss: 31.320 [-31.310, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [324/340] - Loss: 26.606 [-26.597, 0.010, 0.000]- Time: 0.030\n",
      "Epoch 2 [325/340] - Loss: 27.573 [-27.563, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [326/340] - Loss: 31.204 [-31.194, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 2 [327/340] - Loss: 26.766 [-26.756, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 2 [328/340] - Loss: 31.082 [-31.072, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [329/340] - Loss: 30.605 [-30.595, 0.010, 0.000]- Time: 0.029\n",
      "Epoch 2 [330/340] - Loss: 27.427 [-27.417, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 2 [331/340] - Loss: 30.300 [-30.291, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [332/340] - Loss: 28.565 [-28.556, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [333/340] - Loss: 30.182 [-30.172, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 2 [334/340] - Loss: 30.913 [-30.903, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [335/340] - Loss: 30.744 [-30.734, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 2 [336/340] - Loss: 28.402 [-28.393, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 2 [337/340] - Loss: 28.638 [-28.629, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 2 [338/340] - Loss: 31.337 [-31.327, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 2 [339/340] - Loss: 29.313 [-29.303, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [0/340] - Loss: 28.304 [-28.294, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [1/340] - Loss: 29.264 [-29.255, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [2/340] - Loss: 29.311 [-29.301, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [3/340] - Loss: 30.845 [-30.835, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [4/340] - Loss: 30.393 [-30.383, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [5/340] - Loss: 28.176 [-28.166, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [6/340] - Loss: 27.543 [-27.533, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [7/340] - Loss: 29.469 [-29.459, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [8/340] - Loss: 27.788 [-27.778, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [9/340] - Loss: 28.788 [-28.779, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [10/340] - Loss: 27.427 [-27.417, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [11/340] - Loss: 29.890 [-29.880, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [12/340] - Loss: 29.747 [-29.737, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [13/340] - Loss: 31.444 [-31.434, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [14/340] - Loss: 29.615 [-29.605, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [15/340] - Loss: 27.876 [-27.866, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [16/340] - Loss: 31.155 [-31.145, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [17/340] - Loss: 26.910 [-26.900, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [18/340] - Loss: 29.456 [-29.446, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 3 [19/340] - Loss: 28.882 [-28.872, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [20/340] - Loss: 28.406 [-28.396, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [21/340] - Loss: 31.286 [-31.276, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [22/340] - Loss: 26.775 [-26.765, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [23/340] - Loss: 26.619 [-26.609, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [24/340] - Loss: 30.396 [-30.386, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [25/340] - Loss: 29.086 [-29.076, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [26/340] - Loss: 27.729 [-27.719, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [27/340] - Loss: 27.753 [-27.743, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [28/340] - Loss: 26.478 [-26.468, 0.010, 0.000]- Time: 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [29/340] - Loss: 29.979 [-29.969, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [30/340] - Loss: 27.879 [-27.869, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [31/340] - Loss: 27.717 [-27.707, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [32/340] - Loss: 28.116 [-28.106, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [33/340] - Loss: 29.994 [-29.983, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 3 [34/340] - Loss: 30.252 [-30.242, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [35/340] - Loss: 31.304 [-31.294, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [36/340] - Loss: 30.255 [-30.245, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [37/340] - Loss: 29.974 [-29.964, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 3 [38/340] - Loss: 28.745 [-28.735, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [39/340] - Loss: 26.900 [-26.890, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 3 [40/340] - Loss: 26.331 [-26.321, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [41/340] - Loss: 28.628 [-28.617, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [42/340] - Loss: 28.648 [-28.638, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [43/340] - Loss: 29.083 [-29.072, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [44/340] - Loss: 29.101 [-29.091, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [45/340] - Loss: 26.493 [-26.482, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [46/340] - Loss: 28.431 [-28.420, 0.010, 0.000]- Time: 0.028\n",
      "Epoch 3 [47/340] - Loss: 26.060 [-26.049, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [48/340] - Loss: 27.582 [-27.571, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 3 [49/340] - Loss: 28.497 [-28.487, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [50/340] - Loss: 27.339 [-27.329, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [51/340] - Loss: 25.662 [-25.651, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [52/340] - Loss: 28.195 [-28.185, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [53/340] - Loss: 30.994 [-30.984, 0.010, 0.000]- Time: 0.021\n",
      "Epoch 3 [54/340] - Loss: 28.568 [-28.557, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [55/340] - Loss: 26.657 [-26.647, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [56/340] - Loss: 27.440 [-27.430, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [57/340] - Loss: 27.802 [-27.792, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [58/340] - Loss: 26.203 [-26.193, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [59/340] - Loss: 27.108 [-27.098, 0.010, 0.000]- Time: 0.028\n",
      "Epoch 3 [60/340] - Loss: 25.511 [-25.500, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [61/340] - Loss: 28.053 [-28.043, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [62/340] - Loss: 27.476 [-27.466, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [63/340] - Loss: 29.002 [-28.992, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [64/340] - Loss: 26.996 [-26.986, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [65/340] - Loss: 26.863 [-26.853, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [66/340] - Loss: 29.385 [-29.375, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [67/340] - Loss: 28.752 [-28.742, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [68/340] - Loss: 26.180 [-26.169, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [69/340] - Loss: 29.757 [-29.747, 0.010, 0.000]- Time: 0.027\n",
      "Epoch 3 [70/340] - Loss: 30.109 [-30.099, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [71/340] - Loss: 27.464 [-27.454, 0.010, 0.000]- Time: 0.028\n",
      "Epoch 3 [72/340] - Loss: 26.719 [-26.709, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [73/340] - Loss: 33.974 [-33.963, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [74/340] - Loss: 26.742 [-26.732, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [75/340] - Loss: 27.001 [-26.991, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [76/340] - Loss: 25.434 [-25.423, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [77/340] - Loss: 27.717 [-27.706, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [78/340] - Loss: 28.729 [-28.718, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [79/340] - Loss: 26.115 [-26.105, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [80/340] - Loss: 28.940 [-28.930, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [81/340] - Loss: 29.336 [-29.325, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [82/340] - Loss: 28.300 [-28.289, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [83/340] - Loss: 28.871 [-28.861, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [84/340] - Loss: 25.963 [-25.953, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [85/340] - Loss: 27.343 [-27.333, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [86/340] - Loss: 25.287 [-25.277, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [87/340] - Loss: 26.122 [-26.111, 0.010, 0.000]- Time: 0.022\n",
      "Epoch 3 [88/340] - Loss: 28.595 [-28.584, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [89/340] - Loss: 28.869 [-28.859, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [90/340] - Loss: 26.748 [-26.737, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [91/340] - Loss: 26.644 [-26.634, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [92/340] - Loss: 27.516 [-27.505, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [93/340] - Loss: 26.993 [-26.983, 0.010, 0.000]- Time: 0.024\n",
      "Epoch 3 [94/340] - Loss: 29.699 [-29.688, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [95/340] - Loss: 25.215 [-25.204, 0.010, 0.000]- Time: 0.026\n",
      "Epoch 3 [96/340] - Loss: 27.833 [-27.822, 0.010, 0.000]- Time: 0.023\n",
      "Epoch 3 [97/340] - Loss: 26.260 [-26.249, 0.010, 0.000]- Time: 0.025\n",
      "Epoch 3 [98/340] - Loss: 26.293 [-26.282, 0.011, 0.000]- Time: 0.030\n",
      "Epoch 3 [99/340] - Loss: 25.555 [-25.545, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [100/340] - Loss: 26.319 [-26.308, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [101/340] - Loss: 29.649 [-29.639, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [102/340] - Loss: 27.163 [-27.152, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [103/340] - Loss: 24.830 [-24.820, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [104/340] - Loss: 26.507 [-26.497, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [105/340] - Loss: 29.729 [-29.718, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [106/340] - Loss: 25.486 [-25.475, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [107/340] - Loss: 27.289 [-27.279, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [108/340] - Loss: 25.818 [-25.807, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [109/340] - Loss: 26.998 [-26.987, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [110/340] - Loss: 28.027 [-28.017, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [111/340] - Loss: 25.701 [-25.691, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [112/340] - Loss: 26.141 [-26.131, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [113/340] - Loss: 26.230 [-26.219, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [114/340] - Loss: 25.918 [-25.908, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [115/340] - Loss: 28.386 [-28.375, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [116/340] - Loss: 25.735 [-25.724, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [117/340] - Loss: 25.001 [-24.991, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [118/340] - Loss: 24.464 [-24.454, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [119/340] - Loss: 26.674 [-26.664, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [120/340] - Loss: 28.182 [-28.171, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [121/340] - Loss: 29.184 [-29.173, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [122/340] - Loss: 28.311 [-28.300, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [123/340] - Loss: 29.196 [-29.185, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [124/340] - Loss: 25.273 [-25.262, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [125/340] - Loss: 25.845 [-25.835, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [126/340] - Loss: 25.673 [-25.663, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [127/340] - Loss: 26.432 [-26.421, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [128/340] - Loss: 25.535 [-25.525, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [129/340] - Loss: 26.227 [-26.216, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [130/340] - Loss: 30.484 [-30.474, 0.011, 0.000]- Time: 0.029\n",
      "Epoch 3 [131/340] - Loss: 27.824 [-27.813, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [132/340] - Loss: 27.157 [-27.147, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [133/340] - Loss: 26.823 [-26.812, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [134/340] - Loss: 27.342 [-27.331, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [135/340] - Loss: 28.823 [-28.812, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [136/340] - Loss: 25.124 [-25.113, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [137/340] - Loss: 26.278 [-26.267, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [138/340] - Loss: 25.338 [-25.327, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [139/340] - Loss: 28.416 [-28.405, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [140/340] - Loss: 26.873 [-26.863, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [141/340] - Loss: 26.875 [-26.865, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [142/340] - Loss: 25.037 [-25.026, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [143/340] - Loss: 26.367 [-26.356, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [144/340] - Loss: 26.187 [-26.176, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [145/340] - Loss: 23.925 [-23.914, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [146/340] - Loss: 30.509 [-30.498, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [147/340] - Loss: 27.786 [-27.775, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [148/340] - Loss: 28.636 [-28.625, 0.011, 0.000]- Time: 0.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [149/340] - Loss: 25.331 [-25.320, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [150/340] - Loss: 26.010 [-25.999, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [151/340] - Loss: 26.932 [-26.921, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [152/340] - Loss: 26.978 [-26.968, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [153/340] - Loss: 26.089 [-26.078, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [154/340] - Loss: 25.033 [-25.022, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [155/340] - Loss: 25.343 [-25.332, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [156/340] - Loss: 22.961 [-22.950, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [157/340] - Loss: 24.391 [-24.380, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [158/340] - Loss: 28.486 [-28.475, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [159/340] - Loss: 25.895 [-25.884, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [160/340] - Loss: 23.731 [-23.720, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [161/340] - Loss: 24.278 [-24.267, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [162/340] - Loss: 25.650 [-25.639, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [163/340] - Loss: 27.460 [-27.449, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [164/340] - Loss: 22.489 [-22.479, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [165/340] - Loss: 23.675 [-23.664, 0.011, 0.000]- Time: 0.021\n",
      "Epoch 3 [166/340] - Loss: 28.252 [-28.241, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [167/340] - Loss: 27.155 [-27.144, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [168/340] - Loss: 26.634 [-26.624, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [169/340] - Loss: 29.047 [-29.036, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [170/340] - Loss: 25.521 [-25.510, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [171/340] - Loss: 27.637 [-27.626, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [172/340] - Loss: 25.504 [-25.493, 0.011, 0.000]- Time: 0.029\n",
      "Epoch 3 [173/340] - Loss: 28.431 [-28.420, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [174/340] - Loss: 26.662 [-26.651, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [175/340] - Loss: 28.282 [-28.271, 0.011, 0.000]- Time: 0.029\n",
      "Epoch 3 [176/340] - Loss: 26.718 [-26.707, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [177/340] - Loss: 26.276 [-26.265, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [178/340] - Loss: 26.174 [-26.163, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [179/340] - Loss: 26.396 [-26.385, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [180/340] - Loss: 25.277 [-25.266, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [181/340] - Loss: 25.949 [-25.938, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [182/340] - Loss: 25.586 [-25.575, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [183/340] - Loss: 26.867 [-26.856, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [184/340] - Loss: 25.835 [-25.824, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [185/340] - Loss: 27.604 [-27.593, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [186/340] - Loss: 23.902 [-23.891, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [187/340] - Loss: 25.063 [-25.052, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [188/340] - Loss: 22.520 [-22.509, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [189/340] - Loss: 26.871 [-26.860, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [190/340] - Loss: 24.030 [-24.019, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [191/340] - Loss: 28.368 [-28.357, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [192/340] - Loss: 25.940 [-25.929, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [193/340] - Loss: 26.229 [-26.218, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [194/340] - Loss: 25.431 [-25.420, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [195/340] - Loss: 27.441 [-27.430, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [196/340] - Loss: 25.258 [-25.247, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [197/340] - Loss: 27.491 [-27.480, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [198/340] - Loss: 27.375 [-27.364, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [199/340] - Loss: 25.919 [-25.908, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [200/340] - Loss: 22.320 [-22.309, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [201/340] - Loss: 26.073 [-26.062, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [202/340] - Loss: 25.742 [-25.731, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [203/340] - Loss: 23.369 [-23.358, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [204/340] - Loss: 26.946 [-26.935, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [205/340] - Loss: 27.075 [-27.064, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [206/340] - Loss: 26.710 [-26.699, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [207/340] - Loss: 26.953 [-26.942, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [208/340] - Loss: 25.468 [-25.457, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [209/340] - Loss: 21.698 [-21.687, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [210/340] - Loss: 24.116 [-24.105, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [211/340] - Loss: 23.105 [-23.094, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [212/340] - Loss: 25.679 [-25.667, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [213/340] - Loss: 25.427 [-25.416, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [214/340] - Loss: 23.581 [-23.570, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [215/340] - Loss: 23.883 [-23.871, 0.011, 0.000]- Time: 0.021\n",
      "Epoch 3 [216/340] - Loss: 24.096 [-24.085, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [217/340] - Loss: 22.411 [-22.400, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [218/340] - Loss: 24.085 [-24.074, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [219/340] - Loss: 25.398 [-25.387, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [220/340] - Loss: 22.558 [-22.547, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [221/340] - Loss: 23.318 [-23.307, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [222/340] - Loss: 26.821 [-26.810, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [223/340] - Loss: 23.641 [-23.630, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [224/340] - Loss: 25.776 [-25.765, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [225/340] - Loss: 25.599 [-25.588, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [226/340] - Loss: 25.464 [-25.453, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [227/340] - Loss: 24.211 [-24.200, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [228/340] - Loss: 23.631 [-23.620, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [229/340] - Loss: 23.490 [-23.479, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [230/340] - Loss: 25.057 [-25.046, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [231/340] - Loss: 22.734 [-22.723, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [232/340] - Loss: 25.126 [-25.115, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [233/340] - Loss: 23.880 [-23.868, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [234/340] - Loss: 22.014 [-22.003, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [235/340] - Loss: 25.749 [-25.738, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [236/340] - Loss: 22.817 [-22.806, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [237/340] - Loss: 24.038 [-24.027, 0.011, 0.000]- Time: 0.030\n",
      "Epoch 3 [238/340] - Loss: 27.916 [-27.905, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [239/340] - Loss: 22.180 [-22.169, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [240/340] - Loss: 25.993 [-25.982, 0.011, 0.000]- Time: 0.041\n",
      "Epoch 3 [241/340] - Loss: 25.229 [-25.217, 0.011, 0.000]- Time: 0.033\n",
      "Epoch 3 [242/340] - Loss: 22.904 [-22.893, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [243/340] - Loss: 25.893 [-25.882, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [244/340] - Loss: 22.447 [-22.436, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [245/340] - Loss: 22.414 [-22.403, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [246/340] - Loss: 24.727 [-24.716, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [247/340] - Loss: 21.710 [-21.699, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [248/340] - Loss: 23.962 [-23.951, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [249/340] - Loss: 27.146 [-27.134, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [250/340] - Loss: 24.865 [-24.854, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [251/340] - Loss: 23.317 [-23.306, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [252/340] - Loss: 22.360 [-22.349, 0.011, 0.000]- Time: 0.030\n",
      "Epoch 3 [253/340] - Loss: 24.242 [-24.230, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [254/340] - Loss: 23.593 [-23.582, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [255/340] - Loss: 23.226 [-23.215, 0.011, 0.000]- Time: 0.029\n",
      "Epoch 3 [256/340] - Loss: 24.939 [-24.928, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [257/340] - Loss: 23.028 [-23.017, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [258/340] - Loss: 21.979 [-21.968, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [259/340] - Loss: 27.274 [-27.263, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [260/340] - Loss: 26.231 [-26.220, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [261/340] - Loss: 23.184 [-23.173, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [262/340] - Loss: 26.439 [-26.427, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [263/340] - Loss: 25.808 [-25.796, 0.011, 0.000]- Time: 0.036\n",
      "Epoch 3 [264/340] - Loss: 23.644 [-23.633, 0.011, 0.000]- Time: 0.030\n",
      "Epoch 3 [265/340] - Loss: 24.197 [-24.186, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [266/340] - Loss: 27.338 [-27.326, 0.011, 0.000]- Time: 0.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [267/340] - Loss: 24.949 [-24.937, 0.011, 0.000]- Time: 0.030\n",
      "Epoch 3 [268/340] - Loss: 22.701 [-22.690, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [269/340] - Loss: 25.812 [-25.801, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [270/340] - Loss: 23.419 [-23.408, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [271/340] - Loss: 22.973 [-22.961, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [272/340] - Loss: 23.832 [-23.821, 0.011, 0.000]- Time: 0.029\n",
      "Epoch 3 [273/340] - Loss: 25.813 [-25.801, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [274/340] - Loss: 22.197 [-22.185, 0.011, 0.000]- Time: 0.029\n",
      "Epoch 3 [275/340] - Loss: 22.074 [-22.063, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [276/340] - Loss: 21.468 [-21.456, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [277/340] - Loss: 21.544 [-21.532, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [278/340] - Loss: 21.347 [-21.336, 0.011, 0.000]- Time: 0.029\n",
      "Epoch 3 [279/340] - Loss: 24.099 [-24.087, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [280/340] - Loss: 24.721 [-24.710, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [281/340] - Loss: 21.245 [-21.234, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [282/340] - Loss: 23.615 [-23.603, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [283/340] - Loss: 23.072 [-23.061, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [284/340] - Loss: 21.875 [-21.864, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [285/340] - Loss: 24.400 [-24.388, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [286/340] - Loss: 23.916 [-23.904, 0.011, 0.000]- Time: 0.027\n",
      "Epoch 3 [287/340] - Loss: 21.328 [-21.317, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [288/340] - Loss: 25.618 [-25.606, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [289/340] - Loss: 26.144 [-26.133, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [290/340] - Loss: 22.506 [-22.494, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [291/340] - Loss: 26.400 [-26.389, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [292/340] - Loss: 23.610 [-23.599, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [293/340] - Loss: 23.399 [-23.388, 0.011, 0.000]- Time: 0.022\n",
      "Epoch 3 [294/340] - Loss: 22.617 [-22.606, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [295/340] - Loss: 23.831 [-23.819, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [296/340] - Loss: 23.027 [-23.016, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [297/340] - Loss: 22.681 [-22.669, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [298/340] - Loss: 23.341 [-23.330, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [299/340] - Loss: 19.264 [-19.252, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [300/340] - Loss: 23.117 [-23.106, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [301/340] - Loss: 24.678 [-24.667, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [302/340] - Loss: 23.371 [-23.359, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [303/340] - Loss: 24.565 [-24.553, 0.011, 0.000]- Time: 0.026\n",
      "Epoch 3 [304/340] - Loss: 21.798 [-21.787, 0.011, 0.000]- Time: 0.025\n",
      "Epoch 3 [305/340] - Loss: 24.080 [-24.069, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [306/340] - Loss: 22.958 [-22.946, 0.011, 0.000]- Time: 0.028\n",
      "Epoch 3 [307/340] - Loss: 24.919 [-24.907, 0.011, 0.000]- Time: 0.024\n",
      "Epoch 3 [308/340] - Loss: 21.121 [-21.109, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [309/340] - Loss: 21.605 [-21.593, 0.011, 0.000]- Time: 0.023\n",
      "Epoch 3 [310/340] - Loss: 23.100 [-23.088, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 3 [311/340] - Loss: 22.890 [-22.879, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 3 [312/340] - Loss: 24.034 [-24.023, 0.012, 0.000]- Time: 0.028\n",
      "Epoch 3 [313/340] - Loss: 22.462 [-22.450, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 3 [314/340] - Loss: 25.185 [-25.173, 0.012, 0.000]- Time: 0.025\n",
      "Epoch 3 [315/340] - Loss: 22.139 [-22.128, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 3 [316/340] - Loss: 25.160 [-25.149, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 3 [317/340] - Loss: 21.177 [-21.165, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 3 [318/340] - Loss: 23.171 [-23.159, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 3 [319/340] - Loss: 24.255 [-24.244, 0.012, 0.000]- Time: 0.028\n",
      "Epoch 3 [320/340] - Loss: 22.839 [-22.827, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 3 [321/340] - Loss: 23.403 [-23.392, 0.012, 0.000]- Time: 0.028\n",
      "Epoch 3 [322/340] - Loss: 22.653 [-22.641, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 3 [323/340] - Loss: 21.689 [-21.678, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 3 [324/340] - Loss: 23.201 [-23.189, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 3 [325/340] - Loss: 23.336 [-23.324, 0.012, 0.000]- Time: 0.025\n",
      "Epoch 3 [326/340] - Loss: 22.334 [-22.323, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 3 [327/340] - Loss: 22.805 [-22.794, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 3 [328/340] - Loss: 24.761 [-24.749, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 3 [329/340] - Loss: 21.699 [-21.687, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 3 [330/340] - Loss: 23.381 [-23.370, 0.012, 0.000]- Time: 0.028\n",
      "Epoch 3 [331/340] - Loss: 22.721 [-22.709, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 3 [332/340] - Loss: 22.956 [-22.944, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 3 [333/340] - Loss: 21.830 [-21.818, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 3 [334/340] - Loss: 20.407 [-20.395, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 3 [335/340] - Loss: 24.448 [-24.436, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 3 [336/340] - Loss: 22.224 [-22.212, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 3 [337/340] - Loss: 23.601 [-23.590, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 3 [338/340] - Loss: 22.329 [-22.318, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 3 [339/340] - Loss: 21.225 [-21.214, 0.012, 0.000]- Time: 0.028\n",
      "Epoch 4 [0/340] - Loss: 23.233 [-23.222, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 4 [1/340] - Loss: 24.473 [-24.461, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [2/340] - Loss: 21.950 [-21.938, 0.012, 0.000]- Time: 0.025\n",
      "Epoch 4 [3/340] - Loss: 22.789 [-22.777, 0.012, 0.000]- Time: 0.025\n",
      "Epoch 4 [4/340] - Loss: 24.480 [-24.469, 0.012, 0.000]- Time: 0.025\n",
      "Epoch 4 [5/340] - Loss: 23.624 [-23.612, 0.012, 0.000]- Time: 0.021\n",
      "Epoch 4 [6/340] - Loss: 22.971 [-22.960, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [7/340] - Loss: 21.206 [-21.194, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 4 [8/340] - Loss: 19.627 [-19.615, 0.012, 0.000]- Time: 0.028\n",
      "Epoch 4 [9/340] - Loss: 24.109 [-24.098, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 4 [10/340] - Loss: 21.273 [-21.261, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [11/340] - Loss: 24.068 [-24.057, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [12/340] - Loss: 21.304 [-21.292, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [13/340] - Loss: 23.142 [-23.131, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [14/340] - Loss: 21.304 [-21.292, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 4 [15/340] - Loss: 23.533 [-23.521, 0.012, 0.000]- Time: 0.033\n",
      "Epoch 4 [16/340] - Loss: 21.686 [-21.675, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 4 [17/340] - Loss: 22.294 [-22.282, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [18/340] - Loss: 22.968 [-22.957, 0.012, 0.000]- Time: 0.034\n",
      "Epoch 4 [19/340] - Loss: 24.085 [-24.073, 0.012, 0.000]- Time: 0.028\n",
      "Epoch 4 [20/340] - Loss: 21.505 [-21.493, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [21/340] - Loss: 22.716 [-22.705, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [22/340] - Loss: 25.599 [-25.588, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [23/340] - Loss: 23.816 [-23.805, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [24/340] - Loss: 22.914 [-22.902, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 4 [25/340] - Loss: 21.283 [-21.271, 0.012, 0.000]- Time: 0.030\n",
      "Epoch 4 [26/340] - Loss: 21.327 [-21.316, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [27/340] - Loss: 23.885 [-23.873, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 4 [28/340] - Loss: 22.924 [-22.913, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [29/340] - Loss: 21.882 [-21.870, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [30/340] - Loss: 23.203 [-23.191, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [31/340] - Loss: 20.718 [-20.706, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [32/340] - Loss: 22.639 [-22.627, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [33/340] - Loss: 23.771 [-23.759, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [34/340] - Loss: 22.127 [-22.115, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [35/340] - Loss: 20.913 [-20.902, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 4 [36/340] - Loss: 21.906 [-21.895, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [37/340] - Loss: 22.743 [-22.732, 0.012, 0.000]- Time: 0.025\n",
      "Epoch 4 [38/340] - Loss: 23.853 [-23.841, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 4 [39/340] - Loss: 19.616 [-19.605, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [40/340] - Loss: 21.787 [-21.776, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 4 [41/340] - Loss: 23.194 [-23.182, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [42/340] - Loss: 18.955 [-18.944, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [43/340] - Loss: 18.342 [-18.330, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [44/340] - Loss: 21.514 [-21.502, 0.012, 0.000]- Time: 0.025\n",
      "Epoch 4 [45/340] - Loss: 22.801 [-22.789, 0.012, 0.000]- Time: 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [46/340] - Loss: 23.833 [-23.822, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [47/340] - Loss: 24.151 [-24.139, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [48/340] - Loss: 20.129 [-20.118, 0.012, 0.000]- Time: 0.026\n",
      "Epoch 4 [49/340] - Loss: 22.382 [-22.370, 0.012, 0.000]- Time: 0.024\n",
      "Epoch 4 [50/340] - Loss: 20.419 [-20.407, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 4 [51/340] - Loss: 20.146 [-20.134, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [52/340] - Loss: 22.353 [-22.341, 0.012, 0.000]- Time: 0.027\n",
      "Epoch 4 [53/340] - Loss: 20.425 [-20.413, 0.012, 0.000]- Time: 0.022\n",
      "Epoch 4 [54/340] - Loss: 21.152 [-21.140, 0.012, 0.000]- Time: 0.023\n",
      "Epoch 4 [55/340] - Loss: 22.264 [-22.253, 0.012, 0.000]- Time: 0.023\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 6 epochs of training in this tutorial\n",
    "num_epochs = 4\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "# We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "# effective for VI.\n",
    "from time import time\n",
    "for i in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        st = time()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        # with combine_terms=False, we get the terms of the ELBO separated so we can print them individually if we'd like.\n",
    "        # loss = -mll(output, y_batch) would also work.\n",
    "        log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "        loss = -(log_lik - kl_div + log_prior)\n",
    "        print(\n",
    "            f'Epoch {i + 1} [{minibatch_i}/{len(train_loader)}]'\n",
    "            f' - Loss: {loss.item():.3f} [{log_lik.item():.3f}, {kl_div.item():.3f}, {log_prior.item():.3f}]'\n",
    "            f'- Time: {time() - st:.3f}'\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`). Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although this can be done by passing in minibatches of `test_x` rather than the full tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "means = means[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(means - test_y.cpu()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
