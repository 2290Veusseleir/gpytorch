{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use SVGP stochastic variational regression ((https://arxiv.org/pdf/1411.2005.pdf)) to rapidly train using minibatches on the `3droad` UCI dataset with hundreds of thousands of training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `song` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~136 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the SVGP Model\n",
    "\n",
    "We now define the GP regression module that, intuitvely, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `AbstractVariationalGP`. In this example, because we will be learning the inducing point locations, we'll be using a base `VariationalStrategy` with `learn_inducing_locations=True`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import HalfWhitenedVariationalStrategy\n",
    "\n",
    "class GPModel(AbstractVariationalGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = HalfWhitenedVariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "inducing_points = train_x[:500, :]\n",
    "model = GPModel(inducing_points=inducing_points).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalMarginalLogLikelihood` or ELBO), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake.gardner/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NormBackward0>)\n",
      "tensor(6.1839e-05, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "> \u001b[0;32m/home/jake.gardner/git/gpytorch/gpytorch/variational/half_whitened_variational_strategy.py\u001b[0m(181)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    179 \u001b[0;31m            \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    180 \u001b[0;31m            \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 181 \u001b[0;31m            \u001b[0mpredictive_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_data_covar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfull_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    182 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    183 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> covar_cache\n",
      "tensor([[ 2.0988e-05,  7.2612e-06, -2.1143e-05,  ...,  2.5563e-08,\n",
      "          1.7376e-05, -5.5743e-06],\n",
      "        [-2.1663e-05,  2.9096e-05,  5.3868e-05,  ...,  3.9912e-08,\n",
      "          3.3303e-05,  8.1069e-06],\n",
      "        [ 1.7505e-04, -1.1799e-04, -3.2948e-04,  ..., -8.7491e-08,\n",
      "         -4.3049e-05,  6.3796e-05],\n",
      "        ...,\n",
      "        [-2.9459e-03,  2.7300e-03, -4.3676e-03,  ...,  1.1121e-01,\n",
      "         -3.3216e-02, -3.0152e-03],\n",
      "        [-2.7687e-02,  1.0093e-01,  6.0739e-02,  ..., -4.6503e-02,\n",
      "          6.4545e-01, -1.2580e-01],\n",
      "        [-8.3571e-03, -4.1267e-02, -1.5450e-02,  ..., -7.1632e-03,\n",
      "         -1.3031e-01,  2.9438e-02]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "ipdb> kuu = full_covar[..., :num_induc, :num_induc]\n",
      "ipdb> L = kuu.add_jitter().evaluate().cholesky()\n",
      "ipdb> inn = torch.eye(500).cuda() - variational_dist.covariance_matrix\n",
      "ipdb> inn\n",
      "tensor([[ 0.0000e+00, -8.5640e-10,  9.6806e-09,  ...,  0.0000e+00,\n",
      "          4.7684e-07, -1.7881e-07],\n",
      "        [-8.5640e-10,  0.0000e+00,  1.3367e-07,  ...,  0.0000e+00,\n",
      "          9.5367e-07,  2.3842e-07],\n",
      "        [ 9.6806e-09,  1.3367e-07, -2.3842e-07,  ...,  0.0000e+00,\n",
      "          4.7684e-07,  9.5367e-07],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.1321e-04,\n",
      "         -4.6683e-05, -2.2774e-06],\n",
      "        [ 4.7684e-07,  9.5367e-07,  4.7684e-07,  ..., -4.6683e-05,\n",
      "          7.2670e-04, -1.1615e-04],\n",
      "        [-1.7881e-07,  2.3842e-07,  9.5367e-07,  ..., -2.2774e-06,\n",
      "         -1.1615e-04,  6.6876e-05]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "ipdb> l1 = torch.triangular_solve(inn, L, upper=False)\n",
      "ipdb> l1.matmul(L.inverse())\n",
      "*** AttributeError: 'torch.return_types.triangular_solve' object has no attribute 'matmul'\n",
      "ipdb> l1 = torch.triangular_solve(inn, L, upper=False)[0]\n",
      "ipdb> l1.matmul(L.inverse())\n",
      "tensor([[ 2.0989e-05,  7.2612e-06, -2.1143e-05,  ...,  2.5563e-08,\n",
      "          1.7376e-05, -5.5743e-06],\n",
      "        [-2.1663e-05,  2.9097e-05,  5.3868e-05,  ...,  3.9912e-08,\n",
      "          3.3303e-05,  8.1069e-06],\n",
      "        [ 1.7506e-04, -1.1800e-04, -3.2948e-04,  ..., -8.7491e-08,\n",
      "         -4.3049e-05,  6.3796e-05],\n",
      "        ...,\n",
      "        [-2.9464e-03,  2.7298e-03, -4.3676e-03,  ...,  1.1121e-01,\n",
      "         -3.3216e-02, -3.0152e-03],\n",
      "        [-2.7687e-02,  1.0093e-01,  6.0738e-02,  ..., -4.6503e-02,\n",
      "          6.4545e-01, -1.2580e-01],\n",
      "        [-8.3569e-03, -4.1268e-02, -1.5452e-02,  ..., -7.1632e-03,\n",
      "         -1.3031e-01,  2.9438e-02]], device='cuda:0', grad_fn=<MmBackward>)\n",
      "ipdb> covar_cache\n",
      "tensor([[ 2.0988e-05,  7.2612e-06, -2.1143e-05,  ...,  2.5563e-08,\n",
      "          1.7376e-05, -5.5743e-06],\n",
      "        [-2.1663e-05,  2.9096e-05,  5.3868e-05,  ...,  3.9912e-08,\n",
      "          3.3303e-05,  8.1069e-06],\n",
      "        [ 1.7505e-04, -1.1799e-04, -3.2948e-04,  ..., -8.7491e-08,\n",
      "         -4.3049e-05,  6.3796e-05],\n",
      "        ...,\n",
      "        [-2.9459e-03,  2.7300e-03, -4.3676e-03,  ...,  1.1121e-01,\n",
      "         -3.3216e-02, -3.0152e-03],\n",
      "        [-2.7687e-02,  1.0093e-01,  6.0739e-02,  ..., -4.6503e-02,\n",
      "          6.4545e-01, -1.2580e-01],\n",
      "        [-8.3571e-03, -4.1267e-02, -1.5450e-02,  ..., -7.1632e-03,\n",
      "         -1.3031e-01,  2.9438e-02]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "ipdb> qq, ss = torch.symeig(kuu.evaluate())\n",
      "ipdb> ss\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<SymeigBackward>)\n",
      "ipdb> qq, ss = torch.symeig(kuu.evaluate(), eigenvectors=True))\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> ss, qq = torch.symeig(kuu.evaluate(), eigenvectors=True)\n",
      "ipdb> ss\n",
      "tensor([-6.1204e-06, -4.2017e-06, -2.9080e-06, -2.6696e-06, -1.7365e-06,\n",
      "        -1.1069e-06, -9.2339e-07, -8.6509e-07, -8.2536e-07, -8.0258e-07,\n",
      "        -7.9327e-07, -7.7035e-07, -7.6098e-07, -7.5127e-07, -7.4789e-07,\n",
      "        -7.4277e-07, -7.2974e-07, -7.1923e-07, -7.0561e-07, -6.9987e-07,\n",
      "        -6.8009e-07, -6.7563e-07, -6.7085e-07, -6.6266e-07, -6.5003e-07,\n",
      "        -6.4465e-07, -6.3866e-07, -6.2138e-07, -6.1985e-07, -6.1243e-07,\n",
      "        -6.1021e-07, -6.0521e-07, -5.9598e-07, -5.8506e-07, -5.8239e-07,\n",
      "        -5.8004e-07, -5.7120e-07, -5.6224e-07, -5.4922e-07, -5.4325e-07,\n",
      "        -5.3870e-07, -5.3623e-07, -5.3238e-07, -5.2321e-07, -5.1658e-07,\n",
      "        -5.0862e-07, -5.0378e-07, -5.0194e-07, -4.9816e-07, -4.9423e-07,\n",
      "        -4.8408e-07, -4.7709e-07, -4.7662e-07, -4.7229e-07, -4.6969e-07,\n",
      "        -4.6165e-07, -4.5721e-07, -4.5266e-07, -4.4902e-07, -4.4724e-07,\n",
      "        -4.4398e-07, -4.3783e-07, -4.2742e-07, -4.2685e-07, -4.1856e-07,\n",
      "        -4.1753e-07, -4.1003e-07, -4.0969e-07, -3.9740e-07, -3.8907e-07,\n",
      "        -3.8640e-07, -3.8513e-07, -3.7504e-07, -3.6870e-07, -3.6783e-07,\n",
      "        -3.5904e-07, -3.5137e-07, -3.4855e-07, -3.4818e-07, -3.4600e-07,\n",
      "        -3.4539e-07, -3.3331e-07, -3.2746e-07, -3.2182e-07, -3.2097e-07,\n",
      "        -3.1394e-07, -3.0880e-07, -3.0186e-07, -2.9774e-07, -2.9506e-07,\n",
      "        -2.9175e-07, -2.8688e-07, -2.8366e-07, -2.7145e-07, -2.6631e-07,\n",
      "        -2.6590e-07, -2.6563e-07, -2.5909e-07, -2.5693e-07, -2.4865e-07,\n",
      "        -2.4564e-07, -2.4419e-07, -2.3949e-07, -2.3796e-07, -2.3066e-07,\n",
      "        -2.2522e-07, -2.2020e-07, -2.1747e-07, -2.0830e-07, -2.0743e-07,\n",
      "        -2.0331e-07, -1.9794e-07, -1.9678e-07, -1.9229e-07, -1.8578e-07,\n",
      "        -1.7918e-07, -1.7759e-07, -1.6781e-07, -1.6716e-07, -1.6389e-07,\n",
      "        -1.6278e-07, -1.5289e-07, -1.5041e-07, -1.3734e-07, -1.3294e-07,\n",
      "        -1.3273e-07, -1.3004e-07, -1.2507e-07, -1.1841e-07, -1.1508e-07,\n",
      "        -1.1175e-07, -1.0298e-07, -1.0113e-07, -1.0046e-07, -9.3232e-08,\n",
      "        -8.9162e-08, -8.6228e-08, -8.2863e-08, -7.6457e-08, -7.4904e-08,\n",
      "        -7.1983e-08, -6.7440e-08, -5.8697e-08, -5.7273e-08, -5.5271e-08,\n",
      "        -4.9622e-08, -4.8990e-08, -4.2641e-08, -3.7477e-08, -3.7019e-08,\n",
      "        -2.9503e-08, -2.5234e-08, -2.3226e-08, -1.6231e-08, -1.2549e-08,\n",
      "        -8.4476e-09, -7.2791e-09,  3.4352e-09,  9.7794e-09,  1.2492e-08,\n",
      "         1.3649e-08,  1.9058e-08,  2.4166e-08,  2.8080e-08,  3.3787e-08,\n",
      "         3.5933e-08,  3.9022e-08,  4.7871e-08,  5.4701e-08,  5.5683e-08,\n",
      "         5.8311e-08,  6.2251e-08,  6.9628e-08,  7.5135e-08,  8.4904e-08,\n",
      "         8.5867e-08,  8.9480e-08,  9.2632e-08,  9.4631e-08,  9.9367e-08,\n",
      "         1.0457e-07,  1.1089e-07,  1.1114e-07,  1.1777e-07,  1.2449e-07,\n",
      "         1.2698e-07,  1.3289e-07,  1.3715e-07,  1.3865e-07,  1.4021e-07,\n",
      "         1.4445e-07,  1.5061e-07,  1.5142e-07,  1.5433e-07,  1.6079e-07,\n",
      "         1.7082e-07,  1.7161e-07,  1.7438e-07,  1.7618e-07,  1.7688e-07,\n",
      "         1.8381e-07,  1.8973e-07,  1.9014e-07,  2.0030e-07,  2.0120e-07,\n",
      "         2.0385e-07,  2.0779e-07,  2.0838e-07,  2.1948e-07,  2.1980e-07,\n",
      "         2.2718e-07,  2.3038e-07,  2.3377e-07,  2.3499e-07,  2.3797e-07,\n",
      "         2.4067e-07,  2.4910e-07,  2.5389e-07,  2.6789e-07,  2.6806e-07,\n",
      "         2.7597e-07,  2.7722e-07,  2.7931e-07,  2.8189e-07,  2.8312e-07,\n",
      "         2.8624e-07,  2.9447e-07,  2.9963e-07,  3.0344e-07,  3.0484e-07,\n",
      "         3.0814e-07,  3.1812e-07,  3.2089e-07,  3.2254e-07,  3.2754e-07,\n",
      "         3.4324e-07,  3.4768e-07,  3.4951e-07,  3.5753e-07,  3.5918e-07,\n",
      "         3.6104e-07,  3.6393e-07,  3.6901e-07,  3.7269e-07,  3.7699e-07,\n",
      "         3.8355e-07,  3.9036e-07,  3.9425e-07,  4.0083e-07,  4.0141e-07,\n",
      "         4.0298e-07,  4.1401e-07,  4.1738e-07,  4.2030e-07,  4.2611e-07,\n",
      "         4.3012e-07,  4.3573e-07,  4.3694e-07,  4.3878e-07,  4.4694e-07,\n",
      "         4.5428e-07,  4.5807e-07,  4.6498e-07,  4.7057e-07,  4.7127e-07,\n",
      "         4.7723e-07,  4.8523e-07,  4.8957e-07,  4.9242e-07,  5.0137e-07,\n",
      "         5.0583e-07,  5.1019e-07,  5.1058e-07,  5.1758e-07,  5.2728e-07,\n",
      "         5.2751e-07,  5.3285e-07,  5.3704e-07,  5.4030e-07,  5.4268e-07,\n",
      "         5.4904e-07,  5.5671e-07,  5.5962e-07,  5.6504e-07,  5.7786e-07,\n",
      "         5.8119e-07,  5.8450e-07,  5.8606e-07,  6.0103e-07,  6.1543e-07,\n",
      "         6.1558e-07,  6.1924e-07,  6.3185e-07,  6.3660e-07,  6.4763e-07,\n",
      "         6.4968e-07,  6.5208e-07,  6.6015e-07,  6.7057e-07,  6.7872e-07,\n",
      "         6.7934e-07,  6.8125e-07,  6.8937e-07,  7.0582e-07,  7.0586e-07,\n",
      "         7.1245e-07,  7.2216e-07,  7.3119e-07,  7.4182e-07,  7.4540e-07,\n",
      "         7.6935e-07,  7.7141e-07,  7.8519e-07,  7.9854e-07,  8.0265e-07,\n",
      "         8.1770e-07,  8.3345e-07,  8.4395e-07,  8.5931e-07,  8.8405e-07,\n",
      "         9.0642e-07,  9.0913e-07,  9.2145e-07,  9.4747e-07,  9.5518e-07,\n",
      "         1.0577e-06,  1.0698e-06,  1.0772e-06,  1.1283e-06,  1.1966e-06,\n",
      "         1.2290e-06,  1.2813e-06,  1.3456e-06,  1.3636e-06,  1.4042e-06,\n",
      "         1.5426e-06,  1.5601e-06,  1.6272e-06,  1.7009e-06,  1.8418e-06,\n",
      "         1.9766e-06,  2.2382e-06,  2.3222e-06,  2.4835e-06,  2.7158e-06,\n",
      "         2.8826e-06,  3.0525e-06,  3.1172e-06,  3.2427e-06,  3.4683e-06,\n",
      "         3.8471e-06,  3.9653e-06,  4.4610e-06,  4.5812e-06,  5.0896e-06,\n",
      "         5.3420e-06,  5.4837e-06,  5.6378e-06,  6.2991e-06,  6.5152e-06,\n",
      "         6.9779e-06,  7.3739e-06,  7.4364e-06,  8.1112e-06,  8.9958e-06,\n",
      "         1.0067e-05,  1.0168e-05,  1.0616e-05,  1.1413e-05,  1.2072e-05,\n",
      "         1.4949e-05,  1.5435e-05,  1.6731e-05,  1.8768e-05,  2.0374e-05,\n",
      "         2.2518e-05,  2.2958e-05,  2.3139e-05,  2.8193e-05,  2.9857e-05,\n",
      "         3.3047e-05,  3.5144e-05,  3.6020e-05,  4.1054e-05,  4.1316e-05,\n",
      "         4.6809e-05,  4.8750e-05,  5.7151e-05,  6.4702e-05,  6.6831e-05,\n",
      "         6.8678e-05,  7.6289e-05,  8.8280e-05,  1.0057e-04,  1.0633e-04,\n",
      "         1.0834e-04,  1.1734e-04,  1.3158e-04,  1.3560e-04,  1.5270e-04,\n",
      "         1.6184e-04,  1.7901e-04,  1.9885e-04,  2.0737e-04,  2.4937e-04,\n",
      "         2.6091e-04,  2.7070e-04,  2.8824e-04,  3.0733e-04,  3.1650e-04,\n",
      "         3.6833e-04,  3.8637e-04,  4.5822e-04,  4.8002e-04,  5.1735e-04,\n",
      "         5.8395e-04,  6.3627e-04,  7.2032e-04,  7.9305e-04,  8.0566e-04,\n",
      "         8.8056e-04,  9.5410e-04,  1.0766e-03,  1.1228e-03,  1.3605e-03,\n",
      "         1.3781e-03,  1.5949e-03,  1.6826e-03,  1.7709e-03,  1.8581e-03,\n",
      "         2.0707e-03,  2.4116e-03,  2.7794e-03,  3.0011e-03,  3.1439e-03,\n",
      "         3.2635e-03,  4.0668e-03,  4.8354e-03,  5.2052e-03,  5.7627e-03,\n",
      "         6.1227e-03,  6.4906e-03,  7.2833e-03,  8.2936e-03,  8.9887e-03,\n",
      "         9.6459e-03,  1.1243e-02,  1.1865e-02,  1.2965e-02,  1.4379e-02,\n",
      "         1.5995e-02,  1.7322e-02,  1.9234e-02,  2.4242e-02,  2.6047e-02,\n",
      "         2.7604e-02,  2.8684e-02,  3.3941e-02,  3.6676e-02,  4.0061e-02,\n",
      "         4.5389e-02,  5.0888e-02,  6.0223e-02,  7.3143e-02,  8.8794e-02,\n",
      "         1.0491e-01,  1.0801e-01,  1.2522e-01,  1.5109e-01,  1.5916e-01,\n",
      "         1.9136e-01,  1.9519e-01,  2.2401e-01,  2.7709e-01,  2.8946e-01,\n",
      "         3.4978e-01,  3.8229e-01,  4.4052e-01,  5.6472e-01,  6.0338e-01,\n",
      "         7.2730e-01,  8.0650e-01,  1.0346e+00,  1.2459e+00,  1.4166e+00,\n",
      "         2.0735e+00,  2.3826e+00,  2.4632e+00,  3.1176e+00,  3.6131e+00,\n",
      "         4.4411e+00,  6.5132e+00,  6.8865e+00,  1.1645e+01,  1.3619e+01,\n",
      "         1.4819e+01,  2.4718e+01,  4.0720e+01,  5.8228e+01,  1.4119e+02],\n",
      "       device='cuda:0', grad_fn=<SymeigBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> qq\n",
      "tensor([[-2.3888e-02,  2.0590e-02, -4.0280e-02,  ...,  5.9359e-03,\n",
      "          6.7584e-02,  4.3529e-02],\n",
      "        [ 1.5953e-02,  4.1121e-02, -5.7521e-03,  ..., -6.1103e-02,\n",
      "         -4.7232e-02,  4.4257e-02],\n",
      "        [ 2.5312e-02,  1.7891e-04, -9.0191e-03,  ..., -6.0704e-02,\n",
      "         -2.5618e-02,  5.1091e-02],\n",
      "        ...,\n",
      "        [-1.8273e-02, -1.7307e-01, -2.1893e-01,  ...,  5.0359e-02,\n",
      "          2.8332e-02,  2.0349e-02],\n",
      "        [-6.5715e-01,  2.7181e-02, -1.2818e-02,  ..., -2.2878e-02,\n",
      "         -4.3554e-02,  6.0219e-02],\n",
      "        [ 7.9669e-02, -1.0110e-02, -1.8297e-01,  ..., -3.3258e-02,\n",
      "         -5.4535e-02,  2.4553e-02]], device='cuda:0', grad_fn=<SymeigBackward>)\n",
      "ipdb> print(qq.shape)\n",
      "torch.Size([500, 500])\n",
      "ipdb> DiagLazyTensor\n",
      "<class 'gpytorch.lazy.diag_lazy_tensor.DiagLazyTensor'>\n",
      "ipdb> ss = DiagLazyTensor(ss).evaluate()\n",
      "ipdb> ss\n",
      "tensor([[-6.1204e-06, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -4.2017e-06, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -2.9080e-06,  ..., -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.0720e+01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          5.8228e+01,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  1.4119e+02]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "ipdb> torch.norm((qq.t() @ ss @ qq) - kuu)\n",
      "*** TypeError: sub(): argument 'other' (position 1) must be Tensor, not LazyEvaluatedKernelTensor\n",
      "ipdb> torch.norm((qq.t() @ ss @ qq) - kuu.evaluate())\n",
      "tensor(227.9309, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ipdb> torch.norm((qq @ ss @ qq.t()) - kuu.evaluate())\n",
      "tensor(9.8708e-05, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ipdb> ss.min()\n",
      "tensor(-6.1204e-06, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "ipdb> ss = ss + 1e-4 * torch.eye(500, device='cuda')\n",
      "ipdb> ss.min()\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "ipdb> ss.diag().min()\n",
      "tensor(9.3880e-05, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "ipdb> ss.diag().max()\n",
      "tensor(141.1904, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "ipdb> ssr = ss.sqrt()\n",
      "ipdb> zzz = (qq @ ssr @ qq.t() @ inn @ qq @ ssr @ qq.t())\n",
      "ipdb> zzz\n",
      "tensor([[ 5.3258e-04,  1.8187e-04,  1.7357e-04,  ...,  4.4082e-04,\n",
      "          2.1942e-04,  2.4023e-05],\n",
      "        [ 1.8187e-04,  1.3317e-04,  1.8350e-04,  ...,  2.4164e-04,\n",
      "          2.7818e-04, -2.4657e-04],\n",
      "        [ 1.7357e-04,  1.8350e-04,  2.1091e-04,  ...,  2.5825e-04,\n",
      "          2.7874e-04, -1.9608e-04],\n",
      "        ...,\n",
      "        [ 4.4082e-04,  2.4164e-04,  2.5825e-04,  ...,  3.6733e-04,\n",
      "          3.1814e-04,  1.1969e-04],\n",
      "        [ 2.1942e-04,  2.7818e-04,  2.7874e-04,  ...,  3.1814e-04,\n",
      "          4.6067e-04,  2.7758e-06],\n",
      "        [ 2.4023e-05, -2.4657e-04, -1.9608e-04,  ...,  1.1969e-04,\n",
      "          2.7758e-06, -1.1560e-04]], device='cuda:0', grad_fn=<MmBackward>)\n",
      "ipdb> L\n",
      "tensor([[ 8.3315e-01,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 9.3213e-02,  8.2792e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.8246e-01,  7.5207e-01,  3.0863e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 3.9609e-01, -3.2163e-02, -8.5689e-02,  ...,  3.9280e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.3500e-01,  6.0818e-01,  1.5999e-01,  ..., -5.1696e-05,\n",
      "          3.2776e-02,  0.0000e+00],\n",
      "        [ 1.6970e-02,  4.7093e-01, -3.3074e-01,  ...,  1.8985e-05,\n",
      "         -5.0527e-04,  3.8502e-02]], device='cuda:0',\n",
      "       grad_fn=<CholeskyBackward>)\n",
      "ipdb> L.inverse().transpose(-2, -1)\n",
      "tensor([[ 1.2003e+00, -1.3513e-01, -3.8029e-01,  ...,  1.1950e-01,\n",
      "         -7.4543e-02, -2.8638e-02],\n",
      "        [ 0.0000e+00,  1.2078e+00, -2.9432e+00,  ..., -2.0510e-02,\n",
      "          4.8488e-02, -3.7271e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  3.2401e+00,  ..., -4.0819e-02,\n",
      "         -2.4213e-01, -1.1595e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.5458e+01,\n",
      "          4.0155e-02, -1.2026e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          3.0510e+01,  4.0039e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  2.5972e+01]], device='cuda:0',\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "ipdb> L.transpose(-2, -1).inverse()\n",
      "tensor([[ 1.2003e+00, -1.3513e-01, -3.8029e-01,  ...,  1.1949e-01,\n",
      "         -7.4544e-02, -2.8637e-02],\n",
      "        [ 0.0000e+00,  1.2078e+00, -2.9432e+00,  ..., -2.0507e-02,\n",
      "          4.8487e-02, -3.7272e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  3.2401e+00,  ..., -4.0820e-02,\n",
      "         -2.4212e-01, -1.1594e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.5458e+01,\n",
      "          4.0155e-02, -1.2026e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          3.0510e+01,  4.0039e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  2.5972e+01]], device='cuda:0',\n",
      "       grad_fn=<InverseBackward>)\n",
      "ipdb> torch.norm(L.inverse().transpose(-2, -1) - L.transpose(-2, -1).inverse())\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ipdb> zzz2 = L.inverse().transpose(-2, -1) @ inn @ L.inverse()\n",
      "ipdb> zzz2\n",
      "tensor([[ 5.1652e-02, -1.2946e-02, -7.2219e-03,  ...,  1.1619e-04,\n",
      "         -2.5843e-02, -5.9405e-04],\n",
      "        [-1.2946e-02,  2.5440e-02, -1.4775e-02,  ...,  2.5721e-03,\n",
      "          8.6492e-02,  1.0962e-02],\n",
      "        [-7.2219e-03, -1.4775e-02, -1.1962e-02,  ..., -9.6185e-03,\n",
      "          6.5650e-02,  1.3304e-02],\n",
      "        ...,\n",
      "        [ 1.1619e-04,  2.5721e-03, -9.6185e-03,  ...,  1.3809e-01,\n",
      "         -3.5353e-02, -1.6479e-03],\n",
      "        [-2.5843e-02,  8.6492e-02,  6.5650e-02,  ..., -3.5353e-02,\n",
      "          6.7364e-01, -9.1344e-02],\n",
      "        [-5.9405e-04,  1.0962e-02,  1.3304e-02,  ..., -1.6479e-03,\n",
      "         -9.1344e-02,  4.5113e-02]], device='cuda:0', grad_fn=<MmBackward>)\n",
      "ipdb> zzz\n",
      "tensor([[ 5.3258e-04,  1.8187e-04,  1.7357e-04,  ...,  4.4082e-04,\n",
      "          2.1942e-04,  2.4023e-05],\n",
      "        [ 1.8187e-04,  1.3317e-04,  1.8350e-04,  ...,  2.4164e-04,\n",
      "          2.7818e-04, -2.4657e-04],\n",
      "        [ 1.7357e-04,  1.8350e-04,  2.1091e-04,  ...,  2.5825e-04,\n",
      "          2.7874e-04, -1.9608e-04],\n",
      "        ...,\n",
      "        [ 4.4082e-04,  2.4164e-04,  2.5825e-04,  ...,  3.6733e-04,\n",
      "          3.1814e-04,  1.1969e-04],\n",
      "        [ 2.1942e-04,  2.7818e-04,  2.7874e-04,  ...,  3.1814e-04,\n",
      "          4.6067e-04,  2.7758e-06],\n",
      "        [ 2.4023e-05, -2.4657e-04, -1.9608e-04,  ...,  1.1969e-04,\n",
      "          2.7758e-06, -1.1560e-04]], device='cuda:0', grad_fn=<MmBackward>)\n",
      "ipdb> qqq = torch.triangular_solve(inn, L, upper=True, transpose=True)\n",
      "ipdb> qqq2 = L.inverse().transpose(-2, -1) @ inn\n",
      "ipdb> torch.norm(qqq - qqq2)\n",
      "*** TypeError: rsub() received an invalid combination of arguments - got (Tensor, torch.return_types.triangular_solve), but expected one of:\n",
      " * (Tensor input, Tensor other, Number alpha)\n",
      " * (Tensor input, Number other, Number alpha)\n",
      "ipdb> torch.norm(qqq[0] - qqq2)\n",
      "tensor(0.5942, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "ipdb> torch.norm(qqq[0] - qqq2) / qqq2.norm()\n",
      "tensor(0.4215, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "ipdb> qqq[0]\n",
      "tensor([[ 0.0000e+00, -1.0279e-09,  1.1619e-08,  ...,  0.0000e+00,\n",
      "          5.7233e-07, -2.1462e-07],\n",
      "        [-1.0344e-09,  0.0000e+00,  1.6145e-07,  ...,  0.0000e+00,\n",
      "          1.1519e-06,  2.8797e-07],\n",
      "        [ 3.1366e-08,  4.3309e-07, -7.7250e-07,  ...,  0.0000e+00,\n",
      "          1.5450e-06,  3.0900e-06],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  5.4279e-03,\n",
      "         -1.1885e-03, -5.7980e-05],\n",
      "        [ 1.4548e-05,  2.9097e-05,  1.4548e-05,  ..., -1.4243e-03,\n",
      "          2.2172e-02, -3.5437e-03],\n",
      "        [-4.6442e-06,  6.1923e-06,  2.4769e-05,  ..., -5.9150e-05,\n",
      "         -3.0167e-03,  1.7369e-03]], device='cuda:0',\n",
      "       grad_fn=<TriangularSolveBackward>)\n",
      "ipdb> qqq2\n",
      "tensor([[ 1.7487e-05, -1.5979e-05,  4.1565e-05,  ...,  5.8886e-06,\n",
      "         -8.4672e-04, -2.2872e-05],\n",
      "        [ 6.0497e-06,  2.4767e-05, -1.3210e-05,  ...,  9.6769e-05,\n",
      "          2.8293e-03,  4.2204e-04],\n",
      "        [-1.7615e-05,  4.2628e-05, -6.5033e-05,  ..., -3.8095e-04,\n",
      "          2.1450e-03,  5.1224e-04],\n",
      "        ...,\n",
      "        [ 2.1298e-08,  3.5427e-08,  7.6783e-09,  ...,  5.4260e-03,\n",
      "         -1.1579e-03, -6.3448e-05],\n",
      "        [ 1.4477e-05,  2.9192e-05,  1.4930e-05,  ..., -1.4252e-03,\n",
      "          2.2125e-02, -3.5170e-03],\n",
      "        [-4.6442e-06,  6.1923e-06,  2.4769e-05,  ..., -5.9150e-05,\n",
      "         -3.0167e-03,  1.7369e-03]], device='cuda:0', grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> qqq = torch.triangular_solve(inn, L, transpose=True)\n",
      "ipdb> qqq\n",
      "torch.return_types.triangular_solve(\n",
      "solution=tensor([[ 0.0000e+00, -1.0279e-09,  1.1619e-08,  ...,  0.0000e+00,\n",
      "          5.7233e-07, -2.1462e-07],\n",
      "        [-1.0344e-09,  0.0000e+00,  1.6145e-07,  ...,  0.0000e+00,\n",
      "          1.1519e-06,  2.8797e-07],\n",
      "        [ 3.1366e-08,  4.3309e-07, -7.7250e-07,  ...,  0.0000e+00,\n",
      "          1.5450e-06,  3.0900e-06],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  5.4279e-03,\n",
      "         -1.1885e-03, -5.7980e-05],\n",
      "        [ 1.4548e-05,  2.9097e-05,  1.4548e-05,  ..., -1.4243e-03,\n",
      "          2.2172e-02, -3.5437e-03],\n",
      "        [-4.6442e-06,  6.1923e-06,  2.4769e-05,  ..., -5.9150e-05,\n",
      "         -3.0167e-03,  1.7369e-03]], device='cuda:0',\n",
      "       grad_fn=<TriangularSolveBackward>),\n",
      "cloned_coefficient=tensor([[ 8.3315e-01,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 9.3213e-02,  8.2792e-01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.8246e-01,  7.5207e-01,  3.0863e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 3.9609e-01, -3.2163e-02, -8.5689e-02,  ...,  3.9280e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.3500e-01,  6.0818e-01,  1.5999e-01,  ..., -5.1696e-05,\n",
      "          3.2776e-02,  0.0000e+00],\n",
      "        [ 1.6970e-02,  4.7093e-01, -3.3074e-01,  ...,  1.8985e-05,\n",
      "         -5.0527e-04,  3.8502e-02]], device='cuda:0',\n",
      "       grad_fn=<TriangularSolveBackward>))\n",
      "ipdb> qqq[0]\n",
      "tensor([[ 0.0000e+00, -1.0279e-09,  1.1619e-08,  ...,  0.0000e+00,\n",
      "          5.7233e-07, -2.1462e-07],\n",
      "        [-1.0344e-09,  0.0000e+00,  1.6145e-07,  ...,  0.0000e+00,\n",
      "          1.1519e-06,  2.8797e-07],\n",
      "        [ 3.1366e-08,  4.3309e-07, -7.7250e-07,  ...,  0.0000e+00,\n",
      "          1.5450e-06,  3.0900e-06],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  5.4279e-03,\n",
      "         -1.1885e-03, -5.7980e-05],\n",
      "        [ 1.4548e-05,  2.9097e-05,  1.4548e-05,  ..., -1.4243e-03,\n",
      "          2.2172e-02, -3.5437e-03],\n",
      "        [-4.6442e-06,  6.1923e-06,  2.4769e-05,  ..., -5.9150e-05,\n",
      "         -3.0167e-03,  1.7369e-03]], device='cuda:0',\n",
      "       grad_fn=<TriangularSolveBackward>)\n",
      "ipdb> qqq2 = L.inverse().transpose(-2, -1) @ inn\n",
      "ipdb> qqq1 = torch.triangular_solve(inn, L, upper=True, transpose=True)\n",
      "ipdb> qqq1[:2, :2]\n",
      "*** TypeError: tuple indices must be integers or slices, not tuple\n",
      "ipdb> qqq1[0][:2, :2]\n",
      "tensor([[ 0.0000e+00, -1.0279e-09],\n",
      "        [-1.0344e-09,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "ipdb> qqq2[:2, :2]\n",
      "tensor([[ 1.7487e-05, -1.5979e-05],\n",
      "        [ 6.0497e-06,  2.4767e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "ipdb> qqq1 = torch.triangular_solve(inn, L.transpose(-2, -1), upper=True)\n",
      "ipdb> qqq1[0][:2, :2]\n",
      "tensor([[ 1.7486e-05, -1.5979e-05],\n",
      "        [ 6.0497e-06,  2.4766e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "ipdb> qqq1 = torch.triangular_solve(inn, L, transpose=True)\n",
      "ipdb> qqq1[0][:2, :2]\n",
      "tensor([[ 0.0000e+00, -1.0279e-09],\n",
      "        [-1.0344e-09,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "ipdb> qqq1 = torch.triangular_solve(inn, L, upper=True, transpose=True)\n",
      "ipdb> qqq1[0][:2, :2]\n",
      "tensor([[ 0.0000e+00, -1.0279e-09],\n",
      "        [-1.0344e-09,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "ipdb> qqq1 = torch.triangular_solve(inn, L, transpose=False)\n",
      "ipdb> qqq1[0][:2, :2]\n",
      "tensor([[ 0.0000e+00, -1.0279e-09],\n",
      "        [-1.0344e-09,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "ipdb> right_rinv = torch.triangular_solve(inner_mat, L, upper=False)[0].transpose(-2, -1)\n",
      "*** NameError: name 'inner_mat' is not defined\n",
      "ipdb> right_rinv = torch.triangular_solve(inn, L, upper=False)[0].transpose(-2, -1)\n",
      "ipdb> inner_mat = inn\n",
      "ipdb> right_rinv = torch.triangular_solve(inner_mat, L, upper=False)[0].transpose(-2, -1)\n",
      "ipdb> torch.triangular_solve(right_rinv, L.transpose(-2, -1), upper=True)\n",
      "torch.return_types.triangular_solve(\n",
      "solution=tensor([[ 2.0989e-05, -2.1663e-05,  1.7506e-04,  ..., -2.9468e-03,\n",
      "         -2.7687e-02, -8.3568e-03],\n",
      "        [ 7.2612e-06,  2.9097e-05, -1.1800e-04,  ...,  2.7298e-03,\n",
      "          1.0093e-01, -4.1267e-02],\n",
      "        [-2.1143e-05,  5.3868e-05, -3.2948e-04,  ..., -4.3681e-03,\n",
      "          6.0738e-02, -1.5452e-02],\n",
      "        ...,\n",
      "        [ 2.5563e-08,  3.9912e-08, -8.7491e-08,  ...,  1.1121e-01,\n",
      "         -4.6503e-02, -7.1632e-03],\n",
      "        [ 1.7376e-05,  3.3303e-05, -4.3049e-05,  ..., -3.3216e-02,\n",
      "          6.4545e-01, -1.3031e-01],\n",
      "        [-5.5743e-06,  8.1069e-06,  6.3796e-05,  ..., -3.0152e-03,\n",
      "         -1.2580e-01,  2.9438e-02]], device='cuda:0',\n",
      "       grad_fn=<TriangularSolveBackward>),\n",
      "cloned_coefficient=tensor([[ 8.3315e-01,  9.3213e-02,  1.8246e-01,  ...,  3.9609e-01,\n",
      "          2.3500e-01,  1.6970e-02],\n",
      "        [ 0.0000e+00,  8.2792e-01,  7.5207e-01,  ..., -3.2163e-02,\n",
      "          6.0818e-01,  4.7093e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  3.0863e-01,  ..., -8.5689e-02,\n",
      "          1.5999e-01, -3.3074e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.9280e-02,\n",
      "         -5.1696e-05,  1.8985e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          3.2776e-02, -5.0527e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  3.8502e-02]], device='cuda:0',\n",
      "       grad_fn=<TriangularSolveBackward>))\n",
      "ipdb> real_ans = L.inverse().transpose(-2, -1) @ inn @ L.inverse()\n",
      "ipdb> real_ans\n",
      "tensor([[ 5.1652e-02, -1.2946e-02, -7.2219e-03,  ...,  1.1619e-04,\n",
      "         -2.5843e-02, -5.9405e-04],\n",
      "        [-1.2946e-02,  2.5440e-02, -1.4775e-02,  ...,  2.5721e-03,\n",
      "          8.6492e-02,  1.0962e-02],\n",
      "        [-7.2219e-03, -1.4775e-02, -1.1962e-02,  ..., -9.6185e-03,\n",
      "          6.5650e-02,  1.3304e-02],\n",
      "        ...,\n",
      "        [ 1.1619e-04,  2.5721e-03, -9.6185e-03,  ...,  1.3809e-01,\n",
      "         -3.5353e-02, -1.6479e-03],\n",
      "        [-2.5843e-02,  8.6492e-02,  6.5650e-02,  ..., -3.5353e-02,\n",
      "          6.7364e-01, -9.1344e-02],\n",
      "        [-5.9405e-04,  1.0962e-02,  1.3304e-02,  ..., -1.6479e-03,\n",
      "         -9.1344e-02,  4.5113e-02]], device='cuda:0', grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 6 epochs of training in this tutorial\n",
    "num_epochs = 4\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "# We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "# effective for VI.\n",
    "with gpytorch.settings.max_cg_iterations(45):\n",
    "    for i in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            # with combine_terms=False, we get the terms of the ELBO separated so we can print them individually if we'd like.\n",
    "            # loss = -mll(output, y_batch) would also work.\n",
    "            log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "            loss = -(log_lik - kl_div + log_prior)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader), loss.item(), log_lik.item(), kl_div.item(), log_prior.item()))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling libKeOpstorch8dcd847cfd in /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37//build-libKeOpstorch8dcd847cfd:\n",
      "       formula: Sum_Reduction((Exp((Minus(Sum(Square((Var(0,3,0) - Var(1,3,1))))) / IntCst(2))) * Var(2,1,1)),0)\n",
      "       aliases: Var(0,3,0); Var(1,3,1); Var(2,1,1); \n",
      "       dtype  : float32\n",
      "... \n",
      "--------------------- MAKE DEBUG -----------------\n",
      "Command '['cmake', '--build', '.', '--target', 'libKeOpstorch8dcd847cfd', '--', 'VERBOSE=1']' returned non-zero exit status 2.\n",
      "/usr/local/bin/cmake -S/home/jake.gardner/anaconda3/lib/python3.7/site-packages/pykeops -B/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd --check-build-system CMakeFiles/Makefile.cmake 0\n",
      "/usr/bin/make -f CMakeFiles/Makefile2 libKeOpstorch8dcd847cfd\n",
      "make[1]: Entering directory '/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd'\n",
      "/usr/local/bin/cmake -S/home/jake.gardner/anaconda3/lib/python3.7/site-packages/pykeops -B/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd --check-build-system CMakeFiles/Makefile.cmake 0\n",
      "/usr/local/bin/cmake -E cmake_progress_start /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles 5\n",
      "/usr/bin/make -f CMakeFiles/Makefile2 CMakeFiles/libKeOpstorch8dcd847cfd.dir/all\n",
      "make[2]: Entering directory '/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd'\n",
      "/usr/bin/make -f CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/build.make CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/depend\n",
      "make[3]: Entering directory '/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd'\n",
      "[ 20%] Building NVCC (Device) object CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o\n",
      "cd /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core && /usr/local/bin/cmake -E make_directory /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/.\n",
      "cd /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core && /usr/local/bin/cmake -D verbose:BOOL=1 -D build_configuration:STRING=Release -D generated_file:STRING=/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/./keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o -D generated_cubin_file:STRING=/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/./keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o.cubin.txt -P /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o.Release.cmake\n",
      "-- Removing /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/./keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o\n",
      "/usr/local/bin/cmake -E remove /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/./keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o\n",
      "-- Generating dependency file: /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o.NVCC-depend\n",
      "/usr/local/cuda/bin/nvcc -M -D__CUDACC__ /home/jake.gardner/anaconda3/lib/python3.7/site-packages/pykeops/keops/core/link_autodiff.cu -o /home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd/CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o.NVCC-depend -m64 -DkeopslibKeOpstorch8dcd847cfd_EXPORTS -DMAXIDGPU=1 -DMAXTHREADSPERBLOCK0=1024 -DSHAREDMEMPERBLOCK0=49152 -DMAXTHREADSPERBLOCK1=1024 -DSHAREDMEMPERBLOCK1=49152 -D_FORCE_INLINES -DCUDA_BLOCK_SIZE=192 -DUSE_CUDA=1 -D__TYPE__=float -DC_CONTIGUOUS=1 -DMODULE_NAME=libKeOpstorch8dcd847cfd -D_GLIBCXX_USE_CXX11_ABI=0 -DUSE_DOUBLE=0 -DKERNEL_GEOM_TYPE=0 -DKERNEL_SIG_TYPE=0 -DKERNEL_SPHERE_TYPE=0 -DMODULE_NAME_FSHAPE_SCP=fshape_scp_gaussiangaussiangaussian_unoriented_float -Xcompiler ,\\\"-Wall\\\",\\\"-fmax-errors=2\\\",\\\"-fPIC\\\",\\\"-O3\\\",\\\"-DNDEBUG\\\",\\\"-O3\\\" -gencode arch=compute_75,code=sm_75 --use_fast_math --compiler-options=-fPIC --expt-relaxed-constexpr --pre-include=libKeOpstorch8dcd847cfd.h -DNVCC -I/usr/local/cuda/include -I/home/jake.gardner/anaconda3/lib/python3.7/site-packages/pykeops -I/home/jake.gardner/anaconda3/lib/python3.7/site-packages/pykeops/keops -I/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd -I/home/jake.gardner/anaconda3/lib/python3.7/site-packages/torch/include -I/home/jake.gardner/anaconda3/lib/python3.7/site-packages/torch/include/torch/csrc/api/include\n",
      "CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/build.make:63: recipe for target 'CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/keops/core/keopslibKeOpstorch8dcd847cfd_generated_link_autodiff.cu.o' failed\n",
      "make[3]: Leaving directory '/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd'\n",
      "CMakeFiles/Makefile2:146: recipe for target 'CMakeFiles/keopslibKeOpstorch8dcd847cfd.dir/all' failed\n",
      "make[2]: Leaving directory '/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd'\n",
      "CMakeFiles/Makefile2:269: recipe for target 'CMakeFiles/libKeOpstorch8dcd847cfd.dir/rule' failed\n",
      "make[1]: Leaving directory '/home/jake.gardner/.cache/pykeops-1.1.2-cpython-37/build-libKeOpstorch8dcd847cfd'\n",
      "Makefile:183: recipe for target 'libKeOpstorch8dcd847cfd' failed\n",
      "\n",
      "--------------------- ----------- -----------------\n",
      "Done.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'libKeOpstorch8dcd847cfd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/models/abstract_variational_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/gpytorch/gpytorch/variational/half_whitened_variational_strategy.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_params_initialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variational_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_params_initialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/variational/half_whitened_variational_strategy.py\u001b[0m in \u001b[0;36minitialize_variational_dist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m         eval_prior_dist = torch.distributions.MultivariateNormal(\n\u001b[1;32m    117\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mscale_tril\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpsd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variational_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_prior_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mcovariance_matrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcache_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0madd_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcache_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0madd_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/lazy/constant_mul_lazy_tensor.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanded_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcache_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0madd_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0meye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0meye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meye\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/functions/_matmul.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, representation_tree, rhs, *matrix_args)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlazy_tsr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmatrix_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mto_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0morig_rhs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gpytorch/gpytorch/lazy/keops_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovar_mat\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovar_mat\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pykeops/common/lazy_tensor.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mv_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (Nv,1,..,1,batchdimsv,1,N,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m             \u001b[0mv_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             \u001b[0mKv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mv_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (Nv,outbatchdims,M,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m             \u001b[0mKv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (outbatchdims,M,1,Nv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0mKv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pykeops/common/lazy_tensor.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self, axis, dim, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msum_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pykeops/common/lazy_tensor.py\u001b[0m in \u001b[0;36mreduction\u001b[0;34m(self, reduction_op, other, opt_arg, axis, dim, call, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_variables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pykeops/common/lazy_tensor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pykeops/torch/generic/generic_red.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, backend, device_id, ranges, *args)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenredAutograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maliases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maliases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mnout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pykeops/torch/generic/generic_red.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, formula, aliases, backend, dtype, device_id, ranges, *args)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         myconv = LoadKEops(formula, aliases, dtype, 'torch',\n\u001b[0;32m---> 21\u001b[0;31m                            ['-DPYTORCH_INCLUDE_DIR=' + ';'.join(include_dirs)]).import_module()\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Context variables: save everything to compute the gradient:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pykeops/common/keops_io.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdll_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'libKeOpstorch8dcd847cfd'"
     ]
    }
   ],
   "source": [
    "%time output = model(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`). Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although this can be done by passing in minibatches of `test_x` rather than the full tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/cached_cg_lazy_tensor.py:161: UserWarning: CachedCGLazyTensor had to run CG on a tensor of size torch.Size([500, 1918]). For best performance, this LazyTensor should pre-register all vectors to run CG against.\n",
      "  \"LazyTensor should pre-register all vectors to run CG against.\".format(rhs.shape)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "means = means[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 9.761589050292969\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(means - test_y.cpu()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
