{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Variational Models to TorchScript\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to convert a variational GPyTorch model to a ScriptModule that can e.g. be exported to LibTorch.\n",
    "\n",
    "In general the process is quite similar to standard torch models, where we will trace them using `torch.jit.trace`. However there are two key differences:\n",
    "\n",
    "1. The first time you make predictions with a GPyTorch model (exact or approximate), we cache certain computations. These computations can't be traced, but the results of them can be. Therefore, we'll need to pass data through the untraced model once, and then trace the model.\n",
    "1. You can't trace models that return Distribution objects. Therefore, we'll write a simple wrapper than unpacks the MultivariateNormal that our GPs return in to just a mean and variance tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Define Model\n",
    "\n",
    "In this tutorial, we'll be tracing an SVGP model trained for just 10 epochs on the `elevators` UCI dataset. The next two cells are copied directly from our variational tutorial, and download the data and define the variational GP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "\n",
    "if not smoke_test and not os.path.isfile('../elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', '../elevators.mat')\n",
    "\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(1000, 3), torch.randn(1000)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('../elevators.mat')['data'])\n",
    "    X = data[:, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=18))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "inducing_points = torch.randn(500, 18)\n",
    "model = GPModel(inducing_points=inducing_points)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model\n",
    "\n",
    "To keep things simple for this notebook, we won't be training here. Instead, we'll be loading the parameters for a pre-trained model on elevators that we trained in the SVGP example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict, likelihood_state_dict = torch.load('svgp_elevators.pt')\n",
    "model.load_state_dict(model_state_dict)\n",
    "likelihood.load_state_dict(likelihood_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Wrapper\n",
    "\n",
    "Instead of directly tracing the GP, we'll need to trace a PyTorch Module that returns tensors. In the next cell, we define a wrapper that calls a GP and then unpacks the resulting Distribution in to a mean and variance.\n",
    "\n",
    "You could also return the full `covariance_matrix` if you wanted that rather than the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanVarModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, gp):\n",
    "        super().__init__()\n",
    "        self.gp = gp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output_dist = self.gp(x)\n",
    "        return output_dist.mean, output_dist.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace the Model\n",
    "\n",
    "In the next cell, we trace the model as normal, with the exception that we first pass data through the wrapped model so that GPyTorch can compute all of the things it needs to cache that can't be traced. Mostly, this just involves some complex linear algebra operations for variational GPs.\n",
    "\n",
    "**Note:** You'll get a lot of warnings from the tracer. That's fine. GPyTorch models are pretty large graphs, and include things like `.item()` calls that you wouldn't normally encounter in a basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake.gardner/git/gpytorch/gpytorch/variational/variational_strategy.py:124: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.updated_strategy.item() and not prior:\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/variational/_variational_strategy.py:106: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.variational_params_initialized.item():\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/chol_lazy_tensor.py:19: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.max(delazy_chol.mul(mask)).item() > 1e-3 and torch.equal(delazy_chol, delazy_chol):\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/variational/variational_strategy.py:91: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  test_mean = full_output.mean[..., num_induc:]\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:88: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x1 = x1[(*batch_indices, row_index, dim_index)]\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:105: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x2 = x2[(*batch_indices, col_index, dim_index)]\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/kernels/kernel.py:368: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not x1_.size(-1) == x2_.size(-1):\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/kernels/kernel.py:302: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  postprocess = torch.tensor(postprocess)\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/kernels/kernel.py:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self._postprocess(res) if postprocess else res\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:283: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if res.shape != self.shape:\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/lazy_tensor.py:717: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  diag = torch.tensor(jitter_val, dtype=self.dtype, device=self.device)\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/lazy_tensor.py:696: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.is_square:\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/utils/broadcasting.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  non_singleton_sizes = tuple(size for size in size_by_dim if size != 1)\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/utils/broadcasting.py:18: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if any(size != non_singleton_sizes[0] for size in non_singleton_sizes):\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/lazy/lazy_tensor.py:1132: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  other = torch.tensor(other, dtype=self.dtype, device=self.device)\n",
      "/home/jake.gardner/git/gpytorch/gpytorch/utils/broadcasting.py:43: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if n != shape_b[-2]:\n"
     ]
    }
   ],
   "source": [
    "wrapped_model = MeanVarModelWrapper(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_input = test_x[:1024, :]\n",
    "    pred = wrapped_model(fake_input)  # Compute caches\n",
    "    traced_model = torch.jit.trace(wrapped_model, fake_input, check_trace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Compute Errors on a minibatch\n",
    "\n",
    "mean1 = wrapped_model(test_x[:1024, :])[0]\n",
    "mean2 = traced_model(test_x[:1024, :])[0]\n",
    "\n",
    "print(torch.mean(torch.abs(mean1 - test_y[:1024])))\n",
    "print(torch.mean(torch.abs(mean2 - test_y[:1024])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
