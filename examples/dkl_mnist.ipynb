{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our GPyTorch library\n",
    "import gpytorch\n",
    "\n",
    "# Import some classes we will use from torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# While we have theoretically fast algorithms for toeplitz matrix-vector multiplication, the hardware of GPUs\n",
    "# is so well-designed that naive multiplication on them beats the current implementation of our algorith (despite\n",
    "# theoretically fast computation). Because of this, we set the use_toeplitz flag to false to minimize runtime\n",
    "gpytorch.functions.use_toeplitz = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import datasets to access MNISTS and transforms to format data for learning\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Download and load the MNIST dataset to train on\n",
    "# Compose lets us do multiple transformations. Specically make the data a torch.FloatTensor of shape\n",
    "# (colors x height x width) in the range [0.0, 1.0] as opposed to an RGB image with shape (height x width x colors)\n",
    "# then normalize using  mean (0.1317) and standard deviation (0.3081) already calculated (not here)\n",
    "\n",
    "# Transformation documentation here: http://pytorch.org/docs/master/torchvision/transforms.html\n",
    "train_dataset = datasets.MNIST('/tmp', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "test_dataset = datasets.MNIST('/tmp', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "# But the data into a DataLoader. We shuffle the training data but not the test data because the order\n",
    "# training data is presented will affect the outcome unlike the test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature extractor for our deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import torch's neural network\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html\n",
    "from torch import nn\n",
    "# Import torch.nn.functional for various activation/pooling functions\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html#torch-nn-functional\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# We make a classic LeNet Architecture sans a final prediction layer to 10 outputs. This will serve as a feature\n",
    "# extractor reducing the dimensionality of our data down to 64. We will pretrain these layers by adding on a \n",
    "# final classifying 64-->10 layer\n",
    "# https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "class LeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc3 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.norm3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "feature_extractor = LeNetFeatureExtractor().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the feature extractor a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.151788\n",
      "Test set: Average loss: 0.0388, Accuracy: 9882/10000 (98.820%)\n",
      "Train Epoch: 2\tLoss: 0.036721\n",
      "Test set: Average loss: 0.0341, Accuracy: 9885/10000 (98.850%)\n",
      "Train Epoch: 3\tLoss: 0.025171\n",
      "Test set: Average loss: 0.0375, Accuracy: 9876/10000 (98.760%)\n"
     ]
    }
   ],
   "source": [
    "# Make a final classifier layer that operates on the feature extractor's output\n",
    "classifier = nn.Linear(64, 10).cuda()\n",
    "# Make list of parameters to optimize (both the parameters of the feature extractor and classifier)\n",
    "params = list(feature_extractor.parameters()) + list(classifier.parameters())\n",
    "# We train the network using stochastic gradient descent\n",
    "optimizer = SGD(params, lr=0.1, momentum=0.9)\n",
    "\n",
    "# Define our pretraining function\n",
    "#    Set feature extractor to train mode (need b/c module unlike classifier which is just a single layer)\n",
    "#    iterate through train_loader\n",
    "#    put the data on the GPU as a variable\n",
    "#    Zero out the gradients from/for back_prop (needed b/c otherwise would hurt RNNs by default)\n",
    "#    Extract the 64-dimensional feature vector\n",
    "#    Feed the features into the classifying layer and output the log softmax\n",
    "#    Calculate negative log likelihood loss\n",
    "#    COULD REPLACE ABOVE WITH torch.nn.functional.cross_entropy? Says it combines them\n",
    "#    Backprop\n",
    "#    Incrementally optimize parameters\n",
    "#    Accumulate training loss\n",
    "#    Print result of epoch\n",
    "def pretrain(epoch):\n",
    "    feature_extractor.train()\n",
    "    train_loss = 0.\n",
    "    for data, target in train_loader:\n",
    "        #data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * len(data)\n",
    "    print('Train Epoch: %d\\tLoss: %.6f' % (epoch, train_loss / len(train_dataset)))\n",
    "\n",
    "# Set feature extractor to eval mode (these should actually only effect Dropout and BatchNorm which we aren't?)\n",
    "# http://pytorch.org/docs/master/nn.html#torch.nn.Module.train\n",
    "# Set test_loss accumulator and correct counter\n",
    "# Iterate through test data\n",
    "#    volatile is something about not saving gradients because not needed in test mode? Basically just not\n",
    "#          storing some type of information. Makes sense\n",
    "#    calculate loss and accumulate\n",
    "#    make prediction and check accuracy\n",
    "def pretest():\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pretrain(epoch)\n",
    "    pretest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep kernel GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now this is our first exposure to the usefulness of gpytorch\n",
    "\n",
    "# A gpytorch module is superclass of torch.nn.Module\n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, n_features=64, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        # We add the feature-extracting network to the class\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # The latent function is what transforms the features into the output\n",
    "        self.latent_functions = LatentFunctions(n_features=n_features, grid_bounds=grid_bounds)\n",
    "        # The grid bounds are the range we expect the features to fall into\n",
    "        self.grid_bounds = grid_bounds\n",
    "        # n_features in the dimension of the vector extracted (64)\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # For the forward method of the Module, first feed the xdata through the\n",
    "        # feature extraction network\n",
    "        features = self.feature_extractor(x)\n",
    "        # Scale to fit inside grid bounds\n",
    "        features = gpytorch.utils.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        # The result is hte output of the latent functions\n",
    "        res = self.latent_functions(features.unsqueeze(-1))\n",
    "        return res\n",
    "    \n",
    "# The AdditiveGridInducingVariationalGP trains multiple GPs on the features\n",
    "# These are mixed together by the likelihoo function to generate the final\n",
    "# classification output\n",
    "\n",
    "# Grid bounds specify the allowed values of features\n",
    "# grid_size is the number of subdivisions along each dimension\n",
    "class LatentFunctions(gpytorch.models.AdditiveGridInducingVariationalGP):\n",
    "    # n_features is the number of features from feature extractor\n",
    "    # mixing params = False means the result of the GPs will simply be summed instead of mixed\n",
    "    def __init__(self, n_features=64, grid_bounds=(-10., 10.), grid_size=128):\n",
    "        super(LatentFunctions, self).__init__(grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                                              n_components=n_features, mixing_params=False, sum_output=False)\n",
    "        #  We will use the very common universal approximator RBF Kernel\n",
    "        cov_module = gpytorch.kernels.RBFKernel()\n",
    "        # Initialize the lengthscale of the kernel\n",
    "        cov_module.initialize(log_lengthscale=0)\n",
    "        self.cov_module = cov_module\n",
    "        self.grid_bounds = grid_bounds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Zero mean\n",
    "        mean = Variable(x.data.new(len(x)).zero_())\n",
    "        # Covariance using RBF kernel as described in __init__\n",
    "        covar = self.cov_module(x)\n",
    "        # Return as Gaussian\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean, covar)\n",
    "    \n",
    "# Intialize the model  \n",
    "model = DKLModel(feature_extractor).cuda()\n",
    "# Choose that likelihood function to use\n",
    "# Here we use the softmax likelihood (e^z_i)/SUM_over_i(e^z_i)\n",
    "# https://en.wikipedia.org/wiki/Softmax_function\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(n_features=model.n_features, n_classes=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [001/030], Loss: 41.579403\n",
      "Train Epoch: 1 [002/030], Loss: 92.042076\n",
      "Train Epoch: 1 [003/030], Loss: 73.519318\n",
      "Train Epoch: 1 [004/030], Loss: 42.514950\n",
      "Train Epoch: 1 [005/030], Loss: 37.503212\n",
      "Train Epoch: 1 [006/030], Loss: 35.392902\n",
      "Train Epoch: 1 [007/030], Loss: 29.345600\n",
      "Train Epoch: 1 [008/030], Loss: 59.559193\n",
      "Train Epoch: 1 [009/030], Loss: 30.034393\n",
      "Train Epoch: 1 [010/030], Loss: 22.256338\n",
      "Train Epoch: 1 [011/030], Loss: 42.953094\n",
      "Train Epoch: 1 [012/030], Loss: 16.092073\n",
      "Train Epoch: 1 [013/030], Loss: 14.936320\n",
      "Train Epoch: 1 [014/030], Loss: 12.845394\n",
      "Train Epoch: 1 [015/030], Loss: 12.620340\n",
      "Train Epoch: 1 [016/030], Loss: 19.153234\n",
      "Train Epoch: 1 [017/030], Loss: 9.901564\n",
      "Train Epoch: 1 [018/030], Loss: 9.940659\n",
      "Train Epoch: 1 [019/030], Loss: 7.963514\n",
      "Train Epoch: 1 [020/030], Loss: 9.141623\n",
      "Train Epoch: 1 [021/030], Loss: 13.696498\n",
      "Train Epoch: 1 [022/030], Loss: 6.201659\n",
      "Train Epoch: 1 [023/030], Loss: 6.377803\n",
      "Train Epoch: 1 [024/030], Loss: 7.014893\n",
      "Train Epoch: 1 [025/030], Loss: 19.723495\n",
      "Train Epoch: 1 [026/030], Loss: 4.861204\n",
      "Train Epoch: 1 [027/030], Loss: 4.739460\n",
      "Train Epoch: 1 [028/030], Loss: 5.197290\n",
      "Train Epoch: 1 [029/030], Loss: 3.880935\n",
      "Train Epoch: 1 [030/030], Loss: 5.640519\n",
      "CPU times: user 11 s, sys: 508 ms, total: 11.5 s\n",
      "Wall time: 11.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bw462/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 7907/10000 (79.070%)\n",
      "Train Epoch: 2 [001/030], Loss: 5.199722\n",
      "Train Epoch: 2 [002/030], Loss: 3.798968\n",
      "Train Epoch: 2 [003/030], Loss: 10.037350\n",
      "Train Epoch: 2 [004/030], Loss: 3.029057\n",
      "Train Epoch: 2 [005/030], Loss: 3.282596\n",
      "Train Epoch: 2 [006/030], Loss: 3.209903\n",
      "Train Epoch: 2 [007/030], Loss: 2.640893\n",
      "Train Epoch: 2 [008/030], Loss: 2.577655\n",
      "Train Epoch: 2 [009/030], Loss: 2.529065\n",
      "Train Epoch: 2 [010/030], Loss: 2.375546\n",
      "Train Epoch: 2 [011/030], Loss: 2.017558\n",
      "Train Epoch: 2 [012/030], Loss: 1.960126\n",
      "Train Epoch: 2 [013/030], Loss: 2.555728\n",
      "Train Epoch: 2 [014/030], Loss: 1.938028\n",
      "Train Epoch: 2 [015/030], Loss: 1.651893\n",
      "Train Epoch: 2 [016/030], Loss: 3.425699\n",
      "Train Epoch: 2 [017/030], Loss: 1.628858\n",
      "Train Epoch: 2 [018/030], Loss: 1.355417\n",
      "Train Epoch: 2 [019/030], Loss: 1.539618\n",
      "Train Epoch: 2 [020/030], Loss: 2.777413\n",
      "Train Epoch: 2 [021/030], Loss: 1.441511\n",
      "Train Epoch: 2 [022/030], Loss: 1.551285\n",
      "Train Epoch: 2 [023/030], Loss: 1.213492\n",
      "Train Epoch: 2 [024/030], Loss: 1.237918\n",
      "Train Epoch: 2 [025/030], Loss: 1.039291\n",
      "Train Epoch: 2 [026/030], Loss: 1.004779\n",
      "Train Epoch: 2 [027/030], Loss: 2.668581\n",
      "Train Epoch: 2 [028/030], Loss: 0.862648\n",
      "Train Epoch: 2 [029/030], Loss: 0.839099\n",
      "Train Epoch: 2 [030/030], Loss: 0.733762\n",
      "CPU times: user 10.8 s, sys: 404 ms, total: 11.2 s\n",
      "Wall time: 11.1 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9664/10000 (96.640%)\n",
      "Train Epoch: 3 [001/030], Loss: 0.862237\n",
      "Train Epoch: 3 [002/030], Loss: 0.685825\n",
      "Train Epoch: 3 [003/030], Loss: 0.681913\n",
      "Train Epoch: 3 [004/030], Loss: 0.806338\n",
      "Train Epoch: 3 [005/030], Loss: 0.614334\n",
      "Train Epoch: 3 [006/030], Loss: 0.645095\n",
      "Train Epoch: 3 [007/030], Loss: 0.727210\n",
      "Train Epoch: 3 [008/030], Loss: 0.635892\n",
      "Train Epoch: 3 [009/030], Loss: 0.670984\n",
      "Train Epoch: 3 [010/030], Loss: 0.515131\n",
      "Train Epoch: 3 [011/030], Loss: 0.482547\n",
      "Train Epoch: 3 [012/030], Loss: 0.509327\n",
      "Train Epoch: 3 [013/030], Loss: 0.429103\n",
      "Train Epoch: 3 [014/030], Loss: 0.506312\n",
      "Train Epoch: 3 [015/030], Loss: 0.474959\n",
      "Train Epoch: 3 [016/030], Loss: 0.464692\n",
      "Train Epoch: 3 [017/030], Loss: 0.405043\n",
      "Train Epoch: 3 [018/030], Loss: 0.367233\n",
      "Train Epoch: 3 [019/030], Loss: 0.303063\n",
      "Train Epoch: 3 [020/030], Loss: 0.511409\n",
      "Train Epoch: 3 [021/030], Loss: 0.322134\n",
      "Train Epoch: 3 [022/030], Loss: 0.352610\n",
      "Train Epoch: 3 [023/030], Loss: 0.343733\n",
      "Train Epoch: 3 [024/030], Loss: 0.328418\n",
      "Train Epoch: 3 [025/030], Loss: 0.317597\n",
      "Train Epoch: 3 [026/030], Loss: 0.259453\n",
      "Train Epoch: 3 [027/030], Loss: 0.254090\n",
      "Train Epoch: 3 [028/030], Loss: 0.273161\n",
      "Train Epoch: 3 [029/030], Loss: 0.260687\n",
      "Train Epoch: 3 [030/030], Loss: 0.282613\n",
      "CPU times: user 10.9 s, sys: 504 ms, total: 11.4 s\n",
      "Wall time: 11.3 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9828/10000 (98.280%)\n",
      "Train Epoch: 4 [001/030], Loss: 0.255540\n",
      "Train Epoch: 4 [002/030], Loss: 0.253233\n",
      "Train Epoch: 4 [003/030], Loss: 0.240980\n",
      "Train Epoch: 4 [004/030], Loss: 0.234234\n",
      "Train Epoch: 4 [005/030], Loss: 0.236166\n",
      "Train Epoch: 4 [006/030], Loss: 0.201617\n",
      "Train Epoch: 4 [007/030], Loss: 0.865823\n",
      "Train Epoch: 4 [008/030], Loss: 0.204859\n",
      "Train Epoch: 4 [009/030], Loss: 0.177367\n",
      "Train Epoch: 4 [010/030], Loss: 0.244486\n",
      "Train Epoch: 4 [011/030], Loss: 0.161293\n",
      "Train Epoch: 4 [012/030], Loss: 0.152845\n",
      "Train Epoch: 4 [013/030], Loss: 0.180846\n",
      "Train Epoch: 4 [014/030], Loss: 0.187815\n",
      "Train Epoch: 4 [015/030], Loss: 0.113925\n",
      "Train Epoch: 4 [016/030], Loss: 0.157855\n",
      "Train Epoch: 4 [017/030], Loss: 0.154577\n",
      "Train Epoch: 4 [018/030], Loss: 0.127644\n",
      "Train Epoch: 4 [019/030], Loss: 1.026250\n",
      "Train Epoch: 4 [020/030], Loss: 0.137851\n",
      "Train Epoch: 4 [021/030], Loss: 0.137409\n",
      "Train Epoch: 4 [022/030], Loss: 0.098961\n",
      "Train Epoch: 4 [023/030], Loss: 0.136192\n",
      "Train Epoch: 4 [024/030], Loss: 0.090396\n",
      "Train Epoch: 4 [025/030], Loss: 0.104438\n",
      "Train Epoch: 4 [026/030], Loss: 0.118013\n",
      "Train Epoch: 4 [027/030], Loss: 0.076502\n",
      "Train Epoch: 4 [028/030], Loss: 0.108525\n",
      "Train Epoch: 4 [029/030], Loss: 0.107910\n",
      "Train Epoch: 4 [030/030], Loss: 0.128320\n",
      "CPU times: user 10.8 s, sys: 452 ms, total: 11.3 s\n",
      "Wall time: 11.2 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9860/10000 (98.600%)\n",
      "Train Epoch: 5 [001/030], Loss: 0.107996\n",
      "Train Epoch: 5 [002/030], Loss: 0.099561\n",
      "Train Epoch: 5 [003/030], Loss: 0.078857\n",
      "Train Epoch: 5 [004/030], Loss: 0.138164\n",
      "Train Epoch: 5 [005/030], Loss: 0.053193\n",
      "Train Epoch: 5 [006/030], Loss: 0.097932\n",
      "Train Epoch: 5 [007/030], Loss: 0.078083\n",
      "Train Epoch: 5 [008/030], Loss: 0.050935\n",
      "Train Epoch: 5 [009/030], Loss: 0.066582\n",
      "Train Epoch: 5 [010/030], Loss: 0.123081\n",
      "Train Epoch: 5 [011/030], Loss: 0.058601\n",
      "Train Epoch: 5 [012/030], Loss: 0.068265\n",
      "Train Epoch: 5 [013/030], Loss: 0.050432\n",
      "Train Epoch: 5 [014/030], Loss: 0.090640\n",
      "Train Epoch: 5 [015/030], Loss: 0.060112\n",
      "Train Epoch: 5 [016/030], Loss: 0.083635\n",
      "Train Epoch: 5 [017/030], Loss: 0.068374\n",
      "Train Epoch: 5 [018/030], Loss: 0.047461\n",
      "Train Epoch: 5 [019/030], Loss: 0.055990\n",
      "Train Epoch: 5 [020/030], Loss: 0.081016\n",
      "Train Epoch: 5 [021/030], Loss: 0.077630\n",
      "Train Epoch: 5 [022/030], Loss: 0.060772\n",
      "Train Epoch: 5 [023/030], Loss: 0.034763\n",
      "Train Epoch: 5 [024/030], Loss: 0.046932\n",
      "Train Epoch: 5 [025/030], Loss: 0.019860\n",
      "Train Epoch: 5 [026/030], Loss: 0.063889\n",
      "Train Epoch: 5 [027/030], Loss: 0.064323\n",
      "Train Epoch: 5 [028/030], Loss: 0.040488\n",
      "Train Epoch: 5 [029/030], Loss: 0.030980\n",
      "Train Epoch: 5 [030/030], Loss: 0.076001\n",
      "CPU times: user 11.1 s, sys: 404 ms, total: 11.5 s\n",
      "Wall time: 11.4 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9895/10000 (98.950%)\n",
      "Train Epoch: 6 [001/030], Loss: 0.033918\n",
      "Train Epoch: 6 [002/030], Loss: 0.011537\n",
      "Train Epoch: 6 [003/030], Loss: 0.017067\n",
      "Train Epoch: 6 [004/030], Loss: 0.045496\n",
      "Train Epoch: 6 [005/030], Loss: 0.076764\n",
      "Train Epoch: 6 [006/030], Loss: 0.043114\n",
      "Train Epoch: 6 [007/030], Loss: 0.068504\n",
      "Train Epoch: 6 [008/030], Loss: 0.052493\n",
      "Train Epoch: 6 [009/030], Loss: 0.011607\n",
      "Train Epoch: 6 [010/030], Loss: 0.046147\n",
      "Train Epoch: 6 [011/030], Loss: 0.026785\n",
      "Train Epoch: 6 [012/030], Loss: -0.005455\n",
      "Train Epoch: 6 [013/030], Loss: 0.049287\n",
      "Train Epoch: 6 [014/030], Loss: 0.007629\n",
      "Train Epoch: 6 [015/030], Loss: 0.040852\n",
      "Train Epoch: 6 [016/030], Loss: 0.003255\n",
      "Train Epoch: 6 [017/030], Loss: 0.001684\n",
      "Train Epoch: 6 [018/030], Loss: 0.444653\n",
      "Train Epoch: 6 [019/030], Loss: 0.001290\n",
      "Train Epoch: 6 [020/030], Loss: -0.003779\n",
      "Train Epoch: 6 [021/030], Loss: -0.027986\n",
      "Train Epoch: 6 [022/030], Loss: 0.000980\n",
      "Train Epoch: 6 [023/030], Loss: 0.023227\n",
      "Train Epoch: 6 [024/030], Loss: 0.019471\n",
      "Train Epoch: 6 [025/030], Loss: 0.000323\n",
      "Train Epoch: 6 [026/030], Loss: 0.008099\n",
      "Train Epoch: 6 [027/030], Loss: 0.007797\n",
      "Train Epoch: 6 [028/030], Loss: -0.008749\n",
      "Train Epoch: 6 [029/030], Loss: 0.012410\n",
      "Train Epoch: 6 [030/030], Loss: 0.029947\n",
      "CPU times: user 10.9 s, sys: 428 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9901/10000 (99.010%)\n",
      "Train Epoch: 7 [001/030], Loss: -0.009216\n",
      "Train Epoch: 7 [002/030], Loss: -0.002211\n",
      "Train Epoch: 7 [003/030], Loss: -0.027594\n",
      "Train Epoch: 7 [004/030], Loss: -0.001944\n",
      "Train Epoch: 7 [005/030], Loss: 0.002497\n",
      "Train Epoch: 7 [006/030], Loss: 0.012079\n",
      "Train Epoch: 7 [007/030], Loss: -0.021183\n",
      "Train Epoch: 7 [008/030], Loss: 0.061799\n",
      "Train Epoch: 7 [009/030], Loss: -0.002695\n",
      "Train Epoch: 7 [010/030], Loss: -0.024982\n",
      "Train Epoch: 7 [011/030], Loss: -0.010839\n",
      "Train Epoch: 7 [012/030], Loss: 0.043181\n",
      "Train Epoch: 7 [013/030], Loss: 0.087894\n",
      "Train Epoch: 7 [014/030], Loss: -0.035573\n",
      "Train Epoch: 7 [015/030], Loss: 0.095602\n",
      "Train Epoch: 7 [016/030], Loss: -0.031650\n",
      "Train Epoch: 7 [017/030], Loss: -0.031672\n",
      "Train Epoch: 7 [018/030], Loss: 0.000298\n",
      "Train Epoch: 7 [019/030], Loss: -0.003071\n",
      "Train Epoch: 7 [020/030], Loss: -0.004925\n",
      "Train Epoch: 7 [021/030], Loss: 0.001206\n",
      "Train Epoch: 7 [022/030], Loss: -0.015723\n",
      "Train Epoch: 7 [023/030], Loss: -0.025673\n",
      "Train Epoch: 7 [024/030], Loss: -0.014170\n",
      "Train Epoch: 7 [025/030], Loss: -0.018512\n",
      "Train Epoch: 7 [026/030], Loss: -0.012377\n",
      "Train Epoch: 7 [027/030], Loss: -0.027016\n",
      "Train Epoch: 7 [028/030], Loss: -0.017437\n",
      "Train Epoch: 7 [029/030], Loss: -0.028910\n",
      "Train Epoch: 7 [030/030], Loss: -0.021458\n",
      "CPU times: user 11.8 s, sys: 380 ms, total: 12.2 s\n",
      "Wall time: 12.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9916/10000 (99.160%)\n",
      "Train Epoch: 8 [001/030], Loss: -0.040436\n",
      "Train Epoch: 8 [002/030], Loss: -0.026243\n",
      "Train Epoch: 8 [003/030], Loss: -0.020521\n",
      "Train Epoch: 8 [004/030], Loss: -0.032855\n",
      "Train Epoch: 8 [005/030], Loss: -0.027933\n",
      "Train Epoch: 8 [006/030], Loss: -0.036021\n",
      "Train Epoch: 8 [007/030], Loss: -0.021509\n",
      "Train Epoch: 8 [008/030], Loss: -0.040184\n",
      "Train Epoch: 8 [009/030], Loss: -0.038900\n",
      "Train Epoch: 8 [010/030], Loss: -0.032960\n",
      "Train Epoch: 8 [011/030], Loss: -0.013044\n",
      "Train Epoch: 8 [012/030], Loss: -0.027975\n",
      "Train Epoch: 8 [013/030], Loss: -0.049556\n",
      "Train Epoch: 8 [014/030], Loss: -0.022028\n",
      "Train Epoch: 8 [015/030], Loss: 0.021581\n",
      "Train Epoch: 8 [016/030], Loss: -0.013311\n",
      "Train Epoch: 8 [017/030], Loss: -0.013941\n",
      "Train Epoch: 8 [018/030], Loss: 0.015906\n",
      "Train Epoch: 8 [019/030], Loss: -0.025901\n",
      "Train Epoch: 8 [020/030], Loss: -0.040361\n",
      "Train Epoch: 8 [021/030], Loss: -0.042751\n",
      "Train Epoch: 8 [022/030], Loss: -0.031754\n",
      "Train Epoch: 8 [023/030], Loss: -0.044211\n",
      "Train Epoch: 8 [024/030], Loss: -0.039247\n",
      "Train Epoch: 8 [025/030], Loss: -0.044228\n",
      "Train Epoch: 8 [026/030], Loss: -0.015676\n",
      "Train Epoch: 8 [027/030], Loss: -0.046027\n",
      "Train Epoch: 8 [028/030], Loss: -0.007056\n",
      "Train Epoch: 8 [029/030], Loss: -0.041491\n",
      "Train Epoch: 8 [030/030], Loss: -0.027966\n",
      "CPU times: user 11 s, sys: 484 ms, total: 11.4 s\n",
      "Wall time: 11.4 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9918/10000 (99.180%)\n",
      "Train Epoch: 9 [001/030], Loss: -0.053858\n",
      "Train Epoch: 9 [002/030], Loss: -0.022715\n",
      "Train Epoch: 9 [003/030], Loss: -0.037832\n",
      "Train Epoch: 9 [004/030], Loss: -0.030948\n",
      "Train Epoch: 9 [005/030], Loss: -0.021559\n",
      "Train Epoch: 9 [006/030], Loss: -0.032630\n",
      "Train Epoch: 9 [007/030], Loss: -0.037311\n",
      "Train Epoch: 9 [008/030], Loss: 0.215574\n",
      "Train Epoch: 9 [009/030], Loss: -0.013539\n",
      "Train Epoch: 9 [010/030], Loss: -0.045930\n",
      "Train Epoch: 9 [011/030], Loss: -0.022787\n",
      "Train Epoch: 9 [012/030], Loss: -0.026927\n",
      "Train Epoch: 9 [013/030], Loss: -0.041714\n",
      "Train Epoch: 9 [014/030], Loss: -0.056268\n",
      "Train Epoch: 9 [015/030], Loss: 0.004070\n",
      "Train Epoch: 9 [016/030], Loss: -0.038552\n",
      "Train Epoch: 9 [017/030], Loss: -0.046906\n",
      "Train Epoch: 9 [018/030], Loss: -0.033406\n",
      "Train Epoch: 9 [019/030], Loss: -0.045254\n",
      "Train Epoch: 9 [020/030], Loss: -0.026228\n",
      "Train Epoch: 9 [021/030], Loss: -0.045094\n",
      "Train Epoch: 9 [022/030], Loss: -0.046924\n",
      "Train Epoch: 9 [023/030], Loss: -0.039170\n",
      "Train Epoch: 9 [024/030], Loss: -0.024259\n",
      "Train Epoch: 9 [025/030], Loss: -0.068995\n",
      "Train Epoch: 9 [026/030], Loss: -0.047838\n",
      "Train Epoch: 9 [027/030], Loss: -0.030361\n",
      "Train Epoch: 9 [028/030], Loss: -0.054829\n",
      "Train Epoch: 9 [029/030], Loss: -0.031896\n",
      "Train Epoch: 9 [030/030], Loss: -0.043563\n",
      "CPU times: user 10.7 s, sys: 448 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9914/10000 (99.140%)\n",
      "Train Epoch: 10 [001/030], Loss: -0.036163\n",
      "Train Epoch: 10 [002/030], Loss: -0.051495\n",
      "Train Epoch: 10 [003/030], Loss: -0.038878\n",
      "Train Epoch: 10 [004/030], Loss: -0.037484\n",
      "Train Epoch: 10 [005/030], Loss: -0.030889\n",
      "Train Epoch: 10 [006/030], Loss: 0.204158\n",
      "Train Epoch: 10 [007/030], Loss: -0.050660\n",
      "Train Epoch: 10 [008/030], Loss: -0.057550\n",
      "Train Epoch: 10 [009/030], Loss: -0.053906\n",
      "Train Epoch: 10 [010/030], Loss: -0.062051\n",
      "Train Epoch: 10 [011/030], Loss: -0.040790\n",
      "Train Epoch: 10 [012/030], Loss: -0.051470\n",
      "Train Epoch: 10 [013/030], Loss: -0.044975\n",
      "Train Epoch: 10 [014/030], Loss: -0.043412\n",
      "Train Epoch: 10 [015/030], Loss: -0.051908\n",
      "Train Epoch: 10 [016/030], Loss: -0.048006\n",
      "Train Epoch: 10 [017/030], Loss: -0.059417\n",
      "Train Epoch: 10 [018/030], Loss: -0.053077\n",
      "Train Epoch: 10 [019/030], Loss: -0.031433\n",
      "Train Epoch: 10 [020/030], Loss: -0.041645\n",
      "Train Epoch: 10 [021/030], Loss: -0.060858\n",
      "Train Epoch: 10 [022/030], Loss: -0.043748\n",
      "Train Epoch: 10 [023/030], Loss: -0.050675\n",
      "Train Epoch: 10 [024/030], Loss: -0.041127\n",
      "Train Epoch: 10 [025/030], Loss: -0.056238\n",
      "Train Epoch: 10 [026/030], Loss: -0.045114\n",
      "Train Epoch: 10 [027/030], Loss: -0.049273\n",
      "Train Epoch: 10 [028/030], Loss: 0.065613\n",
      "Train Epoch: 10 [029/030], Loss: -0.051594\n",
      "Train Epoch: 10 [030/030], Loss: -0.038776\n",
      "CPU times: user 11 s, sys: 560 ms, total: 11.6 s\n",
      "Wall time: 11.5 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9922/10000 (99.220%)\n"
     ]
    }
   ],
   "source": [
    "# Simple DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True, pin_memory=True)\n",
    "# We use an adam optimizer over both the model and likelihood parameters\n",
    "# https://arxiv.org/abs/1412.6980\n",
    "optimizer = Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.01)\n",
    "#optimizer = Adam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -model.latent_functions.marginal_log_likelihood(likelihood, output, target, n_data=len(train_dataset))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = likelihood(model(data))\n",
    "        pred = output.argmax()\n",
    "        correct += pred.eq(target.view_as(pred)).data.cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    %time train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
