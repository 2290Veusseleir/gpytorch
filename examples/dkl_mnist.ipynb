{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we demonstrate the potential of combining the deep learning capabilities of PyTorch with Gaussian process models using GPyTorch. In this notebook, we will use deep kernel learning to train a deep neural network with a Gaussian process prediction layer for classification, using the MNIST dataset as a simple example.\n",
    "\n",
    "For an introduction to DKL see these papers:\n",
    "https://arxiv.org/abs/1511.02222\n",
    "https://arxiv.org/abs/1611.00336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our GPyTorch library\n",
    "import gpytorch\n",
    "\n",
    "# Import some classes we will use from torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "First, we must load the standard train and test sets for MNIST. To do this, we use the standard MNIST dataset available through torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import datasets to access MNISTS and transforms to format data for learning\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Download and load the MNIST dataset to train on\n",
    "# Compose lets us do multiple transformations. Specically make the data a torch.FloatTensor of shape\n",
    "# (colors x height x width) in the range [0.0, 1.0] as opposed to an RGB image with shape (height x width x colors)\n",
    "# then normalize using  mean (0.1317) and standard deviation (0.3081) already calculated (not here)\n",
    "\n",
    "# Transformation documentation here: http://pytorch.org/docs/master/torchvision/transforms.html\n",
    "train_dataset = datasets.MNIST('/tmp', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "test_dataset = datasets.MNIST('/tmp', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "# But the data into a DataLoader. We shuffle the training data but not the test data because the order\n",
    "# training data is presented will affect the outcome unlike the test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature extractor for our deep kernel\n",
    "\n",
    "In this cell, we define the deep neural network we will use as the basis for our deep kernel. To keep things simple, we use the classic LeNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import torch's neural network\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html\n",
    "from torch import nn\n",
    "# Import torch.nn.functional for various activation/pooling functions\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html#torch-nn-functional\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# We make a classic LeNet Architecture sans a final prediction layer to 10 outputs. This will serve as a feature\n",
    "# extractor reducing the dimensionality of our data down to 64. We will pretrain these layers by adding on a \n",
    "# final classifying 64-->10 layer\n",
    "# https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "class LeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc3 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.norm3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "feature_extractor = LeNetFeatureExtractor().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the feature extractor a bit\n",
    "\n",
    "We next pretrain the deep feature extractor using a simple linear classifier. While this step is in general not necessary, we include it to demonstrate that GPs can be added on to a neural network as a simple fine-tuning step that adds minimal training overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a final classifier layer that operates on the feature extractor's output\n",
    "classifier = nn.Linear(64, 10).cuda()\n",
    "# Make list of parameters to optimize (both the parameters of the feature extractor and classifier)\n",
    "params = list(feature_extractor.parameters()) + list(classifier.parameters())\n",
    "# We train the network using stochastic gradient descent\n",
    "optimizer = SGD(params, lr=0.1, momentum=0.9)\n",
    "\n",
    "def pretrain(epoch):\n",
    "    # Set feature extract to training model\n",
    "    feature_extractor.train()\n",
    "    train_loss = 0.\n",
    "    # Basic training loop for a DNN\n",
    "    for data, target in train_loader:\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        # Forward data through the feature extractor and soft max\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        # Compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Back propagate and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * len(data)\n",
    "    print('Train Epoch: %d\\tLoss: %.6f' % (epoch, train_loss / len(train_dataset)))\n",
    "\n",
    "def pretest():\n",
    "    # Change feature extract to eval mode\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # Loop over minibatches of test data and compute accuracy\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pretrain(epoch)\n",
    "    pretest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep kernel GP\n",
    "\n",
    "We next define a DKLModel that uses the feature extractor. This is a Gaussian process that applies an additive RBF kernel to the features extracted by the deep neural network. The key thing that is different between this model and models we've seen in other example notebooks is in forward: rather than working directly with x, we first extract features using the deep feature extractor.\n",
    "\n",
    "The loss used for training is the standard variational lower bound used for training Gaussian processes. Since we use an additive RBF kernel, we can make use of the AdditiveGridInducingVariationalGP model, which efficiently performs inference using SKI in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now this is our first exposure to the usefulness of gpytorch\n",
    "\n",
    "# A gpytorch module is superclass of torch.nn.Module\n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, n_features=64, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        # We add the feature-extracting network to the class\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # The latent function is what transforms the features into the output\n",
    "        self.latent_functions = LatentFunctions(n_features=n_features, grid_bounds=grid_bounds)\n",
    "        # The grid bounds are the range we expect the features to fall into\n",
    "        self.grid_bounds = grid_bounds\n",
    "        # n_features in the dimension of the vector extracted (64)\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # For the forward method of the Module, first feed the xdata through the\n",
    "        # feature extraction network\n",
    "        features = self.feature_extractor(x)\n",
    "        # Scale to fit inside grid bounds\n",
    "        features = gpytorch.utils.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        # The result is hte output of the latent functions\n",
    "        res = self.latent_functions(features.unsqueeze(-1))\n",
    "        return res\n",
    "    \n",
    "# The AdditiveGridInducingVariationalGP trains multiple GPs on the features\n",
    "# These are mixed together by the likelihoo function to generate the final\n",
    "# classification output\n",
    "\n",
    "# Grid bounds specify the allowed values of features\n",
    "# grid_size is the number of subdivisions along each dimension\n",
    "class LatentFunctions(gpytorch.models.AdditiveGridInducingVariationalGP):\n",
    "    # n_features is the number of features from feature extractor\n",
    "    # mixing params = False means the result of the GPs will simply be summed instead of mixed\n",
    "    def __init__(self, n_features=64, grid_bounds=(-10., 10.), grid_size=128):\n",
    "        super(LatentFunctions, self).__init__(grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                                              n_components=n_features, mixing_params=False, sum_output=False)\n",
    "        #  We will use the very common universal approximator RBF Kernel\n",
    "        cov_module = gpytorch.kernels.RBFKernel()\n",
    "        # Initialize the lengthscale of the kernel\n",
    "        cov_module.initialize(log_lengthscale=0)\n",
    "        self.cov_module = cov_module\n",
    "        self.grid_bounds = grid_bounds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Zero mean\n",
    "        mean = Variable(x.data.new(len(x)).zero_())\n",
    "        # Covariance using RBF kernel as described in __init__\n",
    "        covar = self.cov_module(x)\n",
    "        # Return as Gaussian\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean, covar)\n",
    "    \n",
    "# Intialize the model  \n",
    "model = DKLModel(feature_extractor).cuda()\n",
    "# Choose that likelihood function to use\n",
    "# Here we use the softmax likelihood (e^z_i)/SUM_over_i(e^z_i)\n",
    "# https://en.wikipedia.org/wiki/Softmax_function\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(n_features=model.n_features, n_classes=10).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DKL model\n",
    "In this cell we train the DKL model we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True, pin_memory=True)\n",
    "\n",
    "# We use an adam optimizer over both the model and likelihood parameters\n",
    "# https://arxiv.org/abs/1412.6980\n",
    "optimizer = Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},  # SoftmaxLikelihood contains parameters\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=len(train_dataset))\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -mll(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = likelihood(model(data))\n",
    "        pred = output.argmax()\n",
    "        correct += pred.eq(target.view_as(pred)).data.cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "# While we have theoretically fast algorithms for toeplitz matrix-vector multiplication, the hardware of GPUs\n",
    "# is so well-designed that naive multiplication on them beats the current implementation of our algorith (despite\n",
    "# theoretically fast computation). Because of this, we set the use_toeplitz flag to false to minimize runtime\n",
    "with gpytorch.settings.use_toeplitz(False):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        %time train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
