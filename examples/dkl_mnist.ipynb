{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use deep kernel learning (DKL) for classification. This is useful when you have very complex high-dimensional inputs (such as an image)\n",
    "\n",
    "The example here is MNIST classification\n",
    "\n",
    "For an introduction to DKL see these papers:\n",
    "https://arxiv.org/abs/1511.02222\n",
    "https://arxiv.org/abs/1611.00336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our GPyTorch library\n",
    "import gpytorch\n",
    "\n",
    "# Import some classes we will use from torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets to access MNISTS and transforms to format data for learning\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Download and load the MNIST dataset to train on\n",
    "# Compose lets us do multiple transformations. Specically make the data a torch.FloatTensor of shape\n",
    "# (colors x height x width) in the range [0.0, 1.0] as opposed to an RGB image with shape (height x width x colors)\n",
    "# then normalize using  mean (0.1317) and standard deviation (0.3081) already calculated (not here)\n",
    "\n",
    "# Transformation documentation here: http://pytorch.org/docs/master/torchvision/transforms.html\n",
    "train_dataset = datasets.MNIST('/tmp', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "test_dataset = datasets.MNIST('/tmp', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "# But the data into a DataLoader. We shuffle the training data but not the test data because the order\n",
    "# training data is presented will affect the outcome unlike the test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature extractor for our deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch's neural network\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html\n",
    "from torch import nn\n",
    "# Import torch.nn.functional for various activation/pooling functions\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html#torch-nn-functional\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# We make a classic LeNet Architecture sans a final prediction layer to 10 outputs. This will serve as a feature\n",
    "# extractor reducing the dimensionality of our data down to 64. We will pretrain these layers by adding on a \n",
    "# final classifying 64-->10 layer\n",
    "# https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "class LeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc3 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.norm3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "feature_extractor = LeNetFeatureExtractor().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the feature extractor a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.150723\n",
      "Test set: Average loss: 0.0457, Accuracy: 9851/10000 (98.510%)\n",
      "Train Epoch: 2\tLoss: 0.035508\n",
      "Test set: Average loss: 0.0292, Accuracy: 9900/10000 (99.000%)\n",
      "Train Epoch: 3\tLoss: 0.024795\n",
      "Test set: Average loss: 0.0406, Accuracy: 9877/10000 (98.770%)\n"
     ]
    }
   ],
   "source": [
    "# Make a final classifier layer that operates on the feature extractor's output\n",
    "classifier = nn.Linear(64, 10).cuda()\n",
    "# Make list of parameters to optimize (both the parameters of the feature extractor and classifier)\n",
    "params = list(feature_extractor.parameters()) + list(classifier.parameters())\n",
    "# We train the network using stochastic gradient descent\n",
    "optimizer = SGD(params, lr=0.1, momentum=0.9)\n",
    "\n",
    "# Define our pretraining function\n",
    "#    Set feature extractor to train mode (need b/c module unlike classifier which is just a single layer)\n",
    "#    iterate through train_loader\n",
    "#    put the data on the GPU as a variable\n",
    "#    Zero out the gradients from/for back_prop (needed b/c otherwise would hurt RNNs by default)\n",
    "#    Extract the 64-dimensional feature vector\n",
    "#    Feed the features into the classifying layer and output the log softmax\n",
    "#    Calculate negative log likelihood loss\n",
    "#    COULD REPLACE ABOVE WITH torch.nn.functional.cross_entropy? Says it combines them\n",
    "#    Backprop\n",
    "#    Incrementally optimize parameters\n",
    "#    Accumulate training loss\n",
    "#    Print result of epoch\n",
    "def pretrain(epoch):\n",
    "    feature_extractor.train()\n",
    "    train_loss = 0.\n",
    "    for data, target in train_loader:\n",
    "        #data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * len(data)\n",
    "    print('Train Epoch: %d\\tLoss: %.6f' % (epoch, train_loss / len(train_dataset)))\n",
    "\n",
    "# Set feature extractor to eval mode (these should actually only effect Dropout and BatchNorm which we aren't?)\n",
    "# http://pytorch.org/docs/master/nn.html#torch.nn.Module.train\n",
    "# Set test_loss accumulator and correct counter\n",
    "# Iterate through test data\n",
    "#    volatile is something about not saving gradients because not needed in test mode? Basically just not\n",
    "#          storing some type of information. Makes sense\n",
    "#    calculate loss and accumulate\n",
    "#    make prediction and check accuracy\n",
    "def pretest():\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pretrain(epoch)\n",
    "    pretest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep kernel GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now this is our first exposure to the usefulness of gpytorch\n",
    "\n",
    "# A gpytorch module is superclass of torch.nn.Module\n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, n_features=64, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        # We add the feature-extracting network to the class\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # The latent function is what transforms the features into the output\n",
    "        self.latent_functions = LatentFunctions(n_features=n_features, grid_bounds=grid_bounds)\n",
    "        # The grid bounds are the range we expect the features to fall into\n",
    "        self.grid_bounds = grid_bounds\n",
    "        # n_features in the dimension of the vector extracted (64)\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # For the forward method of the Module, first feed the xdata through the\n",
    "        # feature extraction network\n",
    "        features = self.feature_extractor(x)\n",
    "        # Scale to fit inside grid bounds\n",
    "        features = gpytorch.utils.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        # The result is hte output of the latent functions\n",
    "        res = self.latent_functions(features.unsqueeze(-1))\n",
    "        return res\n",
    "    \n",
    "# The AdditiveGridInducingVariationalGP trains multiple GPs on the features\n",
    "# These are mixed together by the likelihoo function to generate the final\n",
    "# classification output\n",
    "\n",
    "# Grid bounds specify the allowed values of features\n",
    "# grid_size is the number of subdivisions along each dimension\n",
    "class LatentFunctions(gpytorch.models.AdditiveGridInducingVariationalGP):\n",
    "    # n_features is the number of features from feature extractor\n",
    "    # mixing params = False means the result of the GPs will simply be summed instead of mixed\n",
    "    def __init__(self, n_features=64, grid_bounds=(-10., 10.), grid_size=128):\n",
    "        super(LatentFunctions, self).__init__(grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                                              n_components=n_features, mixing_params=False, sum_output=False)\n",
    "        #  We will use the very common universal approximator RBF Kernel\n",
    "        cov_module = gpytorch.kernels.RBFKernel()\n",
    "        # Initialize the lengthscale of the kernel\n",
    "        cov_module.initialize(log_lengthscale=0)\n",
    "        self.cov_module = cov_module\n",
    "        self.grid_bounds = grid_bounds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Zero mean\n",
    "        mean = Variable(x.data.new(len(x)).zero_())\n",
    "        # Covariance using RBF kernel as described in __init__\n",
    "        covar = self.cov_module(x)\n",
    "        # Return as Gaussian\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean, covar)\n",
    "    \n",
    "# Intialize the model  \n",
    "model = DKLModel(feature_extractor).cuda()\n",
    "# Choose that likelihood function to use\n",
    "# Here we use the softmax likelihood (e^z_i)/SUM_over_i(e^z_i)\n",
    "# https://en.wikipedia.org/wiki/Softmax_function\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(n_features=model.n_features, n_classes=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [001/030], Loss: 36.142605\n",
      "Train Epoch: 1 [002/030], Loss: 36.090061\n",
      "Train Epoch: 1 [003/030], Loss: 35.627544\n",
      "Train Epoch: 1 [004/030], Loss: 34.603943\n",
      "Train Epoch: 1 [005/030], Loss: 33.154297\n",
      "Train Epoch: 1 [006/030], Loss: 31.405090\n",
      "Train Epoch: 1 [007/030], Loss: 29.622997\n",
      "Train Epoch: 1 [008/030], Loss: 27.900564\n",
      "Train Epoch: 1 [009/030], Loss: 26.234581\n",
      "Train Epoch: 1 [010/030], Loss: 24.712194\n",
      "Train Epoch: 1 [011/030], Loss: 23.318537\n",
      "Train Epoch: 1 [012/030], Loss: 22.062542\n",
      "Train Epoch: 1 [013/030], Loss: 20.924089\n",
      "Train Epoch: 1 [014/030], Loss: 19.880186\n",
      "Train Epoch: 1 [015/030], Loss: 18.957228\n",
      "Train Epoch: 1 [016/030], Loss: 18.153137\n",
      "Train Epoch: 1 [017/030], Loss: 17.367653\n",
      "Train Epoch: 1 [018/030], Loss: 16.663593\n",
      "Train Epoch: 1 [019/030], Loss: 16.012274\n",
      "Train Epoch: 1 [020/030], Loss: 15.308195\n",
      "Train Epoch: 1 [021/030], Loss: 14.672304\n",
      "Train Epoch: 1 [022/030], Loss: 14.030581\n",
      "Train Epoch: 1 [023/030], Loss: 13.444436\n",
      "Train Epoch: 1 [024/030], Loss: 12.949938\n",
      "Train Epoch: 1 [025/030], Loss: 12.294259\n",
      "Train Epoch: 1 [026/030], Loss: 11.809763\n",
      "Train Epoch: 1 [027/030], Loss: 11.364057\n",
      "Train Epoch: 1 [028/030], Loss: 10.680386\n",
      "Train Epoch: 1 [029/030], Loss: 10.153752\n",
      "Train Epoch: 1 [030/030], Loss: 9.689010\n",
      "CPU times: user 6.64 s, sys: 416 ms, total: 7.06 s\n",
      "Wall time: 7.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 6121/10000 (61.210%)\n",
      "Train Epoch: 2 [001/030], Loss: 9.258375\n",
      "Train Epoch: 2 [002/030], Loss: 8.867882\n",
      "Train Epoch: 2 [003/030], Loss: 8.495071\n",
      "Train Epoch: 2 [004/030], Loss: 8.101971\n",
      "Train Epoch: 2 [005/030], Loss: 7.802680\n",
      "Train Epoch: 2 [006/030], Loss: 7.402121\n",
      "Train Epoch: 2 [007/030], Loss: 7.032426\n",
      "Train Epoch: 2 [008/030], Loss: 6.643226\n",
      "Train Epoch: 2 [009/030], Loss: 6.348939\n",
      "Train Epoch: 2 [010/030], Loss: 6.104055\n",
      "Train Epoch: 2 [011/030], Loss: 5.998545\n",
      "Train Epoch: 2 [012/030], Loss: 5.626659\n",
      "Train Epoch: 2 [013/030], Loss: 5.302618\n",
      "Train Epoch: 2 [014/030], Loss: 5.285032\n",
      "Train Epoch: 2 [015/030], Loss: 4.887465\n",
      "Train Epoch: 2 [016/030], Loss: 4.772480\n",
      "Train Epoch: 2 [017/030], Loss: 4.505405\n",
      "Train Epoch: 2 [018/030], Loss: 4.331192\n",
      "Train Epoch: 2 [019/030], Loss: 4.121179\n",
      "Train Epoch: 2 [020/030], Loss: 4.084509\n",
      "Train Epoch: 2 [021/030], Loss: 3.933098\n",
      "Train Epoch: 2 [022/030], Loss: 3.813060\n",
      "Train Epoch: 2 [023/030], Loss: 3.624206\n",
      "Train Epoch: 2 [024/030], Loss: 3.527125\n",
      "Train Epoch: 2 [025/030], Loss: 3.385017\n",
      "Train Epoch: 2 [026/030], Loss: 3.324082\n",
      "Train Epoch: 2 [027/030], Loss: 3.147472\n",
      "Train Epoch: 2 [028/030], Loss: 3.107167\n",
      "Train Epoch: 2 [029/030], Loss: 3.003148\n",
      "Train Epoch: 2 [030/030], Loss: 2.974749\n",
      "CPU times: user 6.58 s, sys: 456 ms, total: 7.03 s\n",
      "Wall time: 7.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9703/10000 (97.030%)\n",
      "Train Epoch: 3 [001/030], Loss: 2.849145\n",
      "Train Epoch: 3 [002/030], Loss: 2.737780\n",
      "Train Epoch: 3 [003/030], Loss: 2.630193\n",
      "Train Epoch: 3 [004/030], Loss: 2.631416\n",
      "Train Epoch: 3 [005/030], Loss: 2.499823\n",
      "Train Epoch: 3 [006/030], Loss: 2.418246\n",
      "Train Epoch: 3 [007/030], Loss: 2.367687\n",
      "Train Epoch: 3 [008/030], Loss: 2.339429\n",
      "Train Epoch: 3 [009/030], Loss: 2.255776\n",
      "Train Epoch: 3 [010/030], Loss: 2.225314\n",
      "Train Epoch: 3 [011/030], Loss: 2.154285\n",
      "Train Epoch: 3 [012/030], Loss: 2.085375\n",
      "Train Epoch: 3 [013/030], Loss: 2.034397\n",
      "Train Epoch: 3 [014/030], Loss: 2.007726\n",
      "Train Epoch: 3 [015/030], Loss: 1.919988\n",
      "Train Epoch: 3 [016/030], Loss: 1.904859\n",
      "Train Epoch: 3 [017/030], Loss: 1.878610\n",
      "Train Epoch: 3 [018/030], Loss: 1.832925\n",
      "Train Epoch: 3 [019/030], Loss: 1.773328\n",
      "Train Epoch: 3 [020/030], Loss: 1.738720\n",
      "Train Epoch: 3 [021/030], Loss: 1.738039\n",
      "Train Epoch: 3 [022/030], Loss: 1.649673\n",
      "Train Epoch: 3 [023/030], Loss: 1.636243\n",
      "Train Epoch: 3 [024/030], Loss: 1.619721\n",
      "Train Epoch: 3 [025/030], Loss: 1.572071\n",
      "Train Epoch: 3 [026/030], Loss: 1.563496\n",
      "Train Epoch: 3 [027/030], Loss: 1.506922\n",
      "Train Epoch: 3 [028/030], Loss: 1.483922\n",
      "Train Epoch: 3 [029/030], Loss: 1.464683\n",
      "Train Epoch: 3 [030/030], Loss: 1.423833\n",
      "CPU times: user 6.56 s, sys: 460 ms, total: 7.02 s\n",
      "Wall time: 7.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9763/10000 (97.630%)\n",
      "Train Epoch: 4 [001/030], Loss: 1.402782\n",
      "Train Epoch: 4 [002/030], Loss: 1.380410\n",
      "Train Epoch: 4 [003/030], Loss: 1.351071\n",
      "Train Epoch: 4 [004/030], Loss: 1.345011\n",
      "Train Epoch: 4 [005/030], Loss: 1.307389\n",
      "Train Epoch: 4 [006/030], Loss: 1.262983\n",
      "Train Epoch: 4 [007/030], Loss: 1.260225\n",
      "Train Epoch: 4 [008/030], Loss: 1.252252\n",
      "Train Epoch: 4 [009/030], Loss: 1.243247\n",
      "Train Epoch: 4 [010/030], Loss: 1.212812\n",
      "Train Epoch: 4 [011/030], Loss: 1.200291\n",
      "Train Epoch: 4 [012/030], Loss: 1.171254\n",
      "Train Epoch: 4 [013/030], Loss: 1.166869\n",
      "Train Epoch: 4 [014/030], Loss: 1.120746\n",
      "Train Epoch: 4 [015/030], Loss: 1.121987\n",
      "Train Epoch: 4 [016/030], Loss: 1.094936\n",
      "Train Epoch: 4 [017/030], Loss: 1.086679\n",
      "Train Epoch: 4 [018/030], Loss: 1.078507\n",
      "Train Epoch: 4 [019/030], Loss: 1.057760\n",
      "Train Epoch: 4 [020/030], Loss: 1.040013\n",
      "Train Epoch: 4 [021/030], Loss: 1.033480\n",
      "Train Epoch: 4 [022/030], Loss: 1.021665\n",
      "Train Epoch: 4 [023/030], Loss: 0.993896\n",
      "Train Epoch: 4 [024/030], Loss: 0.991789\n",
      "Train Epoch: 4 [025/030], Loss: 0.981772\n",
      "Train Epoch: 4 [026/030], Loss: 0.962018\n",
      "Train Epoch: 4 [027/030], Loss: 0.969571\n",
      "Train Epoch: 4 [028/030], Loss: 0.943551\n",
      "Train Epoch: 4 [029/030], Loss: 0.942181\n",
      "Train Epoch: 4 [030/030], Loss: 0.941332\n",
      "CPU times: user 6.65 s, sys: 392 ms, total: 7.04 s\n",
      "Wall time: 7.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9864/10000 (98.640%)\n",
      "Train Epoch: 5 [001/030], Loss: 0.923116\n",
      "Train Epoch: 5 [002/030], Loss: 0.891815\n",
      "Train Epoch: 5 [003/030], Loss: 0.884920\n",
      "Train Epoch: 5 [004/030], Loss: 0.865727\n",
      "Train Epoch: 5 [005/030], Loss: 0.879008\n",
      "Train Epoch: 5 [006/030], Loss: 0.859716\n",
      "Train Epoch: 5 [007/030], Loss: 0.857543\n",
      "Train Epoch: 5 [008/030], Loss: 0.835173\n",
      "Train Epoch: 5 [009/030], Loss: 0.836762\n",
      "Train Epoch: 5 [010/030], Loss: 0.826224\n",
      "Train Epoch: 5 [011/030], Loss: 0.811641\n",
      "Train Epoch: 5 [012/030], Loss: 0.802732\n",
      "Train Epoch: 5 [013/030], Loss: 0.787749\n",
      "Train Epoch: 5 [014/030], Loss: 0.791700\n",
      "Train Epoch: 5 [015/030], Loss: 0.779832\n",
      "Train Epoch: 5 [016/030], Loss: 0.778598\n",
      "Train Epoch: 5 [017/030], Loss: 0.757305\n",
      "Train Epoch: 5 [018/030], Loss: 0.766895\n",
      "Train Epoch: 5 [019/030], Loss: 0.742244\n",
      "Train Epoch: 5 [020/030], Loss: 0.752526\n",
      "Train Epoch: 5 [021/030], Loss: 0.724963\n",
      "Train Epoch: 5 [022/030], Loss: 0.737638\n",
      "Train Epoch: 5 [023/030], Loss: 0.724808\n",
      "Train Epoch: 5 [024/030], Loss: 0.725261\n",
      "Train Epoch: 5 [025/030], Loss: 0.717833\n",
      "Train Epoch: 5 [026/030], Loss: 0.703432\n",
      "Train Epoch: 5 [027/030], Loss: 0.698857\n",
      "Train Epoch: 5 [028/030], Loss: 0.690661\n",
      "Train Epoch: 5 [029/030], Loss: 0.691556\n",
      "Train Epoch: 5 [030/030], Loss: 0.670466\n",
      "CPU times: user 6.72 s, sys: 348 ms, total: 7.06 s\n",
      "Wall time: 7.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9901/10000 (99.010%)\n",
      "Train Epoch: 6 [001/030], Loss: 0.677170\n",
      "Train Epoch: 6 [002/030], Loss: 0.667035\n",
      "Train Epoch: 6 [003/030], Loss: 0.658754\n",
      "Train Epoch: 6 [004/030], Loss: 0.659502\n",
      "Train Epoch: 6 [005/030], Loss: 0.648770\n",
      "Train Epoch: 6 [006/030], Loss: 0.646387\n",
      "Train Epoch: 6 [007/030], Loss: 0.640344\n",
      "Train Epoch: 6 [008/030], Loss: 0.641842\n",
      "Train Epoch: 6 [009/030], Loss: 0.626409\n",
      "Train Epoch: 6 [010/030], Loss: 0.628503\n",
      "Train Epoch: 6 [011/030], Loss: 0.624434\n",
      "Train Epoch: 6 [012/030], Loss: 0.614327\n",
      "Train Epoch: 6 [013/030], Loss: 0.616552\n",
      "Train Epoch: 6 [014/030], Loss: 0.607694\n",
      "Train Epoch: 6 [015/030], Loss: 0.599544\n",
      "Train Epoch: 6 [016/030], Loss: 0.593831\n",
      "Train Epoch: 6 [017/030], Loss: 0.602182\n",
      "Train Epoch: 6 [018/030], Loss: 0.608630\n",
      "Train Epoch: 6 [019/030], Loss: 0.576198\n",
      "Train Epoch: 6 [020/030], Loss: 0.600555\n",
      "Train Epoch: 6 [021/030], Loss: 0.587777\n",
      "Train Epoch: 6 [022/030], Loss: 0.576071\n",
      "Train Epoch: 6 [023/030], Loss: 0.580227\n",
      "Train Epoch: 6 [024/030], Loss: 0.573566\n",
      "Train Epoch: 6 [025/030], Loss: 0.578572\n",
      "Train Epoch: 6 [026/030], Loss: 0.569147\n",
      "Train Epoch: 6 [027/030], Loss: 0.560565\n",
      "Train Epoch: 6 [028/030], Loss: 0.566358\n",
      "Train Epoch: 6 [029/030], Loss: 0.553476\n",
      "Train Epoch: 6 [030/030], Loss: 0.540924\n",
      "CPU times: user 7.08 s, sys: 676 ms, total: 7.76 s\n",
      "Wall time: 7.74 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9909/10000 (99.090%)\n",
      "Train Epoch: 7 [001/030], Loss: 0.539820\n",
      "Train Epoch: 7 [002/030], Loss: 0.541358\n",
      "Train Epoch: 7 [003/030], Loss: 0.549363\n",
      "Train Epoch: 7 [004/030], Loss: 0.523764\n",
      "Train Epoch: 7 [005/030], Loss: 0.525894\n",
      "Train Epoch: 7 [006/030], Loss: 0.515665\n",
      "Train Epoch: 7 [007/030], Loss: 0.524350\n",
      "Train Epoch: 7 [008/030], Loss: 0.523155\n",
      "Train Epoch: 7 [009/030], Loss: 0.514502\n",
      "Train Epoch: 7 [010/030], Loss: 0.520955\n",
      "Train Epoch: 7 [011/030], Loss: 0.515079\n",
      "Train Epoch: 7 [012/030], Loss: 0.517062\n",
      "Train Epoch: 7 [013/030], Loss: 0.502428\n",
      "Train Epoch: 7 [014/030], Loss: 0.505979\n",
      "Train Epoch: 7 [015/030], Loss: 0.507256\n",
      "Train Epoch: 7 [016/030], Loss: 0.500486\n",
      "Train Epoch: 7 [017/030], Loss: 0.511843\n",
      "Train Epoch: 7 [018/030], Loss: 0.504095\n",
      "Train Epoch: 7 [019/030], Loss: 0.490187\n",
      "Train Epoch: 7 [020/030], Loss: 0.489616\n",
      "Train Epoch: 7 [021/030], Loss: 0.493186\n",
      "Train Epoch: 7 [022/030], Loss: 0.478395\n",
      "Train Epoch: 7 [023/030], Loss: 0.487424\n",
      "Train Epoch: 7 [024/030], Loss: 0.482165\n",
      "Train Epoch: 7 [025/030], Loss: 0.483524\n",
      "Train Epoch: 7 [026/030], Loss: 0.474303\n",
      "Train Epoch: 7 [027/030], Loss: 0.472141\n",
      "Train Epoch: 7 [028/030], Loss: 0.471610\n",
      "Train Epoch: 7 [029/030], Loss: 0.475382\n",
      "Train Epoch: 7 [030/030], Loss: 0.484576\n",
      "CPU times: user 7.96 s, sys: 1.19 s, total: 9.15 s\n",
      "Wall time: 9.13 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9918/10000 (99.180%)\n",
      "Train Epoch: 8 [001/030], Loss: 0.467670\n",
      "Train Epoch: 8 [002/030], Loss: 0.455673\n",
      "Train Epoch: 8 [003/030], Loss: 0.458428\n",
      "Train Epoch: 8 [004/030], Loss: 0.459142\n",
      "Train Epoch: 8 [005/030], Loss: 0.457363\n",
      "Train Epoch: 8 [006/030], Loss: 0.451025\n",
      "Train Epoch: 8 [007/030], Loss: 0.452753\n",
      "Train Epoch: 8 [008/030], Loss: 0.464538\n",
      "Train Epoch: 8 [009/030], Loss: 0.445390\n",
      "Train Epoch: 8 [010/030], Loss: 0.446881\n",
      "Train Epoch: 8 [011/030], Loss: 0.447128\n",
      "Train Epoch: 8 [012/030], Loss: 0.443721\n",
      "Train Epoch: 8 [013/030], Loss: 0.441874\n",
      "Train Epoch: 8 [014/030], Loss: 0.445817\n",
      "Train Epoch: 8 [015/030], Loss: 0.441609\n",
      "Train Epoch: 8 [016/030], Loss: 0.443518\n",
      "Train Epoch: 8 [017/030], Loss: 0.435089\n",
      "Train Epoch: 8 [018/030], Loss: 0.451810\n",
      "Train Epoch: 8 [019/030], Loss: 0.430029\n",
      "Train Epoch: 8 [020/030], Loss: 0.434117\n",
      "Train Epoch: 8 [021/030], Loss: 0.426290\n",
      "Train Epoch: 8 [022/030], Loss: 0.423751\n",
      "Train Epoch: 8 [023/030], Loss: 0.440929\n",
      "Train Epoch: 8 [024/030], Loss: 0.418410\n",
      "Train Epoch: 8 [025/030], Loss: 0.421810\n",
      "Train Epoch: 8 [026/030], Loss: 0.418552\n",
      "Train Epoch: 8 [027/030], Loss: 0.414173\n",
      "Train Epoch: 8 [028/030], Loss: 0.428579\n",
      "Train Epoch: 8 [029/030], Loss: 0.424956\n",
      "Train Epoch: 8 [030/030], Loss: 0.411987\n",
      "CPU times: user 7.97 s, sys: 968 ms, total: 8.94 s\n",
      "Wall time: 8.92 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9912/10000 (99.120%)\n",
      "Train Epoch: 9 [001/030], Loss: 0.418398\n",
      "Train Epoch: 9 [002/030], Loss: 0.408011\n",
      "Train Epoch: 9 [003/030], Loss: 0.415424\n",
      "Train Epoch: 9 [004/030], Loss: 0.402562\n",
      "Train Epoch: 9 [005/030], Loss: 0.406093\n",
      "Train Epoch: 9 [006/030], Loss: 0.403615\n",
      "Train Epoch: 9 [007/030], Loss: 0.402306\n",
      "Train Epoch: 9 [008/030], Loss: 0.413138\n",
      "Train Epoch: 9 [009/030], Loss: 0.404397\n",
      "Train Epoch: 9 [010/030], Loss: 0.398019\n",
      "Train Epoch: 9 [011/030], Loss: 0.407999\n",
      "Train Epoch: 9 [012/030], Loss: 0.397591\n",
      "Train Epoch: 9 [013/030], Loss: 0.396837\n",
      "Train Epoch: 9 [014/030], Loss: 0.402960\n",
      "Train Epoch: 9 [015/030], Loss: 0.402641\n",
      "Train Epoch: 9 [016/030], Loss: 0.394820\n",
      "Train Epoch: 9 [017/030], Loss: 0.404263\n",
      "Train Epoch: 9 [018/030], Loss: 0.389264\n",
      "Train Epoch: 9 [019/030], Loss: 0.386715\n",
      "Train Epoch: 9 [020/030], Loss: 0.396539\n",
      "Train Epoch: 9 [021/030], Loss: 0.388339\n",
      "Train Epoch: 9 [022/030], Loss: 0.405376\n",
      "Train Epoch: 9 [023/030], Loss: 0.383053\n",
      "Train Epoch: 9 [024/030], Loss: 0.384746\n",
      "Train Epoch: 9 [025/030], Loss: 0.402037\n",
      "Train Epoch: 9 [026/030], Loss: 0.383885\n",
      "Train Epoch: 9 [027/030], Loss: 0.381524\n",
      "Train Epoch: 9 [028/030], Loss: 0.384947\n",
      "Train Epoch: 9 [029/030], Loss: 0.383321\n",
      "Train Epoch: 9 [030/030], Loss: 0.383537\n",
      "CPU times: user 8.2 s, sys: 840 ms, total: 9.04 s\n",
      "Wall time: 9.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9917/10000 (99.170%)\n",
      "Train Epoch: 10 [001/030], Loss: 0.381225\n",
      "Train Epoch: 10 [002/030], Loss: 0.372451\n",
      "Train Epoch: 10 [003/030], Loss: 0.374265\n",
      "Train Epoch: 10 [004/030], Loss: 0.374987\n",
      "Train Epoch: 10 [005/030], Loss: 0.372894\n",
      "Train Epoch: 10 [006/030], Loss: 0.368853\n",
      "Train Epoch: 10 [007/030], Loss: 0.369897\n",
      "Train Epoch: 10 [008/030], Loss: 0.374589\n",
      "Train Epoch: 10 [009/030], Loss: 0.370186\n",
      "Train Epoch: 10 [010/030], Loss: 0.367384\n",
      "Train Epoch: 10 [011/030], Loss: 0.367410\n",
      "Train Epoch: 10 [012/030], Loss: 0.370448\n",
      "Train Epoch: 10 [013/030], Loss: 0.371703\n",
      "Train Epoch: 10 [014/030], Loss: 0.371174\n",
      "Train Epoch: 10 [015/030], Loss: 0.362868\n",
      "Train Epoch: 10 [016/030], Loss: 0.360119\n",
      "Train Epoch: 10 [017/030], Loss: 0.363455\n",
      "Train Epoch: 10 [018/030], Loss: 0.363073\n",
      "Train Epoch: 10 [019/030], Loss: 0.359111\n",
      "Train Epoch: 10 [020/030], Loss: 0.363191\n",
      "Train Epoch: 10 [021/030], Loss: 0.364174\n",
      "Train Epoch: 10 [022/030], Loss: 0.367098\n",
      "Train Epoch: 10 [023/030], Loss: 0.360278\n",
      "Train Epoch: 10 [024/030], Loss: 0.361794\n",
      "Train Epoch: 10 [025/030], Loss: 0.354652\n",
      "Train Epoch: 10 [026/030], Loss: 0.364269\n",
      "Train Epoch: 10 [027/030], Loss: 0.366041\n",
      "Train Epoch: 10 [028/030], Loss: 0.357522\n",
      "Train Epoch: 10 [029/030], Loss: 0.363803\n",
      "Train Epoch: 10 [030/030], Loss: 0.370579\n",
      "CPU times: user 8.08 s, sys: 972 ms, total: 9.05 s\n",
      "Wall time: 9.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9923/10000 (99.230%)\n"
     ]
    }
   ],
   "source": [
    "# Simple DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True, pin_memory=True)\n",
    "\n",
    "# We use an adam optimizer over both the model and likelihood parameters\n",
    "# https://arxiv.org/abs/1412.6980\n",
    "optimizer = Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},  # SoftmaxLikelihood contains parameters\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=len(train_dataset))\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -mll(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = likelihood(model(data))\n",
    "        pred = output.argmax()\n",
    "        correct += pred.eq(target.view_as(pred)).data.cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "# While we have theoretically fast algorithms for toeplitz matrix-vector multiplication, the hardware of GPUs\n",
    "# is so well-designed that naive multiplication on them beats the current implementation of our algorith (despite\n",
    "# theoretically fast computation). Because of this, we set the use_toeplitz flag to false to minimize runtime\n",
    "with gpytorch.settings.use_toeplitz(False):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        %time train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
