{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our GPyTorch library\n",
    "import gpytorch\n",
    "\n",
    "# Import some classes we will use from torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets to access MNISTS and transforms to format data for learning\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Download and load the MNIST dataset to train on\n",
    "# Compose lets us do multiple transformations. Specically make the data a torch.FloatTensor of shape\n",
    "# (colors x height x width) in the range [0.0, 1.0] as opposed to an RGB image with shape (height x width x colors)\n",
    "# then normalize using  mean (0.1317) and standard deviation (0.3081) already calculated (not here)\n",
    "\n",
    "# Transformation documentation here: http://pytorch.org/docs/master/torchvision/transforms.html\n",
    "train_dataset = datasets.MNIST('/tmp', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "test_dataset = datasets.MNIST('/tmp', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "# But the data into a DataLoader. We shuffle the training data but not the test data because the order\n",
    "# training data is presented will affect the outcome unlike the test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature extractor for our deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch's neural network\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html\n",
    "from torch import nn\n",
    "# Import torch.nn.functional for various activation/pooling functions\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html#torch-nn-functional\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# We make a classic LeNet Architecture sans a final prediction layer to 10 outputs. This will serve as a feature\n",
    "# extractor reducing the dimensionality of our data down to 64. We will pretrain these layers by adding on a \n",
    "# final classifying 64-->10 layer\n",
    "# https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "class LeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc3 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.norm3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "feature_extractor = LeNetFeatureExtractor().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the feature extractor a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.149688\n",
      "Test set: Average loss: 0.0541, Accuracy: 9827/10000 (98.270%)\n",
      "Train Epoch: 2\tLoss: 0.039770\n",
      "Test set: Average loss: 0.0342, Accuracy: 9883/10000 (98.830%)\n",
      "Train Epoch: 3\tLoss: 0.026879\n",
      "Test set: Average loss: 0.0361, Accuracy: 9879/10000 (98.790%)\n"
     ]
    }
   ],
   "source": [
    "# Make a final classifier layer that operates on the feature extractor's output\n",
    "classifier = nn.Linear(64, 10).cuda()\n",
    "# Make list of parameters to optimize (both the parameters of the feature extractor and classifier)\n",
    "params = list(feature_extractor.parameters()) + list(classifier.parameters())\n",
    "# We train the network using stochastic gradient descent\n",
    "optimizer = SGD(params, lr=0.1, momentum=0.9)\n",
    "\n",
    "# Define our pretraining function\n",
    "#    Set feature extractor to train mode (need b/c module unlike classifier which is just a single layer)\n",
    "#    iterate through train_loader\n",
    "#    put the data on the GPU as a variable\n",
    "#    Zero out the gradients from/for back_prop (needed b/c otherwise would hurt RNNs by default)\n",
    "#    Extract the 64-dimensional feature vector\n",
    "#    Feed the features into the classifying layer and output the log softmax\n",
    "#    Calculate negative log likelihood loss\n",
    "#    COULD REPLACE ABOVE WITH torch.nn.functional.cross_entropy? Says it combines them\n",
    "#    Backprop\n",
    "#    Incrementally optimize parameters\n",
    "#    Accumulate training loss\n",
    "#    Print result of epoch\n",
    "def pretrain(epoch):\n",
    "    feature_extractor.train()\n",
    "    train_loss = 0.\n",
    "    for data, target in train_loader:\n",
    "        #data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * len(data)\n",
    "    print('Train Epoch: %d\\tLoss: %.6f' % (epoch, train_loss / len(train_dataset)))\n",
    "\n",
    "# Set feature extractor to eval mode (these should actually only effect Dropout and BatchNorm which we aren't?)\n",
    "# http://pytorch.org/docs/master/nn.html#torch.nn.Module.train\n",
    "# Set test_loss accumulator and correct counter\n",
    "# Iterate through test data\n",
    "#    volatile is something about not saving gradients because not needed in test mode? Basically just not\n",
    "#          storing some type of information. Makes sense\n",
    "#    calculate loss and accumulate\n",
    "#    make prediction and check accuracy\n",
    "def pretest():\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pretrain(epoch)\n",
    "    pretest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep kernel GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now this is our first exposure to the usefulness of gpytorch\n",
    "\n",
    "# A gpytorch module is superclass of torch.nn.Module\n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, n_features=64, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        # We add the feature-extracting network to the class\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # The latent function is what transforms the features into the output\n",
    "        self.latent_functions = LatentFunctions(n_features=n_features, grid_bounds=grid_bounds)\n",
    "        # The grid bounds are the range we expect the features to fall into\n",
    "        self.grid_bounds = grid_bounds\n",
    "        # n_features in the dimension of the vector extracted (64)\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # For the forward method of the Module, first feed the xdata through the\n",
    "        # feature extraction network\n",
    "        features = self.feature_extractor(x)\n",
    "        # Scale to fit inside grid bounds\n",
    "        features = gpytorch.utils.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        # The result is hte output of the latent functions\n",
    "        res = self.latent_functions(features.unsqueeze(-1))\n",
    "        return res\n",
    "    \n",
    "# The AdditiveGridInducingVariationalGP trains multiple GPs on the features\n",
    "# These are mixed together by the likelihoo function to generate the final\n",
    "# classification output\n",
    "\n",
    "# Grid bounds specify the allowed values of features\n",
    "# grid_size is the number of subdivisions along each dimension\n",
    "class LatentFunctions(gpytorch.models.AdditiveGridInducingVariationalGP):\n",
    "    # n_features is the number of features from feature extractor\n",
    "    # mixing params = False means the result of the GPs will simply be summed instead of mixed\n",
    "    def __init__(self, n_features=64, grid_bounds=(-10., 10.), grid_size=128):\n",
    "        super(LatentFunctions, self).__init__(grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                                              n_components=n_features, mixing_params=False, sum_output=False)\n",
    "        #  We will use the very common universal approximator RBF Kernel\n",
    "        cov_module = gpytorch.kernels.RBFKernel()\n",
    "        # Initialize the lengthscale of the kernel\n",
    "        cov_module.initialize(log_lengthscale=0)\n",
    "        self.cov_module = cov_module\n",
    "        self.grid_bounds = grid_bounds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Zero mean\n",
    "        mean = Variable(x.data.new(len(x)).zero_())\n",
    "        # Covariance using RBF kernel as described in __init__\n",
    "        covar = self.cov_module(x)\n",
    "        # Return as Gaussian\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean, covar)\n",
    "    \n",
    "# Intialize the model  \n",
    "model = DKLModel(feature_extractor).cuda()\n",
    "# Choose that likelihood function to use\n",
    "# Here we use the softmax likelihood (e^z_i)/SUM_over_i(e^z_i)\n",
    "# https://en.wikipedia.org/wiki/Softmax_function\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(n_features=model.n_features, n_classes=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [001/030], Loss: 73.562729\n",
      "Train Epoch: 1 [002/030], Loss: 82.663742\n",
      "Train Epoch: 1 [003/030], Loss: 103.586403\n",
      "Train Epoch: 1 [004/030], Loss: 94.326477\n",
      "Train Epoch: 1 [005/030], Loss: 83.820274\n",
      "Train Epoch: 1 [006/030], Loss: 51.902367\n",
      "Train Epoch: 1 [007/030], Loss: 45.762814\n",
      "Train Epoch: 1 [008/030], Loss: 120.406364\n",
      "Train Epoch: 1 [009/030], Loss: 37.549706\n",
      "Train Epoch: 1 [010/030], Loss: 42.237991\n",
      "Train Epoch: 1 [011/030], Loss: 37.918449\n",
      "Train Epoch: 1 [012/030], Loss: 31.445269\n",
      "Train Epoch: 1 [013/030], Loss: 40.649334\n",
      "Train Epoch: 1 [014/030], Loss: 27.964289\n",
      "Train Epoch: 1 [015/030], Loss: 31.886467\n",
      "Train Epoch: 1 [016/030], Loss: 20.399744\n",
      "Train Epoch: 1 [017/030], Loss: 17.548803\n",
      "Train Epoch: 1 [018/030], Loss: 24.627878\n",
      "Train Epoch: 1 [019/030], Loss: 17.850117\n",
      "Train Epoch: 1 [020/030], Loss: 25.801538\n",
      "Train Epoch: 1 [021/030], Loss: 14.341015\n",
      "Train Epoch: 1 [022/030], Loss: 11.081783\n",
      "Train Epoch: 1 [023/030], Loss: 10.896550\n",
      "Train Epoch: 1 [024/030], Loss: 10.120235\n",
      "Train Epoch: 1 [025/030], Loss: 10.051824\n",
      "Train Epoch: 1 [026/030], Loss: 8.779704\n",
      "Train Epoch: 1 [027/030], Loss: 8.390800\n",
      "Train Epoch: 1 [028/030], Loss: 10.180931\n",
      "Train Epoch: 1 [029/030], Loss: 7.059505\n",
      "Train Epoch: 1 [030/030], Loss: 9.078752\n",
      "CPU times: user 8.26 s, sys: 460 ms, total: 8.72 s\n",
      "Wall time: 8.71 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 7345/10000 (73.450%)\n",
      "Train Epoch: 2 [001/030], Loss: 7.326035\n",
      "Train Epoch: 2 [002/030], Loss: 6.381141\n",
      "Train Epoch: 2 [003/030], Loss: 18.014357\n",
      "Train Epoch: 2 [004/030], Loss: 5.063754\n",
      "Train Epoch: 2 [005/030], Loss: 3.934478\n",
      "Train Epoch: 2 [006/030], Loss: 4.926157\n",
      "Train Epoch: 2 [007/030], Loss: 4.167954\n",
      "Train Epoch: 2 [008/030], Loss: 5.812820\n",
      "Train Epoch: 2 [009/030], Loss: 3.497722\n",
      "Train Epoch: 2 [010/030], Loss: 4.322648\n",
      "Train Epoch: 2 [011/030], Loss: 4.075482\n",
      "Train Epoch: 2 [012/030], Loss: 3.587610\n",
      "Train Epoch: 2 [013/030], Loss: 3.207456\n",
      "Train Epoch: 2 [014/030], Loss: 3.078331\n",
      "Train Epoch: 2 [015/030], Loss: 4.152343\n",
      "Train Epoch: 2 [016/030], Loss: 2.611198\n",
      "Train Epoch: 2 [017/030], Loss: 3.961612\n",
      "Train Epoch: 2 [018/030], Loss: 4.185015\n",
      "Train Epoch: 2 [019/030], Loss: 7.462733\n",
      "Train Epoch: 2 [020/030], Loss: 2.396472\n",
      "Train Epoch: 2 [021/030], Loss: 3.037478\n",
      "Train Epoch: 2 [022/030], Loss: 2.183641\n",
      "Train Epoch: 2 [023/030], Loss: 1.993175\n",
      "Train Epoch: 2 [024/030], Loss: 3.244867\n",
      "Train Epoch: 2 [025/030], Loss: 2.133674\n",
      "Train Epoch: 2 [026/030], Loss: 1.868963\n",
      "Train Epoch: 2 [027/030], Loss: 1.649536\n",
      "Train Epoch: 2 [028/030], Loss: 1.449603\n",
      "Train Epoch: 2 [029/030], Loss: 1.497013\n",
      "Train Epoch: 2 [030/030], Loss: 1.486620\n",
      "CPU times: user 8.23 s, sys: 472 ms, total: 8.7 s\n",
      "Wall time: 8.69 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9562/10000 (95.620%)\n",
      "Train Epoch: 3 [001/030], Loss: 1.349056\n",
      "Train Epoch: 3 [002/030], Loss: 1.561499\n",
      "Train Epoch: 3 [003/030], Loss: 1.211915\n",
      "Train Epoch: 3 [004/030], Loss: 1.286141\n",
      "Train Epoch: 3 [005/030], Loss: 1.183094\n",
      "Train Epoch: 3 [006/030], Loss: 1.949000\n",
      "Train Epoch: 3 [007/030], Loss: 1.319652\n",
      "Train Epoch: 3 [008/030], Loss: 1.486577\n",
      "Train Epoch: 3 [009/030], Loss: 3.139071\n",
      "Train Epoch: 3 [010/030], Loss: 1.188096\n",
      "Train Epoch: 3 [011/030], Loss: 2.681830\n",
      "Train Epoch: 3 [012/030], Loss: 0.863339\n",
      "Train Epoch: 3 [013/030], Loss: 0.820401\n",
      "Train Epoch: 3 [014/030], Loss: 1.016593\n",
      "Train Epoch: 3 [015/030], Loss: 0.746513\n",
      "Train Epoch: 3 [016/030], Loss: 2.122018\n",
      "Train Epoch: 3 [017/030], Loss: 0.788027\n",
      "Train Epoch: 3 [018/030], Loss: 1.953751\n",
      "Train Epoch: 3 [019/030], Loss: 0.791373\n",
      "Train Epoch: 3 [020/030], Loss: 0.695260\n",
      "Train Epoch: 3 [021/030], Loss: 0.721512\n",
      "Train Epoch: 3 [022/030], Loss: 0.705809\n",
      "Train Epoch: 3 [023/030], Loss: 0.675034\n",
      "Train Epoch: 3 [024/030], Loss: 0.558438\n",
      "Train Epoch: 3 [025/030], Loss: 0.609785\n",
      "Train Epoch: 3 [026/030], Loss: 0.895044\n",
      "Train Epoch: 3 [027/030], Loss: 1.107228\n",
      "Train Epoch: 3 [028/030], Loss: 0.775090\n",
      "Train Epoch: 3 [029/030], Loss: 0.583318\n",
      "Train Epoch: 3 [030/030], Loss: 0.746159\n",
      "CPU times: user 8.14 s, sys: 544 ms, total: 8.69 s\n",
      "Wall time: 8.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9770/10000 (97.700%)\n",
      "Train Epoch: 4 [001/030], Loss: 0.536258\n",
      "Train Epoch: 4 [002/030], Loss: 0.472667\n",
      "Train Epoch: 4 [003/030], Loss: 0.508217\n",
      "Train Epoch: 4 [004/030], Loss: 0.536780\n",
      "Train Epoch: 4 [005/030], Loss: 0.437637\n",
      "Train Epoch: 4 [006/030], Loss: 0.430580\n",
      "Train Epoch: 4 [007/030], Loss: 0.445569\n",
      "Train Epoch: 4 [008/030], Loss: 0.404310\n",
      "Train Epoch: 4 [009/030], Loss: 0.479061\n",
      "Train Epoch: 4 [010/030], Loss: 0.540917\n",
      "Train Epoch: 4 [011/030], Loss: 0.416310\n",
      "Train Epoch: 4 [012/030], Loss: 0.404040\n",
      "Train Epoch: 4 [013/030], Loss: 0.374272\n",
      "Train Epoch: 4 [014/030], Loss: 0.398496\n",
      "Train Epoch: 4 [015/030], Loss: 0.385793\n",
      "Train Epoch: 4 [016/030], Loss: 0.656962\n",
      "Train Epoch: 4 [017/030], Loss: 0.567622\n",
      "Train Epoch: 4 [018/030], Loss: 0.400512\n",
      "Train Epoch: 4 [019/030], Loss: 0.356935\n",
      "Train Epoch: 4 [020/030], Loss: 0.687547\n",
      "Train Epoch: 4 [021/030], Loss: 0.330172\n",
      "Train Epoch: 4 [022/030], Loss: 0.332320\n",
      "Train Epoch: 4 [023/030], Loss: 0.349846\n",
      "Train Epoch: 4 [024/030], Loss: 0.298037\n",
      "Train Epoch: 4 [025/030], Loss: 0.283737\n",
      "Train Epoch: 4 [026/030], Loss: 0.337430\n",
      "Train Epoch: 4 [027/030], Loss: 0.277437\n",
      "Train Epoch: 4 [028/030], Loss: 0.315001\n",
      "Train Epoch: 4 [029/030], Loss: 0.341448\n",
      "Train Epoch: 4 [030/030], Loss: 0.364277\n",
      "CPU times: user 8.24 s, sys: 468 ms, total: 8.7 s\n",
      "Wall time: 8.69 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9869/10000 (98.690%)\n",
      "Train Epoch: 5 [001/030], Loss: 0.296992\n",
      "Train Epoch: 5 [002/030], Loss: 0.328858\n",
      "Train Epoch: 5 [003/030], Loss: 0.266479\n",
      "Train Epoch: 5 [004/030], Loss: 0.297663\n",
      "Train Epoch: 5 [005/030], Loss: 0.332768\n",
      "Train Epoch: 5 [006/030], Loss: 0.247213\n",
      "Train Epoch: 5 [007/030], Loss: 0.217652\n",
      "Train Epoch: 5 [008/030], Loss: 0.325998\n",
      "Train Epoch: 5 [009/030], Loss: 0.273159\n",
      "Train Epoch: 5 [010/030], Loss: 0.255474\n",
      "Train Epoch: 5 [011/030], Loss: 0.314953\n",
      "Train Epoch: 5 [012/030], Loss: 0.257866\n",
      "Train Epoch: 5 [013/030], Loss: 0.212163\n",
      "Train Epoch: 5 [014/030], Loss: 0.225794\n",
      "Train Epoch: 5 [015/030], Loss: 0.247043\n",
      "Train Epoch: 5 [016/030], Loss: 0.221152\n",
      "Train Epoch: 5 [017/030], Loss: 0.259933\n",
      "Train Epoch: 5 [018/030], Loss: 0.221741\n",
      "Train Epoch: 5 [019/030], Loss: 0.239763\n",
      "Train Epoch: 5 [020/030], Loss: 0.188205\n",
      "Train Epoch: 5 [021/030], Loss: 0.214881\n",
      "Train Epoch: 5 [022/030], Loss: 0.250654\n",
      "Train Epoch: 5 [023/030], Loss: 0.213395\n",
      "Train Epoch: 5 [024/030], Loss: 0.195484\n",
      "Train Epoch: 5 [025/030], Loss: 0.206214\n",
      "Train Epoch: 5 [026/030], Loss: 0.245297\n",
      "Train Epoch: 5 [027/030], Loss: 0.203378\n",
      "Train Epoch: 5 [028/030], Loss: 0.199433\n",
      "Train Epoch: 5 [029/030], Loss: 0.224561\n",
      "Train Epoch: 5 [030/030], Loss: 0.178073\n",
      "CPU times: user 8.21 s, sys: 500 ms, total: 8.71 s\n",
      "Wall time: 8.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9876/10000 (98.760%)\n",
      "Train Epoch: 6 [001/030], Loss: 0.208963\n",
      "Train Epoch: 6 [002/030], Loss: 0.200370\n",
      "Train Epoch: 6 [003/030], Loss: 0.177837\n",
      "Train Epoch: 6 [004/030], Loss: 0.212964\n",
      "Train Epoch: 6 [005/030], Loss: 0.204766\n",
      "Train Epoch: 6 [006/030], Loss: 0.167096\n",
      "Train Epoch: 6 [007/030], Loss: 0.265125\n",
      "Train Epoch: 6 [008/030], Loss: 0.196995\n",
      "Train Epoch: 6 [009/030], Loss: 0.207403\n",
      "Train Epoch: 6 [010/030], Loss: 0.141273\n",
      "Train Epoch: 6 [011/030], Loss: 0.154239\n",
      "Train Epoch: 6 [012/030], Loss: 0.170697\n",
      "Train Epoch: 6 [013/030], Loss: 0.153261\n",
      "Train Epoch: 6 [014/030], Loss: 0.189697\n",
      "Train Epoch: 6 [015/030], Loss: 0.139502\n",
      "Train Epoch: 6 [016/030], Loss: 0.136670\n",
      "Train Epoch: 6 [017/030], Loss: 0.113855\n",
      "Train Epoch: 6 [018/030], Loss: 0.184707\n",
      "Train Epoch: 6 [019/030], Loss: 0.198732\n",
      "Train Epoch: 6 [020/030], Loss: 0.162072\n",
      "Train Epoch: 6 [021/030], Loss: 0.129305\n",
      "Train Epoch: 6 [022/030], Loss: 0.305793\n",
      "Train Epoch: 6 [023/030], Loss: 0.185865\n",
      "Train Epoch: 6 [024/030], Loss: 0.201982\n",
      "Train Epoch: 6 [025/030], Loss: 0.129591\n",
      "Train Epoch: 6 [026/030], Loss: 0.243068\n",
      "Train Epoch: 6 [027/030], Loss: 0.145171\n",
      "Train Epoch: 6 [028/030], Loss: 0.152177\n",
      "Train Epoch: 6 [029/030], Loss: 0.506476\n",
      "Train Epoch: 6 [030/030], Loss: 0.095718\n",
      "CPU times: user 8.21 s, sys: 512 ms, total: 8.72 s\n",
      "Wall time: 8.71 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9903/10000 (99.030%)\n",
      "Train Epoch: 7 [001/030], Loss: 0.112920\n",
      "Train Epoch: 7 [002/030], Loss: 0.130736\n",
      "Train Epoch: 7 [003/030], Loss: 0.144949\n",
      "Train Epoch: 7 [004/030], Loss: 0.148146\n",
      "Train Epoch: 7 [005/030], Loss: 0.125655\n",
      "Train Epoch: 7 [006/030], Loss: 0.113538\n",
      "Train Epoch: 7 [007/030], Loss: 0.329770\n",
      "Train Epoch: 7 [008/030], Loss: 0.126772\n",
      "Train Epoch: 7 [009/030], Loss: 0.106565\n",
      "Train Epoch: 7 [010/030], Loss: 0.126519\n",
      "Train Epoch: 7 [011/030], Loss: 0.109528\n",
      "Train Epoch: 7 [012/030], Loss: 0.101979\n",
      "Train Epoch: 7 [013/030], Loss: 0.142129\n",
      "Train Epoch: 7 [014/030], Loss: 0.104333\n",
      "Train Epoch: 7 [015/030], Loss: 0.128468\n",
      "Train Epoch: 7 [016/030], Loss: 0.227582\n",
      "Train Epoch: 7 [017/030], Loss: 0.120418\n",
      "Train Epoch: 7 [018/030], Loss: 0.544133\n",
      "Train Epoch: 7 [019/030], Loss: 0.138761\n",
      "Train Epoch: 7 [020/030], Loss: 0.065223\n",
      "Train Epoch: 7 [021/030], Loss: 0.084803\n",
      "Train Epoch: 7 [022/030], Loss: 0.096719\n",
      "Train Epoch: 7 [023/030], Loss: 0.087750\n",
      "Train Epoch: 7 [024/030], Loss: 0.067951\n",
      "Train Epoch: 7 [025/030], Loss: 0.085183\n",
      "Train Epoch: 7 [026/030], Loss: 0.141730\n",
      "Train Epoch: 7 [027/030], Loss: 0.084024\n",
      "Train Epoch: 7 [028/030], Loss: 0.100325\n",
      "Train Epoch: 7 [029/030], Loss: 0.147363\n",
      "Train Epoch: 7 [030/030], Loss: 0.086102\n",
      "CPU times: user 8.24 s, sys: 476 ms, total: 8.72 s\n",
      "Wall time: 8.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9911/10000 (99.110%)\n",
      "Train Epoch: 8 [001/030], Loss: 0.100763\n",
      "Train Epoch: 8 [002/030], Loss: 0.063610\n",
      "Train Epoch: 8 [003/030], Loss: 0.091145\n",
      "Train Epoch: 8 [004/030], Loss: 0.097637\n",
      "Train Epoch: 8 [005/030], Loss: 0.124313\n",
      "Train Epoch: 8 [006/030], Loss: 0.076175\n",
      "Train Epoch: 8 [007/030], Loss: 0.057131\n",
      "Train Epoch: 8 [008/030], Loss: 0.074120\n",
      "Train Epoch: 8 [009/030], Loss: 0.086776\n",
      "Train Epoch: 8 [010/030], Loss: 0.057881\n",
      "Train Epoch: 8 [011/030], Loss: 0.062754\n",
      "Train Epoch: 8 [012/030], Loss: 0.089559\n",
      "Train Epoch: 8 [013/030], Loss: 0.068019\n",
      "Train Epoch: 8 [014/030], Loss: 0.045410\n",
      "Train Epoch: 8 [015/030], Loss: 0.065038\n",
      "Train Epoch: 8 [016/030], Loss: 0.173387\n",
      "Train Epoch: 8 [017/030], Loss: 0.063684\n",
      "Train Epoch: 8 [018/030], Loss: 0.105259\n",
      "Train Epoch: 8 [019/030], Loss: 0.029468\n",
      "Train Epoch: 8 [020/030], Loss: 0.109944\n",
      "Train Epoch: 8 [021/030], Loss: 0.097185\n",
      "Train Epoch: 8 [022/030], Loss: 0.083691\n",
      "Train Epoch: 8 [023/030], Loss: 0.079690\n",
      "Train Epoch: 8 [024/030], Loss: 0.067382\n",
      "Train Epoch: 8 [025/030], Loss: 0.049172\n",
      "Train Epoch: 8 [026/030], Loss: 0.057721\n",
      "Train Epoch: 8 [027/030], Loss: 0.058328\n",
      "Train Epoch: 8 [028/030], Loss: 0.069914\n",
      "Train Epoch: 8 [029/030], Loss: 0.051833\n",
      "Train Epoch: 8 [030/030], Loss: 0.055925\n",
      "CPU times: user 8.04 s, sys: 608 ms, total: 8.65 s\n",
      "Wall time: 8.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9907/10000 (99.070%)\n",
      "Train Epoch: 9 [001/030], Loss: 0.067182\n",
      "Train Epoch: 9 [002/030], Loss: 0.070725\n",
      "Train Epoch: 9 [003/030], Loss: 0.051012\n",
      "Train Epoch: 9 [004/030], Loss: 0.051039\n",
      "Train Epoch: 9 [005/030], Loss: 0.041201\n",
      "Train Epoch: 9 [006/030], Loss: 0.056210\n",
      "Train Epoch: 9 [007/030], Loss: 0.065750\n",
      "Train Epoch: 9 [008/030], Loss: 0.040785\n",
      "Train Epoch: 9 [009/030], Loss: 0.044801\n",
      "Train Epoch: 9 [010/030], Loss: 0.014538\n",
      "Train Epoch: 9 [011/030], Loss: 0.044146\n",
      "Train Epoch: 9 [012/030], Loss: 0.042313\n",
      "Train Epoch: 9 [013/030], Loss: 0.067476\n",
      "Train Epoch: 9 [014/030], Loss: 0.030909\n",
      "Train Epoch: 9 [015/030], Loss: 0.037886\n",
      "Train Epoch: 9 [016/030], Loss: 0.060732\n",
      "Train Epoch: 9 [017/030], Loss: 0.039009\n",
      "Train Epoch: 9 [018/030], Loss: 0.024978\n",
      "Train Epoch: 9 [019/030], Loss: 0.125011\n",
      "Train Epoch: 9 [020/030], Loss: 0.048340\n",
      "Train Epoch: 9 [021/030], Loss: 0.058975\n",
      "Train Epoch: 9 [022/030], Loss: 0.065850\n",
      "Train Epoch: 9 [023/030], Loss: -0.010807\n",
      "Train Epoch: 9 [024/030], Loss: 0.036327\n",
      "Train Epoch: 9 [025/030], Loss: 0.058740\n",
      "Train Epoch: 9 [026/030], Loss: 0.042261\n",
      "Train Epoch: 9 [027/030], Loss: 0.042642\n",
      "Train Epoch: 9 [028/030], Loss: 0.053098\n",
      "Train Epoch: 9 [029/030], Loss: 0.023680\n",
      "Train Epoch: 9 [030/030], Loss: 0.022372\n",
      "CPU times: user 8.12 s, sys: 640 ms, total: 8.76 s\n",
      "Wall time: 8.75 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9907/10000 (99.070%)\n",
      "Train Epoch: 10 [001/030], Loss: 0.044148\n",
      "Train Epoch: 10 [002/030], Loss: 0.032647\n",
      "Train Epoch: 10 [003/030], Loss: 0.084709\n",
      "Train Epoch: 10 [004/030], Loss: 0.034030\n",
      "Train Epoch: 10 [005/030], Loss: 0.037789\n",
      "Train Epoch: 10 [006/030], Loss: 0.018861\n",
      "Train Epoch: 10 [007/030], Loss: 0.041112\n",
      "Train Epoch: 10 [008/030], Loss: 0.029310\n",
      "Train Epoch: 10 [009/030], Loss: 0.056010\n",
      "Train Epoch: 10 [010/030], Loss: 0.009141\n",
      "Train Epoch: 10 [011/030], Loss: 0.021309\n",
      "Train Epoch: 10 [012/030], Loss: 0.031721\n",
      "Train Epoch: 10 [013/030], Loss: 0.032702\n",
      "Train Epoch: 10 [014/030], Loss: 0.010268\n",
      "Train Epoch: 10 [015/030], Loss: 0.019758\n",
      "Train Epoch: 10 [016/030], Loss: 0.042157\n",
      "Train Epoch: 10 [017/030], Loss: 0.031368\n",
      "Train Epoch: 10 [018/030], Loss: 0.032619\n",
      "Train Epoch: 10 [019/030], Loss: -0.005643\n",
      "Train Epoch: 10 [020/030], Loss: 0.040154\n",
      "Train Epoch: 10 [021/030], Loss: 0.014939\n",
      "Train Epoch: 10 [022/030], Loss: 0.003576\n",
      "Train Epoch: 10 [023/030], Loss: 0.054423\n",
      "Train Epoch: 10 [024/030], Loss: 0.009451\n",
      "Train Epoch: 10 [025/030], Loss: 0.034825\n",
      "Train Epoch: 10 [026/030], Loss: 0.005186\n",
      "Train Epoch: 10 [027/030], Loss: 0.023989\n",
      "Train Epoch: 10 [028/030], Loss: 0.037052\n",
      "Train Epoch: 10 [029/030], Loss: 0.031661\n",
      "Train Epoch: 10 [030/030], Loss: 0.019648\n",
      "CPU times: user 8 s, sys: 628 ms, total: 8.63 s\n",
      "Wall time: 8.62 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9905/10000 (99.050%)\n"
     ]
    }
   ],
   "source": [
    "# Simple DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True, pin_memory=True)\n",
    "# We use an adam optimizer over both the model and likelihood parameters\n",
    "# https://arxiv.org/abs/1412.6980\n",
    "optimizer = Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.01)\n",
    "#optimizer = Adam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -model.latent_functions.marginal_log_likelihood(likelihood, output, target, n_data=len(train_dataset))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = likelihood(model(data))\n",
    "        pred = output.argmax()\n",
    "        correct += pred.eq(target.view_as(pred)).data.cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "# While we have theoretically fast algorithms for toeplitz matrix-vector multiplication, the hardware of GPUs\n",
    "# is so well-designed that naive multiplication on them beats the current implementation of our algorith (despite\n",
    "# theoretically fast computation). Because of this, we set the use_toeplitz flag to false to minimize runtime\n",
    "with gpytorch.settings.use_toeplitz(False):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        %time train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
