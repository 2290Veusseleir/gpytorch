{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use deep kernel learning (DKL) for classification. This is useful when you have very complex high-dimensional inputs (such as an image)\n",
    "\n",
    "The example here is MNIST classification\n",
    "\n",
    "For an introduction to DKL see these papers:\n",
    "https://arxiv.org/abs/1511.02222\n",
    "https://arxiv.org/abs/1611.00336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our GPyTorch library\n",
    "import gpytorch\n",
    "\n",
    "# Import some classes we will use from torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from gpytorch.utils.lanczos_bidiagonalize import LanczosBidiagonalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import datasets to access MNISTS and transforms to format data for learning\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Download and load the MNIST dataset to train on\n",
    "# Compose lets us do multiple transformations. Specically make the data a torch.FloatTensor of shape\n",
    "# (colors x height x width) in the range [0.0, 1.0] as opposed to an RGB image with shape (height x width x colors)\n",
    "# then normalize using  mean (0.1317) and standard deviation (0.3081) already calculated (not here)\n",
    "\n",
    "# Transformation documentation here: http://pytorch.org/docs/master/torchvision/transforms.html\n",
    "train_dataset = datasets.MNIST('/tmp', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "test_dataset = datasets.MNIST('/tmp', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "# But the data into a DataLoader. We shuffle the training data but not the test data because the order\n",
    "# training data is presented will affect the outcome unlike the test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg\n",
    "model = vgg.vgg19(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = model.features[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_dense(conv_layer, probe_img, rank):\n",
    "    # probe_img should be 1 x in_channels x imgw x imgh\n",
    "    imgw = probe_img.size(-2)\n",
    "    imgh = probe_img.size(-1)\n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    \n",
    "    probe_vector = probe_img.view(in_channels * imgw * imgh)\n",
    "    \n",
    "    new_conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, bias=False).cuda()\n",
    "    new_conv_layer.load_state_dict(conv_layer.state_dict(), strict=False)\n",
    "    new_conv_layer_t = nn.ConvTranspose2d(out_channels, in_channels, kernel_size=kernel_size, padding=padding, stride=stride, bias=False).cuda()\n",
    "    new_conv_layer_t.load_state_dict(new_conv_layer.state_dict(), strict=False)\n",
    "    \n",
    "    matmul_closure = lambda v: new_conv_layer(Variable(v.contiguous().view(1, in_channels, imgw, imgh)).cuda()).data\n",
    "    matmul_t_closure = lambda v: new_conv_layer_t(Variable(v.contiguous().view(1, out_channels, imgw, imgh)).cuda()).data\n",
    "    \n",
    "    lb = LanczosBidiagonalize(max_iter=rank, cls=type(probe_vector))\n",
    "    n_rows = out_channels * imgw * imgh\n",
    "    n_cols = in_channels * imgw * imgh\n",
    "    P, B, Q = lb.lanczos_bidiagonalize(matmul_closure, matmul_t_closure, probe_vector, n_rows, n_cols)\n",
    "    return P, B, Q, new_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrg365/anaconda3/lib/python3.6/site-packages/torch/tensor.py:300: UserWarning: self and other not broadcastable, but have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.sub(other)\n"
     ]
    }
   ],
   "source": [
    "probe_img = torch.randn(1, 512, 9, 9).cuda()\n",
    "P, B, Q, new_conv_layer = conv_to_dense(model.features[21].cuda(), probe_img, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.rand(1, 512, 9, 9).cuda()\n",
    "my_out = P.matmul(B.matmul(Q.t().matmul(test_img.view(512*9*9)))).view(1, 512, 9, 9)\n",
    "real_out = new_conv_layer(Variable(test_img).cuda()).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.7855 -2.5733 -3.0703 -3.1339 -2.9088 -2.8971 -2.9612 -2.8048 -1.7661\n",
       "-2.8222 -4.2871 -4.8346 -4.8978 -4.7709 -4.9101 -4.5188 -4.4545 -3.0383\n",
       "-2.6252 -4.3977 -4.6451 -4.5744 -4.2396 -4.3563 -4.0236 -4.3268 -3.0988\n",
       "-2.4851 -4.4187 -4.4565 -4.4548 -4.0000 -4.3433 -4.3857 -4.4935 -2.8816\n",
       "-2.5956 -4.1987 -4.1852 -4.0755 -3.2991 -4.1004 -4.5548 -4.9095 -3.0058\n",
       "-2.5259 -3.9930 -4.4466 -4.4880 -3.5658 -4.1926 -4.3241 -4.5980 -2.8738\n",
       "-2.4565 -3.8954 -4.0036 -4.3539 -4.0968 -4.3666 -3.8871 -3.9920 -2.5937\n",
       "-2.7292 -4.3585 -4.3107 -4.3678 -4.2729 -4.2637 -3.8704 -4.0373 -2.5673\n",
       "-1.8696 -2.9644 -2.8937 -2.9995 -2.9951 -2.8629 -2.5754 -2.5467 -1.6593\n",
       "[torch.cuda.FloatTensor of size 9x9 (GPU 0)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_out[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.6695 -3.0159 -2.7022 -2.8106 -3.0404 -3.5181 -2.9043 -2.7308 -2.7054\n",
       "-2.3611 -4.4625 -3.8304 -3.9910 -4.1196 -4.0219 -3.6126 -3.2272 -3.2531\n",
       "-2.8150 -3.8669 -3.7856 -4.4363 -4.5302 -4.1949 -3.5616 -3.8418 -3.1242\n",
       "-2.3715 -3.7058 -3.5787 -4.1800 -3.7834 -3.5073 -3.4099 -3.9138 -2.9160\n",
       "-2.4590 -3.8683 -3.5172 -3.5815 -3.6392 -4.1192 -3.8915 -3.7258 -3.3866\n",
       "-2.5089 -3.9754 -3.2735 -3.7530 -4.1245 -3.4110 -3.2895 -3.2688 -3.4373\n",
       "-2.5451 -3.7677 -3.4878 -4.2226 -4.4720 -3.9447 -3.4788 -3.8035 -3.1894\n",
       "-2.5814 -3.2681 -3.7609 -4.0304 -4.2612 -3.8233 -3.7195 -3.8331 -3.1859\n",
       "-1.6157 -2.2874 -2.9639 -2.4920 -2.7240 -2.9744 -3.4825 -3.0419 -2.1320\n",
       "[torch.cuda.FloatTensor of size 9x9 (GPU 0)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_out[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature extractor for our deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import torch's neural network\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html\n",
    "from torch import nn\n",
    "# Import torch.nn.functional for various activation/pooling functions\n",
    "# Documentation here: http://pytorch.org/docs/master/nn.html#torch-nn-functional\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# We make a classic LeNet Architecture sans a final prediction layer to 10 outputs. This will serve as a feature\n",
    "# extractor reducing the dimensionality of our data down to 64. We will pretrain these layers by adding on a \n",
    "# final classifying 64-->10 layer\n",
    "# https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "class LeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc3 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.norm3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "feature_extractor = LeNetFeatureExtractor().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the feature extractor a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.158651\n",
      "Test set: Average loss: 0.0402, Accuracy: 9869/10000 (98.690%)\n",
      "Train Epoch: 2\tLoss: 0.038372\n",
      "Test set: Average loss: 0.0329, Accuracy: 9890/10000 (98.900%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b37e484cb952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mpretest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-b37e484cb952>\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#data, target = data.cuda(), target.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#data, target = Variable(data), Variable(target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2378\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2324\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# may change to (mode, 0, 1) post-1.1.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m             im = im._new(\n\u001b[1;32m   2328\u001b[0m                 \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make a final classifier layer that operates on the feature extractor's output\n",
    "classifier = nn.Linear(64, 10).cuda()\n",
    "# Make list of parameters to optimize (both the parameters of the feature extractor and classifier)\n",
    "params = list(feature_extractor.parameters()) + list(classifier.parameters())\n",
    "# We train the network using stochastic gradient descent\n",
    "optimizer = SGD(params, lr=0.1, momentum=0.9)\n",
    "\n",
    "# Define our pretraining function\n",
    "#    Set feature extractor to train mode (need b/c module unlike classifier which is just a single layer)\n",
    "#    iterate through train_loader\n",
    "#    put the data on the GPU as a variable\n",
    "#    Zero out the gradients from/for back_prop (needed b/c otherwise would hurt RNNs by default)\n",
    "#    Extract the 64-dimensional feature vector\n",
    "#    Feed the features into the classifying layer and output the log softmax\n",
    "#    Calculate negative log likelihood loss\n",
    "#    COULD REPLACE ABOVE WITH torch.nn.functional.cross_entropy? Says it combines them\n",
    "#    Backprop\n",
    "#    Incrementally optimize parameters\n",
    "#    Accumulate training loss\n",
    "#    Print result of epoch\n",
    "def pretrain(epoch):\n",
    "    feature_extractor.train()\n",
    "    train_loss = 0.\n",
    "    for data, target in train_loader:\n",
    "        #data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * len(data)\n",
    "    print('Train Epoch: %d\\tLoss: %.6f' % (epoch, train_loss / len(train_dataset)))\n",
    "\n",
    "# Set feature extractor to eval mode (these should actually only effect Dropout and BatchNorm which we aren't?)\n",
    "# http://pytorch.org/docs/master/nn.html#torch.nn.Module.train\n",
    "# Set test_loss accumulator and correct counter\n",
    "# Iterate through test data\n",
    "#    volatile is something about not saving gradients because not needed in test mode? Basically just not\n",
    "#          storing some type of information. Makes sense\n",
    "#    calculate loss and accumulate\n",
    "#    make prediction and check accuracy\n",
    "def pretest():\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pretrain(epoch)\n",
    "    pretest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep kernel GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extractor.conv1(Variable(train_dataset[0][0]).unsqueeze(0).cuda()).shape\n",
    "\n",
    "n_rows = 16*28*28\n",
    "n_cols = 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import ConvTranspose2d\n",
    "conv1_transpose = ConvTranspose2d(16, 1, kernel_size=5, padding=2, bias=False).cuda()\n",
    "conv1_transpose.load_state_dict(feature_extractor.conv1.state_dict())\n",
    "\n",
    "matmul_closure = lambda v: feature_extractor.conv1(Variable(v.contiguous().view(1, 1, 28, 28)).cuda()).data\n",
    "matmul_t_closure = lambda v: conv1_transpose(Variable(v.contiguous().view(1, 16, 28, 28)).cuda()).data\n",
    "v = train_dataset[0][0].view(28*28).cuda()\n",
    "\n",
    "from gpytorch.utils.lanczos_bidiagonalize import LanczosBidiagonalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrg365/anaconda3/lib/python3.6/site-packages/torch/tensor.py:300: UserWarning: self and other not broadcastable, but have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.sub(other)\n"
     ]
    }
   ],
   "source": [
    "from gpytorch.utils.lanczos_bidiagonalize import LanczosBidiagonalize\n",
    "lb = LanczosBidiagonalize(max_iter=800, cls=type(v))\n",
    "P, B, Q = lb.lanczos_bidiagonalize(matmul_closure, matmul_t_closure, v, n_rows, n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get output of \"convolution\" using Lanczos decomposition\n",
    "my_out = P.matmul(B.matmul(Q.t())).matmul(train_dataset[1][0].view(28*28).cuda()).view(1, 16, 28, 28)\n",
    "# Get output of convolution using actual convolutional layer forward\n",
    "real_out = feature_extractor.conv1(Variable(train_dataset[1][0]).unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.5168  0.3948  0.1532  0.1532  0.1532  0.1532  0.1532  0.1532  0.1532\n",
       "  0.7371  0.6906  0.4688  0.4688  0.4688  0.4688  0.4688  0.4688  0.4688\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.3846\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.3931 -0.2094\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.4860 -0.3472 -0.1353 -0.1353 -0.1353 -0.1353 -0.1353 -0.1353 -0.1353\n",
       " -0.5565 -0.3557 -0.0543 -0.0543 -0.0543 -0.0543 -0.0543 -0.0543 -0.0543\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1085 -0.1085 -0.1085 -0.1084\n",
       " -0.5038 -0.3589 -0.1084 -0.1085 -0.1085 -0.1084 -0.1085 -0.1085 -0.1084\n",
       " -0.5038 -0.3589 -0.1084 -0.1085 -0.1084 -0.1085 -0.1084 -0.1085 -0.1085\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1085 -0.1084 -0.1085\n",
       " -0.5038 -0.3589 -0.1084 -0.1085 -0.1085 -0.1085 -0.1084 -0.1085 -0.1085\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1085 -0.1085 -0.1084 -0.1084 -0.0336\n",
       " -0.5038 -0.3589 -0.1084 -0.1085 -0.1084 -0.1085 -0.1085 -0.0380  0.4304\n",
       "[torch.cuda.FloatTensor of size 2x9x9 (GPU 0)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_out[0, :2, :9, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.5168  0.3948  0.1532  0.1532  0.1532  0.1532  0.1532  0.1532  0.1532\n",
       "  0.7372  0.6906  0.4688  0.4688  0.4688  0.4688  0.4688  0.4688  0.4688\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.5276  0.3846\n",
       "  0.6974  0.7143  0.5276  0.5276  0.5276  0.5276  0.5276  0.3930 -0.2094\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.4860 -0.3472 -0.1353 -0.1353 -0.1353 -0.1353 -0.1353 -0.1353 -0.1353\n",
       " -0.5565 -0.3557 -0.0543 -0.0543 -0.0543 -0.0543 -0.0543 -0.0543 -0.0543\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.0336\n",
       " -0.5038 -0.3589 -0.1084 -0.1084 -0.1084 -0.1084 -0.1084 -0.0380  0.4304\n",
       "[torch.cuda.FloatTensor of size 2x9x9 (GPU 0)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_out[0, :2, :9, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 4.2183  1.5955\n",
       " 0.0000  4.2027\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
